import torch
import torch.nn.functional as F
from torch.nn.modules.loss import _Loss


class FocalLoss(_Loss):
    def __init__(self, alpha: float = 1.0, gamma: float = 1.0, reduction: str = "sum") -> None:
        """
        Focal Loss for binary segmentation task. The Focal Loss is simply a dynamically scaled cross entropy loss,
        where the scaling factor decays to zero as the confidence in the correct class increases.

        Args:
            alpha (float): The weight associated with the the positive class. Usually set inversely proportional
                to the amount of samples in a given class.
            gamma (float): The exponent to raise the residual between the predicted probability and and ground truth.
                Higher values of gamma lead to emphasizing the contribution of harder examples in the loss.
            reduction (str): The manner in which to reduce the the per sample loss.
        """
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        """
        Returns the focal loss on batch of inputs and targets.

        Args:
            inputs (torch.Tensor): The predictions generated by the model.
            targets (torch.Tensor): The ground truth segmentation labels.

        Returns:
            torch.Tensor: The focal loss for the batch.

        """
        inputs = torch.sigmoid(inputs)
        ce_loss = F.binary_cross_entropy(inputs, targets.float(), reduction="none")
        p_t = (inputs * targets) + ((1 - inputs) * (1 - targets))
        loss = ce_loss * ((1 - p_t) ** self.gamma)

        if self.alpha >= 0:
            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
            loss = alpha_t * loss

        if self.reduction == "mean":
            loss = loss.mean()
        elif self.reduction == "sum":
            loss = loss.sum()
        else:
            raise NotImplementedError

        return loss
