# Parameters that describe server
n_server_rounds: 200 # 30 # The number of rounds to run FL

# Parameters that describe clients
n_clients: 4 # The number of clients in the FL experiment
local_epochs: 1 # The number of epochs to complete for client (NOT USED FOR FLAMBY)
batch_size: 4 # 64 # The batch size for client training (NOT USED FOR FLAMBY)
local_steps: 30 # 100 # The number of local training steps to perform.



# privacy settings (defaults to discrete Gaussian noise)
clipping_threshold: 0.01
# results 1
# granularity: 0.001
# noise_scale: 0.001

granularity: 0.0001
noise_scale: 0.0001

bias: 0.5

# not used by client directly
model_integer_range_exponent: 16   # modulus = 2^model_integer_range_exponent

# things to try
#1 reduce local_steps (or sample batches
# (consults appropriate amplication theorem relevant for zero-concentrated-DP)

#2 train epochs, increase epochs, increase noise variance (?) - try first

#3 increase noise + more global rounds
