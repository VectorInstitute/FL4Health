#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --gres=gpu:1
#SBATCH --mem=32G
#SBATCH --partition=rtx6000
#SBATCH --qos=m2
#SBATCH --job-name=fl_five_fold_exp
#SBATCH --output=%j_%x.out
#SBATCH --error=%j_%x.err
#SBATCH --time=4:00:00

###############################################
# Usage:
#
# sbatch research/cifar10/adaptive_pfl/mrmtl/run_fold_experiment.slrm \
#   path_to_config.yaml \
#   path_to_folder_for_artifacts/ \
#   path_to_folder_for_dataset/ \
#   path_to_desired_venv/ \
#   client_side_learning_rate_value \
#   lambda value \
#   server_address \
#   client_beta \
#   adapt
#
# Example:
# sbatch research/cifar10/adaptive_pfl/mrmtl/run_fold_experiment.slrm \
#   research/cifar10/adaptive_pfl/mrmtl/config.yaml \
#   research/cifar10/adaptive_pfl/mrmtl/hp_results/ \
#   /datasets/cifar10 \
#   /h/demerson/vector_repositories/fl4health_env/ \
#   0.0001 \
#   0.01 \
#   0.0.0.0:8080\
#   0.1 \
#   "TRUE"
#
# Notes:
# 1) The sbatch command above should be run from the top level directory of the repository.
# 2) This example runs mrmtl. As such the data paths and python launch commands are hardcoded. If you want to change
# the example you run, you need to explicitly modify the code below.
# 3) The logging directories need to ALREADY EXIST. The script does not create them.
###############################################

# Note:
#	  ntasks: Total number of processes to use across world
#	  ntasks-per-node: How many processes each node should create

# Set NCCL options
# export NCCL_DEBUG=INFO
# NCCL backend to communicate between GPU workers is not provided in vector's cluster.
# Disable this option in slurm.
export NCCL_IB_DISABLE=1

if [[ "${SLURM_JOB_PARTITION}" == "t4v2" ]] || \
    [[ "${SLURM_JOB_PARTITION}" == "rtx6000" ]]; then
    echo export NCCL_SOCKET_IFNAME=bond0 on "${SLURM_JOB_PARTITION}"
    export NCCL_SOCKET_IFNAME=bond0
fi

# This environment variable must be set in order to force torch to use deterministic algorithms. See documentation
# in fl4health/utils/random.py for more information
export CUBLAS_WORKSPACE_CONFIG=:4096:8

# Process Inputs

SERVER_CONFIG_PATH=$1
ARTIFACT_DIR=$2
DATASET_DIR=$3
VENV_PATH=$4
CLIENT_LR=$5
LAM_VALUE=$6
SERVER_ADDRESS=$7
CLIENT_BETA=$8
ADAPT=$9

# Create the artifact directory
mkdir "${ARTIFACT_DIR}"

RUN_NAMES=( "Run1" "Run2" "Run3" "Run4" "Run5" )
SEEDS=( 2021 2022 2023 2024 2025 )

echo "Python Venv Path: ${VENV_PATH}"

echo "World size: ${SLURM_NTASKS}"
echo "Number of nodes: ${SLURM_NNODES}"
NUM_GPUs=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
echo "GPUs per node: ${NUM_GPUs}"

# Source the environment
source ${VENV_PATH}bin/activate
echo "Active Environment:"
which python

for ((i=0; i<${#RUN_NAMES[@]}; i++));
do
    RUN_NAME="${RUN_NAMES[i]}"
    SEED="${SEEDS[i]}"
    # create the run directory
    RUN_DIR="${ARTIFACT_DIR}${RUN_NAME}/"
    echo "Starting Run and logging artifacts at ${RUN_DIR}"
    if [ -d "${RUN_DIR}" ]
    then
        # Directory already exists, we check if the done.out file exists
        if [ -f "${RUN_DIR}done.out" ]
        then
            # Done file already exists so we skip this run
            echo "Run already completed. Skipping Run."
            continue
        else
            # Done file doesn't exists (assume pre-emption happened)
            # Delete the partially finished contents and start over
            echo "Run did not finished correctly. Re-running."
            rm -r "${RUN_DIR}"
            mkdir "${RUN_DIR}"
        fi
    else
        # Directory doesn't exist yet, so we create it.
        echo "Run directory does not exist. Creating it."
        mkdir "${RUN_DIR}"
    fi

    SERVER_OUTPUT_FILE="${RUN_DIR}server.out"

    # Start the server, divert the outputs to a server file

    echo "Server logging at: ${SERVER_OUTPUT_FILE}"
    echo "Launching Server"

    if [[ ${ADAPT} == "TRUE" ]]; then
        nohup python -m research.cifar10.adaptive_pfl.mrmtl.server \
            --config_path ${SERVER_CONFIG_PATH} \
            --server_address ${SERVER_ADDRESS} \
            --seed ${SEED} \
            --lam ${LAM_VALUE} \
            --use_adaptation \
            > ${SERVER_OUTPUT_FILE} 2>&1 &
    else
        nohup python -m research.cifar10.adaptive_pfl.mrmtl.server \
            --config_path ${SERVER_CONFIG_PATH} \
            --server_address ${SERVER_ADDRESS} \
            --seed ${SEED} \
            --lam ${LAM_VALUE} \
            > ${SERVER_OUTPUT_FILE} 2>&1 &
    fi

    # Sleep for 20 seconds to allow the server to come up.
    sleep 20

    # Start n number of clients and divert the outputs to their own files
    n_clients=7
    for (( c=0; c<${n_clients}; c++ ))
    do
        CLIENT_NAME="client_${c}"
        echo "Launching ${CLIENT_NAME}"

        CLIENT_LOG_PATH="${RUN_DIR}${CLIENT_NAME}.out"
        echo "${CLIENT_NAME} logging at: ${CLIENT_LOG_PATH}"
        nohup python -m research.cifar10.adaptive_pfl.mrmtl.client \
            --artifact_dir ${ARTIFACT_DIR} \
            --dataset_dir ${DATASET_DIR} \
            --run_name ${RUN_NAME} \
            --client_number ${c} \
            --learning_rate ${CLIENT_LR} \
            --server_address ${SERVER_ADDRESS} \
            --seed ${SEED} \
            --beta ${CLIENT_BETA} \
            > ${CLIENT_LOG_PATH} 2>&1 &
    done

    echo "FL Processes Running"

    wait

    # Create a file that verifies that the Run concluded properly
    touch "${RUN_DIR}done.out"
    echo "Finished FL Processes"

done
