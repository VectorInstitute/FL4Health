#!/bin/bash

#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --gres=gpu:0
#SBATCH --mem=32G
#SBATCH --partition=cpu
#SBATCH --qos=high

# Note:
#	  ntasks: Total number of processes to use across world
#	  ntasks-per-node: How many processes each node should create

# Set NCCL options
# export NCCL_DEBUG=INFO
# NCCL backend to communicate between GPU workers is not provided in vector's cluster.
# Disable this option in slurm.
export NCCL_IB_DISABLE=1

if [[ "${SLURM_JOB_PARTITION}" == "t4v2" ]] || \
    [[ "${SLURM_JOB_PARTITION}" == "rtx6000" ]]; then
    echo export NCCL_SOCKET_IFNAME=bond0 on "${SLURM_JOB_PARTITION}"
    export NCCL_SOCKET_IFNAME=bond0
fi

# Process inputs
SERVER_PORT=$1
CONFIG_PATH=$2
ARTIFACT_DIR=$3
VENV_PATH=$4
JOB_HASH=$5

# Print the name of the node, this will be the network address
echo "Node Name: ${SLURMD_NODENAME}"
echo "Server Port number: ${SERVER_PORT}"
echo "Config Path: ${CONFIG_PATH}"
echo "Python Venv Path: ${VENV_PATH}"
echo "Job Hash: ${JOB_HASH}"

SERVER_ADDRESS="${SLURMD_NODENAME}:${SERVER_PORT}"

echo "Server Address: ${SERVER_ADDRESS}"

LOG_PATH="${ARTIFACT_DIR}server_log_${JOB_HASH}.log"

echo "Placing logs in: ${ARTIFACT_DIR}"
echo "World size: ${SLURM_NTASKS}"
echo "Number of nodes: ${SLURM_NNODES}"

# Source the environment
source ${VENV_PATH}bin/activate
echo "Active Environment"
which python

nohup python -m research.flamby.fed_isic2019.fenda.server \
        --config_path ${CONFIG_PATH} \
        --server_address ${SERVER_ADDRESS} \
        > ${LOG_PATH} 2>&1 &

echo ${SERVER_ADDRESS}

wait
echo "Finished FL Processes"
