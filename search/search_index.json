{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-fl4health","title":"Welcome to FL4Health \u2728","text":"<p>A flexible, modular, and easy to use library to facilitate federated learning research and development in healthcare settings.</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Principally, this library contains the federated learning (FL) engine aimed at facilitating FL research, experimentation, and exploration, with a specific focus on health applications.</p> <p>This library is built on the foundational components of Flower, an open-source FL library in its own right. The documentation is here. This library contains a number of unique components that extend the functionality of Flower in a number of directions.</p>"},{"location":"#summary-of-currently-implemented-approaches","title":"Summary of Currently Implemented Approaches","text":"<p>The present set of FL approaches implemented in the library are:</p>"},{"location":"#non-personalized-methods","title":"Non-Personalized Methods","text":"Method Notes FedAvg Weights are aggregated on the server-side through averaging. Weighted and unweighted averaging is available. FedOpt A recent extension of FedAvg that includes adaptive optimization on the server-side aggregation. Implementations through Flower include FedAdam, FedAdaGrad, and FedYogi. FedProx An extension of FedAvg that attempts to control local weight drift through a penalty term added to each client's local loss function. Both fixed and adaptive FedProx implementations are available. SCAFFOLD Another extension of FedAvg that attempts to correct for local gradient drift due to heterogenous data representations. This is done through the calculation of control variates used to modify the weight updates. In addition to standard SCAFFOLD, a version with warm-up is implemented, along with DP-SCAFFOLD. MOON MOON adds a contrastive loss function that attempts to ensure that the feature representations learned on the client-side do not significantly drift from those of the previous server model. FedDG-GA FedDG-GA is a domain generalization approach that aims to ensure that the models trained during FL generalize well to unseen domains, potentially outside of the training distribution. The method applies an adjustment algorithm which modifies the client coefficients used during weighted averaging on the server-side. FLASH FLASH incorporates a modification to the server-side aggregation algorithm, adding an additional term that is meant to modify the server side learning rate if a data distribution shift occurs during training. In the absence of distribution shifts, the modified aggregation approach is nearly equivalent to the existing FedAvg or FedOpt approaches. FedPM FedPM is a recent sparse, communication efficient approach to federated learning. The method has been shown to have exceptional information compression while maintaining good performance. Interestingly, it is also connected to the Lottery Ticket Hypothesis."},{"location":"#personalized-methods","title":"Personalized Methods","text":"Method Notes Personal FL This method strictly considers the effect of continuing local training on each client model, locally, after federated training has completed. FedBN FedBN implements a very light version of personalization wherein clients exchange all parameters in their model except for anything related to batch normalization layers, which are only learned locally. FedPer Trains a global feature extractor shared by all clients through FedAvg and a private classifier that is unique to each client. FedRep Similar to FedPer, FedRep trains a global feature extractor shared by all clients through FedAvg and a private classifier that is unique to each client. However, FedRep breaks up the client-side training of these components into two phases. First the local classifier is trained with the feature extractor frozen. Next, the classifier is frozen and the feature extractor is trained. Ditto Trains a global model with FedAvg and a personal model that is constrained by the l2-norm of the difference between the personal model weights and the previous global model. MR-MTL Trains a personal model that is constrained by the l2-norm of the difference between the personal model weights and the previous aggregation of all client's models. Aggregation of the personal models is done through FedAvg. Unlike Ditto, no global model is optimized during client-side training. APFL Twin models are trained. One of them is globally shared by all clients and aggregated on the server. The other is strictly trained locally by each client. Predictions are made by a convex combination of the models. PerFCL PerFCL extends MOON to consider separate globally and locally trained feature extractors and a locally trained classifier. Contrastive loss functions are used to ensure that, during client training, the global features stay close to the original server model features and that the local features are not close to the global features. FENDA-FL FENDA is an ablation of PerFCL that strictly considers globally and locally trained feature extractors and a locally trained classifier. The contrastive loss functions are removed from the training procedure to allow for less constrained feature learning and more flexible model architecture design. FENDA+Ditto This is a combination of two state-of-the-art approaches above: FENDA-FL and Ditto. The idea is to merge the two approaches to yield a \"best of both\" set of modeling with the flexibility of FENDA-FL for local adaptation and the global-model constrained optimization of Ditto. GPFL GPFL trains a global feature extractor and a local classifier head. A global mapping model transforms the feature extractor's output into both global and personalized feature representations. Global feature representations are trained to be close to their respective class embeddings stored in a global lookup table, whereas personalized feature embeddings are learned for prediction. <p>More approaches are being implemented as they are prioritized. However, the library also provides significant flexibility to implement strategies of your own.</p>"},{"location":"#privacy-capabilities","title":"Privacy Capabilities","text":"<p>In addition to the FL strategies, we also support several differentially private FL training approaches. These include:</p> <ul> <li>Instance-level FL privacy</li> <li>Client-level FL privacy with Adaptive Clipping<ul> <li>Weighted and Unweighted FedAvg</li> </ul> </li> </ul> <p>The addition of Distributed Differential Privacy (DDP) with Secure Aggregation is also anticipated soon.</p>"},{"location":"#community","title":"Community","text":"<p>Need a specific FL algorithm implemented? Submit an issue in our Github, or even better contribute to our open-source project!</p> <ul> <li>FL4Health Python Github</li> <li>FL4Health on PyPI</li> <li>FL4Health Contributing Guide</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>Quick Start - Get started with simple federated system in a few lines of code.</li> <li>Module Guides - Explore the various modules of <code>fl4health</code>!</li> <li>Examples - A wide-ranging set of FL example tasks.</li> <li>API Reference - Thorough documentation of our classes.</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Full API documentation for FL4Health, automatically generated from docstrings.</p>"},{"location":"api/#fl4health","title":"<code>fl4health</code>","text":""},{"location":"api/#fl4health.checkpointing","title":"<code>checkpointing</code>","text":""},{"location":"api/#fl4health.checkpointing.checkpointer","title":"<code>checkpointer</code>","text":""},{"location":"api/#fl4health.checkpointing.checkpointer.TorchModuleCheckpointer","title":"<code>TorchModuleCheckpointer</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>class TorchModuleCheckpointer(ABC):\n    def __init__(self, checkpoint_dir: str, checkpoint_name: str) -&gt; None:\n        \"\"\"\n        Basic abstract base class to handle checkpointing pytorch models. Models are saved with ``torch.save`` by\n        default.\n\n        Args:\n            checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n                checkpointer will not create it if it does not.\n            checkpoint_name (str): Name of the checkpoint to be saved.\n        \"\"\"\n        self.checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n\n    @abstractmethod\n    def maybe_checkpoint(self, model: nn.Module, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n        \"\"\"\n        Abstract method to be implemented by every ``TorchCheckpointer``. Based on the loss and metrics provided it\n        should determine whether to produce a checkpoint AND save it if applicable.\n\n        Args:\n            model (nn.Module): Model to potentially save via the checkpointer\n            loss (float): Computed loss associated with the model.\n            metrics (dict[str, float]): Computed metrics associated with the model.\n\n        Raises:\n            NotImplementedError: Must be implemented by the checkpointer.\n        \"\"\"\n        raise NotImplementedError(\"maybe_checkpoint must be implemented by inheriting classes\")\n\n    def load_checkpoint(self, path_to_checkpoint: str | None = None) -&gt; nn.Module:\n        \"\"\"\n        Checkpointer with the option to either specify a checkpoint path or fall back on the internal path of the\n        checkpointer. The flexibility to specify a load path is useful, for example, if you are not overwriting\n        checkpoints when saving and need to load a specific past checkpoint for whatever reason.\n\n        Args:\n            path_to_checkpoint (str | None, optional): If provided, the checkpoint will be loaded from this path.\n                If not specified, the checkpointer will load from ``self.checkpoint_path``. Defaults to None.\n\n        Returns:\n            (nn.Module): Returns a torch module loaded from the proper checkpoint path.\n        \"\"\"\n        if path_to_checkpoint is None:\n            return torch.load(self.checkpoint_path, weights_only=False)\n        return torch.load(path_to_checkpoint, weights_only=False)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.TorchModuleCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name)</code>","text":"<p>Basic abstract base class to handle checkpointing pytorch models. Models are saved with <code>torch.save</code> by default.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Directory to which the model is saved. This directory should already exist. The checkpointer will not create it if it does not.</p> required <code>checkpoint_name</code> <code>str</code> <p>Name of the checkpoint to be saved.</p> required Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>def __init__(self, checkpoint_dir: str, checkpoint_name: str) -&gt; None:\n    \"\"\"\n    Basic abstract base class to handle checkpointing pytorch models. Models are saved with ``torch.save`` by\n    default.\n\n    Args:\n        checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n            checkpointer will not create it if it does not.\n        checkpoint_name (str): Name of the checkpoint to be saved.\n    \"\"\"\n    self.checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.TorchModuleCheckpointer.maybe_checkpoint","title":"<code>maybe_checkpoint(model, loss, metrics)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be implemented by every <code>TorchCheckpointer</code>. Based on the loss and metrics provided it should determine whether to produce a checkpoint AND save it if applicable.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to potentially save via the checkpointer</p> required <code>loss</code> <code>float</code> <p>Computed loss associated with the model.</p> required <code>metrics</code> <code>dict[str, float]</code> <p>Computed metrics associated with the model.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by the checkpointer.</p> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>@abstractmethod\ndef maybe_checkpoint(self, model: nn.Module, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n    \"\"\"\n    Abstract method to be implemented by every ``TorchCheckpointer``. Based on the loss and metrics provided it\n    should determine whether to produce a checkpoint AND save it if applicable.\n\n    Args:\n        model (nn.Module): Model to potentially save via the checkpointer\n        loss (float): Computed loss associated with the model.\n        metrics (dict[str, float]): Computed metrics associated with the model.\n\n    Raises:\n        NotImplementedError: Must be implemented by the checkpointer.\n    \"\"\"\n    raise NotImplementedError(\"maybe_checkpoint must be implemented by inheriting classes\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.TorchModuleCheckpointer.load_checkpoint","title":"<code>load_checkpoint(path_to_checkpoint=None)</code>","text":"<p>Checkpointer with the option to either specify a checkpoint path or fall back on the internal path of the checkpointer. The flexibility to specify a load path is useful, for example, if you are not overwriting checkpoints when saving and need to load a specific past checkpoint for whatever reason.</p> <p>Parameters:</p> Name Type Description Default <code>path_to_checkpoint</code> <code>str | None</code> <p>If provided, the checkpoint will be loaded from this path. If not specified, the checkpointer will load from <code>self.checkpoint_path</code>. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>Returns a torch module loaded from the proper checkpoint path.</p> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>def load_checkpoint(self, path_to_checkpoint: str | None = None) -&gt; nn.Module:\n    \"\"\"\n    Checkpointer with the option to either specify a checkpoint path or fall back on the internal path of the\n    checkpointer. The flexibility to specify a load path is useful, for example, if you are not overwriting\n    checkpoints when saving and need to load a specific past checkpoint for whatever reason.\n\n    Args:\n        path_to_checkpoint (str | None, optional): If provided, the checkpoint will be loaded from this path.\n            If not specified, the checkpointer will load from ``self.checkpoint_path``. Defaults to None.\n\n    Returns:\n        (nn.Module): Returns a torch module loaded from the proper checkpoint path.\n    \"\"\"\n    if path_to_checkpoint is None:\n        return torch.load(self.checkpoint_path, weights_only=False)\n    return torch.load(path_to_checkpoint, weights_only=False)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.FunctionTorchModuleCheckpointer","title":"<code>FunctionTorchModuleCheckpointer</code>","text":"<p>               Bases: <code>TorchModuleCheckpointer</code></p> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>class FunctionTorchModuleCheckpointer(TorchModuleCheckpointer):\n    def __init__(\n        self,\n        checkpoint_dir: str,\n        checkpoint_name: str,\n        checkpoint_score_function: CheckpointScoreFunctionType,\n        checkpoint_score_name: str | None = None,\n        maximize: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        A general torch checkpointer base class that allows for flexible definition of how to decide when to checkpoint\n        based on the loss and metrics provided. The score function should compute a score from these values and\n        maximize specifies whether we are hoping to maximize or minimize that score.\n\n        Args:\n            checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n                checkpointer will not create it if it does not.\n            checkpoint_name (str): Name of the checkpoint to be saved.\n            checkpoint_score_function (CheckpointScoreFunctionType): Function taking in a loss value and dictionary of\n                metrics and produces a score based on these.\n            checkpoint_score_name (str | None, optional): Name of the score produced by the scoring function. This is\n                used for logging purposes. If not provided, the name of the function will be used. Defaults to None.\n            maximize (bool, optional): Specifies whether we're trying to minimize or maximize the score produced\n                by the scoring function. Defaults to False.\n        \"\"\"\n        super().__init__(checkpoint_dir, checkpoint_name)\n        self.best_score: float | None = None\n        self.checkpoint_score_function = checkpoint_score_function\n        if checkpoint_score_name is not None:\n            self.checkpoint_score_name = checkpoint_score_name\n        else:\n            log(\n                WARNING,\n                \"No checkpoint_score_name provided. Name will default to the checkpoint score function \"\n                f\"name of {checkpoint_score_function.__name__}\",\n            )\n            self.checkpoint_score_name = checkpoint_score_function.__name__\n        # Whether we're looking to maximize (or minimize) the score produced by the checkpoint score function\n        self.maximize = maximize\n        self.comparison_str = \"&gt;=\" if self.maximize else \"&lt;=\"\n\n    def _should_checkpoint(self, comparison_score: float) -&gt; bool:\n        \"\"\"\n        Compares the current score to the best previously recorded, returns true if should checkpoint and false\n        otherwise. If the previous best score is None, then we always checkpoint.\n\n        Args:\n            comparison_score (float): Score that is being maximized or minimized. Will be compared against the previous\n                best score seen by this checkpointer.\n\n        Returns:\n            (bool): Whether or not to checkpoint the model based on the provided score\n        \"\"\"\n        if self.best_score:\n            if self.maximize:\n                return self.best_score &lt;= comparison_score\n            return self.best_score &gt;= comparison_score\n\n        # If best score is none, then this is the first checkpoint\n        return True\n\n    def maybe_checkpoint(self, model: nn.Module, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n        \"\"\"\n        Given the loss/metrics associated with the provided model, the checkpointer uses the scoring function to\n        produce a score. This score will then be used to determine whether the model should be checkpointed or not.\n\n        Args:\n            model (nn.Module): Model that might be persisted if the scoring function determines it should be.\n            loss (float): Loss associated with the provided model. Will potentially contribute to checkpointing\n                decision, based on the score function.\n            metrics (dict[str, Scalar]): Metrics associated with the provided model. Will potentially contribute to\n                the checkpointing decision, based on the score function.\n\n        Raises:\n            e: Will throw an error if there is an issue saving the model. ``Torch.save`` seems to swallow errors in\n                this context, so we explicitly surface the error with a try/except.\n        \"\"\"\n        # First we use the provided scoring function to produce a score\n        comparison_score = self.checkpoint_score_function(loss, metrics)\n        if self._should_checkpoint(comparison_score):\n            log(\n                INFO,\n                f\"Checkpointing the model: Current {self.checkpoint_score_name} score ({comparison_score}) \"\n                f\"{self.comparison_str} Best score ({self.best_score})\",\n            )\n            self.best_score = comparison_score\n            try:\n                log(INFO, f\"Saving checkpoint as {str(self.checkpoint_path)}\")\n                torch.save(model, self.checkpoint_path)\n            except Exception as e:\n                log(ERROR, f\"Encountered the following error while saving the checkpoint: {e}\")\n                raise e\n        else:\n            log(\n                INFO,\n                f\"Not checkpointing the model: Current {self.checkpoint_score_name} score ({comparison_score}) is not \"\n                f\"{self.comparison_str} Best score ({self.best_score})\",\n            )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.FunctionTorchModuleCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name, checkpoint_score_function, checkpoint_score_name=None, maximize=False)</code>","text":"<p>A general torch checkpointer base class that allows for flexible definition of how to decide when to checkpoint based on the loss and metrics provided. The score function should compute a score from these values and maximize specifies whether we are hoping to maximize or minimize that score.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Directory to which the model is saved. This directory should already exist. The checkpointer will not create it if it does not.</p> required <code>checkpoint_name</code> <code>str</code> <p>Name of the checkpoint to be saved.</p> required <code>checkpoint_score_function</code> <code>CheckpointScoreFunctionType</code> <p>Function taking in a loss value and dictionary of metrics and produces a score based on these.</p> required <code>checkpoint_score_name</code> <code>str | None</code> <p>Name of the score produced by the scoring function. This is used for logging purposes. If not provided, the name of the function will be used. Defaults to None.</p> <code>None</code> <code>maximize</code> <code>bool</code> <p>Specifies whether we're trying to minimize or maximize the score produced by the scoring function. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: str,\n    checkpoint_name: str,\n    checkpoint_score_function: CheckpointScoreFunctionType,\n    checkpoint_score_name: str | None = None,\n    maximize: bool = False,\n) -&gt; None:\n    \"\"\"\n    A general torch checkpointer base class that allows for flexible definition of how to decide when to checkpoint\n    based on the loss and metrics provided. The score function should compute a score from these values and\n    maximize specifies whether we are hoping to maximize or minimize that score.\n\n    Args:\n        checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n            checkpointer will not create it if it does not.\n        checkpoint_name (str): Name of the checkpoint to be saved.\n        checkpoint_score_function (CheckpointScoreFunctionType): Function taking in a loss value and dictionary of\n            metrics and produces a score based on these.\n        checkpoint_score_name (str | None, optional): Name of the score produced by the scoring function. This is\n            used for logging purposes. If not provided, the name of the function will be used. Defaults to None.\n        maximize (bool, optional): Specifies whether we're trying to minimize or maximize the score produced\n            by the scoring function. Defaults to False.\n    \"\"\"\n    super().__init__(checkpoint_dir, checkpoint_name)\n    self.best_score: float | None = None\n    self.checkpoint_score_function = checkpoint_score_function\n    if checkpoint_score_name is not None:\n        self.checkpoint_score_name = checkpoint_score_name\n    else:\n        log(\n            WARNING,\n            \"No checkpoint_score_name provided. Name will default to the checkpoint score function \"\n            f\"name of {checkpoint_score_function.__name__}\",\n        )\n        self.checkpoint_score_name = checkpoint_score_function.__name__\n    # Whether we're looking to maximize (or minimize) the score produced by the checkpoint score function\n    self.maximize = maximize\n    self.comparison_str = \"&gt;=\" if self.maximize else \"&lt;=\"\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.FunctionTorchModuleCheckpointer.maybe_checkpoint","title":"<code>maybe_checkpoint(model, loss, metrics)</code>","text":"<p>Given the loss/metrics associated with the provided model, the checkpointer uses the scoring function to produce a score. This score will then be used to determine whether the model should be checkpointed or not.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model that might be persisted if the scoring function determines it should be.</p> required <code>loss</code> <code>float</code> <p>Loss associated with the provided model. Will potentially contribute to checkpointing decision, based on the score function.</p> required <code>metrics</code> <code>dict[str, Scalar]</code> <p>Metrics associated with the provided model. Will potentially contribute to the checkpointing decision, based on the score function.</p> required <p>Raises:</p> Type Description <code>e</code> <p>Will throw an error if there is an issue saving the model. <code>Torch.save</code> seems to swallow errors in this context, so we explicitly surface the error with a try/except.</p> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>def maybe_checkpoint(self, model: nn.Module, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n    \"\"\"\n    Given the loss/metrics associated with the provided model, the checkpointer uses the scoring function to\n    produce a score. This score will then be used to determine whether the model should be checkpointed or not.\n\n    Args:\n        model (nn.Module): Model that might be persisted if the scoring function determines it should be.\n        loss (float): Loss associated with the provided model. Will potentially contribute to checkpointing\n            decision, based on the score function.\n        metrics (dict[str, Scalar]): Metrics associated with the provided model. Will potentially contribute to\n            the checkpointing decision, based on the score function.\n\n    Raises:\n        e: Will throw an error if there is an issue saving the model. ``Torch.save`` seems to swallow errors in\n            this context, so we explicitly surface the error with a try/except.\n    \"\"\"\n    # First we use the provided scoring function to produce a score\n    comparison_score = self.checkpoint_score_function(loss, metrics)\n    if self._should_checkpoint(comparison_score):\n        log(\n            INFO,\n            f\"Checkpointing the model: Current {self.checkpoint_score_name} score ({comparison_score}) \"\n            f\"{self.comparison_str} Best score ({self.best_score})\",\n        )\n        self.best_score = comparison_score\n        try:\n            log(INFO, f\"Saving checkpoint as {str(self.checkpoint_path)}\")\n            torch.save(model, self.checkpoint_path)\n        except Exception as e:\n            log(ERROR, f\"Encountered the following error while saving the checkpoint: {e}\")\n            raise e\n    else:\n        log(\n            INFO,\n            f\"Not checkpointing the model: Current {self.checkpoint_score_name} score ({comparison_score}) is not \"\n            f\"{self.comparison_str} Best score ({self.best_score})\",\n        )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.LatestTorchModuleCheckpointer","title":"<code>LatestTorchModuleCheckpointer</code>","text":"<p>               Bases: <code>FunctionTorchModuleCheckpointer</code></p> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>class LatestTorchModuleCheckpointer(FunctionTorchModuleCheckpointer):\n    def __init__(self, checkpoint_dir: str, checkpoint_name: str) -&gt; None:\n        \"\"\"\n        A checkpointer that always checkpoints the model, regardless of the loss/metrics provided. As such, the score\n        function is essentially a dummy.\n\n        Args:\n            checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n                checkpointer will not create it if it does not.\n            checkpoint_name (str): Name of the checkpoint to be saved.\n        \"\"\"\n\n        # This function is required by the parent class, but not used in the LatestTorchCheckpointer\n        def null_score_function(loss: float, _: dict[str, Scalar]) -&gt; float:\n            return 0.0\n\n        super().__init__(checkpoint_dir, checkpoint_name, null_score_function, \"Latest\", False)\n\n    def maybe_checkpoint(self, model: nn.Module, loss: float, _: dict[str, Scalar]) -&gt; None:\n        \"\"\"\n        This function is essentially a pass through, as this class always checkpoints the provided model.\n\n        Args:\n            model (nn.Module): Model to be checkpointed whenever this function is called\n            loss (float): Loss associated with the provided model. Will potentially contribute to checkpointing\n                decision, based on the score function. NOT USED.\n            _ (dict[str, Scalar]): Metrics associated with the provided model. Will potentially contribute to\n                the checkpointing decision, based on the score function. NOT USED.\n\n        Raises:\n            e: Will throw an error if there is an issue saving the model. ``Torch.save`` seems to swallow errors in\n                this context, so we explicitly surface the error with a try/except.\n        \"\"\"\n        # Always checkpoint the latest model\n        log(INFO, f\"Saving latest checkpoint with LatestTorchCheckpointer as {str(self.checkpoint_path)}\")\n        try:\n            torch.save(model, self.checkpoint_path)\n        except Exception as e:\n            log(ERROR, f\"Encountered the following error while saving the checkpoint: {e}\")\n            raise e\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.LatestTorchModuleCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name)</code>","text":"<p>A checkpointer that always checkpoints the model, regardless of the loss/metrics provided. As such, the score function is essentially a dummy.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Directory to which the model is saved. This directory should already exist. The checkpointer will not create it if it does not.</p> required <code>checkpoint_name</code> <code>str</code> <p>Name of the checkpoint to be saved.</p> required Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>def __init__(self, checkpoint_dir: str, checkpoint_name: str) -&gt; None:\n    \"\"\"\n    A checkpointer that always checkpoints the model, regardless of the loss/metrics provided. As such, the score\n    function is essentially a dummy.\n\n    Args:\n        checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n            checkpointer will not create it if it does not.\n        checkpoint_name (str): Name of the checkpoint to be saved.\n    \"\"\"\n\n    # This function is required by the parent class, but not used in the LatestTorchCheckpointer\n    def null_score_function(loss: float, _: dict[str, Scalar]) -&gt; float:\n        return 0.0\n\n    super().__init__(checkpoint_dir, checkpoint_name, null_score_function, \"Latest\", False)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.LatestTorchModuleCheckpointer.maybe_checkpoint","title":"<code>maybe_checkpoint(model, loss, _)</code>","text":"<p>This function is essentially a pass through, as this class always checkpoints the provided model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to be checkpointed whenever this function is called</p> required <code>loss</code> <code>float</code> <p>Loss associated with the provided model. Will potentially contribute to checkpointing decision, based on the score function. NOT USED.</p> required <code>_</code> <code>dict[str, Scalar]</code> <p>Metrics associated with the provided model. Will potentially contribute to the checkpointing decision, based on the score function. NOT USED.</p> required <p>Raises:</p> Type Description <code>e</code> <p>Will throw an error if there is an issue saving the model. <code>Torch.save</code> seems to swallow errors in this context, so we explicitly surface the error with a try/except.</p> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>def maybe_checkpoint(self, model: nn.Module, loss: float, _: dict[str, Scalar]) -&gt; None:\n    \"\"\"\n    This function is essentially a pass through, as this class always checkpoints the provided model.\n\n    Args:\n        model (nn.Module): Model to be checkpointed whenever this function is called\n        loss (float): Loss associated with the provided model. Will potentially contribute to checkpointing\n            decision, based on the score function. NOT USED.\n        _ (dict[str, Scalar]): Metrics associated with the provided model. Will potentially contribute to\n            the checkpointing decision, based on the score function. NOT USED.\n\n    Raises:\n        e: Will throw an error if there is an issue saving the model. ``Torch.save`` seems to swallow errors in\n            this context, so we explicitly surface the error with a try/except.\n    \"\"\"\n    # Always checkpoint the latest model\n    log(INFO, f\"Saving latest checkpoint with LatestTorchCheckpointer as {str(self.checkpoint_path)}\")\n    try:\n        torch.save(model, self.checkpoint_path)\n    except Exception as e:\n        log(ERROR, f\"Encountered the following error while saving the checkpoint: {e}\")\n        raise e\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.BestLossTorchModuleCheckpointer","title":"<code>BestLossTorchModuleCheckpointer</code>","text":"<p>               Bases: <code>FunctionTorchModuleCheckpointer</code></p> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>class BestLossTorchModuleCheckpointer(FunctionTorchModuleCheckpointer):\n    def __init__(self, checkpoint_dir: str, checkpoint_name: str) -&gt; None:\n        \"\"\"\n        This checkpointer only uses the loss value provided to the ``maybe_checkpoint`` function to determine whether a\n        checkpoint should be save. We are always attempting to minimize the loss. So maximize is always set to false.\n\n        Args:\n            checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n                checkpointer will not create it if it does not.\n            checkpoint_name (str): Name of the checkpoint to be saved.\n        \"\"\"\n\n        # The BestLossTorchCheckpointer just uses the provided loss to scoring checkpoints. More complicated\n        # approaches may be used by other classes.\n        def loss_score_function(loss: float, _: dict[str, Scalar]) -&gt; float:\n            return loss\n\n        super().__init__(\n            checkpoint_dir=checkpoint_dir,\n            checkpoint_name=checkpoint_name,\n            checkpoint_score_function=loss_score_function,\n            checkpoint_score_name=\"Loss\",\n            maximize=False,\n        )\n\n    def maybe_checkpoint(self, model: nn.Module, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n        \"\"\"\n        This function will decide whether to checkpoint the provided model based on the loss argument. If the provided\n        loss is better than any previous losses seen by this checkpointer, the model will be saved.\n\n        Args:\n            model (nn.Module): Model that might be persisted if the scoring function determines it should be.\n            loss (float): Loss associated with the provided model. This value is used to determine whether to save the\n                model or not.\n            metrics (dict[str, Scalar]): Metrics associated with the provided model. Will not be used by this\n                checkpointer.\n\n        Raises:\n            e: Will throw an error if there is an issue saving the model. ``Torch.save`` seems to swallow errors in\n                this context, so we explicitly surface the error with a try/except.\n        \"\"\"\n        # First we use the provided scoring function to produce a score\n        comparison_score = self.checkpoint_score_function(loss, metrics)\n        if self._should_checkpoint(comparison_score):\n            log(\n                INFO,\n                f\"Current Loss ({comparison_score}) {self.comparison_str} Best Loss ({self.best_score})\\n \"\n                f\"Checkpointing the model as {self.checkpoint_path}\",\n            )\n            self.best_score = comparison_score\n            try:\n                torch.save(model, self.checkpoint_path)\n            except Exception as e:\n                log(ERROR, f\"Encountered the following error while saving the checkpoint: {e}\")\n                raise e\n        else:\n            log(\n                INFO,\n                f\"Not checkpointing the model: Current Loss ({comparison_score}) is not \"\n                f\"{self.comparison_str} Best Loss ({self.best_score})\",\n            )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.BestLossTorchModuleCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name)</code>","text":"<p>This checkpointer only uses the loss value provided to the <code>maybe_checkpoint</code> function to determine whether a checkpoint should be save. We are always attempting to minimize the loss. So maximize is always set to false.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Directory to which the model is saved. This directory should already exist. The checkpointer will not create it if it does not.</p> required <code>checkpoint_name</code> <code>str</code> <p>Name of the checkpoint to be saved.</p> required Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>def __init__(self, checkpoint_dir: str, checkpoint_name: str) -&gt; None:\n    \"\"\"\n    This checkpointer only uses the loss value provided to the ``maybe_checkpoint`` function to determine whether a\n    checkpoint should be save. We are always attempting to minimize the loss. So maximize is always set to false.\n\n    Args:\n        checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n            checkpointer will not create it if it does not.\n        checkpoint_name (str): Name of the checkpoint to be saved.\n    \"\"\"\n\n    # The BestLossTorchCheckpointer just uses the provided loss to scoring checkpoints. More complicated\n    # approaches may be used by other classes.\n    def loss_score_function(loss: float, _: dict[str, Scalar]) -&gt; float:\n        return loss\n\n    super().__init__(\n        checkpoint_dir=checkpoint_dir,\n        checkpoint_name=checkpoint_name,\n        checkpoint_score_function=loss_score_function,\n        checkpoint_score_name=\"Loss\",\n        maximize=False,\n    )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.BestLossTorchModuleCheckpointer.maybe_checkpoint","title":"<code>maybe_checkpoint(model, loss, metrics)</code>","text":"<p>This function will decide whether to checkpoint the provided model based on the loss argument. If the provided loss is better than any previous losses seen by this checkpointer, the model will be saved.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model that might be persisted if the scoring function determines it should be.</p> required <code>loss</code> <code>float</code> <p>Loss associated with the provided model. This value is used to determine whether to save the model or not.</p> required <code>metrics</code> <code>dict[str, Scalar]</code> <p>Metrics associated with the provided model. Will not be used by this checkpointer.</p> required <p>Raises:</p> Type Description <code>e</code> <p>Will throw an error if there is an issue saving the model. <code>Torch.save</code> seems to swallow errors in this context, so we explicitly surface the error with a try/except.</p> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>def maybe_checkpoint(self, model: nn.Module, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n    \"\"\"\n    This function will decide whether to checkpoint the provided model based on the loss argument. If the provided\n    loss is better than any previous losses seen by this checkpointer, the model will be saved.\n\n    Args:\n        model (nn.Module): Model that might be persisted if the scoring function determines it should be.\n        loss (float): Loss associated with the provided model. This value is used to determine whether to save the\n            model or not.\n        metrics (dict[str, Scalar]): Metrics associated with the provided model. Will not be used by this\n            checkpointer.\n\n    Raises:\n        e: Will throw an error if there is an issue saving the model. ``Torch.save`` seems to swallow errors in\n            this context, so we explicitly surface the error with a try/except.\n    \"\"\"\n    # First we use the provided scoring function to produce a score\n    comparison_score = self.checkpoint_score_function(loss, metrics)\n    if self._should_checkpoint(comparison_score):\n        log(\n            INFO,\n            f\"Current Loss ({comparison_score}) {self.comparison_str} Best Loss ({self.best_score})\\n \"\n            f\"Checkpointing the model as {self.checkpoint_path}\",\n        )\n        self.best_score = comparison_score\n        try:\n            torch.save(model, self.checkpoint_path)\n        except Exception as e:\n            log(ERROR, f\"Encountered the following error while saving the checkpoint: {e}\")\n            raise e\n    else:\n        log(\n            INFO,\n            f\"Not checkpointing the model: Current Loss ({comparison_score}) is not \"\n            f\"{self.comparison_str} Best Loss ({self.best_score})\",\n        )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.BestMetricTorchModuleCheckpointer","title":"<code>BestMetricTorchModuleCheckpointer</code>","text":"<p>               Bases: <code>FunctionTorchModuleCheckpointer</code></p> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>class BestMetricTorchModuleCheckpointer(FunctionTorchModuleCheckpointer):\n    def __init__(\n        self,\n        checkpoint_dir: str,\n        checkpoint_name: str,\n        metric: str,\n        prefix: str = \"val - prediction - \",\n        maximize: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Checkpointer that checkpoints based on the value of a user defined metric.\n\n        Args:\n            checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n                checkpointer will not create it if it does not.\n            checkpoint_name (str): Name of the checkpoint to be saved.\n            metric (str): The name of the metric to base checkpointing on. After prepending the prefix, should be a\n                key in the metrics dictionary passed in ``self.maybe_checkpoint``. In BasicClient this is the 'name'\n                attribute of the corresponding ``fl4health.utils.metrics.Metric`` that was provided to the clients.\n            prefix (str, optional): A prefix to add to the metric name to create the key used to find the metric.\n                Usually a prefix is added by the client's metric manager. Defaults to 'val - prediction - '.\n            maximize (bool, optional): If True maximizes the metric instead of minimizing it. Defaults to False.\n        \"\"\"\n        self.metric_key = f\"{prefix}{metric}\"\n\n        def metric_score_function(_: float, metrics: dict[str, Scalar]) -&gt; float:\n            try:\n                val = metrics[self.metric_key]\n            except KeyError as e:\n                log(ERROR, f\"Could not find '{self.metric_key}' in metrics dict. Available keys are: {metrics.keys()}\")\n                raise e\n            try:\n                val_float = float(val)\n            except ValueError as e:\n                log(ERROR, f\"Could not convert {self.metric_key} into a float score for best metric checkpointing.\")\n                raise e\n            return val_float\n\n        super().__init__(\n            checkpoint_dir=checkpoint_dir,\n            checkpoint_name=checkpoint_name,\n            checkpoint_score_function=metric_score_function,\n            checkpoint_score_name=metric,\n            maximize=maximize,\n        )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.checkpointer.BestMetricTorchModuleCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name, metric, prefix='val - prediction - ', maximize=False)</code>","text":"<p>Checkpointer that checkpoints based on the value of a user defined metric.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Directory to which the model is saved. This directory should already exist. The checkpointer will not create it if it does not.</p> required <code>checkpoint_name</code> <code>str</code> <p>Name of the checkpoint to be saved.</p> required <code>metric</code> <code>str</code> <p>The name of the metric to base checkpointing on. After prepending the prefix, should be a key in the metrics dictionary passed in <code>self.maybe_checkpoint</code>. In BasicClient this is the 'name' attribute of the corresponding <code>fl4health.utils.metrics.Metric</code> that was provided to the clients.</p> required <code>prefix</code> <code>str</code> <p>A prefix to add to the metric name to create the key used to find the metric. Usually a prefix is added by the client's metric manager. Defaults to 'val - prediction - '.</p> <code>'val - prediction - '</code> <code>maximize</code> <code>bool</code> <p>If True maximizes the metric instead of minimizing it. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/checkpointing/checkpointer.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: str,\n    checkpoint_name: str,\n    metric: str,\n    prefix: str = \"val - prediction - \",\n    maximize: bool = False,\n) -&gt; None:\n    \"\"\"\n    Checkpointer that checkpoints based on the value of a user defined metric.\n\n    Args:\n        checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n            checkpointer will not create it if it does not.\n        checkpoint_name (str): Name of the checkpoint to be saved.\n        metric (str): The name of the metric to base checkpointing on. After prepending the prefix, should be a\n            key in the metrics dictionary passed in ``self.maybe_checkpoint``. In BasicClient this is the 'name'\n            attribute of the corresponding ``fl4health.utils.metrics.Metric`` that was provided to the clients.\n        prefix (str, optional): A prefix to add to the metric name to create the key used to find the metric.\n            Usually a prefix is added by the client's metric manager. Defaults to 'val - prediction - '.\n        maximize (bool, optional): If True maximizes the metric instead of minimizing it. Defaults to False.\n    \"\"\"\n    self.metric_key = f\"{prefix}{metric}\"\n\n    def metric_score_function(_: float, metrics: dict[str, Scalar]) -&gt; float:\n        try:\n            val = metrics[self.metric_key]\n        except KeyError as e:\n            log(ERROR, f\"Could not find '{self.metric_key}' in metrics dict. Available keys are: {metrics.keys()}\")\n            raise e\n        try:\n            val_float = float(val)\n        except ValueError as e:\n            log(ERROR, f\"Could not convert {self.metric_key} into a float score for best metric checkpointing.\")\n            raise e\n        return val_float\n\n    super().__init__(\n        checkpoint_dir=checkpoint_dir,\n        checkpoint_name=checkpoint_name,\n        checkpoint_score_function=metric_score_function,\n        checkpoint_score_name=metric,\n        maximize=maximize,\n    )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.client_module","title":"<code>client_module</code>","text":""},{"location":"api/#fl4health.checkpointing.client_module.ClientCheckpointAndStateModule","title":"<code>ClientCheckpointAndStateModule</code>","text":"Source code in <code>fl4health/checkpointing/client_module.py</code> <pre><code>class ClientCheckpointAndStateModule:\n    def __init__(\n        self,\n        pre_aggregation: ModelCheckpointers = None,\n        post_aggregation: ModelCheckpointers = None,\n        state_checkpointer: ClientStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to hold up three to major components that determine how clients handle model and state\n        checkpointing, where state checkpointing is meant to allow clients to restart if FL training is interrupted.\n        For model checkpointing, there are two distinct types.\n\n        - The first type, if defined, is used to checkpoint local models **BEFORE** server-side aggregation, but\n          after local training. **NOTE**: This is akin to \"further fine-tuning\" approaches for global models.\n        - The second type, if defined, is used to checkpoint local models **AFTER** server-side aggregation, but\n          before local training **NOTE**: This is the \"traditional\" mechanism for global models.\n\n        As a final note, for some methods, such as Ditto or MR-MTL, these checkpoints will actually be identical.\n        That's because the target model for these methods is never globally aggregated. That is, they remain local.\n\n        Args:\n            pre_aggregation (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their validation metrics/losses **BEFORE**\n                server-side aggregation. Defaults to None.\n            post_aggregation (ModelCheckpointers, optional): If defined, this checkpointer (or sequence\n                of checkpointers) is used to checkpoint models based on their validation metrics/losses **AFTER**\n                server-side aggregation. Defaults to None.\n            state_checkpointer (ClientStateCheckpointer | None, optional): If defined, this checkpointer\n                is used to preserve client state (not just models), in the event one wants to restart\n                federated training. Defaults to None.\n        \"\"\"\n        self.pre_aggregation = (\n            [pre_aggregation] if isinstance(pre_aggregation, TorchModuleCheckpointer) else pre_aggregation\n        )\n        self.post_aggregation = (\n            [post_aggregation] if isinstance(post_aggregation, TorchModuleCheckpointer) else post_aggregation\n        )\n        self._check_if_shared_checkpoint_names()\n        self.state_checkpointer = state_checkpointer\n\n    def _check_if_shared_checkpoint_names(self) -&gt; None:\n        \"\"\"\n        This function checks whether there is overlap in the paths to which the checkpointers of this module are\n        supposed to write. This is to ensure that there isn't any accidental overwriting of checkpoints that is\n        unintended.\n\n        Raises:\n            ValueError: If any of the pre- or post-aggregation model checkpointer paths are not unique.\n        \"\"\"\n        pre_aggregation_paths = (\n            [checkpointer.checkpoint_path for checkpointer in self.pre_aggregation] if self.pre_aggregation else []\n        )\n        post_aggregation_paths = (\n            [checkpointer.checkpoint_path for checkpointer in self.post_aggregation] if self.post_aggregation else []\n        )\n\n        all_paths = pre_aggregation_paths + post_aggregation_paths\n        unique_paths = set(all_paths)\n\n        if len(unique_paths) != len(all_paths):\n            formatted_all_paths = \"\\n\".join(all_paths)\n            raise ValueError(\n                \"The paths of all of your checkpointers should be unique otherwise overwrites are possible and data \"\n                f\"will be lost. The current paths are:\\n{formatted_all_paths}\"\n            )\n\n    def maybe_checkpoint(\n        self, model: nn.Module, loss: float, metrics: dict[str, Scalar], mode: CheckpointMode\n    ) -&gt; None:\n        \"\"\"\n        Performs model checkpointing for a particular mode (either pre- or post-aggregation) if any checkpointers are\n        provided for that particular mode in this module. If present, the various checkpointers will decide whether\n        or not to checkpoint based on their internal criterion and the loss/metrics provided.\n\n        Args:\n            model (nn.Module): The model that might be checkpointed by the checkpointers.\n            loss (float): The metric value obtained by the provided model. Used by the checkpointer(s) to decide\n                whether to checkpoint the model.\n            metrics (dict[str, Scalar]): The metrics obtained by the provided model. Potentially used by checkpointer\n                to decide whether to checkpoint the model.\n            mode (CheckpointMode): Determines which of the types of checkpointers to use. Currently, the only modes\n                available are pre- and post-aggregation.\n\n        Raises:\n            ValueError: Thrown if the model checkpointing mode is not recognized.\n        \"\"\"\n        if mode == CheckpointMode.PRE_AGGREGATION:\n            if self.pre_aggregation is not None:\n                for checkpointer in self.pre_aggregation:\n                    checkpointer.maybe_checkpoint(model, loss, metrics)\n            else:\n                log(INFO, \"No Pre-aggregation checkpoint specified. Skipping.\")\n        elif mode == CheckpointMode.POST_AGGREGATION:\n            if self.post_aggregation is not None:\n                for checkpointer in self.post_aggregation:\n                    checkpointer.maybe_checkpoint(model, loss, metrics)\n            else:\n                log(INFO, \"No Post-aggregation checkpoint specified. Skipping.\")\n        else:\n            raise ValueError(f\"Unrecognized mode for checkpointing: {str(mode)}\")\n\n    def save_state(self, client: BasicClient) -&gt; None:\n        \"\"\"\n        This function is meant to facilitate saving state required to restart an FL process on the client side. This\n        function will simply save all the attributes stated in ``ClientStateCheckpointer.snapshot_attrs``.\n        This function should only be called if a ``state_checkpointer`` exists in this module.\n\n        Args:\n            client (BasicClient): The client object from which state will be saved.\n\n        Raises:\n            ValueError: Throws an error if this function is called, but no state checkpointer has been provided.\n        \"\"\"\n        if self.state_checkpointer is not None:\n            self.state_checkpointer.save_client_state(client)\n        else:\n            raise ValueError(\"Attempting to save state but no state checkpointer is specified\")\n\n    def maybe_load_state(self, client: BasicClient) -&gt; bool:\n        \"\"\"\n        This function facilitates loading of any pre-existing state (with the name ``checkpoint_name``) in the\n        directory of the ``checkpoint_dir``. If the state already exists at the proper path, the state is loaded\n        and will be automatically saved into client's attributes. If it doesn't exist, we return False.\n\n        Args:\n            client (BasicClient): client object into which state will be loaded if a checkpoint exists\n\n        Raises:\n            ValueError: Throws an error if this function is called, but no state checkpointer has been provided.\n\n        Returns:\n            (bool): If the state checkpoint properly exists and is loaded correctly, client's attributes are set to\n                the loaded values, and True is returned. Otherwise, we return False (or throw an exception).\n        \"\"\"\n        if self.state_checkpointer is not None:\n            return self.state_checkpointer.maybe_load_client_state(client)\n        raise ValueError(\"Attempting to load state, but no state checkpointer is specified\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.client_module.ClientCheckpointAndStateModule.__init__","title":"<code>__init__(pre_aggregation=None, post_aggregation=None, state_checkpointer=None)</code>","text":"<p>This module is meant to hold up three to major components that determine how clients handle model and state checkpointing, where state checkpointing is meant to allow clients to restart if FL training is interrupted. For model checkpointing, there are two distinct types.</p> <ul> <li>The first type, if defined, is used to checkpoint local models BEFORE server-side aggregation, but   after local training. NOTE: This is akin to \"further fine-tuning\" approaches for global models.</li> <li>The second type, if defined, is used to checkpoint local models AFTER server-side aggregation, but   before local training NOTE: This is the \"traditional\" mechanism for global models.</li> </ul> <p>As a final note, for some methods, such as Ditto or MR-MTL, these checkpoints will actually be identical. That's because the target model for these methods is never globally aggregated. That is, they remain local.</p> <p>Parameters:</p> Name Type Description Default <code>pre_aggregation</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their validation metrics/losses BEFORE server-side aggregation. Defaults to None.</p> <code>None</code> <code>post_aggregation</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their validation metrics/losses AFTER server-side aggregation. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>ClientStateCheckpointer | None</code> <p>If defined, this checkpointer is used to preserve client state (not just models), in the event one wants to restart federated training. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/client_module.py</code> <pre><code>def __init__(\n    self,\n    pre_aggregation: ModelCheckpointers = None,\n    post_aggregation: ModelCheckpointers = None,\n    state_checkpointer: ClientStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to hold up three to major components that determine how clients handle model and state\n    checkpointing, where state checkpointing is meant to allow clients to restart if FL training is interrupted.\n    For model checkpointing, there are two distinct types.\n\n    - The first type, if defined, is used to checkpoint local models **BEFORE** server-side aggregation, but\n      after local training. **NOTE**: This is akin to \"further fine-tuning\" approaches for global models.\n    - The second type, if defined, is used to checkpoint local models **AFTER** server-side aggregation, but\n      before local training **NOTE**: This is the \"traditional\" mechanism for global models.\n\n    As a final note, for some methods, such as Ditto or MR-MTL, these checkpoints will actually be identical.\n    That's because the target model for these methods is never globally aggregated. That is, they remain local.\n\n    Args:\n        pre_aggregation (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their validation metrics/losses **BEFORE**\n            server-side aggregation. Defaults to None.\n        post_aggregation (ModelCheckpointers, optional): If defined, this checkpointer (or sequence\n            of checkpointers) is used to checkpoint models based on their validation metrics/losses **AFTER**\n            server-side aggregation. Defaults to None.\n        state_checkpointer (ClientStateCheckpointer | None, optional): If defined, this checkpointer\n            is used to preserve client state (not just models), in the event one wants to restart\n            federated training. Defaults to None.\n    \"\"\"\n    self.pre_aggregation = (\n        [pre_aggregation] if isinstance(pre_aggregation, TorchModuleCheckpointer) else pre_aggregation\n    )\n    self.post_aggregation = (\n        [post_aggregation] if isinstance(post_aggregation, TorchModuleCheckpointer) else post_aggregation\n    )\n    self._check_if_shared_checkpoint_names()\n    self.state_checkpointer = state_checkpointer\n</code></pre>"},{"location":"api/#fl4health.checkpointing.client_module.ClientCheckpointAndStateModule.maybe_checkpoint","title":"<code>maybe_checkpoint(model, loss, metrics, mode)</code>","text":"<p>Performs model checkpointing for a particular mode (either pre- or post-aggregation) if any checkpointers are provided for that particular mode in this module. If present, the various checkpointers will decide whether or not to checkpoint based on their internal criterion and the loss/metrics provided.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model that might be checkpointed by the checkpointers.</p> required <code>loss</code> <code>float</code> <p>The metric value obtained by the provided model. Used by the checkpointer(s) to decide whether to checkpoint the model.</p> required <code>metrics</code> <code>dict[str, Scalar]</code> <p>The metrics obtained by the provided model. Potentially used by checkpointer to decide whether to checkpoint the model.</p> required <code>mode</code> <code>CheckpointMode</code> <p>Determines which of the types of checkpointers to use. Currently, the only modes available are pre- and post-aggregation.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Thrown if the model checkpointing mode is not recognized.</p> Source code in <code>fl4health/checkpointing/client_module.py</code> <pre><code>def maybe_checkpoint(\n    self, model: nn.Module, loss: float, metrics: dict[str, Scalar], mode: CheckpointMode\n) -&gt; None:\n    \"\"\"\n    Performs model checkpointing for a particular mode (either pre- or post-aggregation) if any checkpointers are\n    provided for that particular mode in this module. If present, the various checkpointers will decide whether\n    or not to checkpoint based on their internal criterion and the loss/metrics provided.\n\n    Args:\n        model (nn.Module): The model that might be checkpointed by the checkpointers.\n        loss (float): The metric value obtained by the provided model. Used by the checkpointer(s) to decide\n            whether to checkpoint the model.\n        metrics (dict[str, Scalar]): The metrics obtained by the provided model. Potentially used by checkpointer\n            to decide whether to checkpoint the model.\n        mode (CheckpointMode): Determines which of the types of checkpointers to use. Currently, the only modes\n            available are pre- and post-aggregation.\n\n    Raises:\n        ValueError: Thrown if the model checkpointing mode is not recognized.\n    \"\"\"\n    if mode == CheckpointMode.PRE_AGGREGATION:\n        if self.pre_aggregation is not None:\n            for checkpointer in self.pre_aggregation:\n                checkpointer.maybe_checkpoint(model, loss, metrics)\n        else:\n            log(INFO, \"No Pre-aggregation checkpoint specified. Skipping.\")\n    elif mode == CheckpointMode.POST_AGGREGATION:\n        if self.post_aggregation is not None:\n            for checkpointer in self.post_aggregation:\n                checkpointer.maybe_checkpoint(model, loss, metrics)\n        else:\n            log(INFO, \"No Post-aggregation checkpoint specified. Skipping.\")\n    else:\n        raise ValueError(f\"Unrecognized mode for checkpointing: {str(mode)}\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.client_module.ClientCheckpointAndStateModule.save_state","title":"<code>save_state(client)</code>","text":"<p>This function is meant to facilitate saving state required to restart an FL process on the client side. This function will simply save all the attributes stated in <code>ClientStateCheckpointer.snapshot_attrs</code>. This function should only be called if a <code>state_checkpointer</code> exists in this module.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>BasicClient</code> <p>The client object from which state will be saved.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Throws an error if this function is called, but no state checkpointer has been provided.</p> Source code in <code>fl4health/checkpointing/client_module.py</code> <pre><code>def save_state(self, client: BasicClient) -&gt; None:\n    \"\"\"\n    This function is meant to facilitate saving state required to restart an FL process on the client side. This\n    function will simply save all the attributes stated in ``ClientStateCheckpointer.snapshot_attrs``.\n    This function should only be called if a ``state_checkpointer`` exists in this module.\n\n    Args:\n        client (BasicClient): The client object from which state will be saved.\n\n    Raises:\n        ValueError: Throws an error if this function is called, but no state checkpointer has been provided.\n    \"\"\"\n    if self.state_checkpointer is not None:\n        self.state_checkpointer.save_client_state(client)\n    else:\n        raise ValueError(\"Attempting to save state but no state checkpointer is specified\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.client_module.ClientCheckpointAndStateModule.maybe_load_state","title":"<code>maybe_load_state(client)</code>","text":"<p>This function facilitates loading of any pre-existing state (with the name <code>checkpoint_name</code>) in the directory of the <code>checkpoint_dir</code>. If the state already exists at the proper path, the state is loaded and will be automatically saved into client's attributes. If it doesn't exist, we return False.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>BasicClient</code> <p>client object into which state will be loaded if a checkpoint exists</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Throws an error if this function is called, but no state checkpointer has been provided.</p> <p>Returns:</p> Type Description <code>bool</code> <p>If the state checkpoint properly exists and is loaded correctly, client's attributes are set to the loaded values, and True is returned. Otherwise, we return False (or throw an exception).</p> Source code in <code>fl4health/checkpointing/client_module.py</code> <pre><code>def maybe_load_state(self, client: BasicClient) -&gt; bool:\n    \"\"\"\n    This function facilitates loading of any pre-existing state (with the name ``checkpoint_name``) in the\n    directory of the ``checkpoint_dir``. If the state already exists at the proper path, the state is loaded\n    and will be automatically saved into client's attributes. If it doesn't exist, we return False.\n\n    Args:\n        client (BasicClient): client object into which state will be loaded if a checkpoint exists\n\n    Raises:\n        ValueError: Throws an error if this function is called, but no state checkpointer has been provided.\n\n    Returns:\n        (bool): If the state checkpoint properly exists and is loaded correctly, client's attributes are set to\n            the loaded values, and True is returned. Otherwise, we return False (or throw an exception).\n    \"\"\"\n    if self.state_checkpointer is not None:\n        return self.state_checkpointer.maybe_load_client_state(client)\n    raise ValueError(\"Attempting to load state, but no state checkpointer is specified\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.opacus_checkpointer","title":"<code>opacus_checkpointer</code>","text":""},{"location":"api/#fl4health.checkpointing.opacus_checkpointer.OpacusCheckpointer","title":"<code>OpacusCheckpointer</code>","text":"<p>               Bases: <code>FunctionTorchModuleCheckpointer</code></p> <p>This is a specific type of checkpointer to be used in saving models trained using Opacus for differential privacy. Certain layers within Opacus wrapped models do not interact well with <code>torch.save</code> functionality. This checkpointer fixes this issue.</p> Source code in <code>fl4health/checkpointing/opacus_checkpointer.py</code> <pre><code>class OpacusCheckpointer(FunctionTorchModuleCheckpointer):\n    \"\"\"\n    This is a specific type of checkpointer to be used in saving models trained using Opacus for differential privacy.\n    Certain layers within Opacus wrapped models do not interact well with ``torch.save`` functionality. This\n    checkpointer fixes this issue.\n    \"\"\"\n\n    def maybe_checkpoint(self, model: GradSampleModule, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n        \"\"\"\n        Overriding the checkpointing strategy of the ``FunctionTorchCheckpointer`` to save model state dictionaries\n        instead of using the ``torch.save`` workflow.\n\n        Args:\n            model (nn.Module): Model to be potentially saved (should be an Opacus wrapped model)\n            loss (float): Loss value associated with the model to be used in checkpointing decisions.\n            metrics (dict[str, Scalar]): Metrics associated with the model to be used in checkpointing decisions.\n        \"\"\"\n        assert isinstance(model, GradSampleModule), (\n            f\"Model is of type: {type(model)}. This checkpointer need only be used to checkpoint Opacus modules\"\n        )\n        comparison_score = self.checkpoint_score_function(loss, metrics)\n        if self._should_checkpoint(comparison_score):\n            log(\n                INFO,\n                f\"Saving Opacus model state: Current score ({comparison_score}) \"\n                f\"{self.comparison_str} Best score ({self.best_score})\",\n            )\n            self.best_score = comparison_score\n            # Extract the state dictionary for the model and save it.\n            self._extract_and_save_state(model)\n        else:\n            log(\n                INFO,\n                f\"Not saving Opacus model state: Current score ({comparison_score}) is not \"\n                f\"{self.comparison_str} Best score ({self.best_score})\",\n            )\n\n    def _process_state_dict_keys(self, opacus_state_dict: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        State dictionary keys for Opacus modules will be prefixed with an ``_module``. So we remove these when loading\n        the state information into a standard torch model.\n\n        Args:\n            opacus_state_dict (dict[str, Any]): A state dictionary produced by an Opacus ``GradSamplingModule``\n\n        Returns:\n            (dict[str, Any]): A state dictionary with the ``_module``. removed from the key prefixes to facilitate\n                loading the state dictionary into a non-Opacus model.\n        \"\"\"\n        return {key.removeprefix(\"_module.\"): val for key, val in opacus_state_dict.items()}\n\n    def _extract_and_save_state(self, model: nn.Module) -&gt; None:\n        \"\"\"\n        Certain Opacus layers don't integrate nicely with the ``torch.save`` mechanism. So rather than using that\n        approach for checkpointing Opacus models, we extract and save the model state dictionary.\n\n        Args:\n            model (nn.Module): Model to be checkpointed via the state dictionary.\n        \"\"\"\n        model_state_dict = model.state_dict()\n        with open(self.checkpoint_path, \"wb\") as handle:\n            pickle.dump(model_state_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def load_checkpoint(self, path_to_checkpoint: str | None = None) -&gt; nn.Module:\n        raise NotImplementedError(\n            \"When loading from Opacus checkpointers, you need to provide a model into which state is loaded. \"\n            \"Please use load_best_checkpoint_into_model instead and provide model architecture to load state into.\"\n        )\n\n    def load_best_checkpoint_into_model(\n        self, target_model: nn.Module, target_is_grad_sample_module: bool = False\n    ) -&gt; None:\n        \"\"\"\n        State dictionary loading requires a model to be provided (unlike the ``torch.save`` mechanism). So we define\n        this function, which requires the user to provide a model into which the state dictionary is to be loaded.\n\n        Args:\n            target_model (nn.Module): Target model for loading state into.\n            target_is_grad_sample_module (bool, optional): Whether the ``target_model`` that the ``state_dict`` is\n                being loaded into is an Opacus module or just a vanilla Pytorch module. Defaults to False.\n        \"\"\"\n        with open(self.checkpoint_path, \"rb\") as handle:\n            model_state_dict = pickle.load(handle)\n            # If the target is just a plain PyTorch module, we remove the _module key prefix that Opacus inserts into\n            # its GradSampleModules.\n            if not target_is_grad_sample_module:\n                model_state_dict = self._process_state_dict_keys(model_state_dict)\n            target_model.load_state_dict(model_state_dict, strict=True)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.opacus_checkpointer.OpacusCheckpointer.maybe_checkpoint","title":"<code>maybe_checkpoint(model, loss, metrics)</code>","text":"<p>Overriding the checkpointing strategy of the <code>FunctionTorchCheckpointer</code> to save model state dictionaries instead of using the <code>torch.save</code> workflow.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to be potentially saved (should be an Opacus wrapped model)</p> required <code>loss</code> <code>float</code> <p>Loss value associated with the model to be used in checkpointing decisions.</p> required <code>metrics</code> <code>dict[str, Scalar]</code> <p>Metrics associated with the model to be used in checkpointing decisions.</p> required Source code in <code>fl4health/checkpointing/opacus_checkpointer.py</code> <pre><code>def maybe_checkpoint(self, model: GradSampleModule, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n    \"\"\"\n    Overriding the checkpointing strategy of the ``FunctionTorchCheckpointer`` to save model state dictionaries\n    instead of using the ``torch.save`` workflow.\n\n    Args:\n        model (nn.Module): Model to be potentially saved (should be an Opacus wrapped model)\n        loss (float): Loss value associated with the model to be used in checkpointing decisions.\n        metrics (dict[str, Scalar]): Metrics associated with the model to be used in checkpointing decisions.\n    \"\"\"\n    assert isinstance(model, GradSampleModule), (\n        f\"Model is of type: {type(model)}. This checkpointer need only be used to checkpoint Opacus modules\"\n    )\n    comparison_score = self.checkpoint_score_function(loss, metrics)\n    if self._should_checkpoint(comparison_score):\n        log(\n            INFO,\n            f\"Saving Opacus model state: Current score ({comparison_score}) \"\n            f\"{self.comparison_str} Best score ({self.best_score})\",\n        )\n        self.best_score = comparison_score\n        # Extract the state dictionary for the model and save it.\n        self._extract_and_save_state(model)\n    else:\n        log(\n            INFO,\n            f\"Not saving Opacus model state: Current score ({comparison_score}) is not \"\n            f\"{self.comparison_str} Best score ({self.best_score})\",\n        )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.opacus_checkpointer.OpacusCheckpointer.load_best_checkpoint_into_model","title":"<code>load_best_checkpoint_into_model(target_model, target_is_grad_sample_module=False)</code>","text":"<p>State dictionary loading requires a model to be provided (unlike the <code>torch.save</code> mechanism). So we define this function, which requires the user to provide a model into which the state dictionary is to be loaded.</p> <p>Parameters:</p> Name Type Description Default <code>target_model</code> <code>Module</code> <p>Target model for loading state into.</p> required <code>target_is_grad_sample_module</code> <code>bool</code> <p>Whether the <code>target_model</code> that the <code>state_dict</code> is being loaded into is an Opacus module or just a vanilla Pytorch module. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/checkpointing/opacus_checkpointer.py</code> <pre><code>def load_best_checkpoint_into_model(\n    self, target_model: nn.Module, target_is_grad_sample_module: bool = False\n) -&gt; None:\n    \"\"\"\n    State dictionary loading requires a model to be provided (unlike the ``torch.save`` mechanism). So we define\n    this function, which requires the user to provide a model into which the state dictionary is to be loaded.\n\n    Args:\n        target_model (nn.Module): Target model for loading state into.\n        target_is_grad_sample_module (bool, optional): Whether the ``target_model`` that the ``state_dict`` is\n            being loaded into is an Opacus module or just a vanilla Pytorch module. Defaults to False.\n    \"\"\"\n    with open(self.checkpoint_path, \"rb\") as handle:\n        model_state_dict = pickle.load(handle)\n        # If the target is just a plain PyTorch module, we remove the _module key prefix that Opacus inserts into\n        # its GradSampleModules.\n        if not target_is_grad_sample_module:\n            model_state_dict = self._process_state_dict_keys(model_state_dict)\n        target_model.load_state_dict(model_state_dict, strict=True)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.opacus_checkpointer.LatestOpacusCheckpointer","title":"<code>LatestOpacusCheckpointer</code>","text":"<p>               Bases: <code>OpacusCheckpointer</code></p> Source code in <code>fl4health/checkpointing/opacus_checkpointer.py</code> <pre><code>class LatestOpacusCheckpointer(OpacusCheckpointer):\n    def __init__(self, checkpoint_dir: str, checkpoint_name: str) -&gt; None:\n        \"\"\"\n        This class implements a checkpointer that always saves the model state when called. It uses a placeholder\n        scoring function and maximize argument.\n\n        Args:\n            checkpoint_dir (str): Directory to save checkpoint state to\n            checkpoint_name (str): Name of the file to which state is to be saved to.\n        \"\"\"\n\n        # This function is required by the parent class, but not used in the LatestOpacusCheckpointer\n        def latest_score_function(loss: float, _: dict[str, Scalar]) -&gt; float:\n            return 0.0\n\n        super().__init__(checkpoint_dir, checkpoint_name, latest_score_function, \"Latest\", False)\n\n    def maybe_checkpoint(self, model: GradSampleModule, loss: float, _: dict[str, Scalar]) -&gt; None:\n        assert isinstance(model, GradSampleModule), (\n            f\"Model is of type: {type(model)}. This checkpointer need only be used to checkpoint Opacus modules\"\n        )\n        # Always checkpoint the latest model\n        log(INFO, \"Saving latest checkpoint with LatestTorchCheckpointer\")\n        self._extract_and_save_state(model)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.opacus_checkpointer.LatestOpacusCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name)</code>","text":"<p>This class implements a checkpointer that always saves the model state when called. It uses a placeholder scoring function and maximize argument.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Directory to save checkpoint state to</p> required <code>checkpoint_name</code> <code>str</code> <p>Name of the file to which state is to be saved to.</p> required Source code in <code>fl4health/checkpointing/opacus_checkpointer.py</code> <pre><code>def __init__(self, checkpoint_dir: str, checkpoint_name: str) -&gt; None:\n    \"\"\"\n    This class implements a checkpointer that always saves the model state when called. It uses a placeholder\n    scoring function and maximize argument.\n\n    Args:\n        checkpoint_dir (str): Directory to save checkpoint state to\n        checkpoint_name (str): Name of the file to which state is to be saved to.\n    \"\"\"\n\n    # This function is required by the parent class, but not used in the LatestOpacusCheckpointer\n    def latest_score_function(loss: float, _: dict[str, Scalar]) -&gt; float:\n        return 0.0\n\n    super().__init__(checkpoint_dir, checkpoint_name, latest_score_function, \"Latest\", False)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.opacus_checkpointer.BestLossOpacusCheckpointer","title":"<code>BestLossOpacusCheckpointer</code>","text":"<p>               Bases: <code>OpacusCheckpointer</code></p> Source code in <code>fl4health/checkpointing/opacus_checkpointer.py</code> <pre><code>class BestLossOpacusCheckpointer(OpacusCheckpointer):\n    def __init__(self, checkpoint_dir: str, checkpoint_name: str) -&gt; None:\n        \"\"\"\n        This checkpointer only uses the loss value provided to the ``maybe_checkpoint`` function to determine whether a\n        checkpoint should be save. We are always attempting to minimize the loss. So maximize is always set to false.\n\n        Args:\n            checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n                checkpointer will not create it if it does not.\n            checkpoint_name (str): Name of the checkpoint to be saved.\n        \"\"\"\n\n        # The BestLossOpacusCheckpointer just uses the provided loss to scoring checkpoints. More complicated\n        # approaches may be used by other classes.\n        def loss_score_function(loss: float, _: dict[str, Scalar]) -&gt; float:\n            return loss\n\n        super().__init__(checkpoint_dir, checkpoint_name, loss_score_function, \"Loss\", maximize=False)\n\n    def maybe_checkpoint(self, model: GradSampleModule, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n        assert isinstance(model, GradSampleModule), (\n            f\"Model is of type: {type(model)}. This checkpointer need only be used to checkpoint Opacus modules\"\n        )\n        # First we use the provided scoring function to produce a score\n        comparison_score = self.checkpoint_score_function(loss, metrics)\n        if self._should_checkpoint(comparison_score):\n            log(\n                INFO,\n                f\"Checkpointing the model: Current Loss ({comparison_score}) \"\n                f\"{self.comparison_str} Best Loss ({self.best_score})\",\n            )\n            self.best_score = comparison_score\n            self._extract_and_save_state(model)\n        else:\n            log(\n                INFO,\n                f\"Not checkpointing the model: Current Loss ({comparison_score}) is not \"\n                f\"{self.comparison_str} Best Loss ({self.best_score})\",\n            )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.opacus_checkpointer.BestLossOpacusCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name)</code>","text":"<p>This checkpointer only uses the loss value provided to the <code>maybe_checkpoint</code> function to determine whether a checkpoint should be save. We are always attempting to minimize the loss. So maximize is always set to false.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>str</code> <p>Directory to which the model is saved. This directory should already exist. The checkpointer will not create it if it does not.</p> required <code>checkpoint_name</code> <code>str</code> <p>Name of the checkpoint to be saved.</p> required Source code in <code>fl4health/checkpointing/opacus_checkpointer.py</code> <pre><code>def __init__(self, checkpoint_dir: str, checkpoint_name: str) -&gt; None:\n    \"\"\"\n    This checkpointer only uses the loss value provided to the ``maybe_checkpoint`` function to determine whether a\n    checkpoint should be save. We are always attempting to minimize the loss. So maximize is always set to false.\n\n    Args:\n        checkpoint_dir (str): Directory to which the model is saved. This directory should already exist. The\n            checkpointer will not create it if it does not.\n        checkpoint_name (str): Name of the checkpoint to be saved.\n    \"\"\"\n\n    # The BestLossOpacusCheckpointer just uses the provided loss to scoring checkpoints. More complicated\n    # approaches may be used by other classes.\n    def loss_score_function(loss: float, _: dict[str, Scalar]) -&gt; float:\n        return loss\n\n    super().__init__(checkpoint_dir, checkpoint_name, loss_score_function, \"Loss\", maximize=False)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module","title":"<code>server_module</code>","text":""},{"location":"api/#fl4health.checkpointing.server_module.BaseServerCheckpointAndStateModule","title":"<code>BaseServerCheckpointAndStateModule</code>","text":"Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>class BaseServerCheckpointAndStateModule:\n    def __init__(\n        self,\n        model: nn.Module | None = None,\n        parameter_exchanger: ExchangerType | None = None,\n        model_checkpointers: ModelCheckpointers = None,\n        state_checkpointer: ServerStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to handle basic model and state checkpointing on the server-side of an FL process. Unlike\n        the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only\n        considers checkpointing the global server model after aggregation, perhaps based on validation statistics\n        retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be\n        used. For state checkpointing, which saves the state of the entire server-side FL process to help with\n        FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval\n        round of FL.\n\n        Args:\n            model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n                to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n                Recall that servers only have parameters rather than torch models. So we need to know where to route\n                these parameters to allow for real models to be saved. Defaults to None.\n            parameter_exchanger (ExchangerType | None, optional): This will facilitate routing the\n                server parameters into the right components of the provided model architecture. Note that this\n                exchanger and the model must match the one used for training and exchange with the servers to ensure\n                parameters go to the right places. Defaults to None.\n            model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n            state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n                will be used to preserve FL training state to facilitate restarting training if interrupted.\n                Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n        \"\"\"\n        self.model = model\n        self.parameter_exchanger = parameter_exchanger\n        self.model_checkpointers = (\n            [model_checkpointers] if isinstance(model_checkpointers, TorchModuleCheckpointer) else model_checkpointers\n        )\n        self.state_checkpointer = state_checkpointer\n        if self.model_checkpointers is not None and len(self.model_checkpointers):\n            # If there are model checkpointers, make sure the the model and parameter exchanger are defined.\n            self._validate_model_checkpointer_components()\n        self._check_if_shared_checkpoint_names()\n\n    def _validate_model_checkpointer_components(self) -&gt; None:\n        assert self.model is not None, (\n            \"Checkpointer(s) is (are) defined but no model is defined to hydrate. The functionality of \"\n            \"this class can be overridden in a child class if checkpointing without a parameter exchanger is \"\n            \"possible and desired\"\n        )\n        assert self.parameter_exchanger is not None, (\n            \"Checkpointer(s) is (are) defined but no parameter_exchanger is defined to hydrate. The functionality of \"\n            \"this class can be overridden in a child class if checkpointing without a parameter exchanger is \"\n            \"possible and desired\"\n        )\n\n    def _check_if_shared_checkpoint_names(self) -&gt; None:\n        \"\"\"\n        This function is meant to throw an exception if there is an overlap in the paths to which model checkpointers\n        will save model checkpoints to avoid accidental overwriting.\n        \"\"\"\n        checkpointer_paths = (\n            [checkpointer.checkpoint_path for checkpointer in self.model_checkpointers]\n            if self.model_checkpointers\n            else []\n        )\n        unique_paths = set(checkpointer_paths)\n\n        if len(unique_paths) != len(checkpointer_paths):\n            formatted_all_paths = \"\\n\".join(checkpointer_paths)\n            raise ValueError(\n                \"The paths of all of your checkpointers should be unique otherwise overwrites are possible and data \"\n                f\"will be lost. The current paths are:\\n{formatted_all_paths}\"\n            )\n\n    def maybe_checkpoint(self, server_parameters: Parameters, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n        \"\"\"\n        If there are model checkpointers defined in this class, we hydrate the model for checkpointing with the server\n        parameters and call maybe checkpoint model on each of the checkpointers to decide whether to checkpoint based\n        on the model metrics or loss and the checkpointer definitions.\n\n        Args:\n            server_parameters (Parameters): Parameters held by the server that should be injected into the model\n            loss (float): The aggregated loss value obtained by the current aggregated server model.\n                Potentially used by checkpointer to decide whether to checkpoint the model.\n            metrics (dict[str, Scalar]): The aggregated metrics obtained by the aggregated server model. Potentially\n                used by checkpointer to decide whether to checkpoint the model.\n        \"\"\"\n        if self.model_checkpointers is not None and len(self.model_checkpointers) &gt; 0:\n            assert self.model is not None\n            self._hydrate_model_for_checkpointing(server_parameters)\n            for checkpointer in self.model_checkpointers:\n                checkpointer.maybe_checkpoint(self.model, loss, metrics)\n        else:\n            log(INFO, \"No model checkpointers specified. Skipping any checkpointing.\")\n\n    def _hydrate_model_for_checkpointing(self, server_parameters: Parameters) -&gt; None:\n        \"\"\"\n        This function is used as a means of saving the server-side model after aggregation in the FL training\n        trajectory. Presently, the server only holds Flower Parameters, which are essentially just ndarrays. Without\n        knowledge of a model architecture to which the arrays correspond. Thus, in the default implementation, we\n        require that a torch architecture and a parameter exchanger be provided which handles mapping these numpy\n        arrays into the architecture properly.\n\n        This function may be overridden in a child class if different behavior is desired.\n\n        **NOTE**: This function stores the weights directly in the self.model attribute\n\n        Args:\n            server_parameters (Parameters): Parameters to be injected into the torch model architecture and\n            checkpointed.\n        \"\"\"\n        assert self.model is not None, \"Hydrate model for checkpoint called but self.model is None\"\n        assert self.parameter_exchanger is not None, (\n            \"Hydrate model for checkpoint called but self.parameter_exchanger is None\"\n        )\n        model_ndarrays = parameters_to_ndarrays(server_parameters)\n        self.parameter_exchanger.pull_parameters(model_ndarrays, self.model)\n\n    def save_state(self, server: FlServer, server_parameters: Parameters) -&gt; None:\n        \"\"\"\n        Facilitates saving state required to restart the FL process on the server side. By default, this function\n        will preserve the state of the server as defined by ``snapshot_attrs`` in ``ServerStateCheckpointer`` .\n        Note that ``server_parameters`` will be hydrated and passed to the state checkpointer module to facilitate\n        saving the state of the server's parameters.\n\n        Args:\n            server (FlServer): Server object from which state will be extracted and saved.\n            server_parameters (Parameters): Like model checkpointers, these are the aggregated Parameters stored by\n                the server representing model state. They are mapped to a torch model architecture via the\n                ``_hydrate_model_for_checkpointing`` function.\n\n        Raises:\n            ValueError: Throws an error if this function is called, but no state checkpointer has been provided.\n        \"\"\"\n        if self.state_checkpointer is not None:\n            self._hydrate_model_for_checkpointing(server_parameters)\n            assert self.model is not None\n            self.state_checkpointer.save_server_state(server, self.model)\n        else:\n            raise ValueError(\"Attempting to save state but no state checkpointer is specified\")\n\n    def maybe_load_state(self, server: FlServer) -&gt; Parameters | None:\n        \"\"\"\n        Facilitates loading of any pre-existing state in the directory of the ``state_checkpointer``. If a\n        ``state_checkpointer`` is defined and a checkpoint exists at its ``checkpoint_path``, this method hydrates the\n        model with the saved state and returns the corresponding server Parameters. If no checkpoint exists, it logs\n        this information and returns None.\n\n        Args:\n            server (FlServer): server into which checkpointed state will be loaded if a checkpoint exists\n\n        Raises:\n            ValueError: Throws an error if this function is called, but no state checkpointer has been provided.\n\n        Returns:\n            (Parameters | None): If the state checkpoint properly exists and is loaded correctly, ``server_parameters``\n                is returned. Otherwise, we return a None (or throw an exception).\n        \"\"\"\n        if self.state_checkpointer is not None:\n            assert self.model is not None, (\n                \"Attempting to load state but self.model is None, make sure to pass the model architecture\"\n                \" to checkpointing module\"\n            )\n            server_model = self.state_checkpointer.maybe_load_server_state(server, self.model)\n            if server_model:\n                assert self.parameter_exchanger is not None\n                return ndarrays_to_parameters(self.parameter_exchanger.push_parameters(server_model))\n            return None\n        raise ValueError(\"Attempting to load state, but no state checkpointer is specified\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.BaseServerCheckpointAndStateModule.__init__","title":"<code>__init__(model=None, parameter_exchanger=None, model_checkpointers=None, state_checkpointer=None)</code>","text":"<p>This module is meant to handle basic model and state checkpointing on the server-side of an FL process. Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on validation statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval round of FL.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model architecture to be saved. The module will use this architecture to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger. Recall that servers only have parameters rather than torch models. So we need to know where to route these parameters to allow for real models to be saved. Defaults to None.</p> <code>None</code> <code>parameter_exchanger</code> <code>ExchangerType | None</code> <p>This will facilitate routing the server parameters into the right components of the provided model architecture. Note that this exchanger and the model must match the one used for training and exchange with the servers to ensure parameters go to the right places. Defaults to None.</p> <code>None</code> <code>model_checkpointers</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>ServerStateCheckpointer | None</code> <p>If defined, this checkpointer will be used to preserve FL training state to facilitate restarting training if interrupted. Generally, this checkpointer will save much more than just the model being trained. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module | None = None,\n    parameter_exchanger: ExchangerType | None = None,\n    model_checkpointers: ModelCheckpointers = None,\n    state_checkpointer: ServerStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to handle basic model and state checkpointing on the server-side of an FL process. Unlike\n    the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only\n    considers checkpointing the global server model after aggregation, perhaps based on validation statistics\n    retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be\n    used. For state checkpointing, which saves the state of the entire server-side FL process to help with\n    FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval\n    round of FL.\n\n    Args:\n        model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n            to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n            Recall that servers only have parameters rather than torch models. So we need to know where to route\n            these parameters to allow for real models to be saved. Defaults to None.\n        parameter_exchanger (ExchangerType | None, optional): This will facilitate routing the\n            server parameters into the right components of the provided model architecture. Note that this\n            exchanger and the model must match the one used for training and exchange with the servers to ensure\n            parameters go to the right places. Defaults to None.\n        model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n        state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n            will be used to preserve FL training state to facilitate restarting training if interrupted.\n            Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n    \"\"\"\n    self.model = model\n    self.parameter_exchanger = parameter_exchanger\n    self.model_checkpointers = (\n        [model_checkpointers] if isinstance(model_checkpointers, TorchModuleCheckpointer) else model_checkpointers\n    )\n    self.state_checkpointer = state_checkpointer\n    if self.model_checkpointers is not None and len(self.model_checkpointers):\n        # If there are model checkpointers, make sure the the model and parameter exchanger are defined.\n        self._validate_model_checkpointer_components()\n    self._check_if_shared_checkpoint_names()\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.BaseServerCheckpointAndStateModule.maybe_checkpoint","title":"<code>maybe_checkpoint(server_parameters, loss, metrics)</code>","text":"<p>If there are model checkpointers defined in this class, we hydrate the model for checkpointing with the server parameters and call maybe checkpoint model on each of the checkpointers to decide whether to checkpoint based on the model metrics or loss and the checkpointer definitions.</p> <p>Parameters:</p> Name Type Description Default <code>server_parameters</code> <code>Parameters</code> <p>Parameters held by the server that should be injected into the model</p> required <code>loss</code> <code>float</code> <p>The aggregated loss value obtained by the current aggregated server model. Potentially used by checkpointer to decide whether to checkpoint the model.</p> required <code>metrics</code> <code>dict[str, Scalar]</code> <p>The aggregated metrics obtained by the aggregated server model. Potentially used by checkpointer to decide whether to checkpoint the model.</p> required Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def maybe_checkpoint(self, server_parameters: Parameters, loss: float, metrics: dict[str, Scalar]) -&gt; None:\n    \"\"\"\n    If there are model checkpointers defined in this class, we hydrate the model for checkpointing with the server\n    parameters and call maybe checkpoint model on each of the checkpointers to decide whether to checkpoint based\n    on the model metrics or loss and the checkpointer definitions.\n\n    Args:\n        server_parameters (Parameters): Parameters held by the server that should be injected into the model\n        loss (float): The aggregated loss value obtained by the current aggregated server model.\n            Potentially used by checkpointer to decide whether to checkpoint the model.\n        metrics (dict[str, Scalar]): The aggregated metrics obtained by the aggregated server model. Potentially\n            used by checkpointer to decide whether to checkpoint the model.\n    \"\"\"\n    if self.model_checkpointers is not None and len(self.model_checkpointers) &gt; 0:\n        assert self.model is not None\n        self._hydrate_model_for_checkpointing(server_parameters)\n        for checkpointer in self.model_checkpointers:\n            checkpointer.maybe_checkpoint(self.model, loss, metrics)\n    else:\n        log(INFO, \"No model checkpointers specified. Skipping any checkpointing.\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.BaseServerCheckpointAndStateModule.save_state","title":"<code>save_state(server, server_parameters)</code>","text":"<p>Facilitates saving state required to restart the FL process on the server side. By default, this function will preserve the state of the server as defined by <code>snapshot_attrs</code> in <code>ServerStateCheckpointer</code> . Note that <code>server_parameters</code> will be hydrated and passed to the state checkpointer module to facilitate saving the state of the server's parameters.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FlServer</code> <p>Server object from which state will be extracted and saved.</p> required <code>server_parameters</code> <code>Parameters</code> <p>Like model checkpointers, these are the aggregated Parameters stored by the server representing model state. They are mapped to a torch model architecture via the <code>_hydrate_model_for_checkpointing</code> function.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Throws an error if this function is called, but no state checkpointer has been provided.</p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def save_state(self, server: FlServer, server_parameters: Parameters) -&gt; None:\n    \"\"\"\n    Facilitates saving state required to restart the FL process on the server side. By default, this function\n    will preserve the state of the server as defined by ``snapshot_attrs`` in ``ServerStateCheckpointer`` .\n    Note that ``server_parameters`` will be hydrated and passed to the state checkpointer module to facilitate\n    saving the state of the server's parameters.\n\n    Args:\n        server (FlServer): Server object from which state will be extracted and saved.\n        server_parameters (Parameters): Like model checkpointers, these are the aggregated Parameters stored by\n            the server representing model state. They are mapped to a torch model architecture via the\n            ``_hydrate_model_for_checkpointing`` function.\n\n    Raises:\n        ValueError: Throws an error if this function is called, but no state checkpointer has been provided.\n    \"\"\"\n    if self.state_checkpointer is not None:\n        self._hydrate_model_for_checkpointing(server_parameters)\n        assert self.model is not None\n        self.state_checkpointer.save_server_state(server, self.model)\n    else:\n        raise ValueError(\"Attempting to save state but no state checkpointer is specified\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.BaseServerCheckpointAndStateModule.maybe_load_state","title":"<code>maybe_load_state(server)</code>","text":"<p>Facilitates loading of any pre-existing state in the directory of the <code>state_checkpointer</code>. If a <code>state_checkpointer</code> is defined and a checkpoint exists at its <code>checkpoint_path</code>, this method hydrates the model with the saved state and returns the corresponding server Parameters. If no checkpoint exists, it logs this information and returns None.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FlServer</code> <p>server into which checkpointed state will be loaded if a checkpoint exists</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Throws an error if this function is called, but no state checkpointer has been provided.</p> <p>Returns:</p> Type Description <code>Parameters | None</code> <p>If the state checkpoint properly exists and is loaded correctly, <code>server_parameters</code> is returned. Otherwise, we return a None (or throw an exception).</p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def maybe_load_state(self, server: FlServer) -&gt; Parameters | None:\n    \"\"\"\n    Facilitates loading of any pre-existing state in the directory of the ``state_checkpointer``. If a\n    ``state_checkpointer`` is defined and a checkpoint exists at its ``checkpoint_path``, this method hydrates the\n    model with the saved state and returns the corresponding server Parameters. If no checkpoint exists, it logs\n    this information and returns None.\n\n    Args:\n        server (FlServer): server into which checkpointed state will be loaded if a checkpoint exists\n\n    Raises:\n        ValueError: Throws an error if this function is called, but no state checkpointer has been provided.\n\n    Returns:\n        (Parameters | None): If the state checkpoint properly exists and is loaded correctly, ``server_parameters``\n            is returned. Otherwise, we return a None (or throw an exception).\n    \"\"\"\n    if self.state_checkpointer is not None:\n        assert self.model is not None, (\n            \"Attempting to load state but self.model is None, make sure to pass the model architecture\"\n            \" to checkpointing module\"\n        )\n        server_model = self.state_checkpointer.maybe_load_server_state(server, self.model)\n        if server_model:\n            assert self.parameter_exchanger is not None\n            return ndarrays_to_parameters(self.parameter_exchanger.push_parameters(server_model))\n        return None\n    raise ValueError(\"Attempting to load state, but no state checkpointer is specified\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.PackingServerCheckpointAndAndStateModule","title":"<code>PackingServerCheckpointAndAndStateModule</code>","text":"<p>               Bases: <code>BaseServerCheckpointAndStateModule</code></p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>class PackingServerCheckpointAndAndStateModule(BaseServerCheckpointAndStateModule):\n    def __init__(\n        self,\n        model: nn.Module | None = None,\n        parameter_exchanger: FullParameterExchangerWithPacking | None = None,\n        model_checkpointers: ModelCheckpointers = None,\n        state_checkpointer: ServerStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to be a base class for any server-side checkpointing module that relies on unpacking\n        of parameters to hydrate models for checkpointing. The specifics of the unpacking will be handled by the\n        child classes of the packer within the parameter exchange.\n        **NOTE**: This function ASSUMES full parameter exchange unpacking. If more complex unpacking/parameter exchange\n        is used, this is not the right parent class.\n\n        Args:\n            model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n                to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n                Recall that servers only have parameters rather than torch models. So we need to know where to route\n                these parameters to allow for real models to be saved. Defaults to None.\n            parameter_exchanger (FullParameterExchangerWithPacking | None, optional): This will facilitate routing the\n                server parameters into the right components of the provided model architecture. It specifically also\n                should handle any necessary unpacking of the parameters. Note that this exchanger and the model must\n                match the one used for training and exchange with the servers to ensure parameters go to the right\n                places. Defaults to None.\n            model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n            state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n                will be used to preserve FL training state to facilitate restarting training if interrupted.\n                Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n        \"\"\"\n        if parameter_exchanger is not None:\n            assert isinstance(parameter_exchanger, FullParameterExchangerWithPacking), (\n                \"Parameter exchanger must be of based type FullParameterExchangerWithPacking\"\n            )\n        super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n\n    def _hydrate_model_for_checkpointing(self, server_parameters: Parameters) -&gt; None:\n        \"\"\"\n        This function is used as a means of saving the server-side model after aggregation in the FL training\n        trajectory. Presently, the server only holds Flower Parameters, which are essentially just ndarrays. Without\n        knowledge of a model architecture to which the arrays correspond. Thus, in the default implementation, we\n        require that a torch architecture and a parameter exchanger be provided which handles mapping these numpy\n        arrays into the architecture properly.\n\n        This function overrides the base functionality of model hydration to insert an additional unpacking step\n        using the unpacking function of the specific type of parameter exchanger.\n\n        **NOTE**: This function stores the weights directly in the self.model attribute\n\n        Args:\n            server_parameters (Parameters): Parameters to be injected into the torch model architecture and\n            checkpointed.\n        \"\"\"\n        assert self.model is not None, \"Hydrate model for checkpoint called but self.model is None\"\n        assert self.parameter_exchanger is not None, (\n            \"Hydrate model for checkpoint called but self.parameter_exchanger is None\"\n        )\n        packed_parameters = parameters_to_ndarrays(server_parameters)\n        assert isinstance(self.parameter_exchanger, FullParameterExchangerWithPacking)\n        # Use the unpacking function of the parameter exchange to handle extraction of model parameters\n        model_ndarrays, _ = self.parameter_exchanger.unpack_parameters(packed_parameters)\n        self.parameter_exchanger.pull_parameters(model_ndarrays, self.model)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.PackingServerCheckpointAndAndStateModule.__init__","title":"<code>__init__(model=None, parameter_exchanger=None, model_checkpointers=None, state_checkpointer=None)</code>","text":"<p>This module is meant to be a base class for any server-side checkpointing module that relies on unpacking of parameters to hydrate models for checkpointing. The specifics of the unpacking will be handled by the child classes of the packer within the parameter exchange. NOTE: This function ASSUMES full parameter exchange unpacking. If more complex unpacking/parameter exchange is used, this is not the right parent class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model architecture to be saved. The module will use this architecture to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger. Recall that servers only have parameters rather than torch models. So we need to know where to route these parameters to allow for real models to be saved. Defaults to None.</p> <code>None</code> <code>parameter_exchanger</code> <code>FullParameterExchangerWithPacking | None</code> <p>This will facilitate routing the server parameters into the right components of the provided model architecture. It specifically also should handle any necessary unpacking of the parameters. Note that this exchanger and the model must match the one used for training and exchange with the servers to ensure parameters go to the right places. Defaults to None.</p> <code>None</code> <code>model_checkpointers</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>ServerStateCheckpointer | None</code> <p>If defined, this checkpointer will be used to preserve FL training state to facilitate restarting training if interrupted. Generally, this checkpointer will save much more than just the model being trained. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module | None = None,\n    parameter_exchanger: FullParameterExchangerWithPacking | None = None,\n    model_checkpointers: ModelCheckpointers = None,\n    state_checkpointer: ServerStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to be a base class for any server-side checkpointing module that relies on unpacking\n    of parameters to hydrate models for checkpointing. The specifics of the unpacking will be handled by the\n    child classes of the packer within the parameter exchange.\n    **NOTE**: This function ASSUMES full parameter exchange unpacking. If more complex unpacking/parameter exchange\n    is used, this is not the right parent class.\n\n    Args:\n        model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n            to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n            Recall that servers only have parameters rather than torch models. So we need to know where to route\n            these parameters to allow for real models to be saved. Defaults to None.\n        parameter_exchanger (FullParameterExchangerWithPacking | None, optional): This will facilitate routing the\n            server parameters into the right components of the provided model architecture. It specifically also\n            should handle any necessary unpacking of the parameters. Note that this exchanger and the model must\n            match the one used for training and exchange with the servers to ensure parameters go to the right\n            places. Defaults to None.\n        model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n        state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n            will be used to preserve FL training state to facilitate restarting training if interrupted.\n            Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n    \"\"\"\n    if parameter_exchanger is not None:\n        assert isinstance(parameter_exchanger, FullParameterExchangerWithPacking), (\n            \"Parameter exchanger must be of based type FullParameterExchangerWithPacking\"\n        )\n    super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.ScaffoldServerCheckpointAndStateModule","title":"<code>ScaffoldServerCheckpointAndStateModule</code>","text":"<p>               Bases: <code>PackingServerCheckpointAndAndStateModule</code></p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>class ScaffoldServerCheckpointAndStateModule(PackingServerCheckpointAndAndStateModule):\n    def __init__(\n        self,\n        model: nn.Module | None = None,\n        model_checkpointers: ModelCheckpointers = None,\n        state_checkpointer: ServerStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to handle SCAFFOLD model and state checkpointing on the server-side of an FL process.\n        Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing.\n        It only considers checkpointing the global server model after aggregation, perhaps based on validation\n        statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers\n        may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with\n        FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval\n        round of FL.\n\n        Args:\n            model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n                to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n                Recall that servers only have parameters rather than torch models. So we need to know where to route\n                these parameters to allow for real models to be saved. Defaults to None.\n            model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n            state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n                will be used to preserve FL training state to facilitate restarting training if interrupted.\n                Generally, this  checkpointer will save much more than just the model being trained. Defaults to None.\n        \"\"\"\n        if model is not None:\n            model_size = len(model.state_dict())\n            parameter_exchanger = FullParameterExchangerWithPacking(ParameterPackerWithControlVariates(model_size))\n        else:\n            parameter_exchanger = None\n        super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.ScaffoldServerCheckpointAndStateModule.__init__","title":"<code>__init__(model=None, model_checkpointers=None, state_checkpointer=None)</code>","text":"<p>This module is meant to handle SCAFFOLD model and state checkpointing on the server-side of an FL process. Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on validation statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval round of FL.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model architecture to be saved. The module will use this architecture to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger. Recall that servers only have parameters rather than torch models. So we need to know where to route these parameters to allow for real models to be saved. Defaults to None.</p> <code>None</code> <code>model_checkpointers</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>ServerStateCheckpointer | None</code> <p>If defined, this checkpointer will be used to preserve FL training state to facilitate restarting training if interrupted. Generally, this  checkpointer will save much more than just the model being trained. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module | None = None,\n    model_checkpointers: ModelCheckpointers = None,\n    state_checkpointer: ServerStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to handle SCAFFOLD model and state checkpointing on the server-side of an FL process.\n    Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing.\n    It only considers checkpointing the global server model after aggregation, perhaps based on validation\n    statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers\n    may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with\n    FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval\n    round of FL.\n\n    Args:\n        model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n            to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n            Recall that servers only have parameters rather than torch models. So we need to know where to route\n            these parameters to allow for real models to be saved. Defaults to None.\n        model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n        state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n            will be used to preserve FL training state to facilitate restarting training if interrupted.\n            Generally, this  checkpointer will save much more than just the model being trained. Defaults to None.\n    \"\"\"\n    if model is not None:\n        model_size = len(model.state_dict())\n        parameter_exchanger = FullParameterExchangerWithPacking(ParameterPackerWithControlVariates(model_size))\n    else:\n        parameter_exchanger = None\n    super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.AdaptiveConstraintServerCheckpointAndStateModule","title":"<code>AdaptiveConstraintServerCheckpointAndStateModule</code>","text":"<p>               Bases: <code>PackingServerCheckpointAndAndStateModule</code></p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>class AdaptiveConstraintServerCheckpointAndStateModule(PackingServerCheckpointAndAndStateModule):\n    def __init__(\n        self,\n        model: nn.Module | None = None,\n        model_checkpointers: ModelCheckpointers = None,\n        state_checkpointer: ServerStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to handle FL flows with adaptive constraints, where the server and client communicate\n        a loss weight parameter in addition to the model weights. Unlike the module on the client side, this module\n        has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server\n        model after aggregation, perhaps based on validation statistics retrieved on the client side by running a\n        federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the\n        state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer\n        responsible for saving the state after each fit and eval round of FL.\n\n        Args:\n            model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n                to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n                Recall that servers only have parameters rather than torch models. So we need to know where to route\n                these parameters to allow for real models to be saved. Defaults to None.\n            model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n            state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n                will be used to preserve FL training state to facilitate restarting training if interrupted.\n                Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n        \"\"\"\n        if model is not None:\n            parameter_exchanger = FullParameterExchangerWithPacking(ParameterPackerAdaptiveConstraint())\n        else:\n            parameter_exchanger = None\n        super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.AdaptiveConstraintServerCheckpointAndStateModule.__init__","title":"<code>__init__(model=None, model_checkpointers=None, state_checkpointer=None)</code>","text":"<p>This module is meant to handle FL flows with adaptive constraints, where the server and client communicate a loss weight parameter in addition to the model weights. Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on validation statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval round of FL.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model architecture to be saved. The module will use this architecture to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger. Recall that servers only have parameters rather than torch models. So we need to know where to route these parameters to allow for real models to be saved. Defaults to None.</p> <code>None</code> <code>model_checkpointers</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>ServerStateCheckpointer | None</code> <p>If defined, this checkpointer will be used to preserve FL training state to facilitate restarting training if interrupted. Generally, this checkpointer will save much more than just the model being trained. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module | None = None,\n    model_checkpointers: ModelCheckpointers = None,\n    state_checkpointer: ServerStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to handle FL flows with adaptive constraints, where the server and client communicate\n    a loss weight parameter in addition to the model weights. Unlike the module on the client side, this module\n    has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server\n    model after aggregation, perhaps based on validation statistics retrieved on the client side by running a\n    federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the\n    state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer\n    responsible for saving the state after each fit and eval round of FL.\n\n    Args:\n        model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n            to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n            Recall that servers only have parameters rather than torch models. So we need to know where to route\n            these parameters to allow for real models to be saved. Defaults to None.\n        model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n        state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n            will be used to preserve FL training state to facilitate restarting training if interrupted.\n            Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n    \"\"\"\n    if model is not None:\n        parameter_exchanger = FullParameterExchangerWithPacking(ParameterPackerAdaptiveConstraint())\n    else:\n        parameter_exchanger = None\n    super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.ClippingBitServerCheckpointAndStateModule","title":"<code>ClippingBitServerCheckpointAndStateModule</code>","text":"<p>               Bases: <code>PackingServerCheckpointAndAndStateModule</code></p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>class ClippingBitServerCheckpointAndStateModule(PackingServerCheckpointAndAndStateModule):\n    def __init__(\n        self,\n        model: nn.Module | None = None,\n        model_checkpointers: ModelCheckpointers = None,\n        state_checkpointer: ServerStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to handle FL flows with clipping bits being passed to the server along with the model\n        weights. This is used for DP-FL with adaptive clipping. Unlike the module on the client side, this module\n        has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server\n        model after aggregation, perhaps based on validation statistics retrieved on the client side by running a\n        federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the\n        state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer\n        responsible for saving the state after each fit and eval round of FL.\n\n        Args:\n            model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n                to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n                Recall that servers only have parameters rather than torch models. So we need to know where to route\n                these parameters to allow for real models to be saved. Defaults to None.\n            model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n            state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n                will be used to preserve FL training state to facilitate restarting training if interrupted.\n                Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n        \"\"\"\n        if model is not None:\n            parameter_exchanger = FullParameterExchangerWithPacking(ParameterPackerWithClippingBit())\n        else:\n            parameter_exchanger = None\n        super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.ClippingBitServerCheckpointAndStateModule.__init__","title":"<code>__init__(model=None, model_checkpointers=None, state_checkpointer=None)</code>","text":"<p>This module is meant to handle FL flows with clipping bits being passed to the server along with the model weights. This is used for DP-FL with adaptive clipping. Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on validation statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval round of FL.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model architecture to be saved. The module will use this architecture to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger. Recall that servers only have parameters rather than torch models. So we need to know where to route these parameters to allow for real models to be saved. Defaults to None.</p> <code>None</code> <code>model_checkpointers</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>ServerStateCheckpointer | None</code> <p>If defined, this checkpointer will be used to preserve FL training state to facilitate restarting training if interrupted. Generally, this checkpointer will save much more than just the model being trained. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module | None = None,\n    model_checkpointers: ModelCheckpointers = None,\n    state_checkpointer: ServerStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to handle FL flows with clipping bits being passed to the server along with the model\n    weights. This is used for DP-FL with adaptive clipping. Unlike the module on the client side, this module\n    has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server\n    model after aggregation, perhaps based on validation statistics retrieved on the client side by running a\n    federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the\n    state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer\n    responsible for saving the state after each fit and eval round of FL.\n\n    Args:\n        model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n            to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n            Recall that servers only have parameters rather than torch models. So we need to know where to route\n            these parameters to allow for real models to be saved. Defaults to None.\n        model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n        state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n            will be used to preserve FL training state to facilitate restarting training if interrupted.\n            Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n    \"\"\"\n    if model is not None:\n        parameter_exchanger = FullParameterExchangerWithPacking(ParameterPackerWithClippingBit())\n    else:\n        parameter_exchanger = None\n    super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.LayerNamesServerCheckpointAndStateModule","title":"<code>LayerNamesServerCheckpointAndStateModule</code>","text":"<p>               Bases: <code>PackingServerCheckpointAndAndStateModule</code></p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>class LayerNamesServerCheckpointAndStateModule(PackingServerCheckpointAndAndStateModule):\n    def __init__(\n        self,\n        model: nn.Module | None = None,\n        model_checkpointers: ModelCheckpointers = None,\n        state_checkpointer: ServerStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to handle FL flows with layer names being passed to the server along with the model\n        weights. This is used for adaptive layer exchange FL. Unlike the module on the client side, this module\n        has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server\n        model after aggregation, perhaps based on validation statistics retrieved on the client side by running a\n        federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the\n        state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer\n        responsible for saving the state after each fit and eval round of FL.\n\n        Args:\n            model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n                to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n                Recall that servers only have parameters rather than torch models. So we need to know where to route\n                these parameters to allow for real models to be saved. Defaults to None.\n            model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n            state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n                will be used to preserve FL training state to facilitate restarting training if interrupted.\n                Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n        \"\"\"\n        if model is not None:\n            parameter_exchanger = FullParameterExchangerWithPacking(ParameterPackerWithLayerNames())\n        else:\n            parameter_exchanger = None\n        super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.LayerNamesServerCheckpointAndStateModule.__init__","title":"<code>__init__(model=None, model_checkpointers=None, state_checkpointer=None)</code>","text":"<p>This module is meant to handle FL flows with layer names being passed to the server along with the model weights. This is used for adaptive layer exchange FL. Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on validation statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval round of FL.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model architecture to be saved. The module will use this architecture to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger. Recall that servers only have parameters rather than torch models. So we need to know where to route these parameters to allow for real models to be saved. Defaults to None.</p> <code>None</code> <code>model_checkpointers</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>ServerStateCheckpointer | None</code> <p>If defined, this checkpointer will be used to preserve FL training state to facilitate restarting training if interrupted. Generally, this checkpointer will save much more than just the model being trained. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module | None = None,\n    model_checkpointers: ModelCheckpointers = None,\n    state_checkpointer: ServerStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to handle FL flows with layer names being passed to the server along with the model\n    weights. This is used for adaptive layer exchange FL. Unlike the module on the client side, this module\n    has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server\n    model after aggregation, perhaps based on validation statistics retrieved on the client side by running a\n    federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the\n    state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer\n    responsible for saving the state after each fit and eval round of FL.\n\n    Args:\n        model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n            to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n            Recall that servers only have parameters rather than torch models. So we need to know where to route\n            these parameters to allow for real models to be saved. Defaults to None.\n        model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n        state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n            will be used to preserve FL training state to facilitate restarting training if interrupted.\n            Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n    \"\"\"\n    if model is not None:\n        parameter_exchanger = FullParameterExchangerWithPacking(ParameterPackerWithLayerNames())\n    else:\n        parameter_exchanger = None\n    super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.SparseCooServerCheckpointAndStateModule","title":"<code>SparseCooServerCheckpointAndStateModule</code>","text":"<p>               Bases: <code>PackingServerCheckpointAndAndStateModule</code></p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>class SparseCooServerCheckpointAndStateModule(PackingServerCheckpointAndAndStateModule):\n    def __init__(\n        self,\n        model: nn.Module | None = None,\n        model_checkpointers: ModelCheckpointers = None,\n        state_checkpointer: ServerStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to handle FL flows with parameters encoded in a sparse COO format being passed to the\n        server as the model weights. This is used for adaptive parameter-wise exchange (i.e. unstructured subsets of\n        parameters) . Unlike the module on the client side, this module has no concept of pre- or post-aggregation\n        checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on\n        validation statistics retrieved on the client side by running a federated evaluation step. Multiple model\n        checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process\n        to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit\n        and eval round of FL.\n\n        Args:\n            model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n                to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n                Recall that servers only have parameters rather than torch models. So we need to know where to route\n                these parameters to allow for real models to be saved. Defaults to None.\n            model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n            state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n                will be used to preserve FL training state to facilitate restarting training if interrupted.\n                Generally, this checkpointer will save much more than just the model being trained.\n                Defaults to None.\n        \"\"\"\n        if model is not None:\n            parameter_exchanger = FullParameterExchangerWithPacking(SparseCooParameterPacker())\n        else:\n            parameter_exchanger = None\n        super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.SparseCooServerCheckpointAndStateModule.__init__","title":"<code>__init__(model=None, model_checkpointers=None, state_checkpointer=None)</code>","text":"<p>This module is meant to handle FL flows with parameters encoded in a sparse COO format being passed to the server as the model weights. This is used for adaptive parameter-wise exchange (i.e. unstructured subsets of parameters) . Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on validation statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval round of FL.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model architecture to be saved. The module will use this architecture to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger. Recall that servers only have parameters rather than torch models. So we need to know where to route these parameters to allow for real models to be saved. Defaults to None.</p> <code>None</code> <code>model_checkpointers</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>ServerStateCheckpointer | None</code> <p>If defined, this checkpointer will be used to preserve FL training state to facilitate restarting training if interrupted. Generally, this checkpointer will save much more than just the model being trained. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module | None = None,\n    model_checkpointers: ModelCheckpointers = None,\n    state_checkpointer: ServerStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to handle FL flows with parameters encoded in a sparse COO format being passed to the\n    server as the model weights. This is used for adaptive parameter-wise exchange (i.e. unstructured subsets of\n    parameters) . Unlike the module on the client side, this module has no concept of pre- or post-aggregation\n    checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on\n    validation statistics retrieved on the client side by running a federated evaluation step. Multiple model\n    checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process\n    to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit\n    and eval round of FL.\n\n    Args:\n        model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n            to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n            Recall that servers only have parameters rather than torch models. So we need to know where to route\n            these parameters to allow for real models to be saved. Defaults to None.\n        model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n        state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n            will be used to preserve FL training state to facilitate restarting training if interrupted.\n            Generally, this checkpointer will save much more than just the model being trained.\n            Defaults to None.\n    \"\"\"\n    if model is not None:\n        parameter_exchanger = FullParameterExchangerWithPacking(SparseCooParameterPacker())\n    else:\n        parameter_exchanger = None\n    super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.OpacusServerCheckpointAndStateModule","title":"<code>OpacusServerCheckpointAndStateModule</code>","text":"<p>               Bases: <code>BaseServerCheckpointAndStateModule</code></p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>class OpacusServerCheckpointAndStateModule(BaseServerCheckpointAndStateModule):\n    def __init__(\n        self,\n        model: nn.Module | None = None,\n        parameter_exchanger: ExchangerType | None = None,\n        model_checkpointers: ModelCheckpointers = None,\n        state_checkpointer: ServerStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to handle FL flows with Opacus models where special treatment by the checkpointers is\n        required. This module simply ensures the checkpointers are of the proper type before proceeding.\n        Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing.\n        It only considers checkpointing the global server model after aggregation, perhaps based on validation\n        statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers\n        may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with\n        FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval\n        round of FL.\n\n        Args:\n            model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n                to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n                Recall that servers only have parameters rather than torch models. So we need to know where to route\n                these parameters to allow for real models to be saved. Defaults to None.\n            parameter_exchanger (FullParameterExchangerWithPacking | None, optional): This will facilitate routing the\n                server parameters into the right components of the provided model architecture. Note that this\n                exchanger and the model must match the one used for training and exchange with the servers to ensure\n                parameters go to the right places. Defaults to None.\n            model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n            state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n                will be used to preserve FL training state to facilitate restarting training if interrupted.\n                Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n        \"\"\"\n        super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n        self._ensure_checkpointers_are_of_opacus_type()\n\n    def _ensure_checkpointers_are_of_opacus_type(self) -&gt; None:\n        \"\"\"Helper function to ensure that the provided checkpointers are explicitly compatible with Opacus.\"\"\"\n        if self.model_checkpointers is not None:\n            for checkpointer in self.model_checkpointers:\n                assert isinstance(checkpointer, OpacusCheckpointer), (\n                    \"Provided checkpointers must have base class OpacusCheckpointer\"\n                )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.OpacusServerCheckpointAndStateModule.__init__","title":"<code>__init__(model=None, parameter_exchanger=None, model_checkpointers=None, state_checkpointer=None)</code>","text":"<p>This module is meant to handle FL flows with Opacus models where special treatment by the checkpointers is required. This module simply ensures the checkpointers are of the proper type before proceeding. Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on validation statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval round of FL.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model architecture to be saved. The module will use this architecture to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger. Recall that servers only have parameters rather than torch models. So we need to know where to route these parameters to allow for real models to be saved. Defaults to None.</p> <code>None</code> <code>parameter_exchanger</code> <code>FullParameterExchangerWithPacking | None</code> <p>This will facilitate routing the server parameters into the right components of the provided model architecture. Note that this exchanger and the model must match the one used for training and exchange with the servers to ensure parameters go to the right places. Defaults to None.</p> <code>None</code> <code>model_checkpointers</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>ServerStateCheckpointer | None</code> <p>If defined, this checkpointer will be used to preserve FL training state to facilitate restarting training if interrupted. Generally, this checkpointer will save much more than just the model being trained. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module | None = None,\n    parameter_exchanger: ExchangerType | None = None,\n    model_checkpointers: ModelCheckpointers = None,\n    state_checkpointer: ServerStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to handle FL flows with Opacus models where special treatment by the checkpointers is\n    required. This module simply ensures the checkpointers are of the proper type before proceeding.\n    Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing.\n    It only considers checkpointing the global server model after aggregation, perhaps based on validation\n    statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers\n    may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with\n    FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval\n    round of FL.\n\n    Args:\n        model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n            to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n            Recall that servers only have parameters rather than torch models. So we need to know where to route\n            these parameters to allow for real models to be saved. Defaults to None.\n        parameter_exchanger (FullParameterExchangerWithPacking | None, optional): This will facilitate routing the\n            server parameters into the right components of the provided model architecture. Note that this\n            exchanger and the model must match the one used for training and exchange with the servers to ensure\n            parameters go to the right places. Defaults to None.\n        model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n        state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n            will be used to preserve FL training state to facilitate restarting training if interrupted.\n            Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n    \"\"\"\n    super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n    self._ensure_checkpointers_are_of_opacus_type()\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.NnUnetServerCheckpointAndStateModule","title":"<code>NnUnetServerCheckpointAndStateModule</code>","text":"<p>               Bases: <code>BaseServerCheckpointAndStateModule</code></p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>class NnUnetServerCheckpointAndStateModule(BaseServerCheckpointAndStateModule):\n    def __init__(\n        self,\n        model: nn.Module | None = None,\n        parameter_exchanger: ExchangerType | None = None,\n        model_checkpointers: ModelCheckpointers = None,\n        state_checkpointer: NnUnetServerStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to be used with the ``NnUnetServer`` class to handle model and state checkpointing on the\n        server-side of an FL process. Unlike the module on the client side, this module has no concept of pre- or\n        post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation,\n        perhaps based on validation statistics retrieved on the client side by running a federated evaluation step.\n        Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire\n        server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving\n        the state after each fit and eval round of FL.\n\n        This implementation differs from the base class in the federated NnUnet only initializes its model after an\n        initial communication phase with the clients. As such, the model is not necessarily available upon\n        initialization, but may be set later.\n\n        Args:\n            model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n                to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n                Recall that servers only have parameters rather than torch models. So we need to know where to route\n                these parameters to allow for real models to be saved.\n\n                **NOTE**: For NnUnet, this need not be set upon creation, as the model architecture may only be known\n                later\n\n                Defaults to None.\n\n            parameter_exchanger (FullParameterExchangerWithPacking | None, optional): This will facilitate routing the\n                server parameters into the right components of the provided model architecture. Note that this\n                exchanger and the model must match the one used for training and exchange with the servers to ensure\n                parameters go to the right places. Defaults to None.\n            model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n            state_checkpointer (NnUnetServerStateCheckpointer | None, optional): If defined, this checkpointer\n                will be used to preserve FL training state to facilitate restarting training if interrupted.\n                Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n        \"\"\"\n        super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n\n    def _validate_model_checkpointer_components(self) -&gt; None:\n        # NOTE: We only check if the parameter exchanger is present. Model may be set later.\n        assert self.parameter_exchanger is not None, (\n            \"Checkpointer(s) is (are) defined but no parameter_exchanger is defined to hydrate. The functionality of \"\n            \"this class can be overridden in a child class if checkpointing without a parameter exchanger is \"\n            \"possible and desired\"\n        )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.NnUnetServerCheckpointAndStateModule.__init__","title":"<code>__init__(model=None, parameter_exchanger=None, model_checkpointers=None, state_checkpointer=None)</code>","text":"<p>This module is meant to be used with the <code>NnUnetServer</code> class to handle model and state checkpointing on the server-side of an FL process. Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on validation statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval round of FL.</p> <p>This implementation differs from the base class in the federated NnUnet only initializes its model after an initial communication phase with the clients. As such, the model is not necessarily available upon initialization, but may be set later.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model architecture to be saved. The module will use this architecture to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger. Recall that servers only have parameters rather than torch models. So we need to know where to route these parameters to allow for real models to be saved.</p> <p>NOTE: For NnUnet, this need not be set upon creation, as the model architecture may only be known later</p> <p>Defaults to None.</p> <code>None</code> <code>parameter_exchanger</code> <code>FullParameterExchangerWithPacking | None</code> <p>This will facilitate routing the server parameters into the right components of the provided model architecture. Note that this exchanger and the model must match the one used for training and exchange with the servers to ensure parameters go to the right places. Defaults to None.</p> <code>None</code> <code>model_checkpointers</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>NnUnetServerStateCheckpointer | None</code> <p>If defined, this checkpointer will be used to preserve FL training state to facilitate restarting training if interrupted. Generally, this checkpointer will save much more than just the model being trained. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module | None = None,\n    parameter_exchanger: ExchangerType | None = None,\n    model_checkpointers: ModelCheckpointers = None,\n    state_checkpointer: NnUnetServerStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to be used with the ``NnUnetServer`` class to handle model and state checkpointing on the\n    server-side of an FL process. Unlike the module on the client side, this module has no concept of pre- or\n    post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation,\n    perhaps based on validation statistics retrieved on the client side by running a federated evaluation step.\n    Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire\n    server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving\n    the state after each fit and eval round of FL.\n\n    This implementation differs from the base class in the federated NnUnet only initializes its model after an\n    initial communication phase with the clients. As such, the model is not necessarily available upon\n    initialization, but may be set later.\n\n    Args:\n        model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n            to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n            Recall that servers only have parameters rather than torch models. So we need to know where to route\n            these parameters to allow for real models to be saved.\n\n            **NOTE**: For NnUnet, this need not be set upon creation, as the model architecture may only be known\n            later\n\n            Defaults to None.\n\n        parameter_exchanger (FullParameterExchangerWithPacking | None, optional): This will facilitate routing the\n            server parameters into the right components of the provided model architecture. Note that this\n            exchanger and the model must match the one used for training and exchange with the servers to ensure\n            parameters go to the right places. Defaults to None.\n        model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n        state_checkpointer (NnUnetServerStateCheckpointer | None, optional): If defined, this checkpointer\n            will be used to preserve FL training state to facilitate restarting training if interrupted.\n            Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n    \"\"\"\n    super().__init__(model, parameter_exchanger, model_checkpointers, state_checkpointer)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.DpScaffoldServerCheckpointAndStateModule","title":"<code>DpScaffoldServerCheckpointAndStateModule</code>","text":"<p>               Bases: <code>ScaffoldServerCheckpointAndStateModule</code></p> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>class DpScaffoldServerCheckpointAndStateModule(ScaffoldServerCheckpointAndStateModule):\n    def __init__(\n        self,\n        model: nn.Module | None = None,\n        model_checkpointers: ModelCheckpointers = None,\n        state_checkpointer: ServerStateCheckpointer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This module is meant to handle DP SCAFFOLD model and state checkpointing on the server-side of an FL process.\n        Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing.\n        It only considers checkpointing the global server model after aggregation, perhaps based on validation\n        statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers\n        may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with\n        FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval\n        round of FL.\n\n        Args:\n            model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n                to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n                Recall that servers only have parameters rather than torch models. So we need to know where to route\n                these parameters to allow for real models to be saved. Defaults to None.\n            model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n                checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n            state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n                will be used to preserve FL training state to facilitate restarting training if interrupted.\n                Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n        \"\"\"\n        super().__init__(model, model_checkpointers, state_checkpointer)\n        self._ensure_checkpointers_are_of_opacus_type()\n\n    def _ensure_checkpointers_are_of_opacus_type(self) -&gt; None:\n        \"\"\"Helper function to ensure that the provided checkpointers are explicitly compatible with Opacus.\"\"\"\n        if self.model_checkpointers is not None:\n            for checkpointer in self.model_checkpointers:\n                assert isinstance(checkpointer, OpacusCheckpointer), (\n                    \"Provided checkpointers must have base class OpacusCheckpointer\"\n                )\n</code></pre>"},{"location":"api/#fl4health.checkpointing.server_module.DpScaffoldServerCheckpointAndStateModule.__init__","title":"<code>__init__(model=None, model_checkpointers=None, state_checkpointer=None)</code>","text":"<p>This module is meant to handle DP SCAFFOLD model and state checkpointing on the server-side of an FL process. Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing. It only considers checkpointing the global server model after aggregation, perhaps based on validation statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval round of FL.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module | None</code> <p>Model architecture to be saved. The module will use this architecture to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger. Recall that servers only have parameters rather than torch models. So we need to know where to route these parameters to allow for real models to be saved. Defaults to None.</p> <code>None</code> <code>model_checkpointers</code> <code>ModelCheckpointers</code> <p>If defined, this checkpointer (or sequence of checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.</p> <code>None</code> <code>state_checkpointer</code> <code>ServerStateCheckpointer | None</code> <p>If defined, this checkpointer will be used to preserve FL training state to facilitate restarting training if interrupted. Generally, this checkpointer will save much more than just the model being trained. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/server_module.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module | None = None,\n    model_checkpointers: ModelCheckpointers = None,\n    state_checkpointer: ServerStateCheckpointer | None = None,\n) -&gt; None:\n    \"\"\"\n    This module is meant to handle DP SCAFFOLD model and state checkpointing on the server-side of an FL process.\n    Unlike the module on the client side, this module has no concept of pre- or post-aggregation checkpointing.\n    It only considers checkpointing the global server model after aggregation, perhaps based on validation\n    statistics retrieved on the client side by running a federated evaluation step. Multiple model checkpointers\n    may be used. For state checkpointing, which saves the state of the entire server-side FL process to help with\n    FL restarts, we allow only a single checkpointer responsible for saving the state after each fit and eval\n    round of FL.\n\n    Args:\n        model (nn.Module | None, optional): Model architecture to be saved. The module will use this architecture\n            to hold the server parameters and facilitate checkpointing with the help of the parameter exchanger.\n            Recall that servers only have parameters rather than torch models. So we need to know where to route\n            these parameters to allow for real models to be saved. Defaults to None.\n        model_checkpointers (ModelCheckpointers, optional): If defined, this checkpointer (or sequence of\n            checkpointers) is used to checkpoint models based on their defined scoring function. Defaults to None.\n        state_checkpointer (ServerStateCheckpointer | None, optional): If defined, this checkpointer\n            will be used to preserve FL training state to facilitate restarting training if interrupted.\n            Generally, this checkpointer will save much more than just the model being trained. Defaults to None.\n    \"\"\"\n    super().__init__(model, model_checkpointers, state_checkpointer)\n    self._ensure_checkpointers_are_of_opacus_type()\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer","title":"<code>state_checkpointer</code>","text":""},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer","title":"<code>StateCheckpointer</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>class StateCheckpointer(ABC):\n    def __init__(\n        self,\n        checkpoint_dir: Path,\n        checkpoint_name: str | None,\n        snapshot_attrs: dict[str, tuple[AbstractSnapshotter, Any]],\n    ) -&gt; None:\n        \"\"\"\n        Class for saving and loading the state of the client or server attributes. Attributes are stored in a\n        dictionary to assist saving and are loaded in a dictionary. Checkpointing can be done after client or\n        server round to facilitate restarting federated training if interrupted, or during the client's training\n        loop to facilitate early stopping.\n\n        Server and client state checkpointers will save to disk in the provided directory. A default name for the\n        state checkpoint will be derived if checkpoint name remains none at the time of saving.\n\n        Args:\n            checkpoint_dir (Path): Directory to which checkpoints are saved. This can be modified later with\n                ``set_checkpoint_path``\n            checkpoint_name (str): Name of the checkpoint to be saved. If None at time of state saving, a default name\n                will be given to the checkpoint. This can be changed later with ``set_checkpoint_path``\n            snapshot_attrs (dict[str, tuple[AbstractSnapshotter, Any]]): Attributes that we need to save in order\n                to allow for restarting of training.\n        \"\"\"\n        self.checkpoint_dir = checkpoint_dir\n        self.checkpoint_name = checkpoint_name\n        self.checkpoint_path: str | None = None\n        if self.checkpoint_name is not None:\n            self.checkpoint_path = os.path.join(self.checkpoint_dir, self.checkpoint_name)\n\n        self.snapshot_attrs = snapshot_attrs\n        self.snapshot_ckpt: dict[str, Any] = {}\n\n    def set_checkpoint_path(self, checkpoint_dir: Path, checkpoint_name: str) -&gt; None:\n        \"\"\"\n        Set or update the checkpoint path based on the provided checkpoint name and directory.\n\n        Args:\n            checkpoint_dir (Path): The directory where the checkpoint will be saved.\n            checkpoint_name (str): The name of the checkpoint file.\n        \"\"\"\n        self.checkpoint_dir = checkpoint_dir\n        self.checkpoint_name = checkpoint_name\n        self.checkpoint_path = os.path.join(self.checkpoint_dir, self.checkpoint_name)\n\n    def save_checkpoint(self, checkpoint_dict: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Save ``checkpoint_dict`` to checkpoint path defined based on checkpointer dir and checkpoint name.\n\n        Args:\n            checkpoint_dict (dict[str, Any]): A dictionary with string keys and values of type Any representing the\n                state to be saved.\n\n        Raises:\n            e: Will throw an error if there is an issue saving the model. ``Torch.save`` seems to swallow errors in\n                this context, so we explicitly surface the error with a try/except.\n        \"\"\"\n        assert self.checkpoint_path is not None, \"Checkpoint path is not set but save_checkpoint has been called.\"\n        try:\n            log(INFO, f\"Saving the state as {self.checkpoint_path}\")\n            torch.save(checkpoint_dict, self.checkpoint_path)\n        except Exception as e:\n            log(ERROR, f\"Encountered the following error while saving the checkpoint: {e}\")\n            raise e\n\n    def load_checkpoint(self) -&gt; dict[str, Any]:\n        \"\"\"\n        Load and return the checkpoint stored in ``checkpoint_dir`` under the  ``checkpoint_name`` if it exists. If\n        it does not exist, an assertion error will be thrown.\n\n        Returns:\n            (dict[str, Any]): A dictionary representing the checkpointed state, as loaded by ``torch.load``.\n        \"\"\"\n        assert self.checkpoint_path is not None, \"Checkpoint path is not set but load_checkpoint has been called.\"\n        assert self.checkpoint_exists(), f\"Could not verify existence of checkpoint file at {self.checkpoint_path}\"\n        log(INFO, f\"Loading state from checkpoint at {self.checkpoint_path}\")\n\n        return torch.load(self.checkpoint_path, weights_only=False)\n\n    def checkpoint_exists(self) -&gt; bool:\n        \"\"\"\n        Check if a checkpoint exists at the ``checkpoint_path`` constructed as ``checkpoint_dir`` +\n        ``checkpoint_name``.\n\n        Returns:\n            (bool): True if checkpoint exists, otherwise false.\n        \"\"\"\n        assert self.checkpoint_path is not None, \"A checkpoint_path should be set but is no\"\n        return os.path.exists(self.checkpoint_path)\n\n    def add_to_snapshot_attr(self, name: str, snapshotter: AbstractSnapshotter, input_type: type[T]) -&gt; None:\n        \"\"\"\n        Add new attribute to the default ``snapshot_attrs`` dictionary. For this, we need a snapshotter that\n        provides functionality for loading and saving the state of the attribute based on the type of the attribute.\n\n        Args:\n            name (str): Name of the attribute to be added.\n            snapshotter (AbstractSnapshotter): Snapshotter object to be used for saving and loading the attribute.\n            input_type (type[T]): Expected type of the attribute.\n        \"\"\"\n        self.snapshot_attrs.update({name: (snapshotter, input_type)})\n\n    def delete_from_snapshot_attr(self, name: str) -&gt; None:\n        \"\"\"\n        Delete the attribute from the default ``snapshot_attrs`` dictionary. This is useful for removing attributes\n        that are no longer needed or to avoid saving/loading them.\n\n        Args:\n            name (str): Name of the attribute to be removed from the ``snapshot_attrs`` dictionary.\n        \"\"\"\n        del self.snapshot_attrs[name]\n\n    def save_state(self) -&gt; None:\n        \"\"\"\n        Create a snapshot of the state as defined in ``self.snapshot_attrs``.\n        It is saved at ``self.checkpoint_path``.\n        \"\"\"\n        for attr_name, (snapshotter, expected_type) in self.snapshot_attrs.items():\n            self.snapshot_ckpt.update(self._save_snapshot(snapshotter, attr_name, expected_type))\n\n        assert self.checkpoint_path is not None, \"Attempting to save state but checkpoint_path is None\"\n        log(INFO, f\"Saving the state to checkpoint at {self.checkpoint_path}\")\n        self.save_checkpoint(self.snapshot_ckpt)\n        # Release snapshot memory after disk persistence\n        self.snapshot_ckpt.clear()\n\n    def load_state(self, attributes: list[str] | None = None) -&gt; None:\n        \"\"\"\n        Load checkpointed state dictionary from the checkpoint, potentially restricting the attributes to load.\n\n        Args:\n            attributes (list[str] | None): List of attributes to load from the checkpoint. If None, all attributes\n                specified in ``snapshot_attrs`` are loaded. Defaults to None.\n        \"\"\"\n        assert self.checkpoint_exists(), (\n            f\"No state checkpoint to load. Checkpoint at {self.checkpoint_path} does not exist\"\n        )\n\n        if attributes is None:\n            attributes = list(self.snapshot_attrs.keys())\n            if not attributes:\n                log(WARNING, \"self.snapshot_attrs is empty, which may be undesired behavior.\")\n\n        # If the checkpoint exists, load it into snapshot_ckpt\n        self.snapshot_ckpt = self.load_checkpoint()\n\n        # Load components into target object\n        for attr in attributes:\n            snapshotter, expected_type = self.snapshot_attrs[attr]\n            self._load_snapshot(snapshotter, attr, expected_type)\n        log(INFO, f\"Loaded the checkpointed state from {self.checkpoint_path}\")\n\n        # Release snapshot memory after loading\n        self.snapshot_ckpt.clear()\n\n    @abstractmethod\n    def get_attribute(self, name: str) -&gt; Any:\n        \"\"\"\n        Get the attribute from the client or server.\n\n        Args:\n            name (str): Name of the attribute.\n\n        Returns:\n            (Any): The attribute value.\n        \"\"\"\n        raise NotImplementedError(\"get_attribute must be implemented by inheriting classes\")\n\n    @abstractmethod\n    def set_attribute(self, name: str, value: Any) -&gt; None:\n        \"\"\"\n        Set the attribute on the client or server.\n\n        Args:\n            name (str): Name of the attribute.\n            value (Any): Value to set for the attribute.\n        \"\"\"\n        raise NotImplementedError(\"set_attribute must be implemented by inheriting classes\")\n\n    def _dict_wrap_attr(self, name: str, expected_type: type[T]) -&gt; dict[str, T]:\n        \"\"\"\n        Wrap the attribute in a dictionary if it is not already a dictionary.\n\n        Args:\n            name (str): Name of the attribute.\n            expected_type (type[T]): Expected type of the attribute.\n\n        Returns:\n            (dict[str, T]): Wrapped attribute as a dictionary.\n        \"\"\"\n        attribute = self.get_attribute(name)\n        if isinstance(attribute, expected_type):\n            return {\"None\": attribute}\n        if isinstance(attribute, dict):\n            for key, value in attribute.items():\n                if not isinstance(value, expected_type):\n                    raise ValueError(f\"Incompatible type of attribute {type(attribute)} for key {key}\")\n            return attribute\n        raise ValueError(f\"Incompatible type of attribute {type(attribute)}, expected {expected_type}\")\n\n    def _save_snapshot(self, snapshotter: AbstractSnapshotter, name: str, expected_type: type[T]) -&gt; dict[str, Any]:\n        \"\"\"\n        Save the state of the attribute using the snapshotter's save_attribute functionality.\n\n        Args:\n            snapshotter (AbstractSnapshotter): Snapshotter object to save the attribute.\n            name (str): Name of the attribute.\n            expected_type (type[T]): Expected type of the attribute.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the attribute.\n        \"\"\"\n        attribute = self._dict_wrap_attr(name, expected_type)\n        return {name: snapshotter.save_attribute(attribute)}\n\n    def _load_snapshot(self, snapshotter: AbstractSnapshotter, name: str, expected_type: type[T]) -&gt; None:\n        \"\"\"\n        Load the state of the attribute using the snapshotter's ``load_attribute`` functionality.\n\n        **NOTE**: This function assumes that ``snapshot_ckpt`` has been populated with the right data loaded from disk.\n\n        Args:\n            snapshotter (dict[str, Any]): Snapshotter object to return the state of the attribute.\n            name (str): Name of the attribute.\n            expected_type (type[T]): Expected type of the attribute.\n        \"\"\"\n        attribute = self._dict_wrap_attr(name, expected_type)\n        snapshotter.load_attribute(self.snapshot_ckpt[name], attribute)\n        if list(attribute.keys()) == [\"None\"]:\n            self.set_attribute(name, attribute[\"None\"])\n        else:\n            self.set_attribute(name, attribute)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name, snapshot_attrs)</code>","text":"<p>Class for saving and loading the state of the client or server attributes. Attributes are stored in a dictionary to assist saving and are loaded in a dictionary. Checkpointing can be done after client or server round to facilitate restarting federated training if interrupted, or during the client's training loop to facilitate early stopping.</p> <p>Server and client state checkpointers will save to disk in the provided directory. A default name for the state checkpoint will be derived if checkpoint name remains none at the time of saving.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>Path</code> <p>Directory to which checkpoints are saved. This can be modified later with <code>set_checkpoint_path</code></p> required <code>checkpoint_name</code> <code>str</code> <p>Name of the checkpoint to be saved. If None at time of state saving, a default name will be given to the checkpoint. This can be changed later with <code>set_checkpoint_path</code></p> required <code>snapshot_attrs</code> <code>dict[str, tuple[AbstractSnapshotter, Any]]</code> <p>Attributes that we need to save in order to allow for restarting of training.</p> required Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: Path,\n    checkpoint_name: str | None,\n    snapshot_attrs: dict[str, tuple[AbstractSnapshotter, Any]],\n) -&gt; None:\n    \"\"\"\n    Class for saving and loading the state of the client or server attributes. Attributes are stored in a\n    dictionary to assist saving and are loaded in a dictionary. Checkpointing can be done after client or\n    server round to facilitate restarting federated training if interrupted, or during the client's training\n    loop to facilitate early stopping.\n\n    Server and client state checkpointers will save to disk in the provided directory. A default name for the\n    state checkpoint will be derived if checkpoint name remains none at the time of saving.\n\n    Args:\n        checkpoint_dir (Path): Directory to which checkpoints are saved. This can be modified later with\n            ``set_checkpoint_path``\n        checkpoint_name (str): Name of the checkpoint to be saved. If None at time of state saving, a default name\n            will be given to the checkpoint. This can be changed later with ``set_checkpoint_path``\n        snapshot_attrs (dict[str, tuple[AbstractSnapshotter, Any]]): Attributes that we need to save in order\n            to allow for restarting of training.\n    \"\"\"\n    self.checkpoint_dir = checkpoint_dir\n    self.checkpoint_name = checkpoint_name\n    self.checkpoint_path: str | None = None\n    if self.checkpoint_name is not None:\n        self.checkpoint_path = os.path.join(self.checkpoint_dir, self.checkpoint_name)\n\n    self.snapshot_attrs = snapshot_attrs\n    self.snapshot_ckpt: dict[str, Any] = {}\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.set_checkpoint_path","title":"<code>set_checkpoint_path(checkpoint_dir, checkpoint_name)</code>","text":"<p>Set or update the checkpoint path based on the provided checkpoint name and directory.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>Path</code> <p>The directory where the checkpoint will be saved.</p> required <code>checkpoint_name</code> <code>str</code> <p>The name of the checkpoint file.</p> required Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def set_checkpoint_path(self, checkpoint_dir: Path, checkpoint_name: str) -&gt; None:\n    \"\"\"\n    Set or update the checkpoint path based on the provided checkpoint name and directory.\n\n    Args:\n        checkpoint_dir (Path): The directory where the checkpoint will be saved.\n        checkpoint_name (str): The name of the checkpoint file.\n    \"\"\"\n    self.checkpoint_dir = checkpoint_dir\n    self.checkpoint_name = checkpoint_name\n    self.checkpoint_path = os.path.join(self.checkpoint_dir, self.checkpoint_name)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.save_checkpoint","title":"<code>save_checkpoint(checkpoint_dict)</code>","text":"<p>Save <code>checkpoint_dict</code> to checkpoint path defined based on checkpointer dir and checkpoint name.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dict</code> <code>dict[str, Any]</code> <p>A dictionary with string keys and values of type Any representing the state to be saved.</p> required <p>Raises:</p> Type Description <code>e</code> <p>Will throw an error if there is an issue saving the model. <code>Torch.save</code> seems to swallow errors in this context, so we explicitly surface the error with a try/except.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def save_checkpoint(self, checkpoint_dict: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Save ``checkpoint_dict`` to checkpoint path defined based on checkpointer dir and checkpoint name.\n\n    Args:\n        checkpoint_dict (dict[str, Any]): A dictionary with string keys and values of type Any representing the\n            state to be saved.\n\n    Raises:\n        e: Will throw an error if there is an issue saving the model. ``Torch.save`` seems to swallow errors in\n            this context, so we explicitly surface the error with a try/except.\n    \"\"\"\n    assert self.checkpoint_path is not None, \"Checkpoint path is not set but save_checkpoint has been called.\"\n    try:\n        log(INFO, f\"Saving the state as {self.checkpoint_path}\")\n        torch.save(checkpoint_dict, self.checkpoint_path)\n    except Exception as e:\n        log(ERROR, f\"Encountered the following error while saving the checkpoint: {e}\")\n        raise e\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.load_checkpoint","title":"<code>load_checkpoint()</code>","text":"<p>Load and return the checkpoint stored in <code>checkpoint_dir</code> under the  <code>checkpoint_name</code> if it exists. If it does not exist, an assertion error will be thrown.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary representing the checkpointed state, as loaded by <code>torch.load</code>.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def load_checkpoint(self) -&gt; dict[str, Any]:\n    \"\"\"\n    Load and return the checkpoint stored in ``checkpoint_dir`` under the  ``checkpoint_name`` if it exists. If\n    it does not exist, an assertion error will be thrown.\n\n    Returns:\n        (dict[str, Any]): A dictionary representing the checkpointed state, as loaded by ``torch.load``.\n    \"\"\"\n    assert self.checkpoint_path is not None, \"Checkpoint path is not set but load_checkpoint has been called.\"\n    assert self.checkpoint_exists(), f\"Could not verify existence of checkpoint file at {self.checkpoint_path}\"\n    log(INFO, f\"Loading state from checkpoint at {self.checkpoint_path}\")\n\n    return torch.load(self.checkpoint_path, weights_only=False)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.checkpoint_exists","title":"<code>checkpoint_exists()</code>","text":"<p>Check if a checkpoint exists at the <code>checkpoint_path</code> constructed as <code>checkpoint_dir</code> + <code>checkpoint_name</code>.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if checkpoint exists, otherwise false.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def checkpoint_exists(self) -&gt; bool:\n    \"\"\"\n    Check if a checkpoint exists at the ``checkpoint_path`` constructed as ``checkpoint_dir`` +\n    ``checkpoint_name``.\n\n    Returns:\n        (bool): True if checkpoint exists, otherwise false.\n    \"\"\"\n    assert self.checkpoint_path is not None, \"A checkpoint_path should be set but is no\"\n    return os.path.exists(self.checkpoint_path)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.add_to_snapshot_attr","title":"<code>add_to_snapshot_attr(name, snapshotter, input_type)</code>","text":"<p>Add new attribute to the default <code>snapshot_attrs</code> dictionary. For this, we need a snapshotter that provides functionality for loading and saving the state of the attribute based on the type of the attribute.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the attribute to be added.</p> required <code>snapshotter</code> <code>AbstractSnapshotter</code> <p>Snapshotter object to be used for saving and loading the attribute.</p> required <code>input_type</code> <code>type[T]</code> <p>Expected type of the attribute.</p> required Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def add_to_snapshot_attr(self, name: str, snapshotter: AbstractSnapshotter, input_type: type[T]) -&gt; None:\n    \"\"\"\n    Add new attribute to the default ``snapshot_attrs`` dictionary. For this, we need a snapshotter that\n    provides functionality for loading and saving the state of the attribute based on the type of the attribute.\n\n    Args:\n        name (str): Name of the attribute to be added.\n        snapshotter (AbstractSnapshotter): Snapshotter object to be used for saving and loading the attribute.\n        input_type (type[T]): Expected type of the attribute.\n    \"\"\"\n    self.snapshot_attrs.update({name: (snapshotter, input_type)})\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.delete_from_snapshot_attr","title":"<code>delete_from_snapshot_attr(name)</code>","text":"<p>Delete the attribute from the default <code>snapshot_attrs</code> dictionary. This is useful for removing attributes that are no longer needed or to avoid saving/loading them.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the attribute to be removed from the <code>snapshot_attrs</code> dictionary.</p> required Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def delete_from_snapshot_attr(self, name: str) -&gt; None:\n    \"\"\"\n    Delete the attribute from the default ``snapshot_attrs`` dictionary. This is useful for removing attributes\n    that are no longer needed or to avoid saving/loading them.\n\n    Args:\n        name (str): Name of the attribute to be removed from the ``snapshot_attrs`` dictionary.\n    \"\"\"\n    del self.snapshot_attrs[name]\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.save_state","title":"<code>save_state()</code>","text":"<p>Create a snapshot of the state as defined in <code>self.snapshot_attrs</code>. It is saved at <code>self.checkpoint_path</code>.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def save_state(self) -&gt; None:\n    \"\"\"\n    Create a snapshot of the state as defined in ``self.snapshot_attrs``.\n    It is saved at ``self.checkpoint_path``.\n    \"\"\"\n    for attr_name, (snapshotter, expected_type) in self.snapshot_attrs.items():\n        self.snapshot_ckpt.update(self._save_snapshot(snapshotter, attr_name, expected_type))\n\n    assert self.checkpoint_path is not None, \"Attempting to save state but checkpoint_path is None\"\n    log(INFO, f\"Saving the state to checkpoint at {self.checkpoint_path}\")\n    self.save_checkpoint(self.snapshot_ckpt)\n    # Release snapshot memory after disk persistence\n    self.snapshot_ckpt.clear()\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.load_state","title":"<code>load_state(attributes=None)</code>","text":"<p>Load checkpointed state dictionary from the checkpoint, potentially restricting the attributes to load.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <code>list[str] | None</code> <p>List of attributes to load from the checkpoint. If None, all attributes specified in <code>snapshot_attrs</code> are loaded. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def load_state(self, attributes: list[str] | None = None) -&gt; None:\n    \"\"\"\n    Load checkpointed state dictionary from the checkpoint, potentially restricting the attributes to load.\n\n    Args:\n        attributes (list[str] | None): List of attributes to load from the checkpoint. If None, all attributes\n            specified in ``snapshot_attrs`` are loaded. Defaults to None.\n    \"\"\"\n    assert self.checkpoint_exists(), (\n        f\"No state checkpoint to load. Checkpoint at {self.checkpoint_path} does not exist\"\n    )\n\n    if attributes is None:\n        attributes = list(self.snapshot_attrs.keys())\n        if not attributes:\n            log(WARNING, \"self.snapshot_attrs is empty, which may be undesired behavior.\")\n\n    # If the checkpoint exists, load it into snapshot_ckpt\n    self.snapshot_ckpt = self.load_checkpoint()\n\n    # Load components into target object\n    for attr in attributes:\n        snapshotter, expected_type = self.snapshot_attrs[attr]\n        self._load_snapshot(snapshotter, attr, expected_type)\n    log(INFO, f\"Loaded the checkpointed state from {self.checkpoint_path}\")\n\n    # Release snapshot memory after loading\n    self.snapshot_ckpt.clear()\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.get_attribute","title":"<code>get_attribute(name)</code>  <code>abstractmethod</code>","text":"<p>Get the attribute from the client or server.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the attribute.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The attribute value.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>@abstractmethod\ndef get_attribute(self, name: str) -&gt; Any:\n    \"\"\"\n    Get the attribute from the client or server.\n\n    Args:\n        name (str): Name of the attribute.\n\n    Returns:\n        (Any): The attribute value.\n    \"\"\"\n    raise NotImplementedError(\"get_attribute must be implemented by inheriting classes\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.StateCheckpointer.set_attribute","title":"<code>set_attribute(name, value)</code>  <code>abstractmethod</code>","text":"<p>Set the attribute on the client or server.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the attribute.</p> required <code>value</code> <code>Any</code> <p>Value to set for the attribute.</p> required Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>@abstractmethod\ndef set_attribute(self, name: str, value: Any) -&gt; None:\n    \"\"\"\n    Set the attribute on the client or server.\n\n    Args:\n        name (str): Name of the attribute.\n        value (Any): Value to set for the attribute.\n    \"\"\"\n    raise NotImplementedError(\"set_attribute must be implemented by inheriting classes\")\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ClientStateCheckpointer","title":"<code>ClientStateCheckpointer</code>","text":"<p>               Bases: <code>StateCheckpointer</code></p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>class ClientStateCheckpointer(StateCheckpointer):\n    def __init__(\n        self,\n        checkpoint_dir: Path,\n        checkpoint_name: str | None = None,\n        snapshot_attrs: dict[str, tuple[AbstractSnapshotter, Any]] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Class for saving and loading the state of a client's attributes as specified in ``snapshot_attrs``.\n\n        Args:\n            checkpoint_dir (Path): Directory to which checkpoints are saved. This can be modified later with\n                ``set_checkpoint_path``\n            checkpoint_name (str | None, optional): Name of the checkpoint to be saved. If None, but ``checkpoint_dir``\n                is set then a default ``checkpoint_name`` based on the underlying name of the client to be\n                checkpointed will be set of the form ``f\"client_{client.client_name}_state.pt\"``. This can be changed\n                later with ``set_checkpoint_path``. Defaults to None.\n            snapshot_attrs (dict[str, tuple[AbstractSnapshotter, Any]] | None, optional): Attributes that we need to\n                save in order to allow for restarting of training. If None, a sensible default set of attributes and\n                their associated snapshotters for an FL client are set. Defaults to None.\n        \"\"\"\n        # If snapshot_attrs is None, we set a sensible default set of attributes to be saved. These are a minimal\n        # set of attributes that can be used for per round checkpointing or early stopping.\n        # NOTE: These default attributes are useful for state checkpointing a BasicClient. More sophisticated\n        # clients may require more attributes to fully support training restarts and early stopping. For a server\n        # example, see NnUnetServerStateCheckpointer.\n        if snapshot_attrs is None:\n            snapshot_attrs = {\n                \"model\": (TorchModuleSnapshotter(), nn.Module),\n                \"optimizers\": (OptimizerSnapshotter(), Optimizer),\n                \"lr_schedulers\": (\n                    LRSchedulerSnapshotter(),\n                    LRScheduler,\n                ),\n                \"total_steps\": (SingletonSnapshotter(), int),\n                \"total_epochs\": (SingletonSnapshotter(), int),\n                \"reports_manager\": (\n                    SerializableObjectSnapshotter(),\n                    ReportsManager,\n                ),\n                \"train_loss_meter\": (\n                    SerializableObjectSnapshotter(),\n                    LossMeter,\n                ),\n                \"train_metric_manager\": (\n                    SerializableObjectSnapshotter(),\n                    MetricManager,\n                ),\n            }\n\n        super().__init__(checkpoint_dir, checkpoint_name, snapshot_attrs)\n        self.client: BasicClient | None = None\n\n    def maybe_set_default_checkpoint_name(self) -&gt; None:\n        \"\"\"\n        Potentially sets a default name for the checkpoint to be saved. If ``checkpoint_dir`` is set but\n        ``checkpoint_name`` is None then a default ``checkpoint_name`` based on the underlying name of the client to\n        be checkpointed will be set of the form ``f\"client_{self.client.client_name}_state.pt\"``.\n        \"\"\"\n        assert self.client is not None, \"Attempting to save client state but client is None\"\n        # Set the checkpoint name based on client's name if not already provided.\n        if self.checkpoint_name is None:\n            # If checkpoint_name is not provided, we set it based on the client name.\n            self.checkpoint_name = f\"client_{self.client.client_name}_state.pt\"\n            self.set_checkpoint_path(self.checkpoint_dir, self.checkpoint_name)\n\n    def save_client_state(self, client: BasicClient) -&gt; None:\n        \"\"\"\n        Save the state of the client that is provided.\n\n        Args:\n            client (BasicClient): Client object with state to be saved.\n        \"\"\"\n        # Store client for access in functions\n        self.client = client\n        # Potentially set a default checkpoint name\n        self.maybe_set_default_checkpoint_name()\n        # Saves everything in self.snapshot_attrs\n        self.save_state()\n        # Clear the client after being checkpointed.\n        self.client = None\n\n    def maybe_load_client_state(self, client: BasicClient, attributes: list[str] | None = None) -&gt; bool:\n        \"\"\"\n        Load the state into the client that is being provided.\n\n        Args:\n            client (BasicClient): Target client object into which state will be loaded\n            attributes (list[str] | None, optional): List of attributes to load from the checkpoint. If None, all\n                attributes specified in ``snapshot_attrs`` are loaded. Defaults to None.\n\n        Returns:\n            (bool): True if a checkpoint is successfully loaded. False otherwise.\n        \"\"\"\n        # Store client for access in functions\n        self.client = client\n        # Setting default name if one doesn't exist. If we're here and it doesn't exist yet, user is expecting\n        # a default name for loading anyway.\n        self.maybe_set_default_checkpoint_name()\n        if self.checkpoint_exists():\n            self.load_state(attributes)\n            log(INFO, f\"State checkpoint successfully loaded from: {self.checkpoint_path}\")\n            # Clear the client after we are done updating its attributes.\n            self.client = None\n            return True\n\n        log(INFO, f\"No state checkpoint found at: {self.checkpoint_path}\")\n        # Clear the client since no checkpoint exists.\n        self.client = None\n        return False\n\n    def get_attribute(self, name: str) -&gt; Any:\n        \"\"\"\n        Get the attribute from the client.\n\n        Args:\n            name (str): Name of the attribute.\n\n        Returns:\n            (Any): The attribute value.\n        \"\"\"\n        assert self.client is not None, \"Client is not set.\"\n        return getattr(self.client, name)\n\n    def set_attribute(self, name: str, value: Any) -&gt; None:\n        \"\"\"\n        Set the attribute on the client.\n\n        Args:\n            name (str): Name of the attribute.\n            value (Any): Value to set for the attribute.\n        \"\"\"\n        assert self.client is not None, \"Client is not set.\"\n        setattr(self.client, name, value)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ClientStateCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name=None, snapshot_attrs=None)</code>","text":"<p>Class for saving and loading the state of a client's attributes as specified in <code>snapshot_attrs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>Path</code> <p>Directory to which checkpoints are saved. This can be modified later with <code>set_checkpoint_path</code></p> required <code>checkpoint_name</code> <code>str | None</code> <p>Name of the checkpoint to be saved. If None, but <code>checkpoint_dir</code> is set then a default <code>checkpoint_name</code> based on the underlying name of the client to be checkpointed will be set of the form <code>f\"client_{client.client_name}_state.pt\"</code>. This can be changed later with <code>set_checkpoint_path</code>. Defaults to None.</p> <code>None</code> <code>snapshot_attrs</code> <code>dict[str, tuple[AbstractSnapshotter, Any]] | None</code> <p>Attributes that we need to save in order to allow for restarting of training. If None, a sensible default set of attributes and their associated snapshotters for an FL client are set. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: Path,\n    checkpoint_name: str | None = None,\n    snapshot_attrs: dict[str, tuple[AbstractSnapshotter, Any]] | None = None,\n) -&gt; None:\n    \"\"\"\n    Class for saving and loading the state of a client's attributes as specified in ``snapshot_attrs``.\n\n    Args:\n        checkpoint_dir (Path): Directory to which checkpoints are saved. This can be modified later with\n            ``set_checkpoint_path``\n        checkpoint_name (str | None, optional): Name of the checkpoint to be saved. If None, but ``checkpoint_dir``\n            is set then a default ``checkpoint_name`` based on the underlying name of the client to be\n            checkpointed will be set of the form ``f\"client_{client.client_name}_state.pt\"``. This can be changed\n            later with ``set_checkpoint_path``. Defaults to None.\n        snapshot_attrs (dict[str, tuple[AbstractSnapshotter, Any]] | None, optional): Attributes that we need to\n            save in order to allow for restarting of training. If None, a sensible default set of attributes and\n            their associated snapshotters for an FL client are set. Defaults to None.\n    \"\"\"\n    # If snapshot_attrs is None, we set a sensible default set of attributes to be saved. These are a minimal\n    # set of attributes that can be used for per round checkpointing or early stopping.\n    # NOTE: These default attributes are useful for state checkpointing a BasicClient. More sophisticated\n    # clients may require more attributes to fully support training restarts and early stopping. For a server\n    # example, see NnUnetServerStateCheckpointer.\n    if snapshot_attrs is None:\n        snapshot_attrs = {\n            \"model\": (TorchModuleSnapshotter(), nn.Module),\n            \"optimizers\": (OptimizerSnapshotter(), Optimizer),\n            \"lr_schedulers\": (\n                LRSchedulerSnapshotter(),\n                LRScheduler,\n            ),\n            \"total_steps\": (SingletonSnapshotter(), int),\n            \"total_epochs\": (SingletonSnapshotter(), int),\n            \"reports_manager\": (\n                SerializableObjectSnapshotter(),\n                ReportsManager,\n            ),\n            \"train_loss_meter\": (\n                SerializableObjectSnapshotter(),\n                LossMeter,\n            ),\n            \"train_metric_manager\": (\n                SerializableObjectSnapshotter(),\n                MetricManager,\n            ),\n        }\n\n    super().__init__(checkpoint_dir, checkpoint_name, snapshot_attrs)\n    self.client: BasicClient | None = None\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ClientStateCheckpointer.maybe_set_default_checkpoint_name","title":"<code>maybe_set_default_checkpoint_name()</code>","text":"<p>Potentially sets a default name for the checkpoint to be saved. If <code>checkpoint_dir</code> is set but <code>checkpoint_name</code> is None then a default <code>checkpoint_name</code> based on the underlying name of the client to be checkpointed will be set of the form <code>f\"client_{self.client.client_name}_state.pt\"</code>.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def maybe_set_default_checkpoint_name(self) -&gt; None:\n    \"\"\"\n    Potentially sets a default name for the checkpoint to be saved. If ``checkpoint_dir`` is set but\n    ``checkpoint_name`` is None then a default ``checkpoint_name`` based on the underlying name of the client to\n    be checkpointed will be set of the form ``f\"client_{self.client.client_name}_state.pt\"``.\n    \"\"\"\n    assert self.client is not None, \"Attempting to save client state but client is None\"\n    # Set the checkpoint name based on client's name if not already provided.\n    if self.checkpoint_name is None:\n        # If checkpoint_name is not provided, we set it based on the client name.\n        self.checkpoint_name = f\"client_{self.client.client_name}_state.pt\"\n        self.set_checkpoint_path(self.checkpoint_dir, self.checkpoint_name)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ClientStateCheckpointer.save_client_state","title":"<code>save_client_state(client)</code>","text":"<p>Save the state of the client that is provided.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>BasicClient</code> <p>Client object with state to be saved.</p> required Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def save_client_state(self, client: BasicClient) -&gt; None:\n    \"\"\"\n    Save the state of the client that is provided.\n\n    Args:\n        client (BasicClient): Client object with state to be saved.\n    \"\"\"\n    # Store client for access in functions\n    self.client = client\n    # Potentially set a default checkpoint name\n    self.maybe_set_default_checkpoint_name()\n    # Saves everything in self.snapshot_attrs\n    self.save_state()\n    # Clear the client after being checkpointed.\n    self.client = None\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ClientStateCheckpointer.maybe_load_client_state","title":"<code>maybe_load_client_state(client, attributes=None)</code>","text":"<p>Load the state into the client that is being provided.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>BasicClient</code> <p>Target client object into which state will be loaded</p> required <code>attributes</code> <code>list[str] | None</code> <p>List of attributes to load from the checkpoint. If None, all attributes specified in <code>snapshot_attrs</code> are loaded. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if a checkpoint is successfully loaded. False otherwise.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def maybe_load_client_state(self, client: BasicClient, attributes: list[str] | None = None) -&gt; bool:\n    \"\"\"\n    Load the state into the client that is being provided.\n\n    Args:\n        client (BasicClient): Target client object into which state will be loaded\n        attributes (list[str] | None, optional): List of attributes to load from the checkpoint. If None, all\n            attributes specified in ``snapshot_attrs`` are loaded. Defaults to None.\n\n    Returns:\n        (bool): True if a checkpoint is successfully loaded. False otherwise.\n    \"\"\"\n    # Store client for access in functions\n    self.client = client\n    # Setting default name if one doesn't exist. If we're here and it doesn't exist yet, user is expecting\n    # a default name for loading anyway.\n    self.maybe_set_default_checkpoint_name()\n    if self.checkpoint_exists():\n        self.load_state(attributes)\n        log(INFO, f\"State checkpoint successfully loaded from: {self.checkpoint_path}\")\n        # Clear the client after we are done updating its attributes.\n        self.client = None\n        return True\n\n    log(INFO, f\"No state checkpoint found at: {self.checkpoint_path}\")\n    # Clear the client since no checkpoint exists.\n    self.client = None\n    return False\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ClientStateCheckpointer.get_attribute","title":"<code>get_attribute(name)</code>","text":"<p>Get the attribute from the client.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the attribute.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The attribute value.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def get_attribute(self, name: str) -&gt; Any:\n    \"\"\"\n    Get the attribute from the client.\n\n    Args:\n        name (str): Name of the attribute.\n\n    Returns:\n        (Any): The attribute value.\n    \"\"\"\n    assert self.client is not None, \"Client is not set.\"\n    return getattr(self.client, name)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ClientStateCheckpointer.set_attribute","title":"<code>set_attribute(name, value)</code>","text":"<p>Set the attribute on the client.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the attribute.</p> required <code>value</code> <code>Any</code> <p>Value to set for the attribute.</p> required Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def set_attribute(self, name: str, value: Any) -&gt; None:\n    \"\"\"\n    Set the attribute on the client.\n\n    Args:\n        name (str): Name of the attribute.\n        value (Any): Value to set for the attribute.\n    \"\"\"\n    assert self.client is not None, \"Client is not set.\"\n    setattr(self.client, name, value)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ServerStateCheckpointer","title":"<code>ServerStateCheckpointer</code>","text":"<p>               Bases: <code>StateCheckpointer</code></p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>class ServerStateCheckpointer(StateCheckpointer):\n    def __init__(\n        self,\n        checkpoint_dir: Path,\n        checkpoint_name: str | None = None,\n        snapshot_attrs: dict[str, tuple[AbstractSnapshotter, Any]] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Class for saving and loading the state of a server's attributes as specified in ``snapshot_attrs``.\n\n        Args:\n            checkpoint_dir (Path): Directory to which checkpoints are saved. This can be modified later with\n                ``set_checkpoint_path``\n            checkpoint_name (str | None, optional): Name of the checkpoint to be saved. If None, but ``checkpoint_dir``\n                is set then a default ``checkpoint_name`` based on the underlying name of the client to be\n                checkpointed will be set of the form ``f\"server_{self.server.server_name}_state.pt\"``. This can be\n                updated later  with ``set_checkpoint_path``. Defaults to None.\n            snapshot_attrs (dict[str, tuple[AbstractSnapshotter, Any]] | None, optional): Attributes that we need to\n                save in order to allow for restarting of training. If None, a sensible default set of attributes and\n                their associated snapshotters for an FL client are set. Defaults to None.\n        \"\"\"\n        # If snapshot_attrs is None, we set a sensible default set of attributes to be saved. These are a minimal\n        # set of attributes that can be used for per round checkpointing or early stopping.\n        # NOTE: These default attributes are useful for state checkpointing a FlServer. More sophisticated servers\n        # may require more attributes to fully support training restarts and early stopping. For an example, see\n        # NnUnetServerStateCheckpointer.\n        if snapshot_attrs is None:\n            snapshot_attrs = {\n                \"model\": (TorchModuleSnapshotter(), nn.Module),\n                \"current_round\": (SingletonSnapshotter(), int),\n                \"reports_manager\": (\n                    SerializableObjectSnapshotter(),\n                    ReportsManager,\n                ),\n                \"server_name\": (StringSnapshotter(), str),\n                \"history\": (HistorySnapshotter(), History),\n            }\n        super().__init__(checkpoint_dir, checkpoint_name, snapshot_attrs)\n        self.server: FlServer | None = None\n        self.server_model: nn.Module | None = None\n\n    def maybe_set_default_checkpoint_name(self) -&gt; None:\n        \"\"\"\n        Potentially sets a default name for the checkpoint to be saved. If ``checkpoint_dir`` is set but\n        ``checkpoint_name`` is None then a default ``checkpoint_name`` based on the underlying name of the server to\n        be checkpointed will be set of the form ``f\"server_{self.server.server_name}_state.pt\"``.\n        \"\"\"\n        assert self.server is not None, \"Attempting to save server state but server is None\"\n        # Set the checkpoint name based on server's name if not already provided.\n        if self.checkpoint_name is None:\n            # If checkpoint_name is not provided, we set it based on the server's name.\n            self.checkpoint_name = f\"server_{self.server.server_name}_state.pt\"\n            self.set_checkpoint_path(self.checkpoint_dir, self.checkpoint_name)\n\n    def save_server_state(self, server: FlServer, model: nn.Module) -&gt; None:\n        \"\"\"\n        Save the state of the server, including a torch model, which is not a required component of the server class.\n\n        Args:\n            server (FlServer): Server with state to be saved\n            model (nn.Module): The model to be saved as part of the server state.\n        \"\"\"\n        # Store server and model for access in functions\n        self.server = server\n        # Server object does not have a model attribute, so we handle it separately.\n        self.server_model = model\n        # Potentially set a default checkpoint name\n        self.maybe_set_default_checkpoint_name()\n        # Saves everything in self.snapshot_attrs\n        self.save_state()\n        # Clear the server objects after checkpointing.\n        self.server = None\n        self.server_model = None\n\n    def maybe_load_server_state(\n        self, server: FlServer, model: nn.Module, attributes: list[str] | None = None\n    ) -&gt; nn.Module | None:\n        \"\"\"\n        Load the state of the server from checkpoint.\n\n        Args:\n            server (FlServer): Server into which the attributes will be loaded.\n            model (nn.Module): The model structure to be loaded as part of the server state.\n            attributes (list[str] | None, optional): List of attributes to load from the checkpoint. If None, all\n                attributes specified in ``snapshot_attrs`` are loaded. Defaults to None.\n\n        Returns:\n            (nn.Module | None): Returns a model if a checkpoint exists to load from. Otherwise returns None.\n        \"\"\"\n        # Store server for access in functions\n        self.server = server\n        # Server object does not have a model attribute, so we handle it separately.\n        self.server_model = model\n        # Setting default name if one doesn't exist. If we're here and it doesn't exist yet, user is expecting\n        self.maybe_set_default_checkpoint_name()\n        if self.checkpoint_exists():\n            self.load_state(attributes)\n            log(INFO, f\"State checkpoint successfully loaded from: {self.checkpoint_path}\")\n            # Clear the server after we are done updating its attributes.\n            self.server = None\n            # Server model is saved and returned separately for parameter extraction.\n            return self.server_model\n\n        log(INFO, f\"No state checkpoint found at: {self.checkpoint_path}\")\n        # Clear the server object since checkpoint is not found.\n        self.server = None\n        self.server_model = None\n        return None\n\n    def get_attribute(self, name: str) -&gt; Any:\n        \"\"\"\n        Get the attribute from the server.\n\n        Args:\n            name (str): Name of the attribute.\n\n        Returns:\n            (Any): The attribute value.\n        \"\"\"\n        assert self.server is not None, \"Server is not set.\"\n        if name == \"model\":\n            return self.server_model\n        return getattr(self.server, name)\n\n    def set_attribute(self, name: str, value: Any) -&gt; None:\n        \"\"\"\n        Set the attribute on the server.\n\n        Args:\n            name (str): Name of the attribute.\n            value (Any): Value to set for the attribute.\n        \"\"\"\n        assert self.server is not None, \"Server is not set.\"\n        if name == \"model\":\n            self.server_model = value\n        else:\n            setattr(self.server, name, value)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ServerStateCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name=None, snapshot_attrs=None)</code>","text":"<p>Class for saving and loading the state of a server's attributes as specified in <code>snapshot_attrs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>Path</code> <p>Directory to which checkpoints are saved. This can be modified later with <code>set_checkpoint_path</code></p> required <code>checkpoint_name</code> <code>str | None</code> <p>Name of the checkpoint to be saved. If None, but <code>checkpoint_dir</code> is set then a default <code>checkpoint_name</code> based on the underlying name of the client to be checkpointed will be set of the form <code>f\"server_{self.server.server_name}_state.pt\"</code>. This can be updated later  with <code>set_checkpoint_path</code>. Defaults to None.</p> <code>None</code> <code>snapshot_attrs</code> <code>dict[str, tuple[AbstractSnapshotter, Any]] | None</code> <p>Attributes that we need to save in order to allow for restarting of training. If None, a sensible default set of attributes and their associated snapshotters for an FL client are set. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: Path,\n    checkpoint_name: str | None = None,\n    snapshot_attrs: dict[str, tuple[AbstractSnapshotter, Any]] | None = None,\n) -&gt; None:\n    \"\"\"\n    Class for saving and loading the state of a server's attributes as specified in ``snapshot_attrs``.\n\n    Args:\n        checkpoint_dir (Path): Directory to which checkpoints are saved. This can be modified later with\n            ``set_checkpoint_path``\n        checkpoint_name (str | None, optional): Name of the checkpoint to be saved. If None, but ``checkpoint_dir``\n            is set then a default ``checkpoint_name`` based on the underlying name of the client to be\n            checkpointed will be set of the form ``f\"server_{self.server.server_name}_state.pt\"``. This can be\n            updated later  with ``set_checkpoint_path``. Defaults to None.\n        snapshot_attrs (dict[str, tuple[AbstractSnapshotter, Any]] | None, optional): Attributes that we need to\n            save in order to allow for restarting of training. If None, a sensible default set of attributes and\n            their associated snapshotters for an FL client are set. Defaults to None.\n    \"\"\"\n    # If snapshot_attrs is None, we set a sensible default set of attributes to be saved. These are a minimal\n    # set of attributes that can be used for per round checkpointing or early stopping.\n    # NOTE: These default attributes are useful for state checkpointing a FlServer. More sophisticated servers\n    # may require more attributes to fully support training restarts and early stopping. For an example, see\n    # NnUnetServerStateCheckpointer.\n    if snapshot_attrs is None:\n        snapshot_attrs = {\n            \"model\": (TorchModuleSnapshotter(), nn.Module),\n            \"current_round\": (SingletonSnapshotter(), int),\n            \"reports_manager\": (\n                SerializableObjectSnapshotter(),\n                ReportsManager,\n            ),\n            \"server_name\": (StringSnapshotter(), str),\n            \"history\": (HistorySnapshotter(), History),\n        }\n    super().__init__(checkpoint_dir, checkpoint_name, snapshot_attrs)\n    self.server: FlServer | None = None\n    self.server_model: nn.Module | None = None\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ServerStateCheckpointer.maybe_set_default_checkpoint_name","title":"<code>maybe_set_default_checkpoint_name()</code>","text":"<p>Potentially sets a default name for the checkpoint to be saved. If <code>checkpoint_dir</code> is set but <code>checkpoint_name</code> is None then a default <code>checkpoint_name</code> based on the underlying name of the server to be checkpointed will be set of the form <code>f\"server_{self.server.server_name}_state.pt\"</code>.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def maybe_set_default_checkpoint_name(self) -&gt; None:\n    \"\"\"\n    Potentially sets a default name for the checkpoint to be saved. If ``checkpoint_dir`` is set but\n    ``checkpoint_name`` is None then a default ``checkpoint_name`` based on the underlying name of the server to\n    be checkpointed will be set of the form ``f\"server_{self.server.server_name}_state.pt\"``.\n    \"\"\"\n    assert self.server is not None, \"Attempting to save server state but server is None\"\n    # Set the checkpoint name based on server's name if not already provided.\n    if self.checkpoint_name is None:\n        # If checkpoint_name is not provided, we set it based on the server's name.\n        self.checkpoint_name = f\"server_{self.server.server_name}_state.pt\"\n        self.set_checkpoint_path(self.checkpoint_dir, self.checkpoint_name)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ServerStateCheckpointer.save_server_state","title":"<code>save_server_state(server, model)</code>","text":"<p>Save the state of the server, including a torch model, which is not a required component of the server class.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FlServer</code> <p>Server with state to be saved</p> required <code>model</code> <code>Module</code> <p>The model to be saved as part of the server state.</p> required Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def save_server_state(self, server: FlServer, model: nn.Module) -&gt; None:\n    \"\"\"\n    Save the state of the server, including a torch model, which is not a required component of the server class.\n\n    Args:\n        server (FlServer): Server with state to be saved\n        model (nn.Module): The model to be saved as part of the server state.\n    \"\"\"\n    # Store server and model for access in functions\n    self.server = server\n    # Server object does not have a model attribute, so we handle it separately.\n    self.server_model = model\n    # Potentially set a default checkpoint name\n    self.maybe_set_default_checkpoint_name()\n    # Saves everything in self.snapshot_attrs\n    self.save_state()\n    # Clear the server objects after checkpointing.\n    self.server = None\n    self.server_model = None\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ServerStateCheckpointer.maybe_load_server_state","title":"<code>maybe_load_server_state(server, model, attributes=None)</code>","text":"<p>Load the state of the server from checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>FlServer</code> <p>Server into which the attributes will be loaded.</p> required <code>model</code> <code>Module</code> <p>The model structure to be loaded as part of the server state.</p> required <code>attributes</code> <code>list[str] | None</code> <p>List of attributes to load from the checkpoint. If None, all attributes specified in <code>snapshot_attrs</code> are loaded. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Module | None</code> <p>Returns a model if a checkpoint exists to load from. Otherwise returns None.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def maybe_load_server_state(\n    self, server: FlServer, model: nn.Module, attributes: list[str] | None = None\n) -&gt; nn.Module | None:\n    \"\"\"\n    Load the state of the server from checkpoint.\n\n    Args:\n        server (FlServer): Server into which the attributes will be loaded.\n        model (nn.Module): The model structure to be loaded as part of the server state.\n        attributes (list[str] | None, optional): List of attributes to load from the checkpoint. If None, all\n            attributes specified in ``snapshot_attrs`` are loaded. Defaults to None.\n\n    Returns:\n        (nn.Module | None): Returns a model if a checkpoint exists to load from. Otherwise returns None.\n    \"\"\"\n    # Store server for access in functions\n    self.server = server\n    # Server object does not have a model attribute, so we handle it separately.\n    self.server_model = model\n    # Setting default name if one doesn't exist. If we're here and it doesn't exist yet, user is expecting\n    self.maybe_set_default_checkpoint_name()\n    if self.checkpoint_exists():\n        self.load_state(attributes)\n        log(INFO, f\"State checkpoint successfully loaded from: {self.checkpoint_path}\")\n        # Clear the server after we are done updating its attributes.\n        self.server = None\n        # Server model is saved and returned separately for parameter extraction.\n        return self.server_model\n\n    log(INFO, f\"No state checkpoint found at: {self.checkpoint_path}\")\n    # Clear the server object since checkpoint is not found.\n    self.server = None\n    self.server_model = None\n    return None\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ServerStateCheckpointer.get_attribute","title":"<code>get_attribute(name)</code>","text":"<p>Get the attribute from the server.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the attribute.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The attribute value.</p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def get_attribute(self, name: str) -&gt; Any:\n    \"\"\"\n    Get the attribute from the server.\n\n    Args:\n        name (str): Name of the attribute.\n\n    Returns:\n        (Any): The attribute value.\n    \"\"\"\n    assert self.server is not None, \"Server is not set.\"\n    if name == \"model\":\n        return self.server_model\n    return getattr(self.server, name)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.ServerStateCheckpointer.set_attribute","title":"<code>set_attribute(name, value)</code>","text":"<p>Set the attribute on the server.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the attribute.</p> required <code>value</code> <code>Any</code> <p>Value to set for the attribute.</p> required Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def set_attribute(self, name: str, value: Any) -&gt; None:\n    \"\"\"\n    Set the attribute on the server.\n\n    Args:\n        name (str): Name of the attribute.\n        value (Any): Value to set for the attribute.\n    \"\"\"\n    assert self.server is not None, \"Server is not set.\"\n    if name == \"model\":\n        self.server_model = value\n    else:\n        setattr(self.server, name, value)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.NnUnetServerStateCheckpointer","title":"<code>NnUnetServerStateCheckpointer</code>","text":"<p>               Bases: <code>ServerStateCheckpointer</code></p> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>class NnUnetServerStateCheckpointer(ServerStateCheckpointer):\n    def __init__(\n        self,\n        checkpoint_dir: Path,\n        checkpoint_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Class for saving and loading the state of the server's attributes based on the ``snapshot_attrs`` defined\n        specifically for the nnUNet server.\n\n        Args:\n            checkpoint_dir (Path): Directory to which checkpoints are saved. This can be modified later with\n                ``set_checkpoint_path``\n            checkpoint_name (str | None, optional): Name of the checkpoint to be saved. If None, but ``checkpoint_dir``\n                is set then a default ``checkpoint_name`` based on the underlying name of the client to be\n                checkpointed will be set of the form ``f\"server_{self.server.server_name}_state.pt\"``. This can be\n                updated later  with ``set_checkpoint_path``. Defaults to None.\n        \"\"\"\n        # Go beyond default snapshot_attrs with nnUNet-specific attributes.\n        nnunet_snapshot_attrs: dict[str, tuple[AbstractSnapshotter, Any]] = {\n            \"model\": (TorchModuleSnapshotter(), nn.Module),\n            \"current_round\": (SingletonSnapshotter(), int),\n            \"reports_manager\": (\n                SerializableObjectSnapshotter(),\n                ReportsManager,\n            ),\n            \"server_name\": (StringSnapshotter(), str),\n            \"history\": (HistorySnapshotter(), History),\n            \"nnunet_plans_bytes\": (BytesSnapshotter(), bytes),\n            \"num_segmentation_heads\": (SingletonSnapshotter(), int),\n            \"num_input_channels\": (SingletonSnapshotter(), int),\n            \"global_deep_supervision\": (EnumSnapshotter(), bool),\n            \"nnunet_config\": (EnumSnapshotter(), Enum),\n        }\n\n        super().__init__(checkpoint_dir, checkpoint_name, snapshot_attrs=nnunet_snapshot_attrs)\n</code></pre>"},{"location":"api/#fl4health.checkpointing.state_checkpointer.NnUnetServerStateCheckpointer.__init__","title":"<code>__init__(checkpoint_dir, checkpoint_name=None)</code>","text":"<p>Class for saving and loading the state of the server's attributes based on the <code>snapshot_attrs</code> defined specifically for the nnUNet server.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_dir</code> <code>Path</code> <p>Directory to which checkpoints are saved. This can be modified later with <code>set_checkpoint_path</code></p> required <code>checkpoint_name</code> <code>str | None</code> <p>Name of the checkpoint to be saved. If None, but <code>checkpoint_dir</code> is set then a default <code>checkpoint_name</code> based on the underlying name of the client to be checkpointed will be set of the form <code>f\"server_{self.server.server_name}_state.pt\"</code>. This can be updated later  with <code>set_checkpoint_path</code>. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/checkpointing/state_checkpointer.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_dir: Path,\n    checkpoint_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Class for saving and loading the state of the server's attributes based on the ``snapshot_attrs`` defined\n    specifically for the nnUNet server.\n\n    Args:\n        checkpoint_dir (Path): Directory to which checkpoints are saved. This can be modified later with\n            ``set_checkpoint_path``\n        checkpoint_name (str | None, optional): Name of the checkpoint to be saved. If None, but ``checkpoint_dir``\n            is set then a default ``checkpoint_name`` based on the underlying name of the client to be\n            checkpointed will be set of the form ``f\"server_{self.server.server_name}_state.pt\"``. This can be\n            updated later  with ``set_checkpoint_path``. Defaults to None.\n    \"\"\"\n    # Go beyond default snapshot_attrs with nnUNet-specific attributes.\n    nnunet_snapshot_attrs: dict[str, tuple[AbstractSnapshotter, Any]] = {\n        \"model\": (TorchModuleSnapshotter(), nn.Module),\n        \"current_round\": (SingletonSnapshotter(), int),\n        \"reports_manager\": (\n            SerializableObjectSnapshotter(),\n            ReportsManager,\n        ),\n        \"server_name\": (StringSnapshotter(), str),\n        \"history\": (HistorySnapshotter(), History),\n        \"nnunet_plans_bytes\": (BytesSnapshotter(), bytes),\n        \"num_segmentation_heads\": (SingletonSnapshotter(), int),\n        \"num_input_channels\": (SingletonSnapshotter(), int),\n        \"global_deep_supervision\": (EnumSnapshotter(), bool),\n        \"nnunet_config\": (EnumSnapshotter(), Enum),\n    }\n\n    super().__init__(checkpoint_dir, checkpoint_name, snapshot_attrs=nnunet_snapshot_attrs)\n</code></pre>"},{"location":"api/#fl4health.client_managers","title":"<code>client_managers</code>","text":""},{"location":"api/#fl4health.client_managers.base_sampling_manager","title":"<code>base_sampling_manager</code>","text":""},{"location":"api/#fl4health.client_managers.base_sampling_manager.BaseFractionSamplingManager","title":"<code>BaseFractionSamplingManager</code>","text":"<p>               Bases: <code>SimpleClientManager</code></p> <p>Overrides the <code>SimpleClientManager</code> to Provide Fixed Sampling without replacement for Clients.</p> Source code in <code>fl4health/client_managers/base_sampling_manager.py</code> <pre><code>class BaseFractionSamplingManager(SimpleClientManager):\n    \"\"\"Overrides the ``SimpleClientManager`` to Provide Fixed Sampling without replacement for Clients.\"\"\"\n\n    def sample(\n        self, num_clients: int, min_num_clients: int | None = None, criterion: Criterion | None = None\n    ) -&gt; list[ClientProxy]:\n        raise NotImplementedError(\n            \"The basic sampling function is not implemented for these managers. \"\n            \"Please use the fraction sample function instead\"\n        )\n\n    def sample_fraction(\n        self,\n        sample_fraction: float,\n        min_num_clients: int | None = None,\n        criterion: Criterion | None = None,\n    ) -&gt; list[ClientProxy]:\n        raise NotImplementedError\n\n    def wait_and_filter(self, min_num_clients: int | None, criterion: Criterion | None = None) -&gt; list[str]:\n        \"\"\"\n        Waits for ``min_num_clients`` to become available then select clients from those available and filter them\n        based on the criterion provided. If ``min_num_clients`` is None, then it waits for at least 1 client to be\n        available.\n\n        Args:\n            min_num_clients (int | None): Number of clients to wait for before performing filtration.\n            criterion (Criterion | None, optional): criterion used to filter available clients. Defaults to None.\n\n        Returns:\n            (list[str]): List of CIDs representing available and filtered clients.\n        \"\"\"\n        if min_num_clients is not None:\n            self.wait_for(min_num_clients)\n        else:\n            self.wait_for(1)\n\n        available_cids = list(self.clients)\n        if criterion is not None:\n            available_cids = [cid for cid in available_cids if criterion.select(self.clients[cid])]\n\n        return available_cids\n\n    def sample_one(self, min_num_clients: int | None = None, criterion: Criterion | None = None) -&gt; list[ClientProxy]:\n        \"\"\"\n        Samples exactly one available client randomly. This should only be used for client-side parameter\n        initialization.\n\n        Args:\n            min_num_clients (int | None, optional): minimum number of clients to wait to become available before\n                selecting all available clients. Defaults to None.\n            criterion (Criterion | None, optional): Criterion used to filter returned clients. If none, no filter is\n                applied. Defaults to None.\n\n        Returns:\n            (list[ClientProxy]): Selected client represented by a ClientProxy object in list form as expected by\n                server.\n        \"\"\"\n        available_cids = self.wait_and_filter(min_num_clients, criterion)\n        # Sample exactly on client randomly\n        cids = random.sample(available_cids, 1)\n\n        return [self.clients[cid] for cid in cids]\n\n    def sample_all(self, min_num_clients: int | None = None, criterion: Criterion | None = None) -&gt; list[ClientProxy]:\n        \"\"\"\n        Samples **ALL** available clients.\n\n        Args:\n            min_num_clients (int | None, optional): minimum number of clients to wait to become available before\n                selecting all available clients. Defaults to None.\n            criterion (Criterion | None, optional): Criterion used to filter returned clients. If none, no filter is\n                applied. Defaults to None.\n\n        Returns:\n            (list[ClientProxy]): List of selected clients represented by ``ClientProxy`` objects.\n        \"\"\"\n        available_cids = self.wait_and_filter(min_num_clients, criterion)\n\n        return [self.clients[cid] for cid in available_cids]\n</code></pre>"},{"location":"api/#fl4health.client_managers.base_sampling_manager.BaseFractionSamplingManager.wait_and_filter","title":"<code>wait_and_filter(min_num_clients, criterion=None)</code>","text":"<p>Waits for <code>min_num_clients</code> to become available then select clients from those available and filter them based on the criterion provided. If <code>min_num_clients</code> is None, then it waits for at least 1 client to be available.</p> <p>Parameters:</p> Name Type Description Default <code>min_num_clients</code> <code>int | None</code> <p>Number of clients to wait for before performing filtration.</p> required <code>criterion</code> <code>Criterion | None</code> <p>criterion used to filter available clients. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of CIDs representing available and filtered clients.</p> Source code in <code>fl4health/client_managers/base_sampling_manager.py</code> <pre><code>def wait_and_filter(self, min_num_clients: int | None, criterion: Criterion | None = None) -&gt; list[str]:\n    \"\"\"\n    Waits for ``min_num_clients`` to become available then select clients from those available and filter them\n    based on the criterion provided. If ``min_num_clients`` is None, then it waits for at least 1 client to be\n    available.\n\n    Args:\n        min_num_clients (int | None): Number of clients to wait for before performing filtration.\n        criterion (Criterion | None, optional): criterion used to filter available clients. Defaults to None.\n\n    Returns:\n        (list[str]): List of CIDs representing available and filtered clients.\n    \"\"\"\n    if min_num_clients is not None:\n        self.wait_for(min_num_clients)\n    else:\n        self.wait_for(1)\n\n    available_cids = list(self.clients)\n    if criterion is not None:\n        available_cids = [cid for cid in available_cids if criterion.select(self.clients[cid])]\n\n    return available_cids\n</code></pre>"},{"location":"api/#fl4health.client_managers.base_sampling_manager.BaseFractionSamplingManager.sample_one","title":"<code>sample_one(min_num_clients=None, criterion=None)</code>","text":"<p>Samples exactly one available client randomly. This should only be used for client-side parameter initialization.</p> <p>Parameters:</p> Name Type Description Default <code>min_num_clients</code> <code>int | None</code> <p>minimum number of clients to wait to become available before selecting all available clients. Defaults to None.</p> <code>None</code> <code>criterion</code> <code>Criterion | None</code> <p>Criterion used to filter returned clients. If none, no filter is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ClientProxy]</code> <p>Selected client represented by a ClientProxy object in list form as expected by server.</p> Source code in <code>fl4health/client_managers/base_sampling_manager.py</code> <pre><code>def sample_one(self, min_num_clients: int | None = None, criterion: Criterion | None = None) -&gt; list[ClientProxy]:\n    \"\"\"\n    Samples exactly one available client randomly. This should only be used for client-side parameter\n    initialization.\n\n    Args:\n        min_num_clients (int | None, optional): minimum number of clients to wait to become available before\n            selecting all available clients. Defaults to None.\n        criterion (Criterion | None, optional): Criterion used to filter returned clients. If none, no filter is\n            applied. Defaults to None.\n\n    Returns:\n        (list[ClientProxy]): Selected client represented by a ClientProxy object in list form as expected by\n            server.\n    \"\"\"\n    available_cids = self.wait_and_filter(min_num_clients, criterion)\n    # Sample exactly on client randomly\n    cids = random.sample(available_cids, 1)\n\n    return [self.clients[cid] for cid in cids]\n</code></pre>"},{"location":"api/#fl4health.client_managers.base_sampling_manager.BaseFractionSamplingManager.sample_all","title":"<code>sample_all(min_num_clients=None, criterion=None)</code>","text":"<p>Samples ALL available clients.</p> <p>Parameters:</p> Name Type Description Default <code>min_num_clients</code> <code>int | None</code> <p>minimum number of clients to wait to become available before selecting all available clients. Defaults to None.</p> <code>None</code> <code>criterion</code> <code>Criterion | None</code> <p>Criterion used to filter returned clients. If none, no filter is applied. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ClientProxy]</code> <p>List of selected clients represented by <code>ClientProxy</code> objects.</p> Source code in <code>fl4health/client_managers/base_sampling_manager.py</code> <pre><code>def sample_all(self, min_num_clients: int | None = None, criterion: Criterion | None = None) -&gt; list[ClientProxy]:\n    \"\"\"\n    Samples **ALL** available clients.\n\n    Args:\n        min_num_clients (int | None, optional): minimum number of clients to wait to become available before\n            selecting all available clients. Defaults to None.\n        criterion (Criterion | None, optional): Criterion used to filter returned clients. If none, no filter is\n            applied. Defaults to None.\n\n    Returns:\n        (list[ClientProxy]): List of selected clients represented by ``ClientProxy`` objects.\n    \"\"\"\n    available_cids = self.wait_and_filter(min_num_clients, criterion)\n\n    return [self.clients[cid] for cid in available_cids]\n</code></pre>"},{"location":"api/#fl4health.client_managers.fixed_sampling_client_manager","title":"<code>fixed_sampling_client_manager</code>","text":""},{"location":"api/#fl4health.client_managers.fixed_sampling_client_manager.FixedSamplingClientManager","title":"<code>FixedSamplingClientManager</code>","text":"<p>               Bases: <code>SimpleClientManager</code></p> Source code in <code>fl4health/client_managers/fixed_sampling_client_manager.py</code> <pre><code>class FixedSamplingClientManager(SimpleClientManager):\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Client manager that samples the same set of clients each time until it receives a signal to resample the\n        clients to be selected. This class, for example, helps facilitate the requirements associated with FedDG-GA.\n        Keeps sampling fixed until it's reset.\n        \"\"\"\n        super().__init__()\n        self.current_sample: list[ClientProxy] | None = None\n\n    def reset_sample(self) -&gt; None:\n        \"\"\"Resets the saved sample so ``self.sample`` produces a new sample again.\"\"\"\n        self.current_sample = None\n\n    def sample(\n        self,\n        num_clients: int,\n        min_num_clients: int | None = None,\n        criterion: Criterion | None = None,\n    ) -&gt; list[ClientProxy]:\n        \"\"\"\n        Return a new client sample for the first time it runs. For subsequent runs, it will return the same sampling\n        until ``self.reset_sampling()`` is called.\n\n        Args:\n            num_clients (int): The number of clients to sample.\n            min_num_clients (int | None, optional): The minimum number of clients to return in the sample.\n                Defaults to None.\n            criterion (Criterion | None, optional): A criterion to filter clients to sample. If None, no criterion is\n                applied during selection/sampling. Defaults to None.\n\n        Returns:\n            (list[ClientProxy]): A list of sampled clients as ``ClientProxy`` instances.\n        \"\"\"\n        if self.current_sample is None:\n            self.current_sample = super().sample(num_clients, min_num_clients, criterion)\n        return self.current_sample\n</code></pre>"},{"location":"api/#fl4health.client_managers.fixed_sampling_client_manager.FixedSamplingClientManager.__init__","title":"<code>__init__()</code>","text":"<p>Client manager that samples the same set of clients each time until it receives a signal to resample the clients to be selected. This class, for example, helps facilitate the requirements associated with FedDG-GA. Keeps sampling fixed until it's reset.</p> Source code in <code>fl4health/client_managers/fixed_sampling_client_manager.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Client manager that samples the same set of clients each time until it receives a signal to resample the\n    clients to be selected. This class, for example, helps facilitate the requirements associated with FedDG-GA.\n    Keeps sampling fixed until it's reset.\n    \"\"\"\n    super().__init__()\n    self.current_sample: list[ClientProxy] | None = None\n</code></pre>"},{"location":"api/#fl4health.client_managers.fixed_sampling_client_manager.FixedSamplingClientManager.reset_sample","title":"<code>reset_sample()</code>","text":"<p>Resets the saved sample so <code>self.sample</code> produces a new sample again.</p> Source code in <code>fl4health/client_managers/fixed_sampling_client_manager.py</code> <pre><code>def reset_sample(self) -&gt; None:\n    \"\"\"Resets the saved sample so ``self.sample`` produces a new sample again.\"\"\"\n    self.current_sample = None\n</code></pre>"},{"location":"api/#fl4health.client_managers.fixed_sampling_client_manager.FixedSamplingClientManager.sample","title":"<code>sample(num_clients, min_num_clients=None, criterion=None)</code>","text":"<p>Return a new client sample for the first time it runs. For subsequent runs, it will return the same sampling until <code>self.reset_sampling()</code> is called.</p> <p>Parameters:</p> Name Type Description Default <code>num_clients</code> <code>int</code> <p>The number of clients to sample.</p> required <code>min_num_clients</code> <code>int | None</code> <p>The minimum number of clients to return in the sample. Defaults to None.</p> <code>None</code> <code>criterion</code> <code>Criterion | None</code> <p>A criterion to filter clients to sample. If None, no criterion is applied during selection/sampling. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ClientProxy]</code> <p>A list of sampled clients as <code>ClientProxy</code> instances.</p> Source code in <code>fl4health/client_managers/fixed_sampling_client_manager.py</code> <pre><code>def sample(\n    self,\n    num_clients: int,\n    min_num_clients: int | None = None,\n    criterion: Criterion | None = None,\n) -&gt; list[ClientProxy]:\n    \"\"\"\n    Return a new client sample for the first time it runs. For subsequent runs, it will return the same sampling\n    until ``self.reset_sampling()`` is called.\n\n    Args:\n        num_clients (int): The number of clients to sample.\n        min_num_clients (int | None, optional): The minimum number of clients to return in the sample.\n            Defaults to None.\n        criterion (Criterion | None, optional): A criterion to filter clients to sample. If None, no criterion is\n            applied during selection/sampling. Defaults to None.\n\n    Returns:\n        (list[ClientProxy]): A list of sampled clients as ``ClientProxy`` instances.\n    \"\"\"\n    if self.current_sample is None:\n        self.current_sample = super().sample(num_clients, min_num_clients, criterion)\n    return self.current_sample\n</code></pre>"},{"location":"api/#fl4health.client_managers.fixed_without_replacement_manager","title":"<code>fixed_without_replacement_manager</code>","text":""},{"location":"api/#fl4health.client_managers.fixed_without_replacement_manager.FixedSamplingByFractionClientManager","title":"<code>FixedSamplingByFractionClientManager</code>","text":"<p>               Bases: <code>BaseFractionSamplingManager</code></p> <p>Overrides the <code>BaseFractionSamplingManager</code> to provide Fixed Sampling without replacement for Clients by fraction.</p> Source code in <code>fl4health/client_managers/fixed_without_replacement_manager.py</code> <pre><code>class FixedSamplingByFractionClientManager(BaseFractionSamplingManager):\n    \"\"\"\n    Overrides the ``BaseFractionSamplingManager`` to provide Fixed Sampling without replacement for Clients by\n    fraction.\n    \"\"\"\n\n    def sample_fraction(\n        self,\n        sample_fraction: float,\n        min_num_clients: int | None = None,\n        criterion: Criterion | None = None,\n    ) -&gt; list[ClientProxy]:\n        \"\"\"\n        Sample a number of Flower ``ClientProxy`` instances **WITHOUT** replacement.\n\n        Args:\n            sample_fraction (float): Fraction of clients to sample. Guaranteed to produce this fraction from\n                available clients\n            min_num_clients (int | None, optional): minimum number of clients required to be available.\n                Defaults to None.\n            criterion (Criterion | None, optional): Criterion to help with sampling. No filter is applied if None.\n                Defaults to None.\n\n        Returns:\n            (list[ClientProxy]): List of ClientProxy objects representing the selected clients.\n        \"\"\"\n        available_cids = self.wait_and_filter(min_num_clients, criterion)\n        n_available_cids = len(available_cids)\n        num_to_sample = int(sample_fraction * n_available_cids)\n        if num_to_sample &lt; 1:\n            log(\n                WARNING,\n                f\"Sample fraction of {round(sample_fraction, 3)} resulted in 0 samples to being selected\"\n                f\"from {n_available_cids}.\",\n            )\n            return []\n        sampled_cids = random.sample(available_cids, num_to_sample)\n        return [self.clients[cid] for cid in sampled_cids]\n</code></pre>"},{"location":"api/#fl4health.client_managers.fixed_without_replacement_manager.FixedSamplingByFractionClientManager.sample_fraction","title":"<code>sample_fraction(sample_fraction, min_num_clients=None, criterion=None)</code>","text":"<p>Sample a number of Flower <code>ClientProxy</code> instances WITHOUT replacement.</p> <p>Parameters:</p> Name Type Description Default <code>sample_fraction</code> <code>float</code> <p>Fraction of clients to sample. Guaranteed to produce this fraction from available clients</p> required <code>min_num_clients</code> <code>int | None</code> <p>minimum number of clients required to be available. Defaults to None.</p> <code>None</code> <code>criterion</code> <code>Criterion | None</code> <p>Criterion to help with sampling. No filter is applied if None. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ClientProxy]</code> <p>List of ClientProxy objects representing the selected clients.</p> Source code in <code>fl4health/client_managers/fixed_without_replacement_manager.py</code> <pre><code>def sample_fraction(\n    self,\n    sample_fraction: float,\n    min_num_clients: int | None = None,\n    criterion: Criterion | None = None,\n) -&gt; list[ClientProxy]:\n    \"\"\"\n    Sample a number of Flower ``ClientProxy`` instances **WITHOUT** replacement.\n\n    Args:\n        sample_fraction (float): Fraction of clients to sample. Guaranteed to produce this fraction from\n            available clients\n        min_num_clients (int | None, optional): minimum number of clients required to be available.\n            Defaults to None.\n        criterion (Criterion | None, optional): Criterion to help with sampling. No filter is applied if None.\n            Defaults to None.\n\n    Returns:\n        (list[ClientProxy]): List of ClientProxy objects representing the selected clients.\n    \"\"\"\n    available_cids = self.wait_and_filter(min_num_clients, criterion)\n    n_available_cids = len(available_cids)\n    num_to_sample = int(sample_fraction * n_available_cids)\n    if num_to_sample &lt; 1:\n        log(\n            WARNING,\n            f\"Sample fraction of {round(sample_fraction, 3)} resulted in 0 samples to being selected\"\n            f\"from {n_available_cids}.\",\n        )\n        return []\n    sampled_cids = random.sample(available_cids, num_to_sample)\n    return [self.clients[cid] for cid in sampled_cids]\n</code></pre>"},{"location":"api/#fl4health.client_managers.poisson_sampling_manager","title":"<code>poisson_sampling_manager</code>","text":""},{"location":"api/#fl4health.client_managers.poisson_sampling_manager.PoissonSamplingClientManager","title":"<code>PoissonSamplingClientManager</code>","text":"<p>               Bases: <code>BaseFractionSamplingManager</code></p> <p>Overrides the <code>BaseFractionSamplingManager</code> to provide Poisson sampling for clients rather than fixed without replacement sampling.</p> Source code in <code>fl4health/client_managers/poisson_sampling_manager.py</code> <pre><code>class PoissonSamplingClientManager(BaseFractionSamplingManager):\n    \"\"\"\n    Overrides the ``BaseFractionSamplingManager`` to provide Poisson sampling for clients rather than fixed without\n    replacement sampling.\n    \"\"\"\n\n    def _poisson_sample(self, sampling_probability: float, available_cids: list[str]) -&gt; list[str]:\n        poisson_trials = np.random.binomial(1, sampling_probability, len(available_cids))\n        poisson_mask = poisson_trials.astype(dtype=bool)\n        return list(np.array(available_cids)[poisson_mask])\n\n    def sample_fraction(\n        self,\n        sample_fraction: float,\n        min_num_clients: int | None = None,\n        criterion: Criterion | None = None,\n    ) -&gt; list[ClientProxy]:\n        \"\"\"\n        Poisson Sampling of Flower ClientProxy instances with a probability determine by sample_fraction.\n\n        Args:\n            sample_fraction (float): Fraction, which sets the Poisson sampling probability\n            min_num_clients (int | None, optional): minimum number of clients to be selected (overrides sampling to\n                some extent). Defaults to None.\n            criterion (Criterion | None, optional): Criterion to sample clients based on. Defaults to None.\n\n        Returns:\n            (list[ClientProxy]): List of selected ClientProxy objects represented the clients selected by the process.\n        \"\"\"\n        available_cids = self.wait_and_filter(min_num_clients, criterion)\n        n_available_cids = len(available_cids)\n        expected_clients_selected = sample_fraction * n_available_cids\n        if expected_clients_selected &lt; 1:\n            log(\n                WARNING,\n                f\"Sample fraction of {round(sample_fraction, 3)} from {n_available_cids} clients results \"\n                f\"in expected value of {round(expected_clients_selected, 3)} selected.\",\n            )\n        sampled_cids = self._poisson_sample(sample_fraction, available_cids)\n        return [self.clients[cid] for cid in sampled_cids]\n</code></pre>"},{"location":"api/#fl4health.client_managers.poisson_sampling_manager.PoissonSamplingClientManager.sample_fraction","title":"<code>sample_fraction(sample_fraction, min_num_clients=None, criterion=None)</code>","text":"<p>Poisson Sampling of Flower ClientProxy instances with a probability determine by sample_fraction.</p> <p>Parameters:</p> Name Type Description Default <code>sample_fraction</code> <code>float</code> <p>Fraction, which sets the Poisson sampling probability</p> required <code>min_num_clients</code> <code>int | None</code> <p>minimum number of clients to be selected (overrides sampling to some extent). Defaults to None.</p> <code>None</code> <code>criterion</code> <code>Criterion | None</code> <p>Criterion to sample clients based on. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[ClientProxy]</code> <p>List of selected ClientProxy objects represented the clients selected by the process.</p> Source code in <code>fl4health/client_managers/poisson_sampling_manager.py</code> <pre><code>def sample_fraction(\n    self,\n    sample_fraction: float,\n    min_num_clients: int | None = None,\n    criterion: Criterion | None = None,\n) -&gt; list[ClientProxy]:\n    \"\"\"\n    Poisson Sampling of Flower ClientProxy instances with a probability determine by sample_fraction.\n\n    Args:\n        sample_fraction (float): Fraction, which sets the Poisson sampling probability\n        min_num_clients (int | None, optional): minimum number of clients to be selected (overrides sampling to\n            some extent). Defaults to None.\n        criterion (Criterion | None, optional): Criterion to sample clients based on. Defaults to None.\n\n    Returns:\n        (list[ClientProxy]): List of selected ClientProxy objects represented the clients selected by the process.\n    \"\"\"\n    available_cids = self.wait_and_filter(min_num_clients, criterion)\n    n_available_cids = len(available_cids)\n    expected_clients_selected = sample_fraction * n_available_cids\n    if expected_clients_selected &lt; 1:\n        log(\n            WARNING,\n            f\"Sample fraction of {round(sample_fraction, 3)} from {n_available_cids} clients results \"\n            f\"in expected value of {round(expected_clients_selected, 3)} selected.\",\n        )\n    sampled_cids = self._poisson_sample(sample_fraction, available_cids)\n    return [self.clients[cid] for cid in sampled_cids]\n</code></pre>"},{"location":"api/#fl4health.clients","title":"<code>clients</code>","text":""},{"location":"api/#fl4health.clients.adaptive_drift_constraint_client","title":"<code>adaptive_drift_constraint_client</code>","text":""},{"location":"api/#fl4health.clients.adaptive_drift_constraint_client.AdaptiveDriftConstraintClient","title":"<code>AdaptiveDriftConstraintClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/adaptive_drift_constraint_client.py</code> <pre><code>class AdaptiveDriftConstraintClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client serves as a base for FL methods implementing an auxiliary loss penalty with a weight coefficient\n        that might be adapted via loss trajectories on the server-side. An example of such a method is FedProx, which\n        uses an auxiliary loss penalizing weight drift with a coefficient mu. This client is a simple extension of the\n        ``BasicClient`` that packs the ``self.loss_for_adaptation`` for exchange with the server and expects to\n        receive an updated (or constant if non-adaptive) parameter for the loss weight. In many cases, such as FedProx,\n        the ``loss_for_adaptation`` being packaged is the criterion loss (i.e. loss without the penalty).\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        # These are the tensors that will be used to compute the penalty loss\n        self.drift_penalty_tensors: list[torch.Tensor]\n        # Exchanger with packing to be able to exchange the weights and auxiliary information with the server for\n        # adaptation\n        self.parameter_exchanger: FullParameterExchangerWithPacking[float]\n        # Weight on the penalty loss to be used in backprop. This is what might be adapted via server calculations\n        self.drift_penalty_weight: float\n        # This is the loss value to be sent back to the server on which adaptation decisions will be made.\n        self.loss_for_adaptation: float\n        # Function to compute the penalty loss.\n        self.penalty_loss_function = WeightDriftLoss(self.device)\n\n    def get_parameters(self, config: Config) -&gt; NDArrays:\n        \"\"\"\n        Packs the parameters and training loss into a single ``NDArrays`` to be sent to the server for aggregation. If\n        the client has not been initialized, this means the server is requesting parameters for initialization and\n        just the model parameters are sent. When using the ``FedAvgWithAdaptiveConstraint`` strategy, this should not\n        happen, as that strategy requires server-side initialization parameters. However, other strategies may handle\n        this case.\n\n        Args:\n            config (Config): Configurations to allow for customization of this functions behavior\n\n        Returns:\n            (NDArrays): Parameters and training loss packed together into a list of numpy arrays to be sent to the\n                server.\n        \"\"\"\n        if not self.initialized:\n            return self.setup_client_and_return_all_model_parameters(config)\n\n        # Make sure the proper components are there\n        assert self.model is not None and self.parameter_exchanger is not None and self.loss_for_adaptation is not None\n        model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n\n        # Weights and training loss sent to server for aggregation. Training loss is sent because server will\n        # decide to increase or decrease the penalty weight, if adaptivity is turned on.\n        return self.parameter_exchanger.pack_parameters(model_weights, self.loss_for_adaptation)\n\n    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n        \"\"\"\n        Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n        unpacked for the clients to use in training. In the first fitting round, we assume the full model is being\n        initialized and use the ``FullParameterExchanger()`` to set all model weights.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model and also the penalty weight to be applied during training.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n                round or an evaluation round. This is used to help determine which parameter exchange should be used\n                for pulling parameters. A full parameter exchanger is always used if the current federated learning\n                round is the very first fitting round.\n        \"\"\"\n        assert self.model is not None and self.parameter_exchanger is not None\n\n        server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n        log(INFO, f\"Penalty weight received from the server: {self.drift_penalty_weight}\")\n\n        super().set_parameters(server_model_state, config, fitting_round)\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training loss given predictions of the model and ground truth data. Adds to objective by including\n        penalty loss.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n                dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n                by name. Additional losses includes penalty loss.\n        \"\"\"\n        loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n        if additional_losses is None:\n            additional_losses = {}\n\n        additional_losses[\"loss\"] = loss.clone()\n        # adding the vanilla loss to the additional losses to be used by update_after_train for potential adaptation\n        additional_losses[\"loss_for_adaptation\"] = loss.clone()\n\n        # Compute the drift penalty loss and store it in the additional losses dictionary.\n        penalty_loss = self.compute_penalty_loss()\n        additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n\n        return TrainingLosses(backward=loss + penalty_loss, additional_losses=additional_losses)\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        \"\"\"\n        Setting up the parameter exchanger to include the appropriate packing functionality. By default we assume\n        that we're exchanging all parameters. Can be overridden for other behavior.\n\n        Args:\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n        Returns:\n            (ParameterExchanger): Exchanger that can handle packing/unpacking auxiliary server information.\n        \"\"\"\n        return FullParameterExchangerWithPacking(ParameterPackerAdaptiveConstraint())\n\n    def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n        \"\"\"\n        Called after training with the number of ``local_steps`` performed over the FL round and the corresponding loss\n        dictionary. We use this to store the training loss that we want to use to adapt the penalty weight parameter\n        on the server side.\n\n        Args:\n            local_steps (int): The number of steps so far in the round in the local training.\n            loss_dict (dict[str, float]): A dictionary of losses from local training.\n            config (Config): The config from the server\n        \"\"\"\n        assert \"loss_for_adaptation\" in loss_dict\n        # Store current loss which is the vanilla loss without the penalty term added in\n        self.loss_for_adaptation = loss_dict[\"loss_for_adaptation\"]\n        super().update_after_train(local_steps, loss_dict, config)\n\n    def compute_penalty_loss(self) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the drift loss for the client model and drift tensors.\n\n        Returns:\n            (torch.Tensor): Computed penalty loss tensor.\n        \"\"\"\n        # Penalty tensors must have been set for these clients.\n        assert self.drift_penalty_tensors is not None\n\n        return self.penalty_loss_function(self.model, self.drift_penalty_tensors, self.drift_penalty_weight)\n</code></pre>"},{"location":"api/#fl4health.clients.adaptive_drift_constraint_client.AdaptiveDriftConstraintClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>This client serves as a base for FL methods implementing an auxiliary loss penalty with a weight coefficient that might be adapted via loss trajectories on the server-side. An example of such a method is FedProx, which uses an auxiliary loss penalizing weight drift with a coefficient mu. This client is a simple extension of the <code>BasicClient</code> that packs the <code>self.loss_for_adaptation</code> for exchange with the server and expects to receive an updated (or constant if non-adaptive) parameter for the loss weight. In many cases, such as FedProx, the <code>loss_for_adaptation</code> being packaged is the criterion loss (i.e. loss without the penalty).</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/adaptive_drift_constraint_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    This client serves as a base for FL methods implementing an auxiliary loss penalty with a weight coefficient\n    that might be adapted via loss trajectories on the server-side. An example of such a method is FedProx, which\n    uses an auxiliary loss penalizing weight drift with a coefficient mu. This client is a simple extension of the\n    ``BasicClient`` that packs the ``self.loss_for_adaptation`` for exchange with the server and expects to\n    receive an updated (or constant if non-adaptive) parameter for the loss weight. In many cases, such as FedProx,\n    the ``loss_for_adaptation`` being packaged is the criterion loss (i.e. loss without the penalty).\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    # These are the tensors that will be used to compute the penalty loss\n    self.drift_penalty_tensors: list[torch.Tensor]\n    # Exchanger with packing to be able to exchange the weights and auxiliary information with the server for\n    # adaptation\n    self.parameter_exchanger: FullParameterExchangerWithPacking[float]\n    # Weight on the penalty loss to be used in backprop. This is what might be adapted via server calculations\n    self.drift_penalty_weight: float\n    # This is the loss value to be sent back to the server on which adaptation decisions will be made.\n    self.loss_for_adaptation: float\n    # Function to compute the penalty loss.\n    self.penalty_loss_function = WeightDriftLoss(self.device)\n</code></pre>"},{"location":"api/#fl4health.clients.adaptive_drift_constraint_client.AdaptiveDriftConstraintClient.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>Packs the parameters and training loss into a single <code>NDArrays</code> to be sent to the server for aggregation. If the client has not been initialized, this means the server is requesting parameters for initialization and just the model parameters are sent. When using the <code>FedAvgWithAdaptiveConstraint</code> strategy, this should not happen, as that strategy requires server-side initialization parameters. However, other strategies may handle this case.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configurations to allow for customization of this functions behavior</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Parameters and training loss packed together into a list of numpy arrays to be sent to the server.</p> Source code in <code>fl4health/clients/adaptive_drift_constraint_client.py</code> <pre><code>def get_parameters(self, config: Config) -&gt; NDArrays:\n    \"\"\"\n    Packs the parameters and training loss into a single ``NDArrays`` to be sent to the server for aggregation. If\n    the client has not been initialized, this means the server is requesting parameters for initialization and\n    just the model parameters are sent. When using the ``FedAvgWithAdaptiveConstraint`` strategy, this should not\n    happen, as that strategy requires server-side initialization parameters. However, other strategies may handle\n    this case.\n\n    Args:\n        config (Config): Configurations to allow for customization of this functions behavior\n\n    Returns:\n        (NDArrays): Parameters and training loss packed together into a list of numpy arrays to be sent to the\n            server.\n    \"\"\"\n    if not self.initialized:\n        return self.setup_client_and_return_all_model_parameters(config)\n\n    # Make sure the proper components are there\n    assert self.model is not None and self.parameter_exchanger is not None and self.loss_for_adaptation is not None\n    model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n\n    # Weights and training loss sent to server for aggregation. Training loss is sent because server will\n    # decide to increase or decrease the penalty weight, if adaptivity is turned on.\n    return self.parameter_exchanger.pack_parameters(model_weights, self.loss_for_adaptation)\n</code></pre>"},{"location":"api/#fl4health.clients.adaptive_drift_constraint_client.AdaptiveDriftConstraintClient.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are unpacked for the clients to use in training. In the first fitting round, we assume the full model is being initialized and use the <code>FullParameterExchanger()</code> to set all model weights.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model and also the penalty weight to be applied during training.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. This is used to help determine which parameter exchange should be used for pulling parameters. A full parameter exchanger is always used if the current federated learning round is the very first fitting round.</p> required Source code in <code>fl4health/clients/adaptive_drift_constraint_client.py</code> <pre><code>def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n    \"\"\"\n    Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n    unpacked for the clients to use in training. In the first fitting round, we assume the full model is being\n    initialized and use the ``FullParameterExchanger()`` to set all model weights.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model and also the penalty weight to be applied during training.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n            round or an evaluation round. This is used to help determine which parameter exchange should be used\n            for pulling parameters. A full parameter exchanger is always used if the current federated learning\n            round is the very first fitting round.\n    \"\"\"\n    assert self.model is not None and self.parameter_exchanger is not None\n\n    server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n    log(INFO, f\"Penalty weight received from the server: {self.drift_penalty_weight}\")\n\n    super().set_parameters(server_model_state, config, fitting_round)\n</code></pre>"},{"location":"api/#fl4health.clients.adaptive_drift_constraint_client.AdaptiveDriftConstraintClient.compute_training_loss","title":"<code>compute_training_loss(preds, features, target)</code>","text":"<p>Computes training loss given predictions of the model and ground truth data. Adds to objective by including penalty loss.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name. Additional losses includes penalty loss.</p> Source code in <code>fl4health/clients/adaptive_drift_constraint_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training loss given predictions of the model and ground truth data. Adds to objective by including\n    penalty loss.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n            dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n            by name. Additional losses includes penalty loss.\n    \"\"\"\n    loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n    if additional_losses is None:\n        additional_losses = {}\n\n    additional_losses[\"loss\"] = loss.clone()\n    # adding the vanilla loss to the additional losses to be used by update_after_train for potential adaptation\n    additional_losses[\"loss_for_adaptation\"] = loss.clone()\n\n    # Compute the drift penalty loss and store it in the additional losses dictionary.\n    penalty_loss = self.compute_penalty_loss()\n    additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n\n    return TrainingLosses(backward=loss + penalty_loss, additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.adaptive_drift_constraint_client.AdaptiveDriftConstraintClient.get_parameter_exchanger","title":"<code>get_parameter_exchanger(config)</code>","text":"<p>Setting up the parameter exchanger to include the appropriate packing functionality. By default we assume that we're exchanging all parameters. Can be overridden for other behavior.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <p>Returns:</p> Type Description <code>ParameterExchanger</code> <p>Exchanger that can handle packing/unpacking auxiliary server information.</p> Source code in <code>fl4health/clients/adaptive_drift_constraint_client.py</code> <pre><code>def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n    \"\"\"\n    Setting up the parameter exchanger to include the appropriate packing functionality. By default we assume\n    that we're exchanging all parameters. Can be overridden for other behavior.\n\n    Args:\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n    Returns:\n        (ParameterExchanger): Exchanger that can handle packing/unpacking auxiliary server information.\n    \"\"\"\n    return FullParameterExchangerWithPacking(ParameterPackerAdaptiveConstraint())\n</code></pre>"},{"location":"api/#fl4health.clients.adaptive_drift_constraint_client.AdaptiveDriftConstraintClient.update_after_train","title":"<code>update_after_train(local_steps, loss_dict, config)</code>","text":"<p>Called after training with the number of <code>local_steps</code> performed over the FL round and the corresponding loss dictionary. We use this to store the training loss that we want to use to adapt the penalty weight parameter on the server side.</p> <p>Parameters:</p> Name Type Description Default <code>local_steps</code> <code>int</code> <p>The number of steps so far in the round in the local training.</p> required <code>loss_dict</code> <code>dict[str, float]</code> <p>A dictionary of losses from local training.</p> required <code>config</code> <code>Config</code> <p>The config from the server</p> required Source code in <code>fl4health/clients/adaptive_drift_constraint_client.py</code> <pre><code>def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n    \"\"\"\n    Called after training with the number of ``local_steps`` performed over the FL round and the corresponding loss\n    dictionary. We use this to store the training loss that we want to use to adapt the penalty weight parameter\n    on the server side.\n\n    Args:\n        local_steps (int): The number of steps so far in the round in the local training.\n        loss_dict (dict[str, float]): A dictionary of losses from local training.\n        config (Config): The config from the server\n    \"\"\"\n    assert \"loss_for_adaptation\" in loss_dict\n    # Store current loss which is the vanilla loss without the penalty term added in\n    self.loss_for_adaptation = loss_dict[\"loss_for_adaptation\"]\n    super().update_after_train(local_steps, loss_dict, config)\n</code></pre>"},{"location":"api/#fl4health.clients.adaptive_drift_constraint_client.AdaptiveDriftConstraintClient.compute_penalty_loss","title":"<code>compute_penalty_loss()</code>","text":"<p>Computes the drift loss for the client model and drift tensors.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed penalty loss tensor.</p> Source code in <code>fl4health/clients/adaptive_drift_constraint_client.py</code> <pre><code>def compute_penalty_loss(self) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the drift loss for the client model and drift tensors.\n\n    Returns:\n        (torch.Tensor): Computed penalty loss tensor.\n    \"\"\"\n    # Penalty tensors must have been set for these clients.\n    assert self.drift_penalty_tensors is not None\n\n    return self.penalty_loss_function(self.model, self.drift_penalty_tensors, self.drift_penalty_weight)\n</code></pre>"},{"location":"api/#fl4health.clients.apfl_client","title":"<code>apfl_client</code>","text":""},{"location":"api/#fl4health.clients.apfl_client.ApflClient","title":"<code>ApflClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/apfl_client.py</code> <pre><code>class ApflClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Client specifically implementing the APFL Algorithm: https://arxiv.org/abs/2003.13461.\n\n        Twin models are trained. One of them is globally shared by all clients and aggregated on the server.\n        The other is strictly trained locally by each client. Predictions are made by a convex combination of the\n        models.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n\n        self.model: ApflModule\n        self.learning_rate: float\n        self.optimizers: dict[str, torch.optim.Optimizer]\n\n    def is_start_of_local_training(self, step: int) -&gt; bool:\n        return step == 0\n\n    def update_after_step(self, step: int, current_round: int | None = None) -&gt; None:\n        \"\"\"\n        Called after local train step on client. Step is an integer that represents the local training step that was\n        most recently completed.\n        \"\"\"\n        if self.is_start_of_local_training(step) and self.model.adaptive_alpha:\n            self.model.update_alpha()\n\n    def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n        # Return preds value thats Dict of torch.Tensor containing personal, global and local predictions\n\n        # Mechanics of training loop follow from original implementation\n        # https://github.com/MLOPTPSU/FedTorch/blob/main/fedtorch/comms/trainings/federated/apfl.py\n\n        # Forward pass on global model and update global parameters\n        assert isinstance(input, torch.Tensor)\n        self.optimizers[\"global\"].zero_grad()\n        global_pred = self.model.global_forward(input)\n        global_loss = self.criterion(global_pred, target)\n        global_loss.backward()\n        self.optimizers[\"global\"].step()\n\n        # Make sure gradients are zero prior to forward passes of global and local model\n        # to generate personalized predictions\n        # NOTE: We zero the global optimizer grads because they are used (after the backward calculation below)\n        # to update the scalar alpha (see update_alpha() where .grad is called.)\n        self.optimizers[\"global\"].zero_grad()\n        self.optimizers[\"local\"].zero_grad()\n\n        # Personal predictions are generated as a convex combination of the output\n        # of local and global models\n        preds, features = self.predict(input)\n        target = self.transform_target(target)  # Apply transformation (Defaults to identity)\n\n        # Parameters of local model are updated to minimize loss of personalized model\n        losses = self.compute_training_loss(preds, features, target)\n\n        losses.backward[\"backward\"].backward()\n        self.optimizers[\"local\"].step()\n\n        # Return dictionary of predictions where key is used to name respective MetricMeters\n        return losses, preds\n\n    def get_parameter_exchanger(self, config: Config) -&gt; FixedLayerExchanger:\n        return FixedLayerExchanger(self.model.layers_to_exchange())\n\n    def compute_loss_and_additional_losses(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Computes the loss and any additional losses given predictions of the model and ground truth data.\n        For APFL, the loss will be the personal loss and the additional losses are the global and local loss.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n                - The tensor for the personal loss\n                - A dictionary of with \"global_loss\" and \"local_loss\" keys and their calculated values\n        \"\"\"\n        assert isinstance(preds, dict)\n        personal_loss = self.criterion(preds[\"personal\"], target)\n        global_loss = self.criterion(preds[\"global\"], target)\n        local_loss = self.criterion(preds[\"local\"], target)\n        additional_losses = {\"global\": global_loss, \"local\": local_loss}\n\n        return personal_loss, additional_losses\n\n    def set_optimizer(self, config: Config) -&gt; None:\n        optimizers = self.get_optimizer(config)\n        assert isinstance(optimizers, dict) and {\"global\", \"local\"} == set(optimizers.keys())\n        self.optimizers = optimizers\n\n    def get_optimizer(self, config: Config) -&gt; dict[str, Optimizer]:\n        \"\"\"Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.apfl_client.ApflClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>Client specifically implementing the APFL Algorithm: https://arxiv.org/abs/2003.13461.</p> <p>Twin models are trained. One of them is globally shared by all clients and aggregated on the server. The other is strictly trained locally by each client. Predictions are made by a convex combination of the models.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/apfl_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Client specifically implementing the APFL Algorithm: https://arxiv.org/abs/2003.13461.\n\n    Twin models are trained. One of them is globally shared by all clients and aggregated on the server.\n    The other is strictly trained locally by each client. Predictions are made by a convex combination of the\n    models.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n\n    self.model: ApflModule\n    self.learning_rate: float\n    self.optimizers: dict[str, torch.optim.Optimizer]\n</code></pre>"},{"location":"api/#fl4health.clients.apfl_client.ApflClient.update_after_step","title":"<code>update_after_step(step, current_round=None)</code>","text":"<p>Called after local train step on client. Step is an integer that represents the local training step that was most recently completed.</p> Source code in <code>fl4health/clients/apfl_client.py</code> <pre><code>def update_after_step(self, step: int, current_round: int | None = None) -&gt; None:\n    \"\"\"\n    Called after local train step on client. Step is an integer that represents the local training step that was\n    most recently completed.\n    \"\"\"\n    if self.is_start_of_local_training(step) and self.model.adaptive_alpha:\n        self.model.update_alpha()\n</code></pre>"},{"location":"api/#fl4health.clients.apfl_client.ApflClient.compute_loss_and_additional_losses","title":"<code>compute_loss_and_additional_losses(preds, features, target)</code>","text":"<p>Computes the loss and any additional losses given predictions of the model and ground truth data. For APFL, the loss will be the personal loss and the additional losses are the global and local loss.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor]]</code> <p>A tuple with:</p> <ul> <li>The tensor for the personal loss</li> <li>A dictionary of with \"global_loss\" and \"local_loss\" keys and their calculated values</li> </ul> Source code in <code>fl4health/clients/apfl_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Computes the loss and any additional losses given predictions of the model and ground truth data.\n    For APFL, the loss will be the personal loss and the additional losses are the global and local loss.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n            - The tensor for the personal loss\n            - A dictionary of with \"global_loss\" and \"local_loss\" keys and their calculated values\n    \"\"\"\n    assert isinstance(preds, dict)\n    personal_loss = self.criterion(preds[\"personal\"], target)\n    global_loss = self.criterion(preds[\"global\"], target)\n    local_loss = self.criterion(preds[\"local\"], target)\n    additional_losses = {\"global\": global_loss, \"local\": local_loss}\n\n    return personal_loss, additional_losses\n</code></pre>"},{"location":"api/#fl4health.clients.apfl_client.ApflClient.get_optimizer","title":"<code>get_optimizer(config)</code>","text":"<p>Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.</p> Source code in <code>fl4health/clients/apfl_client.py</code> <pre><code>def get_optimizer(self, config: Config) -&gt; dict[str, Optimizer]:\n    \"\"\"Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client","title":"<code>basic_client</code>","text":""},{"location":"api/#fl4health.clients.basic_client.BasicClient","title":"<code>BasicClient</code>","text":"<p>               Bases: <code>NumPyClient</code></p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>class BasicClient(NumPyClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Base FL Client with functionality to train, evaluate, log, report and checkpoint.\n        User is responsible for implementing methods: ``get_model``, ``get_optimizer``, ``get_data_loaders``,\n        ``get_criterion`` Other methods can be overridden to achieve custom functionality.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        self.data_path = data_path\n        self.device = device\n        self.metrics = metrics\n        self.progress_bar = progress_bar\n\n        self.client_name = client_name if client_name is not None else generate_hash()\n        log(INFO, f\"Client Name: {self.client_name}\")\n\n        if checkpoint_and_state_module is not None:\n            self.checkpoint_and_state_module = checkpoint_and_state_module\n        else:\n            # Define a default module that does nothing.\n            self.checkpoint_and_state_module = ClientCheckpointAndStateModule(\n                pre_aggregation=None, post_aggregation=None, state_checkpointer=None\n            )\n\n        # Initialize reporters with client information.\n        self.reports_manager = ReportsManager(reporters)\n        self.reports_manager.initialize(id=self.client_name, name=self.client_name)\n\n        self.initialized = False  # Whether or not the client has been setup\n\n        # Loss and Metric management\n        self.train_loss_meter = LossMeter[TrainingLosses](loss_meter_type, TrainingLosses)\n        self.val_loss_meter = LossMeter[EvaluationLosses](loss_meter_type, EvaluationLosses)\n        self.train_metric_manager = MetricManager(metrics=self.metrics, metric_manager_name=\"train\")\n        self.val_metric_manager = MetricManager(metrics=self.metrics, metric_manager_name=\"val\")\n        self.test_loss_meter = LossMeter[EvaluationLosses](loss_meter_type, EvaluationLosses)\n        self.test_metric_manager = MetricManager(metrics=self.metrics, metric_manager_name=\"test\")\n\n        # Optional variable to store the weights that the client was initialized with during each round of training\n        self.initial_weights: NDArrays | None = None\n\n        self.total_steps: int = 0  # Need to track total_steps across rounds for WANDB reporting\n        self.total_epochs: int = 0  # Will remain as 0 if training by steps\n\n        # Attributes to be initialized in setup_client\n        self.parameter_exchanger: ParameterExchanger\n        self.model: nn.Module\n        self.optimizers: dict[str, torch.optim.Optimizer]\n        self.train_loader: DataLoader\n        self.val_loader: DataLoader\n        self.test_loader: DataLoader | None\n        self.num_train_samples: int\n        self.num_val_samples: int\n        self.num_test_samples: int | None\n        self.learning_rate: float | None\n\n        # User can set the early stopper for the client by instantiating the EarlyStopper class\n        # and setting the patience and interval_steps attributes. The early stopper will be used to\n        # stop training if the validation loss does not improve for a certain number of steps.\n        self.early_stopper: EarlyStopper | None = None\n        # Config can contain num_validation_steps key, which determines an upper bound\n        # for the validation steps taken. If not specified, no upper bound will be enforced.\n        # By specifying this in the config we cannot guarantee the validation set is the same\n        # across rounds for clients.\n        self.num_validation_steps: int | None = None\n        # NOTE: These iterators are of type _BaseDataLoaderIter, which is not importable...so we're forced to use\n        # Iterator\n        self.train_iterator: Iterator | None = None\n        self.val_iterator: Iterator | None = None\n\n    def _maybe_checkpoint(self, loss: float, metrics: dict[str, Scalar], checkpoint_mode: CheckpointMode) -&gt; None:\n        \"\"\"\n        If checkpointer exists, maybe checkpoint model based on the provided metric values.\n\n        Args:\n            loss (float): Validation loss to potentially be used for checkpointing.\n            metrics (dict[str, Scalar]): Validation metrics to potentially be used for checkpointing\n            checkpoint_mode (CheckpointMode): Whether we're doing checkpointing pre- or post-aggregation on the server\n                side.\n        \"\"\"\n        self.checkpoint_and_state_module.maybe_checkpoint(self.model, loss, metrics, checkpoint_mode)\n\n    def get_parameters(self, config: Config) -&gt; NDArrays:\n        \"\"\"\n        Determines which parameters are sent back to the server for aggregation. This uses a parameter exchanger to\n        determine parameters sent.\n\n        Args:\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n        Returns:\n            (NDArrays): These are the parameters to be sent to the server. At minimum they represent the relevant model\n                parameters to be aggregated, but can contain more information.\n        \"\"\"\n        if not self.initialized:\n            return self.setup_client_and_return_all_model_parameters(config)\n\n        assert self.model is not None and self.parameter_exchanger is not None\n        # If the client has early stopping module and the patience is None, we load the best saved state\n        # to send the best checkpointed local model's parameters to the server\n        self._maybe_load_saved_best_local_model_state()\n        return self.parameter_exchanger.push_parameters(self.model, config=config)\n\n    def _maybe_load_saved_best_local_model_state(self) -&gt; None:\n        if self.early_stopper is not None and self.early_stopper.patience is None:\n            log(INFO, \"Loading saved best model's state before sending model to server.\")\n            self.early_stopper.load_snapshot([\"model\"])\n\n    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n        \"\"\"\n        Sets the local model parameters transferred from the server using a parameter exchanger to coordinate how\n        parameters are set. In the first fitting round, we assume the full model is being\n        initialized and use the ``FullParameterExchanger()`` to set all model weights.\n        Otherwise, we use the appropriate parameter exchanger defined by the user depending on the\n        federated learning algorithm being used.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model but may contain more information than that.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning round is a\n                fitting round or an evaluation round.\n                This is used to help determine which parameter exchange should be used for pulling parameters.\n                A full parameter exchanger is only used if the current federated learning round is the very\n                first fitting round.\n        \"\"\"\n        assert self.model is not None\n        current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n        if current_server_round == 1 and fitting_round:\n            self.initialize_all_model_weights(parameters, config)\n        else:\n            assert self.parameter_exchanger is not None\n            self.parameter_exchanger.pull_parameters(parameters, self.model, config)\n\n    def initialize_all_model_weights(self, parameters: NDArrays, config: Config) -&gt; None:\n        \"\"\"\n        If this is the first time we're initializing the model weights, we use the ``FullParameterExchanger`` to\n        initialize all model components. Subclasses that require custom model initialization can override this.\n\n        Args:\n            parameters (NDArrays): Model parameters to be injected into the client model.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        \"\"\"\n        FullParameterExchanger().pull_parameters(parameters, self.model, config)\n\n    def setup_client_and_return_all_model_parameters(self, config: Config) -&gt; NDArrays:\n        \"\"\"\n        Function used to setup the client using the provided configuration and then exact all model parameters from\n        ``self.model`` and return them. This function is used as a helper for ``get_parameters`` when the client\n        has yet to be initialized.\n\n        Args:\n            config (Config): Configuration to be used  in setting up the client.\n\n        Returns:\n            (NDArrays): All parameters associated with the ``self.model`` property of the client.\n        \"\"\"\n        log(INFO, \"Setting up client and providing full model parameters to the server for initialization\")\n        if not config:\n            log(\n                WARNING,\n                (\n                    \"This client has not yet been initialized and the config is empty. This may cause unexpected \"\n                    \"failures, as setting up a client typically requires several configuration parameters, \"\n                    \"including batch_size and current_server_round.\"\n                ),\n            )\n\n        # If initialized is False, the server is requesting model parameters from which to initialize all other\n        # clients. As such get_parameters is being called before fit or evaluate, so we must call\n        # setup_client first.\n        self.setup_client(config)\n\n        # Need all parameters even if normally exchanging partial\n        return FullParameterExchanger().push_parameters(self.model, config=config)\n\n    def shutdown(self) -&gt; None:\n        \"\"\"Shuts down the client. Involves shutting down W&amp;B reporter if one exists.\"\"\"\n        # Shutdown reporters\n        self.reports_manager.report({\"shutdown\": str(datetime.datetime.now())})\n        self.reports_manager.shutdown()\n\n    def process_config(self, config: Config) -&gt; tuple[int | None, int | None, int, bool, bool]:\n        \"\"\"\n        Method to ensure the required keys are present in config and extracts values to be returned.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (tuple[int | None, int | None, int, bool, bool]): Returns the ``local_epochs``, ``local_steps``,\n                ``current_server_round``, ``evaluate_after_fit`` and ``pack_losses_with_val_metrics``. Ensures only\n                one of ``local_epochs`` and ``local_steps`` is defined in the config and sets the one that is not to\n                None.\n\n        Raises:\n            ValueError: If the config contains both ``local_steps`` and local epochs or if ``local_steps``,\n                ``local_epochs`` or ``current_server_round`` is of the wrong type (int).\n        \"\"\"\n        current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n\n        # Parse config to determine train by steps or train by epochs\n        if (\"local_epochs\" in config) and (\"local_steps\" in config):\n            raise ValueError(\"Config cannot contain both local_epochs and local_steps. Please specify only one.\")\n        if \"local_epochs\" in config:\n            local_epochs = narrow_dict_type(config, \"local_epochs\", int)\n            local_steps = None\n        elif \"local_steps\" in config:\n            local_steps = narrow_dict_type(config, \"local_steps\", int)\n            local_epochs = None\n        else:\n            raise ValueError(\"Must specify either local_epochs or local_steps in the Config.\")\n\n        try:\n            evaluate_after_fit = narrow_dict_type(config, \"evaluate_after_fit\", bool)\n        except ValueError:\n            evaluate_after_fit = False\n\n        pack_losses_with_val_metrics = set_pack_losses_with_val_metrics(config)\n\n        # Either local epochs or local steps is none based on what key is passed in the config\n        return local_epochs, local_steps, current_server_round, evaluate_after_fit, pack_losses_with_val_metrics\n\n    def fit(self, parameters: NDArrays, config: Config) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n        \"\"\"\n        Processes config, initializes client (if first round) and performs training based on the passed config.\n        If ``per_round_checkpointer`` is not None, on initialization the client checks if a checkpointed client state\n        exists to load and at the end of each round the client state is saved.\n\n        Args:\n            parameters (NDArrays): The parameters of the model to be used in fit.\n            config (NDArrays): The config from the server.\n\n        Returns:\n            (tuple[NDArrays, int, dict[str, Scalar]]): The parameters following the local training along with the\n                number of samples in the local training dataset and the computed metrics throughout the fit.\n\n        Raises:\n            ValueError: If ``local_steps`` or ``local_epochs`` is not specified in config.\n        \"\"\"\n        round_start_time = datetime.datetime.now()\n        local_epochs, local_steps, current_server_round, evaluate_after_fit, pack_losses_with_val_metrics = (\n            self.process_config(config)\n        )\n\n        if not self.initialized:\n            self.setup_client(config)\n\n            if self.checkpoint_and_state_module.state_checkpointer is not None:\n                # If this is the first time the client is being setup, we also attempt to load any existing state\n                # If no state exists, we assume this is a fresh run. State is useful, for example, in restarting FL\n                # training that was interrupted or failed part way through.\n                state_load_success = self._load_client_state()\n                if state_load_success:\n                    log(INFO, \"Successfully loaded client state.\")\n                else:\n                    log(INFO, \"Client state was not loaded.\")\n\n        self.set_parameters(parameters, config, fitting_round=True)\n\n        self.update_before_train(current_server_round)\n\n        fit_start_time = datetime.datetime.now()\n        if local_epochs is not None:\n            loss_dict, metrics = self.train_by_epochs(local_epochs, current_server_round)\n            local_steps = len(self.train_loader) * local_epochs  # total steps over training round\n        elif local_steps is not None:\n            loss_dict, metrics = self.train_by_steps(local_steps, current_server_round)\n        else:\n            raise ValueError(\"Must specify either local_epochs or local_steps in the Config.\")\n        fit_end_time = datetime.datetime.now()\n\n        # Perform necessary updates after training has completed for the current FL round\n        self.update_after_train(local_steps, loss_dict, config)\n\n        # Check if we should run an evaluation with validation data after fit\n        # (for example, this is used by FedDGGA)\n        if self._should_evaluate_after_fit(evaluate_after_fit):\n            validation_loss, validation_metrics = self.validate(pack_losses_with_val_metrics)\n            metrics.update(validation_metrics)\n            # We perform a pre-aggregation checkpoint if applicable\n            self._maybe_checkpoint(validation_loss, validation_metrics, CheckpointMode.PRE_AGGREGATION)\n\n        # Notes on report values:\n        #   - Train by steps: round metrics/losses are computed using all samples from the round\n        #   - Train by epochs: round metrics/losses computed using only the samples from the final epoch of the round\n        #   - fit_round_metrics: Computed at the end of the round on the samples directly\n        #   - fit_round_losses: The average of the losses computed for each step.\n        #       * (Hence likely higher than the final loss of the round.)\n        self.reports_manager.report(\n            {\n                \"fit_round_metrics\": metrics,\n                \"fit_round_losses\": loss_dict,\n                \"round\": current_server_round,\n                \"round_start\": str(round_start_time),\n                \"round_end\": str(datetime.datetime.now()),\n                \"fit_round_start\": str(fit_start_time),\n                \"fit_round_time_elapsed\": round((fit_end_time - fit_start_time).total_seconds()),\n                \"fit_round_end\": str(fit_end_time),\n                \"fit_step\": self.total_steps,\n                \"fit_epoch\": self.total_epochs,\n            },\n            current_server_round,\n        )\n\n        # After local client training has finished, checkpoint client state if a state checkpointer is defined\n        if self.checkpoint_and_state_module.state_checkpointer is not None:\n            self._save_client_state()\n\n        # FitRes should contain local parameters, number of examples on client, and a dictionary holding metrics\n        # calculation results.\n        return (\n            self.get_parameters(config),\n            self.num_train_samples,\n            metrics,\n        )\n\n    def evaluate(self, parameters: NDArrays, config: Config) -&gt; tuple[float, int, dict[str, Scalar]]:\n        \"\"\"\n        Evaluates the model on the validation set, and test set (if defined).\n\n        Args:\n            parameters (NDArrays): The parameters of the model to be evaluated.\n            config (NDArrays): The config object from the server.\n\n        Returns:\n            (tuple[float, int, dict[str, Scalar]]): A loss associated with the evaluation, the number of samples in the\n                validation/test set and the ``metric_values`` associated with evaluation.\n        \"\"\"\n        if not self.initialized:\n            self.setup_client(config)\n\n        start_time = datetime.datetime.now()\n        current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n\n        pack_losses_with_val_metrics = set_pack_losses_with_val_metrics(config)\n\n        self.set_parameters(parameters, config, fitting_round=False)\n        loss, metrics = self.validate(pack_losses_with_val_metrics)\n        end_time = datetime.datetime.now()\n        elapsed = end_time - start_time\n\n        # Checkpoint based on the loss and metrics produced during validation AFTER server-side aggregation\n        # NOTE: This assumes that the loss returned in the checkpointing loss\n        self._maybe_checkpoint(loss, metrics, CheckpointMode.POST_AGGREGATION)\n\n        self.reports_manager.report(\n            {\n                \"eval_round_metrics\": metrics,\n                \"eval_round_loss\": loss,\n                \"eval_round_start\": str(start_time),\n                \"eval_round_time_elapsed\": round(elapsed.total_seconds()),\n                \"eval_round_end\": str(end_time),\n                \"fit_step\": self.total_steps,\n                \"fit_epoch\": self.total_epochs,\n                \"round\": current_server_round,\n            },\n            current_server_round,\n        )\n\n        # EvaluateRes should return the loss, number of examples on client, and a dictionary holding metrics\n        # calculation results.\n        return (\n            loss,\n            self.num_val_samples,\n            metrics,\n        )\n\n    def _should_evaluate_after_fit(self, evaluate_after_fit: bool) -&gt; bool:\n        \"\"\"\n        Function to determine whether to trigger an evaluation of the model on the validation set immediately after\n        completing the local training round. The user can request this explicitly by setting evaluate_after_fit to\n        true in the config, or implicitly by specifying a pre-aggregation checkpoint module.\n\n        Args:\n            evaluate_after_fit (bool): Whether the user explicitly specified that they would like an evaluate after\n                fit to be performed through the config.\n\n        Returns:\n            (bool): Whether to perform an evaluation on the client validation set after fitting.\n        \"\"\"\n        pre_aggregation_checkpointing_enabled = (\n            self.checkpoint_and_state_module is not None\n            and self.checkpoint_and_state_module.pre_aggregation is not None\n        )\n        return evaluate_after_fit or pre_aggregation_checkpointing_enabled\n\n    def _log_header_str(\n        self,\n        current_round: int | None = None,\n        current_epoch: int | None = None,\n        logging_mode: LoggingMode = LoggingMode.TRAIN,\n    ) -&gt; None:\n        \"\"\"\n        Logs a header string. By default this is logged at the beginning of each local epoch or at the beginning of\n        the round if training by steps.\n\n        Args:\n            current_round (int | None, optional): The current FL round. (Ie current server round). Defaults to None.\n            current_epoch (int | None, optional): The current epoch of local training. Defaults to None.\n            logging_mode (LoggingMode, optional): The logging mode to be used in logging. This mainly changes the\n                way in which logging is decorated. Defaults to LoggingMode.TRAIN.\n        \"\"\"\n        log_str = f\"Current FL Round: {int(current_round)} \" if current_round is not None else \"\"\n        log_str += f\"Current Epoch: {int(current_epoch)} \" if current_epoch is not None else \"\"\n\n        # Maybe add client specific info to initial log string\n        client_str, _ = self.get_client_specific_logs(current_round, current_epoch, logging_mode)\n\n        log_str += client_str\n\n        log(INFO, \"\")  # For aesthetics\n        log(INFO, log_str)\n\n    def _log_results(\n        self,\n        loss_dict: dict[str, float],\n        metrics_dict: dict[str, Scalar],\n        current_round: int | None = None,\n        current_epoch: int | None = None,\n        logging_mode: LoggingMode = LoggingMode.TRAIN,\n    ) -&gt; None:\n        \"\"\"\n        Handles the logging of losses, metrics, and other information to the output file. Called only at the end of\n        an epoch or server round.\n\n        Args:\n            loss_dict (dict[str, float]): A dictionary of losses to log.\n            metrics_dict (dict[str, Scalar]): A dictionary of the metric to log.\n            current_round (int | None): The current FL round (i.e., current server round).\n            current_epoch (int | None): The current epoch of local training.\n            logging_mode (LoggingMode): The logging mode (Training, Validation, or Testing).\n        \"\"\"\n        _, client_logs = self.get_client_specific_logs(current_round, current_epoch, logging_mode)\n\n        # Get Metric Prefix\n        metric_prefix = logging_mode.value\n\n        # Log losses if any were provided\n        if len(loss_dict.keys()) &gt; 0:\n            log(INFO, f\"Client {metric_prefix} Losses:\")\n            [log(INFO, f\"\\t {key}: {str(val)}\") for key, val in loss_dict.items()]\n\n        # Log metrics if any\n        if len(metrics_dict.keys()) &gt; 0:\n            log(INFO, f\"Client {metric_prefix} Metrics:\")\n            [log(INFO, f\"\\t {key}: {str(val)}\") for key, val in metrics_dict.items()]\n\n        # Add additional logs specific to client\n        if len(client_logs) &gt; 0:\n            [log(level.value, msg) for level, msg in client_logs]\n\n    def get_client_specific_logs(\n        self,\n        current_round: int | None,\n        current_epoch: int | None,\n        logging_mode: LoggingMode,\n    ) -&gt; tuple[str, list[tuple[LogLevel, str]]]:\n        \"\"\"\n        This function can be overridden to provide any client specific information to the basic client logging.\n        For example, perhaps a client uses an LR scheduler and wants the LR to be logged each epoch. Called at the\n        beginning and end of each server round or local epoch. Also called at the end of validation/testing.\n\n        Args:\n            current_round (int | None): The current FL round (i.e., current server round).\n            current_epoch (int | None): The current epoch of local training.\n            logging_mode (LoggingMode): The logging mode (Training, Validation, or Testing).\n\n        Returns:\n            (tuple[str, list[tuple[LogLevel, str]]]):\n\n                - A string to append to the header log string that typically announces the current server round and\n                  current epoch at the beginning of each round or local epoch.\n                - A list of tuples where the first element is a ``LogLevel`` as defined in ``fl4health.utils.``\n                  typing and the second element is a string message. Each item in the list will be logged at the end of\n                  each server round or epoch. Elements will also be logged at the end of validation/testing.\n        \"\"\"\n        return \"\", []\n\n    def get_client_specific_reports(self) -&gt; dict[str, Any]:\n        \"\"\"\n        This function can be overridden by an inheriting client to report additional client specific information to\n        the ``wandb_reporter``.\n\n        Returns:\n            (dict[str, Any]): A dictionary of things to report.\n        \"\"\"\n        return {}\n\n    def update_metric_manager(\n        self,\n        preds: TorchPredType,\n        target: TorchTargetType,\n        metric_manager: MetricManager,\n    ) -&gt; None:\n        \"\"\"\n        Updates a metric manager with the provided model predictions and targets. Can be overridden to modify pred and\n        target inputs to the metric manager. This is useful in cases where the preds and targets needed to compute the\n        loss are different than what is needed to compute metrics.\n\n        Args:\n            preds (TorchPredType): The output predictions from the model returned by ``self.predict``.\n            target (TorchTargetType): The targets generated by the dataloader with which to evaluate the predictions.\n            metric_manager (MetricManager): The metric manager to update.\n        \"\"\"\n        metric_manager.update(preds, target)\n\n    def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Given a single batch of input and target data, generate predictions, compute loss, update parameters and\n        optionally update metrics if they exist. (i.e. backprop on a single batch of data).\n        Assumes ``self.model`` is in train mode already.\n\n        Args:\n            input (TorchInputType): The input to be fed into the model.\n            target (TorchTargetType): The target corresponding to the input.\n\n        Returns:\n            (tuple[TrainingLosses, TorchPredType]): The losses object from the train step along with a dictionary of\n                any predictions produced by the model.\n        \"\"\"\n        # Clear gradients from optimizer if they exist\n        self.optimizers[\"global\"].zero_grad()\n\n        # Call user defined methods to get predictions and compute loss\n        preds, features = self.predict(input)\n        target = self.transform_target(target)\n        losses = self.compute_training_loss(preds, features, target)\n\n        # Compute backward pass and update parameters with optimizer\n        losses.backward[\"backward\"].backward()\n        self.transform_gradients(losses)\n        self.optimizers[\"global\"].step()\n\n        return losses, preds\n\n    def val_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[EvaluationLosses, TorchPredType]:\n        \"\"\"\n        Given input and target, compute loss, update loss and metrics. Assumes ``self.model`` is in eval mode already.\n\n        Args:\n            input (TorchInputType): The input to be fed into the model.\n            target (TorchTargetType): The target corresponding to the input.\n\n        Returns:\n            (tuple[EvaluationLosses, TorchPredType]): The losses object from the val step along with a dictionary of\n                the predictions produced by the model.\n        \"\"\"\n        # Get preds and compute loss\n        with torch.no_grad():\n            preds, features = self.predict(input)\n            target = self.transform_target(target)\n            losses = self.compute_evaluation_loss(preds, features, target)\n\n        return losses, preds\n\n    def train_by_epochs(\n        self,\n        epochs: int,\n        current_round: int | None = None,\n    ) -&gt; tuple[dict[str, float], dict[str, Scalar]]:\n        \"\"\"\n        Train locally for the specified number of epochs.\n\n        Args:\n            epochs (int): The number of epochs for local training.\n            current_round (int | None, optional): The current FL round.\n\n        Returns:\n            (tuple[dict[str, float], dict[str, Scalar]]): The loss and metrics dictionary from the local training.\n                Loss is a dictionary of one or more losses that represent the different components of the loss.\n        \"\"\"\n        self.model.train()\n        steps_this_round = 0  # Reset number of steps this round\n        report_data: dict[str, Any] = {\"round\": current_round}\n        continue_training = True\n        for local_epoch in range(epochs):\n            self.train_metric_manager.clear()\n            self.train_loss_meter.clear()\n            # Print initial log string on epoch start\n            self._log_header_str(current_round, local_epoch)\n            # update before epoch hook\n            self.update_before_epoch(epoch=local_epoch)\n            # Update report data dict\n            report_data.update({\"fit_epoch\": self.total_epochs})\n            for input, target in maybe_progress_bar(self.train_loader, self.progress_bar):\n                self.update_before_step(steps_this_round, current_round)\n                # Assume first dimension is batch size. Sampling iterators (such as Poisson batch sampling), can\n                # construct empty batches. We skip the iteration if this occurs.\n                if check_if_batch_is_empty_and_verify_input(input):\n                    log(INFO, \"Empty batch generated by data loader. Skipping step.\")\n                    continue\n\n                input = move_data_to_device(input, self.device)\n                target = move_data_to_device(target, self.device)\n                losses, preds = self.train_step(input, target)\n                self.train_loss_meter.update(losses)\n                self.update_metric_manager(preds, target, self.train_metric_manager)\n                self.update_after_step(steps_this_round, current_round)\n                self.update_lr_schedulers(epoch=local_epoch)\n                report_data.update({\"fit_step_losses\": losses.as_dict(), \"fit_step\": self.total_steps})\n                report_data.update(self.get_client_specific_reports())\n                self.reports_manager.report(report_data, current_round, self.total_epochs, self.total_steps)\n                self.total_steps += 1\n                steps_this_round += 1\n                if self.early_stopper is not None and self.early_stopper.should_stop(steps_this_round):\n                    log(INFO, \"Early stopping criterion met. Stopping training.\")\n                    self.early_stopper.load_snapshot()\n                    continue_training = False\n                    break\n\n            # Log and report results\n            metrics = self.train_metric_manager.compute()\n            loss_dict = self.train_loss_meter.compute().as_dict()\n            report_data.update({\"fit_epoch_metrics\": metrics, \"fit_epoch_losses\": loss_dict})\n            report_data.update(self.get_client_specific_reports())\n            self.reports_manager.report(report_data, current_round, self.total_epochs)\n            self._log_results(loss_dict, metrics, current_round, local_epoch)\n\n            # Update internal epoch counter\n            self.total_epochs += 1\n\n            if not continue_training:\n                break\n\n        # Return final training metrics\n        return loss_dict, metrics\n\n    def train_by_steps(\n        self,\n        steps: int,\n        current_round: int | None = None,\n    ) -&gt; tuple[dict[str, float], dict[str, Scalar]]:\n        \"\"\"\n        Train locally for the specified number of steps.\n\n        Args:\n            steps (int): The number of steps to train locally.\n            current_round (int | None, optional): The current FL round\n\n        Returns:\n            (tuple[dict[str, float], dict[str, Scalar]]): The loss and metrics dictionary from the local training.\n                Loss is a dictionary of one or more losses that represent the different components of the loss.\n        \"\"\"\n        self.model.train()\n\n        # If the train_iterator hasn't been created before, we do so now.\n        if self.train_iterator is None:\n            # Pass loader to iterator so we can step through train loader\n            self.train_iterator = iter(self.train_loader)\n\n        self.train_loss_meter.clear()\n        self.train_metric_manager.clear()\n        self._log_header_str(current_round)\n        report_data: dict[str, Any] = {\"round\": current_round}\n        for step in maybe_progress_bar(range(steps), self.progress_bar):\n            self.update_before_step(step, current_round)\n\n            try:\n                input, target = next(self.train_iterator)\n            except StopIteration:\n                # StopIteration is thrown if dataset ends. Calling iter() on the dataloader resets the loader\n                # If shuffle=True for the dataloader, the data is also shuffled anew. If not, we pass through\n                # the data in the same order\n                self.train_iterator = iter(self.train_loader)\n                input, target = next(self.train_iterator)\n\n            # Assume first dimension is batch size. Sampling iterators (such as Poisson batch sampling), can\n            # construct empty batches. We skip the iteration if this occurs.\n            if check_if_batch_is_empty_and_verify_input(input):\n                log(INFO, \"Empty batch generated by data loader. Skipping step.\")\n                continue\n\n            input = move_data_to_device(input, self.device)\n            target = move_data_to_device(target, self.device)\n            losses, preds = self.train_step(input, target)\n            self.train_loss_meter.update(losses)\n            self.update_metric_manager(preds, target, self.train_metric_manager)\n            self.update_after_step(step, current_round)\n            self.update_lr_schedulers(step=step)\n            report_data.update({\"fit_step_losses\": losses.as_dict(), \"fit_step\": self.total_steps})\n            report_data.update(self.get_client_specific_reports())\n            self.reports_manager.report(report_data, current_round, None, self.total_steps)\n            self.total_steps += 1\n            if self.early_stopper is not None and self.early_stopper.should_stop(step):\n                log(INFO, \"Early stopping criterion met. Stopping training.\")\n                self.early_stopper.load_snapshot()\n                break\n\n        loss_dict = self.train_loss_meter.compute().as_dict()\n        metrics = self.train_metric_manager.compute()\n\n        # Log and report results\n        self._log_results(loss_dict, metrics, current_round)\n\n        return loss_dict, metrics\n\n    def _validate_by_steps(\n        self, loss_meter: LossMeter, metric_manager: MetricManager, include_losses_in_metrics: bool = False\n    ) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Evaluate the model on the validation set for a fixed number of steps set by ``self.num_validation_steps``.\n\n        Args:\n            loss_meter (LossMeter): The meter to track the losses.\n            metric_manager (MetricManager): The manager to track the metrics.\n            include_losses_in_metrics (bool, optional): Whether or not to pack the additional losses into the metrics\n                dictionary. Defaults to False.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The loss and a dictionary of metrics from evaluation.\n        \"\"\"\n        assert self.num_validation_steps is not None, \"num_validation_steps must be defined to use this function\"\n\n        self.model.eval()\n        metric_manager.clear()\n        loss_meter.clear()\n\n        # If the val_iterator hasn't been created before, we do so now.\n        if self.val_iterator is None:\n            # Pass loader to iterator so we can step through validation loader\n            self.val_iterator = iter(self.val_loader)\n\n        with torch.no_grad():\n            for _ in maybe_progress_bar(range(self.num_validation_steps), self.progress_bar):\n                try:\n                    input, target = next(self.val_iterator)\n                except StopIteration:\n                    # StopIteration is thrown if dataset ends. Calling iter() on the dataloader resets the loader\n                    # If shuffle=True for the dataloader, the data is also shuffled anew. If not, we pass through\n                    # the data in the same order\n                    self.val_iterator = iter(self.val_loader)\n                    input, target = next(self.val_iterator)\n\n                input = move_data_to_device(input, self.device)\n                target = move_data_to_device(target, self.device)\n                losses, preds = self.val_step(input, target)\n                loss_meter.update(losses)\n                self.update_metric_manager(preds, target, metric_manager)\n\n        # Compute losses and metrics over validation set\n        loss_dict = loss_meter.compute().as_dict()\n        metrics = metric_manager.compute()\n        self._log_results(loss_dict, metrics, logging_mode=LoggingMode.VALIDATION)\n\n        if include_losses_in_metrics:\n            fold_loss_dict_into_metrics(metrics, loss_dict, LoggingMode.VALIDATION)\n\n        return loss_dict[\"checkpoint\"], metrics\n\n    def _fully_validate_or_test(\n        self,\n        loader: DataLoader,\n        loss_meter: LossMeter,\n        metric_manager: MetricManager,\n        logging_mode: LoggingMode = LoggingMode.VALIDATION,\n        include_losses_in_metrics: bool = False,\n    ) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Evaluate the model on the given validation or test dataset.\n\n        Args:\n            loader (DataLoader): The data loader for the dataset (validation or test).\n            loss_meter (LossMeter): The meter to track the losses.\n            metric_manager (MetricManager): The manager to track the metrics.\n            logging_mode (LoggingMode, optional): The ``LoggingMode`` for whether this evaluation is for validation or\n                test. Defaults to ``LoggingMode.VALIDATION``.\n            include_losses_in_metrics (bool, optional): Whether or not to pack the additional losses into the metrics\n                dictionary. Defaults to False.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The loss and a dictionary of metrics from evaluation.\n        \"\"\"\n        assert logging_mode in [LoggingMode.VALIDATION, LoggingMode.TEST], \"logging_mode must be VALIDATION or TEST\"\n\n        self.model.eval()\n        metric_manager.clear()\n        loss_meter.clear()\n        with torch.no_grad():\n            for input, target in maybe_progress_bar(loader, self.progress_bar):\n                input = move_data_to_device(input, self.device)\n                target = move_data_to_device(target, self.device)\n                losses, preds = self.val_step(input, target)\n                loss_meter.update(losses)\n                self.update_metric_manager(preds, target, metric_manager)\n\n        # Compute losses and metrics over validation set\n        loss_dict = loss_meter.compute().as_dict()\n        metrics = metric_manager.compute()\n        self._log_results(loss_dict, metrics, logging_mode=logging_mode)\n\n        if include_losses_in_metrics:\n            fold_loss_dict_into_metrics(metrics, loss_dict, logging_mode)\n\n        return loss_dict[\"checkpoint\"], metrics\n\n    def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Validate the current model on the entire validation (or a subset thereof if ``num_validation_steps`` is not\n        None) and potentially an entire test dataset if it has been defined.\n\n        Args:\n            include_losses_in_metrics (bool, optional): Determines whether to include the calculated losses into the\n                metrics that are sent back to the server. Defaults to False.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation\n                (and test if present).\n        \"\"\"\n        if self.num_validation_steps is None:\n            val_loss, val_metrics = self._fully_validate_or_test(\n                self.val_loader,\n                self.val_loss_meter,\n                self.val_metric_manager,\n                include_losses_in_metrics=include_losses_in_metrics,\n            )\n        else:\n            val_loss, val_metrics = self._validate_by_steps(\n                self.val_loss_meter,\n                self.val_metric_manager,\n                include_losses_in_metrics=include_losses_in_metrics,\n            )\n\n        if self.test_loader:\n            test_loss, test_metrics = self._fully_validate_or_test(\n                self.test_loader,\n                self.test_loss_meter,\n                self.test_metric_manager,\n                LoggingMode.TEST,\n                include_losses_in_metrics=include_losses_in_metrics,\n            )\n            # There will be no clashes due to the naming convention associated with the metric managers\n            if self.num_test_samples is not None:\n                val_metrics[TEST_NUM_EXAMPLES_KEY] = self.num_test_samples\n            val_metrics[TEST_LOSS_KEY] = test_loss\n            val_metrics.update(test_metrics)\n\n        return val_loss, val_metrics\n\n    def get_properties(self, config: Config) -&gt; dict[str, Scalar]:\n        \"\"\"\n        Return properties (train and validation dataset sample counts) of client.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (dict[str, Scalar]): A dictionary with two entries corresponding to the sample counts in\n                the train and validation set.\n        \"\"\"\n        if not self.initialized:\n            self.setup_client(config)\n\n        return {\n            \"num_train_samples\": self.num_train_samples,\n            \"num_val_samples\": self.num_val_samples,\n        }\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n        Then set initialized attribute to True.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        # Explicitly send the model to the desired device. This is idempotent.\n        self.model = self.get_model(config).to(self.device)\n        train_loader, val_loader = self.get_data_loaders(config)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.test_loader = self.get_test_data_loader(config)\n\n        self.num_validation_steps = process_and_check_validation_steps(config, self.val_loader)\n\n        # The following lines are type ignored because torch datasets are not \"Sized\"\n        # IE __len__ is considered optionally defined. In practice, it is almost always defined\n        # and as such, we will make that assumption.\n        self.num_train_samples = len(self.train_loader.dataset)  # type: ignore\n\n        # if num_validation_steps is defined, the number of validation samples seen is\n        # batch_size * num_validation_steps\n        self.num_val_samples = len(self.val_loader.dataset)  # type: ignore\n        if self.num_validation_steps is not None:\n            assert self.val_loader.batch_size is not None, (\n                \"Validation batch size must be defined if we want to limit the number of validation steps\"\n            )\n            self.num_val_samples = self.num_validation_steps * self.val_loader.batch_size\n\n        if self.test_loader:\n            self.num_test_samples = len(self.test_loader.dataset)  # type: ignore\n\n        self.set_optimizer(config)\n\n        # Must initialize LR scheduler after parent method initializes optimizer\n        # Add lr_scheduler to dictionary if user overrides get_lr_scheduler to return\n        # scheduler for given optimizer\n        self.lr_schedulers = {}\n        for optimizer_key in self.optimizers:\n            lr_scheduler = self.get_lr_scheduler(optimizer_key, config)\n            if lr_scheduler is not None:\n                self.lr_schedulers[optimizer_key] = lr_scheduler\n\n        self.criterion = self.get_criterion(config).to(self.device)\n        self.parameter_exchanger = self.get_parameter_exchanger(config)\n\n        self.reports_manager.report({\"host_type\": \"client\", \"initialized\": str(datetime.datetime.now())})\n        self.initialized = True\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        \"\"\"\n        Returns Full Parameter Exchangers. Subclasses that require custom Parameter Exchangers can override this.\n\n        Args:\n            config (Config): The config from server.\n\n        Returns:\n            (ParameterExchanger): Used to exchange parameters between server and client.\n        \"\"\"\n        return FullParameterExchanger()\n\n    def predict(self, input: TorchInputType) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Computes the prediction(s), and potentially features, of the model(s) given the input.\n\n        Args:\n            input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n                it is assumed that the keys of input match the names of the keyword arguments of\n                ``self.model.forward().``\n\n        Returns:\n            (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n                predictions indexed by name and the second element contains intermediate activations indexed by name.\n                By passing features, we can compute losses such as the contrastive loss in MOON. All predictions\n                included in dictionary will by default be used to compute metrics separately.\n\n        Raises:\n            TypeError: Occurs when something other than a tensor or dict of tensors is passed in to the model's\n                forward method.\n            ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n                forward.\n        \"\"\"\n        if isinstance(input, torch.Tensor):\n            output = self.model(input)\n        elif isinstance(input, dict):\n            # If input is a dictionary, then we unpack it before computing the forward pass.\n            # Note that this assumes the keys of the input match (exactly) the keyword args\n            # of self.model.forward().\n            output = self.model(**input)\n        else:\n            raise TypeError(\"'input' must be of type torch.Tensor or dict[str, torch.Tensor].\")\n\n        if isinstance(output, dict):\n            return output, {}\n        if isinstance(output, torch.Tensor):\n            return {\"prediction\": output}, {}\n        if isinstance(output, tuple):\n            if len(output) != EXPECTED_OUTPUT_TUPLE_SIZE:\n                raise ValueError(f\"Output tuple should have length 2 but has length {len(output)}\")\n            preds, features = output\n            return preds, features\n        raise ValueError(\"Model forward did not return a tensor, dictionary of tensors, or tuple of tensors\")\n\n    def compute_loss_and_additional_losses(\n        self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor] | None]:\n        \"\"\"\n        Computes the loss and any additional losses given predictions of the model and ground truth data.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor] | None]): A tuple with:\n\n                - The tensor for the loss.\n                - A dictionary of additional losses with their names and values, or None if there are no additional\n                  losses.\n        \"\"\"\n        return self.criterion(preds[\"prediction\"], target), None\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training loss given predictions (and potentially features) of the model and ground truth data.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n                in preds will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses\n                indexed by name.\n        \"\"\"\n        loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n        return TrainingLosses(backward=loss, additional_losses=additional_losses)\n\n    def compute_evaluation_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses:\n        \"\"\"\n        Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n                in preds will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n                indexed by name.\n        \"\"\"\n        loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n        return EvaluationLosses(checkpoint=loss, additional_losses=additional_losses)\n\n    def set_optimizer(self, config: Config) -&gt; None:\n        \"\"\"\n        Method called in the ``setup_client`` method to set optimizer attribute returned by used-defined\n        ``get_optimizer``. In the simplest case, ``get_optimizer`` returns an optimizer. For more advanced use cases\n        where a dictionary of string and optimizer are returned (i.e. APFL), the user must override this method.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        optimizer = self.get_optimizer(config)\n        assert not isinstance(optimizer, dict)\n        self.optimizers = {\"global\": optimizer}\n\n    def get_data_loaders(self, config: Config) -&gt; tuple[DataLoader, DataLoader]:\n        \"\"\"\n        User defined method that returns a PyTorch Train ``DataLoader`` and a PyTorch Validation ``DataLoader``.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (tuple[DataLoader, DataLoader]) Tuple of length 2. The client train and validation loader.\n\n        Raises:\n            NotImplementedError: To be defined in child class.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_test_data_loader(self, config: Config) -&gt; DataLoader | None:\n        \"\"\"\n        User defined method that returns a PyTorch Test DataLoader. By default, this function returns None,\n        assuming that there is no test dataset to be used. If the user would like to load and evaluate a dataset,\n        they need only override this function in their client class.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (DataLoader | None): The optional client test loader.\n        \"\"\"\n        return None\n\n    def transform_target(self, target: TorchTargetType) -&gt; TorchTargetType:\n        \"\"\"\n        Method that users can extend to specify an arbitrary transformation to apply to\n        the target prior to the loss being computed. Defaults to the identity transform.\n\n        Overriding this method can be useful in a variety of scenarios such as Self Supervised\n        Learning where the target is derived from the input sample itself. For example, the FedSimClr\n        reference implementation overrides this method to extract features from the target, which\n        is a transformed version of the input image itself.\n\n        Args:\n            target (TorchTargetType): The target or label used to compute the loss.\n\n        Returns:\n            (TorchTargetType): Identical to target.\n        \"\"\"\n        return target\n\n    def get_criterion(self, config: Config) -&gt; _Loss:\n        \"\"\"\n        User defined method that returns PyTorch loss to train model.\n\n        Args:\n            config (Config): The config from the server.\n\n        Raises:\n            NotImplementedError: To be defined in child class.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_optimizer(self, config: Config) -&gt; Optimizer | dict[str, Optimizer]:\n        \"\"\"\n        Method to be defined by user that returns the PyTorch optimizer used to train models locally\n        Return value can be a single torch optimizer or a dictionary of string and torch optimizer.\n        Returning multiple optimizers is useful in methods like APFL which has a different optimizer\n        for the local and global models.\n\n        Args:\n            config (Config): The config sent from the server.\n\n        Returns:\n            (Optimizer | dict[str, Optimizer]): An optimizer or dictionary of optimizers to train model.\n\n        Raises:\n            NotImplementedError: To be defined in child class.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_model(self, config: Config) -&gt; nn.Module:\n        \"\"\"\n        User defined method that returns PyTorch model.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (nn.Module): The client model.\n\n        Raises:\n            NotImplementedError: To be defined in child class.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_lr_scheduler(self, optimizer_key: str, config: Config) -&gt; LRScheduler | None:\n        \"\"\"\n        Optional user defined method that returns learning rate scheduler\n        to be used throughout training for the given optimizer. Defaults to None.\n\n        Args:\n            optimizer_key (str): The key in the optimizer dict corresponding\n                to the optimizer we are optionally defining a learning rate\n                scheduler for.\n            config (Config): The config from the server.\n\n        Returns:\n            (LRScheduler | None): Client learning rate schedulers.\n        \"\"\"\n        return None\n\n    def update_lr_schedulers(self, step: int | None = None, epoch: int | None = None) -&gt; None:\n        \"\"\"\n        Updates any schedulers that exist. Can be overridden to customize update logic based on client state\n        (i.e ``self.total_steps``).\n\n        Args:\n            step (int | None): If using ``local_steps``, current step of this round. Otherwise None.\n            epoch (int | None): If using ``local_epochs`` current epoch of this round. Otherwise None.\n        \"\"\"\n        assert (step is None) ^ (epoch is None)\n\n        for lr_scheduler in self.lr_schedulers.values():\n            lr_scheduler.step()  # Update LR\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        \"\"\"\n        Hook method called before training with the number of current server rounds performed.\n\n        **NOTE**: This method is called immediately **AFTER** the aggregated parameters are received from the server.\n        For example, used by MOON and FENDA to save global modules after aggregation.\n\n        Args:\n            current_server_round (int): The number of current server round.\n        \"\"\"\n        pass\n\n    def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n        \"\"\"\n        Hook method called after training with the number of ``local_steps`` performed over the FL round and\n        the corresponding loss dictionary. For example, used by Scaffold to update the control variates\n        after a local round of training. Also used by FedProx to update the current loss based on the loss\n        returned during training. Also used by MOON and FENDA to save trained modules weights before\n        aggregation.\n\n        Args:\n            local_steps (int): The number of steps so far in the round in the local training.\n            loss_dict (dict[str, float]): A dictionary of losses from local training.\n            config (Config): The config from the server\n        \"\"\"\n        pass\n\n    def update_before_step(self, step: int, current_round: int | None = None) -&gt; None:\n        \"\"\"\n        Hook method called before local train step.\n\n        Args:\n            step (int): The local training step that was most recently completed. Resets only at the end of the round.\n            current_round (int | None, optional): The current FL server round.\n        \"\"\"\n        pass\n\n    def update_after_step(self, step: int, current_round: int | None = None) -&gt; None:\n        \"\"\"\n        Hook method called after local train step on client. Step is an integer that represents the local training\n        step that was most recently completed. For example, used by the APFL method to update the alpha value after a\n        training a step. Also used by the MOON, FENDA and Ditto to update optimized beta value for MK-MMD loss after\n        n steps.\n\n        Args:\n            step (int): The step number in local training that was most recently completed. Resets only at the end of\n                the round.\n            current_round (int | None, optional): The current FL server round.\n        \"\"\"\n        pass\n\n    def update_before_epoch(self, epoch: int) -&gt; None:\n        \"\"\"\n        Hook method called before local epoch on client. Only called if client is being trained by epochs\n        (i.e. using ``local_epochs`` key instead of local steps in the server config file).\n\n        Args:\n            epoch (int): Integer representing the epoch about to begin\n        \"\"\"\n        pass\n\n    def transform_gradients(self, losses: TrainingLosses) -&gt; None:\n        \"\"\"\n        Hook function for model training only called after backwards pass but before optimizer step. Useful for\n        transforming the gradients (such as with gradient clipping) before they are applied to the model weights.\n\n        Args:\n            losses (TrainingLosses): The losses object from the train step\n        \"\"\"\n        pass\n\n    def _save_client_state(self) -&gt; None:\n        \"\"\"\n        Save a checkpoint of the client's state as defined by the state_checkpointer's snapshot_attrs.\n        By default, snapshot_attrs includes attributes such as client name, total steps, lr schedulers,\n        metrics reporter, and optimizer states. You can override snapshot_attrs in the state_checkpointer to\n        customize which attributes are saved in the checkpoint.\n        \"\"\"\n        assert self.checkpoint_and_state_module.state_checkpointer is not None\n        self.checkpoint_and_state_module.save_state(self)\n\n    def _load_client_state(self) -&gt; bool:\n        \"\"\"\n        Load checkpoint dict consisting of client name, total steps, lr schedulers, metrics reporter and optimizers\n        state. Method can be overridden to augment loaded checkpointed state.\n        \"\"\"\n        assert self.checkpoint_and_state_module.state_checkpointer is not None\n        log(INFO, \"Loading client state from checkpoint\")\n        return self.checkpoint_and_state_module.maybe_load_state(self)\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>Base FL Client with functionality to train, evaluate, log, report and checkpoint. User is responsible for implementing methods: <code>get_model</code>, <code>get_optimizer</code>, <code>get_data_loaders</code>, <code>get_criterion</code> Other methods can be overridden to achieve custom functionality.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Base FL Client with functionality to train, evaluate, log, report and checkpoint.\n    User is responsible for implementing methods: ``get_model``, ``get_optimizer``, ``get_data_loaders``,\n    ``get_criterion`` Other methods can be overridden to achieve custom functionality.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    self.data_path = data_path\n    self.device = device\n    self.metrics = metrics\n    self.progress_bar = progress_bar\n\n    self.client_name = client_name if client_name is not None else generate_hash()\n    log(INFO, f\"Client Name: {self.client_name}\")\n\n    if checkpoint_and_state_module is not None:\n        self.checkpoint_and_state_module = checkpoint_and_state_module\n    else:\n        # Define a default module that does nothing.\n        self.checkpoint_and_state_module = ClientCheckpointAndStateModule(\n            pre_aggregation=None, post_aggregation=None, state_checkpointer=None\n        )\n\n    # Initialize reporters with client information.\n    self.reports_manager = ReportsManager(reporters)\n    self.reports_manager.initialize(id=self.client_name, name=self.client_name)\n\n    self.initialized = False  # Whether or not the client has been setup\n\n    # Loss and Metric management\n    self.train_loss_meter = LossMeter[TrainingLosses](loss_meter_type, TrainingLosses)\n    self.val_loss_meter = LossMeter[EvaluationLosses](loss_meter_type, EvaluationLosses)\n    self.train_metric_manager = MetricManager(metrics=self.metrics, metric_manager_name=\"train\")\n    self.val_metric_manager = MetricManager(metrics=self.metrics, metric_manager_name=\"val\")\n    self.test_loss_meter = LossMeter[EvaluationLosses](loss_meter_type, EvaluationLosses)\n    self.test_metric_manager = MetricManager(metrics=self.metrics, metric_manager_name=\"test\")\n\n    # Optional variable to store the weights that the client was initialized with during each round of training\n    self.initial_weights: NDArrays | None = None\n\n    self.total_steps: int = 0  # Need to track total_steps across rounds for WANDB reporting\n    self.total_epochs: int = 0  # Will remain as 0 if training by steps\n\n    # Attributes to be initialized in setup_client\n    self.parameter_exchanger: ParameterExchanger\n    self.model: nn.Module\n    self.optimizers: dict[str, torch.optim.Optimizer]\n    self.train_loader: DataLoader\n    self.val_loader: DataLoader\n    self.test_loader: DataLoader | None\n    self.num_train_samples: int\n    self.num_val_samples: int\n    self.num_test_samples: int | None\n    self.learning_rate: float | None\n\n    # User can set the early stopper for the client by instantiating the EarlyStopper class\n    # and setting the patience and interval_steps attributes. The early stopper will be used to\n    # stop training if the validation loss does not improve for a certain number of steps.\n    self.early_stopper: EarlyStopper | None = None\n    # Config can contain num_validation_steps key, which determines an upper bound\n    # for the validation steps taken. If not specified, no upper bound will be enforced.\n    # By specifying this in the config we cannot guarantee the validation set is the same\n    # across rounds for clients.\n    self.num_validation_steps: int | None = None\n    # NOTE: These iterators are of type _BaseDataLoaderIter, which is not importable...so we're forced to use\n    # Iterator\n    self.train_iterator: Iterator | None = None\n    self.val_iterator: Iterator | None = None\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>Determines which parameters are sent back to the server for aggregation. This uses a parameter exchanger to determine parameters sent.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>These are the parameters to be sent to the server. At minimum they represent the relevant model parameters to be aggregated, but can contain more information.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_parameters(self, config: Config) -&gt; NDArrays:\n    \"\"\"\n    Determines which parameters are sent back to the server for aggregation. This uses a parameter exchanger to\n    determine parameters sent.\n\n    Args:\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n    Returns:\n        (NDArrays): These are the parameters to be sent to the server. At minimum they represent the relevant model\n            parameters to be aggregated, but can contain more information.\n    \"\"\"\n    if not self.initialized:\n        return self.setup_client_and_return_all_model_parameters(config)\n\n    assert self.model is not None and self.parameter_exchanger is not None\n    # If the client has early stopping module and the patience is None, we load the best saved state\n    # to send the best checkpointed local model's parameters to the server\n    self._maybe_load_saved_best_local_model_state()\n    return self.parameter_exchanger.push_parameters(self.model, config=config)\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>Sets the local model parameters transferred from the server using a parameter exchanger to coordinate how parameters are set. In the first fitting round, we assume the full model is being initialized and use the <code>FullParameterExchanger()</code> to set all model weights. Otherwise, we use the appropriate parameter exchanger defined by the user depending on the federated learning algorithm being used.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model but may contain more information than that.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. This is used to help determine which parameter exchange should be used for pulling parameters. A full parameter exchanger is only used if the current federated learning round is the very first fitting round.</p> required Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n    \"\"\"\n    Sets the local model parameters transferred from the server using a parameter exchanger to coordinate how\n    parameters are set. In the first fitting round, we assume the full model is being\n    initialized and use the ``FullParameterExchanger()`` to set all model weights.\n    Otherwise, we use the appropriate parameter exchanger defined by the user depending on the\n    federated learning algorithm being used.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model but may contain more information than that.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning round is a\n            fitting round or an evaluation round.\n            This is used to help determine which parameter exchange should be used for pulling parameters.\n            A full parameter exchanger is only used if the current federated learning round is the very\n            first fitting round.\n    \"\"\"\n    assert self.model is not None\n    current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n    if current_server_round == 1 and fitting_round:\n        self.initialize_all_model_weights(parameters, config)\n    else:\n        assert self.parameter_exchanger is not None\n        self.parameter_exchanger.pull_parameters(parameters, self.model, config)\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.initialize_all_model_weights","title":"<code>initialize_all_model_weights(parameters, config)</code>","text":"<p>If this is the first time we're initializing the model weights, we use the <code>FullParameterExchanger</code> to initialize all model components. Subclasses that require custom model initialization can override this.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Model parameters to be injected into the client model.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def initialize_all_model_weights(self, parameters: NDArrays, config: Config) -&gt; None:\n    \"\"\"\n    If this is the first time we're initializing the model weights, we use the ``FullParameterExchanger`` to\n    initialize all model components. Subclasses that require custom model initialization can override this.\n\n    Args:\n        parameters (NDArrays): Model parameters to be injected into the client model.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n    \"\"\"\n    FullParameterExchanger().pull_parameters(parameters, self.model, config)\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.setup_client_and_return_all_model_parameters","title":"<code>setup_client_and_return_all_model_parameters(config)</code>","text":"<p>Function used to setup the client using the provided configuration and then exact all model parameters from <code>self.model</code> and return them. This function is used as a helper for <code>get_parameters</code> when the client has yet to be initialized.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration to be used  in setting up the client.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>All parameters associated with the <code>self.model</code> property of the client.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def setup_client_and_return_all_model_parameters(self, config: Config) -&gt; NDArrays:\n    \"\"\"\n    Function used to setup the client using the provided configuration and then exact all model parameters from\n    ``self.model`` and return them. This function is used as a helper for ``get_parameters`` when the client\n    has yet to be initialized.\n\n    Args:\n        config (Config): Configuration to be used  in setting up the client.\n\n    Returns:\n        (NDArrays): All parameters associated with the ``self.model`` property of the client.\n    \"\"\"\n    log(INFO, \"Setting up client and providing full model parameters to the server for initialization\")\n    if not config:\n        log(\n            WARNING,\n            (\n                \"This client has not yet been initialized and the config is empty. This may cause unexpected \"\n                \"failures, as setting up a client typically requires several configuration parameters, \"\n                \"including batch_size and current_server_round.\"\n            ),\n        )\n\n    # If initialized is False, the server is requesting model parameters from which to initialize all other\n    # clients. As such get_parameters is being called before fit or evaluate, so we must call\n    # setup_client first.\n    self.setup_client(config)\n\n    # Need all parameters even if normally exchanging partial\n    return FullParameterExchanger().push_parameters(self.model, config=config)\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.shutdown","title":"<code>shutdown()</code>","text":"<p>Shuts down the client. Involves shutting down W&amp;B reporter if one exists.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"Shuts down the client. Involves shutting down W&amp;B reporter if one exists.\"\"\"\n    # Shutdown reporters\n    self.reports_manager.report({\"shutdown\": str(datetime.datetime.now())})\n    self.reports_manager.shutdown()\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.process_config","title":"<code>process_config(config)</code>","text":"<p>Method to ensure the required keys are present in config and extracts values to be returned.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>tuple[int | None, int | None, int, bool, bool]</code> <p>Returns the <code>local_epochs</code>, <code>local_steps</code>, <code>current_server_round</code>, <code>evaluate_after_fit</code> and <code>pack_losses_with_val_metrics</code>. Ensures only one of <code>local_epochs</code> and <code>local_steps</code> is defined in the config and sets the one that is not to None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the config contains both <code>local_steps</code> and local epochs or if <code>local_steps</code>, <code>local_epochs</code> or <code>current_server_round</code> is of the wrong type (int).</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def process_config(self, config: Config) -&gt; tuple[int | None, int | None, int, bool, bool]:\n    \"\"\"\n    Method to ensure the required keys are present in config and extracts values to be returned.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (tuple[int | None, int | None, int, bool, bool]): Returns the ``local_epochs``, ``local_steps``,\n            ``current_server_round``, ``evaluate_after_fit`` and ``pack_losses_with_val_metrics``. Ensures only\n            one of ``local_epochs`` and ``local_steps`` is defined in the config and sets the one that is not to\n            None.\n\n    Raises:\n        ValueError: If the config contains both ``local_steps`` and local epochs or if ``local_steps``,\n            ``local_epochs`` or ``current_server_round`` is of the wrong type (int).\n    \"\"\"\n    current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n\n    # Parse config to determine train by steps or train by epochs\n    if (\"local_epochs\" in config) and (\"local_steps\" in config):\n        raise ValueError(\"Config cannot contain both local_epochs and local_steps. Please specify only one.\")\n    if \"local_epochs\" in config:\n        local_epochs = narrow_dict_type(config, \"local_epochs\", int)\n        local_steps = None\n    elif \"local_steps\" in config:\n        local_steps = narrow_dict_type(config, \"local_steps\", int)\n        local_epochs = None\n    else:\n        raise ValueError(\"Must specify either local_epochs or local_steps in the Config.\")\n\n    try:\n        evaluate_after_fit = narrow_dict_type(config, \"evaluate_after_fit\", bool)\n    except ValueError:\n        evaluate_after_fit = False\n\n    pack_losses_with_val_metrics = set_pack_losses_with_val_metrics(config)\n\n    # Either local epochs or local steps is none based on what key is passed in the config\n    return local_epochs, local_steps, current_server_round, evaluate_after_fit, pack_losses_with_val_metrics\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.fit","title":"<code>fit(parameters, config)</code>","text":"<p>Processes config, initializes client (if first round) and performs training based on the passed config. If <code>per_round_checkpointer</code> is not None, on initialization the client checks if a checkpointed client state exists to load and at the end of each round the client state is saved.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>The parameters of the model to be used in fit.</p> required <code>config</code> <code>NDArrays</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>tuple[NDArrays, int, dict[str, Scalar]]</code> <p>The parameters following the local training along with the number of samples in the local training dataset and the computed metrics throughout the fit.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>local_steps</code> or <code>local_epochs</code> is not specified in config.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def fit(self, parameters: NDArrays, config: Config) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n    \"\"\"\n    Processes config, initializes client (if first round) and performs training based on the passed config.\n    If ``per_round_checkpointer`` is not None, on initialization the client checks if a checkpointed client state\n    exists to load and at the end of each round the client state is saved.\n\n    Args:\n        parameters (NDArrays): The parameters of the model to be used in fit.\n        config (NDArrays): The config from the server.\n\n    Returns:\n        (tuple[NDArrays, int, dict[str, Scalar]]): The parameters following the local training along with the\n            number of samples in the local training dataset and the computed metrics throughout the fit.\n\n    Raises:\n        ValueError: If ``local_steps`` or ``local_epochs`` is not specified in config.\n    \"\"\"\n    round_start_time = datetime.datetime.now()\n    local_epochs, local_steps, current_server_round, evaluate_after_fit, pack_losses_with_val_metrics = (\n        self.process_config(config)\n    )\n\n    if not self.initialized:\n        self.setup_client(config)\n\n        if self.checkpoint_and_state_module.state_checkpointer is not None:\n            # If this is the first time the client is being setup, we also attempt to load any existing state\n            # If no state exists, we assume this is a fresh run. State is useful, for example, in restarting FL\n            # training that was interrupted or failed part way through.\n            state_load_success = self._load_client_state()\n            if state_load_success:\n                log(INFO, \"Successfully loaded client state.\")\n            else:\n                log(INFO, \"Client state was not loaded.\")\n\n    self.set_parameters(parameters, config, fitting_round=True)\n\n    self.update_before_train(current_server_round)\n\n    fit_start_time = datetime.datetime.now()\n    if local_epochs is not None:\n        loss_dict, metrics = self.train_by_epochs(local_epochs, current_server_round)\n        local_steps = len(self.train_loader) * local_epochs  # total steps over training round\n    elif local_steps is not None:\n        loss_dict, metrics = self.train_by_steps(local_steps, current_server_round)\n    else:\n        raise ValueError(\"Must specify either local_epochs or local_steps in the Config.\")\n    fit_end_time = datetime.datetime.now()\n\n    # Perform necessary updates after training has completed for the current FL round\n    self.update_after_train(local_steps, loss_dict, config)\n\n    # Check if we should run an evaluation with validation data after fit\n    # (for example, this is used by FedDGGA)\n    if self._should_evaluate_after_fit(evaluate_after_fit):\n        validation_loss, validation_metrics = self.validate(pack_losses_with_val_metrics)\n        metrics.update(validation_metrics)\n        # We perform a pre-aggregation checkpoint if applicable\n        self._maybe_checkpoint(validation_loss, validation_metrics, CheckpointMode.PRE_AGGREGATION)\n\n    # Notes on report values:\n    #   - Train by steps: round metrics/losses are computed using all samples from the round\n    #   - Train by epochs: round metrics/losses computed using only the samples from the final epoch of the round\n    #   - fit_round_metrics: Computed at the end of the round on the samples directly\n    #   - fit_round_losses: The average of the losses computed for each step.\n    #       * (Hence likely higher than the final loss of the round.)\n    self.reports_manager.report(\n        {\n            \"fit_round_metrics\": metrics,\n            \"fit_round_losses\": loss_dict,\n            \"round\": current_server_round,\n            \"round_start\": str(round_start_time),\n            \"round_end\": str(datetime.datetime.now()),\n            \"fit_round_start\": str(fit_start_time),\n            \"fit_round_time_elapsed\": round((fit_end_time - fit_start_time).total_seconds()),\n            \"fit_round_end\": str(fit_end_time),\n            \"fit_step\": self.total_steps,\n            \"fit_epoch\": self.total_epochs,\n        },\n        current_server_round,\n    )\n\n    # After local client training has finished, checkpoint client state if a state checkpointer is defined\n    if self.checkpoint_and_state_module.state_checkpointer is not None:\n        self._save_client_state()\n\n    # FitRes should contain local parameters, number of examples on client, and a dictionary holding metrics\n    # calculation results.\n    return (\n        self.get_parameters(config),\n        self.num_train_samples,\n        metrics,\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.evaluate","title":"<code>evaluate(parameters, config)</code>","text":"<p>Evaluates the model on the validation set, and test set (if defined).</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>The parameters of the model to be evaluated.</p> required <code>config</code> <code>NDArrays</code> <p>The config object from the server.</p> required <p>Returns:</p> Type Description <code>tuple[float, int, dict[str, Scalar]]</code> <p>A loss associated with the evaluation, the number of samples in the validation/test set and the <code>metric_values</code> associated with evaluation.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def evaluate(self, parameters: NDArrays, config: Config) -&gt; tuple[float, int, dict[str, Scalar]]:\n    \"\"\"\n    Evaluates the model on the validation set, and test set (if defined).\n\n    Args:\n        parameters (NDArrays): The parameters of the model to be evaluated.\n        config (NDArrays): The config object from the server.\n\n    Returns:\n        (tuple[float, int, dict[str, Scalar]]): A loss associated with the evaluation, the number of samples in the\n            validation/test set and the ``metric_values`` associated with evaluation.\n    \"\"\"\n    if not self.initialized:\n        self.setup_client(config)\n\n    start_time = datetime.datetime.now()\n    current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n\n    pack_losses_with_val_metrics = set_pack_losses_with_val_metrics(config)\n\n    self.set_parameters(parameters, config, fitting_round=False)\n    loss, metrics = self.validate(pack_losses_with_val_metrics)\n    end_time = datetime.datetime.now()\n    elapsed = end_time - start_time\n\n    # Checkpoint based on the loss and metrics produced during validation AFTER server-side aggregation\n    # NOTE: This assumes that the loss returned in the checkpointing loss\n    self._maybe_checkpoint(loss, metrics, CheckpointMode.POST_AGGREGATION)\n\n    self.reports_manager.report(\n        {\n            \"eval_round_metrics\": metrics,\n            \"eval_round_loss\": loss,\n            \"eval_round_start\": str(start_time),\n            \"eval_round_time_elapsed\": round(elapsed.total_seconds()),\n            \"eval_round_end\": str(end_time),\n            \"fit_step\": self.total_steps,\n            \"fit_epoch\": self.total_epochs,\n            \"round\": current_server_round,\n        },\n        current_server_round,\n    )\n\n    # EvaluateRes should return the loss, number of examples on client, and a dictionary holding metrics\n    # calculation results.\n    return (\n        loss,\n        self.num_val_samples,\n        metrics,\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_client_specific_logs","title":"<code>get_client_specific_logs(current_round, current_epoch, logging_mode)</code>","text":"<p>This function can be overridden to provide any client specific information to the basic client logging. For example, perhaps a client uses an LR scheduler and wants the LR to be logged each epoch. Called at the beginning and end of each server round or local epoch. Also called at the end of validation/testing.</p> <p>Parameters:</p> Name Type Description Default <code>current_round</code> <code>int | None</code> <p>The current FL round (i.e., current server round).</p> required <code>current_epoch</code> <code>int | None</code> <p>The current epoch of local training.</p> required <code>logging_mode</code> <code>LoggingMode</code> <p>The logging mode (Training, Validation, or Testing).</p> required <p>Returns:</p> Type Description <code>tuple[str, list[tuple[LogLevel, str]]]</code> <ul> <li>A string to append to the header log string that typically announces the current server round and   current epoch at the beginning of each round or local epoch.</li> <li>A list of tuples where the first element is a <code>LogLevel</code> as defined in <code>fl4health.utils.</code>   typing and the second element is a string message. Each item in the list will be logged at the end of   each server round or epoch. Elements will also be logged at the end of validation/testing.</li> </ul> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_client_specific_logs(\n    self,\n    current_round: int | None,\n    current_epoch: int | None,\n    logging_mode: LoggingMode,\n) -&gt; tuple[str, list[tuple[LogLevel, str]]]:\n    \"\"\"\n    This function can be overridden to provide any client specific information to the basic client logging.\n    For example, perhaps a client uses an LR scheduler and wants the LR to be logged each epoch. Called at the\n    beginning and end of each server round or local epoch. Also called at the end of validation/testing.\n\n    Args:\n        current_round (int | None): The current FL round (i.e., current server round).\n        current_epoch (int | None): The current epoch of local training.\n        logging_mode (LoggingMode): The logging mode (Training, Validation, or Testing).\n\n    Returns:\n        (tuple[str, list[tuple[LogLevel, str]]]):\n\n            - A string to append to the header log string that typically announces the current server round and\n              current epoch at the beginning of each round or local epoch.\n            - A list of tuples where the first element is a ``LogLevel`` as defined in ``fl4health.utils.``\n              typing and the second element is a string message. Each item in the list will be logged at the end of\n              each server round or epoch. Elements will also be logged at the end of validation/testing.\n    \"\"\"\n    return \"\", []\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_client_specific_reports","title":"<code>get_client_specific_reports()</code>","text":"<p>This function can be overridden by an inheriting client to report additional client specific information to the <code>wandb_reporter</code>.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary of things to report.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_client_specific_reports(self) -&gt; dict[str, Any]:\n    \"\"\"\n    This function can be overridden by an inheriting client to report additional client specific information to\n    the ``wandb_reporter``.\n\n    Returns:\n        (dict[str, Any]): A dictionary of things to report.\n    \"\"\"\n    return {}\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.update_metric_manager","title":"<code>update_metric_manager(preds, target, metric_manager)</code>","text":"<p>Updates a metric manager with the provided model predictions and targets. Can be overridden to modify pred and target inputs to the metric manager. This is useful in cases where the preds and targets needed to compute the loss are different than what is needed to compute metrics.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>The output predictions from the model returned by <code>self.predict</code>.</p> required <code>target</code> <code>TorchTargetType</code> <p>The targets generated by the dataloader with which to evaluate the predictions.</p> required <code>metric_manager</code> <code>MetricManager</code> <p>The metric manager to update.</p> required Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def update_metric_manager(\n    self,\n    preds: TorchPredType,\n    target: TorchTargetType,\n    metric_manager: MetricManager,\n) -&gt; None:\n    \"\"\"\n    Updates a metric manager with the provided model predictions and targets. Can be overridden to modify pred and\n    target inputs to the metric manager. This is useful in cases where the preds and targets needed to compute the\n    loss are different than what is needed to compute metrics.\n\n    Args:\n        preds (TorchPredType): The output predictions from the model returned by ``self.predict``.\n        target (TorchTargetType): The targets generated by the dataloader with which to evaluate the predictions.\n        metric_manager (MetricManager): The metric manager to update.\n    \"\"\"\n    metric_manager.update(preds, target)\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.train_step","title":"<code>train_step(input, target)</code>","text":"<p>Given a single batch of input and target data, generate predictions, compute loss, update parameters and optionally update metrics if they exist. (i.e. backprop on a single batch of data). Assumes <code>self.model</code> is in train mode already.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>The input to be fed into the model.</p> required <code>target</code> <code>TorchTargetType</code> <p>The target corresponding to the input.</p> required <p>Returns:</p> Type Description <code>tuple[TrainingLosses, TorchPredType]</code> <p>The losses object from the train step along with a dictionary of any predictions produced by the model.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n    \"\"\"\n    Given a single batch of input and target data, generate predictions, compute loss, update parameters and\n    optionally update metrics if they exist. (i.e. backprop on a single batch of data).\n    Assumes ``self.model`` is in train mode already.\n\n    Args:\n        input (TorchInputType): The input to be fed into the model.\n        target (TorchTargetType): The target corresponding to the input.\n\n    Returns:\n        (tuple[TrainingLosses, TorchPredType]): The losses object from the train step along with a dictionary of\n            any predictions produced by the model.\n    \"\"\"\n    # Clear gradients from optimizer if they exist\n    self.optimizers[\"global\"].zero_grad()\n\n    # Call user defined methods to get predictions and compute loss\n    preds, features = self.predict(input)\n    target = self.transform_target(target)\n    losses = self.compute_training_loss(preds, features, target)\n\n    # Compute backward pass and update parameters with optimizer\n    losses.backward[\"backward\"].backward()\n    self.transform_gradients(losses)\n    self.optimizers[\"global\"].step()\n\n    return losses, preds\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.val_step","title":"<code>val_step(input, target)</code>","text":"<p>Given input and target, compute loss, update loss and metrics. Assumes <code>self.model</code> is in eval mode already.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>The input to be fed into the model.</p> required <code>target</code> <code>TorchTargetType</code> <p>The target corresponding to the input.</p> required <p>Returns:</p> Type Description <code>tuple[EvaluationLosses, TorchPredType]</code> <p>The losses object from the val step along with a dictionary of the predictions produced by the model.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def val_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[EvaluationLosses, TorchPredType]:\n    \"\"\"\n    Given input and target, compute loss, update loss and metrics. Assumes ``self.model`` is in eval mode already.\n\n    Args:\n        input (TorchInputType): The input to be fed into the model.\n        target (TorchTargetType): The target corresponding to the input.\n\n    Returns:\n        (tuple[EvaluationLosses, TorchPredType]): The losses object from the val step along with a dictionary of\n            the predictions produced by the model.\n    \"\"\"\n    # Get preds and compute loss\n    with torch.no_grad():\n        preds, features = self.predict(input)\n        target = self.transform_target(target)\n        losses = self.compute_evaluation_loss(preds, features, target)\n\n    return losses, preds\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.train_by_epochs","title":"<code>train_by_epochs(epochs, current_round=None)</code>","text":"<p>Train locally for the specified number of epochs.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>The number of epochs for local training.</p> required <code>current_round</code> <code>int | None</code> <p>The current FL round.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict[str, float], dict[str, Scalar]]</code> <p>The loss and metrics dictionary from the local training. Loss is a dictionary of one or more losses that represent the different components of the loss.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def train_by_epochs(\n    self,\n    epochs: int,\n    current_round: int | None = None,\n) -&gt; tuple[dict[str, float], dict[str, Scalar]]:\n    \"\"\"\n    Train locally for the specified number of epochs.\n\n    Args:\n        epochs (int): The number of epochs for local training.\n        current_round (int | None, optional): The current FL round.\n\n    Returns:\n        (tuple[dict[str, float], dict[str, Scalar]]): The loss and metrics dictionary from the local training.\n            Loss is a dictionary of one or more losses that represent the different components of the loss.\n    \"\"\"\n    self.model.train()\n    steps_this_round = 0  # Reset number of steps this round\n    report_data: dict[str, Any] = {\"round\": current_round}\n    continue_training = True\n    for local_epoch in range(epochs):\n        self.train_metric_manager.clear()\n        self.train_loss_meter.clear()\n        # Print initial log string on epoch start\n        self._log_header_str(current_round, local_epoch)\n        # update before epoch hook\n        self.update_before_epoch(epoch=local_epoch)\n        # Update report data dict\n        report_data.update({\"fit_epoch\": self.total_epochs})\n        for input, target in maybe_progress_bar(self.train_loader, self.progress_bar):\n            self.update_before_step(steps_this_round, current_round)\n            # Assume first dimension is batch size. Sampling iterators (such as Poisson batch sampling), can\n            # construct empty batches. We skip the iteration if this occurs.\n            if check_if_batch_is_empty_and_verify_input(input):\n                log(INFO, \"Empty batch generated by data loader. Skipping step.\")\n                continue\n\n            input = move_data_to_device(input, self.device)\n            target = move_data_to_device(target, self.device)\n            losses, preds = self.train_step(input, target)\n            self.train_loss_meter.update(losses)\n            self.update_metric_manager(preds, target, self.train_metric_manager)\n            self.update_after_step(steps_this_round, current_round)\n            self.update_lr_schedulers(epoch=local_epoch)\n            report_data.update({\"fit_step_losses\": losses.as_dict(), \"fit_step\": self.total_steps})\n            report_data.update(self.get_client_specific_reports())\n            self.reports_manager.report(report_data, current_round, self.total_epochs, self.total_steps)\n            self.total_steps += 1\n            steps_this_round += 1\n            if self.early_stopper is not None and self.early_stopper.should_stop(steps_this_round):\n                log(INFO, \"Early stopping criterion met. Stopping training.\")\n                self.early_stopper.load_snapshot()\n                continue_training = False\n                break\n\n        # Log and report results\n        metrics = self.train_metric_manager.compute()\n        loss_dict = self.train_loss_meter.compute().as_dict()\n        report_data.update({\"fit_epoch_metrics\": metrics, \"fit_epoch_losses\": loss_dict})\n        report_data.update(self.get_client_specific_reports())\n        self.reports_manager.report(report_data, current_round, self.total_epochs)\n        self._log_results(loss_dict, metrics, current_round, local_epoch)\n\n        # Update internal epoch counter\n        self.total_epochs += 1\n\n        if not continue_training:\n            break\n\n    # Return final training metrics\n    return loss_dict, metrics\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.train_by_steps","title":"<code>train_by_steps(steps, current_round=None)</code>","text":"<p>Train locally for the specified number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>The number of steps to train locally.</p> required <code>current_round</code> <code>int | None</code> <p>The current FL round</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict[str, float], dict[str, Scalar]]</code> <p>The loss and metrics dictionary from the local training. Loss is a dictionary of one or more losses that represent the different components of the loss.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def train_by_steps(\n    self,\n    steps: int,\n    current_round: int | None = None,\n) -&gt; tuple[dict[str, float], dict[str, Scalar]]:\n    \"\"\"\n    Train locally for the specified number of steps.\n\n    Args:\n        steps (int): The number of steps to train locally.\n        current_round (int | None, optional): The current FL round\n\n    Returns:\n        (tuple[dict[str, float], dict[str, Scalar]]): The loss and metrics dictionary from the local training.\n            Loss is a dictionary of one or more losses that represent the different components of the loss.\n    \"\"\"\n    self.model.train()\n\n    # If the train_iterator hasn't been created before, we do so now.\n    if self.train_iterator is None:\n        # Pass loader to iterator so we can step through train loader\n        self.train_iterator = iter(self.train_loader)\n\n    self.train_loss_meter.clear()\n    self.train_metric_manager.clear()\n    self._log_header_str(current_round)\n    report_data: dict[str, Any] = {\"round\": current_round}\n    for step in maybe_progress_bar(range(steps), self.progress_bar):\n        self.update_before_step(step, current_round)\n\n        try:\n            input, target = next(self.train_iterator)\n        except StopIteration:\n            # StopIteration is thrown if dataset ends. Calling iter() on the dataloader resets the loader\n            # If shuffle=True for the dataloader, the data is also shuffled anew. If not, we pass through\n            # the data in the same order\n            self.train_iterator = iter(self.train_loader)\n            input, target = next(self.train_iterator)\n\n        # Assume first dimension is batch size. Sampling iterators (such as Poisson batch sampling), can\n        # construct empty batches. We skip the iteration if this occurs.\n        if check_if_batch_is_empty_and_verify_input(input):\n            log(INFO, \"Empty batch generated by data loader. Skipping step.\")\n            continue\n\n        input = move_data_to_device(input, self.device)\n        target = move_data_to_device(target, self.device)\n        losses, preds = self.train_step(input, target)\n        self.train_loss_meter.update(losses)\n        self.update_metric_manager(preds, target, self.train_metric_manager)\n        self.update_after_step(step, current_round)\n        self.update_lr_schedulers(step=step)\n        report_data.update({\"fit_step_losses\": losses.as_dict(), \"fit_step\": self.total_steps})\n        report_data.update(self.get_client_specific_reports())\n        self.reports_manager.report(report_data, current_round, None, self.total_steps)\n        self.total_steps += 1\n        if self.early_stopper is not None and self.early_stopper.should_stop(step):\n            log(INFO, \"Early stopping criterion met. Stopping training.\")\n            self.early_stopper.load_snapshot()\n            break\n\n    loss_dict = self.train_loss_meter.compute().as_dict()\n    metrics = self.train_metric_manager.compute()\n\n    # Log and report results\n    self._log_results(loss_dict, metrics, current_round)\n\n    return loss_dict, metrics\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.validate","title":"<code>validate(include_losses_in_metrics=False)</code>","text":"<p>Validate the current model on the entire validation (or a subset thereof if <code>num_validation_steps</code> is not None) and potentially an entire test dataset if it has been defined.</p> <p>Parameters:</p> Name Type Description Default <code>include_losses_in_metrics</code> <code>bool</code> <p>Determines whether to include the calculated losses into the metrics that are sent back to the server. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]]</code> <p>The validation loss and a dictionary of metrics from validation (and test if present).</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n    \"\"\"\n    Validate the current model on the entire validation (or a subset thereof if ``num_validation_steps`` is not\n    None) and potentially an entire test dataset if it has been defined.\n\n    Args:\n        include_losses_in_metrics (bool, optional): Determines whether to include the calculated losses into the\n            metrics that are sent back to the server. Defaults to False.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation\n            (and test if present).\n    \"\"\"\n    if self.num_validation_steps is None:\n        val_loss, val_metrics = self._fully_validate_or_test(\n            self.val_loader,\n            self.val_loss_meter,\n            self.val_metric_manager,\n            include_losses_in_metrics=include_losses_in_metrics,\n        )\n    else:\n        val_loss, val_metrics = self._validate_by_steps(\n            self.val_loss_meter,\n            self.val_metric_manager,\n            include_losses_in_metrics=include_losses_in_metrics,\n        )\n\n    if self.test_loader:\n        test_loss, test_metrics = self._fully_validate_or_test(\n            self.test_loader,\n            self.test_loss_meter,\n            self.test_metric_manager,\n            LoggingMode.TEST,\n            include_losses_in_metrics=include_losses_in_metrics,\n        )\n        # There will be no clashes due to the naming convention associated with the metric managers\n        if self.num_test_samples is not None:\n            val_metrics[TEST_NUM_EXAMPLES_KEY] = self.num_test_samples\n        val_metrics[TEST_LOSS_KEY] = test_loss\n        val_metrics.update(test_metrics)\n\n    return val_loss, val_metrics\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_properties","title":"<code>get_properties(config)</code>","text":"<p>Return properties (train and validation dataset sample counts) of client.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>dict[str, Scalar]</code> <p>A dictionary with two entries corresponding to the sample counts in the train and validation set.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_properties(self, config: Config) -&gt; dict[str, Scalar]:\n    \"\"\"\n    Return properties (train and validation dataset sample counts) of client.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (dict[str, Scalar]): A dictionary with two entries corresponding to the sample counts in\n            the train and validation set.\n    \"\"\"\n    if not self.initialized:\n        self.setup_client(config)\n\n    return {\n        \"num_train_samples\": self.num_train_samples,\n        \"num_val_samples\": self.num_val_samples,\n    }\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set initialized attribute to True.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n    Then set initialized attribute to True.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    # Explicitly send the model to the desired device. This is idempotent.\n    self.model = self.get_model(config).to(self.device)\n    train_loader, val_loader = self.get_data_loaders(config)\n    self.train_loader = train_loader\n    self.val_loader = val_loader\n    self.test_loader = self.get_test_data_loader(config)\n\n    self.num_validation_steps = process_and_check_validation_steps(config, self.val_loader)\n\n    # The following lines are type ignored because torch datasets are not \"Sized\"\n    # IE __len__ is considered optionally defined. In practice, it is almost always defined\n    # and as such, we will make that assumption.\n    self.num_train_samples = len(self.train_loader.dataset)  # type: ignore\n\n    # if num_validation_steps is defined, the number of validation samples seen is\n    # batch_size * num_validation_steps\n    self.num_val_samples = len(self.val_loader.dataset)  # type: ignore\n    if self.num_validation_steps is not None:\n        assert self.val_loader.batch_size is not None, (\n            \"Validation batch size must be defined if we want to limit the number of validation steps\"\n        )\n        self.num_val_samples = self.num_validation_steps * self.val_loader.batch_size\n\n    if self.test_loader:\n        self.num_test_samples = len(self.test_loader.dataset)  # type: ignore\n\n    self.set_optimizer(config)\n\n    # Must initialize LR scheduler after parent method initializes optimizer\n    # Add lr_scheduler to dictionary if user overrides get_lr_scheduler to return\n    # scheduler for given optimizer\n    self.lr_schedulers = {}\n    for optimizer_key in self.optimizers:\n        lr_scheduler = self.get_lr_scheduler(optimizer_key, config)\n        if lr_scheduler is not None:\n            self.lr_schedulers[optimizer_key] = lr_scheduler\n\n    self.criterion = self.get_criterion(config).to(self.device)\n    self.parameter_exchanger = self.get_parameter_exchanger(config)\n\n    self.reports_manager.report({\"host_type\": \"client\", \"initialized\": str(datetime.datetime.now())})\n    self.initialized = True\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_parameter_exchanger","title":"<code>get_parameter_exchanger(config)</code>","text":"<p>Returns Full Parameter Exchangers. Subclasses that require custom Parameter Exchangers can override this.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from server.</p> required <p>Returns:</p> Type Description <code>ParameterExchanger</code> <p>Used to exchange parameters between server and client.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n    \"\"\"\n    Returns Full Parameter Exchangers. Subclasses that require custom Parameter Exchangers can override this.\n\n    Args:\n        config (Config): The config from server.\n\n    Returns:\n        (ParameterExchanger): Used to exchange parameters between server and client.\n    \"\"\"\n    return FullParameterExchanger()\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.predict","title":"<code>predict(input)</code>","text":"<p>Computes the prediction(s), and potentially features, of the model(s) given the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into the model. If input is of type <code>dict[str, torch.Tensor]</code>, it is assumed that the keys of input match the names of the keyword arguments of <code>self.model.forward().</code></p> required <p>Returns:</p> Type Description <code>tuple[TorchPredType, TorchFeatureType]</code> <p>A tuple in which the first element contains a dictionary of predictions indexed by name and the second element contains intermediate activations indexed by name. By passing features, we can compute losses such as the contrastive loss in MOON. All predictions included in dictionary will by default be used to compute metrics separately.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Occurs when something other than a tensor or dict of tensors is passed in to the model's forward method.</p> <code>ValueError</code> <p>Occurs when something other than a tensor or dict of tensors is returned by the model forward.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def predict(self, input: TorchInputType) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Computes the prediction(s), and potentially features, of the model(s) given the input.\n\n    Args:\n        input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n            it is assumed that the keys of input match the names of the keyword arguments of\n            ``self.model.forward().``\n\n    Returns:\n        (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n            predictions indexed by name and the second element contains intermediate activations indexed by name.\n            By passing features, we can compute losses such as the contrastive loss in MOON. All predictions\n            included in dictionary will by default be used to compute metrics separately.\n\n    Raises:\n        TypeError: Occurs when something other than a tensor or dict of tensors is passed in to the model's\n            forward method.\n        ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n            forward.\n    \"\"\"\n    if isinstance(input, torch.Tensor):\n        output = self.model(input)\n    elif isinstance(input, dict):\n        # If input is a dictionary, then we unpack it before computing the forward pass.\n        # Note that this assumes the keys of the input match (exactly) the keyword args\n        # of self.model.forward().\n        output = self.model(**input)\n    else:\n        raise TypeError(\"'input' must be of type torch.Tensor or dict[str, torch.Tensor].\")\n\n    if isinstance(output, dict):\n        return output, {}\n    if isinstance(output, torch.Tensor):\n        return {\"prediction\": output}, {}\n    if isinstance(output, tuple):\n        if len(output) != EXPECTED_OUTPUT_TUPLE_SIZE:\n            raise ValueError(f\"Output tuple should have length 2 but has length {len(output)}\")\n        preds, features = output\n        return preds, features\n    raise ValueError(\"Model forward did not return a tensor, dictionary of tensors, or tuple of tensors\")\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.compute_loss_and_additional_losses","title":"<code>compute_loss_and_additional_losses(preds, features, target)</code>","text":"<p>Computes the loss and any additional losses given predictions of the model and ground truth data.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor] | None]</code> <p>A tuple with:</p> <ul> <li>The tensor for the loss.</li> <li>A dictionary of additional losses with their names and values, or None if there are no additional   losses.</li> </ul> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor] | None]:\n    \"\"\"\n    Computes the loss and any additional losses given predictions of the model and ground truth data.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor] | None]): A tuple with:\n\n            - The tensor for the loss.\n            - A dictionary of additional losses with their names and values, or None if there are no additional\n              losses.\n    \"\"\"\n    return self.criterion(preds[\"prediction\"], target), None\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.compute_training_loss","title":"<code>compute_training_loss(preds, features, target)</code>","text":"<p>Computes training loss given predictions (and potentially features) of the model and ground truth data.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. Anything stored in preds will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training loss given predictions (and potentially features) of the model and ground truth data.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n            in preds will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses\n            indexed by name.\n    \"\"\"\n    loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n    return TrainingLosses(backward=loss, additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.compute_evaluation_loss","title":"<code>compute_evaluation_loss(preds, features, target)</code>","text":"<p>Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. Anything stored in preds will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> containing checkpoint loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def compute_evaluation_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; EvaluationLosses:\n    \"\"\"\n    Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n            in preds will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n            indexed by name.\n    \"\"\"\n    loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n    return EvaluationLosses(checkpoint=loss, additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.set_optimizer","title":"<code>set_optimizer(config)</code>","text":"<p>Method called in the <code>setup_client</code> method to set optimizer attribute returned by used-defined <code>get_optimizer</code>. In the simplest case, <code>get_optimizer</code> returns an optimizer. For more advanced use cases where a dictionary of string and optimizer are returned (i.e. APFL), the user must override this method.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def set_optimizer(self, config: Config) -&gt; None:\n    \"\"\"\n    Method called in the ``setup_client`` method to set optimizer attribute returned by used-defined\n    ``get_optimizer``. In the simplest case, ``get_optimizer`` returns an optimizer. For more advanced use cases\n    where a dictionary of string and optimizer are returned (i.e. APFL), the user must override this method.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    optimizer = self.get_optimizer(config)\n    assert not isinstance(optimizer, dict)\n    self.optimizers = {\"global\": optimizer}\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_data_loaders","title":"<code>get_data_loaders(config)</code>","text":"<p>User defined method that returns a PyTorch Train <code>DataLoader</code> and a PyTorch Validation <code>DataLoader</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader]</code> <p>(tuple[DataLoader, DataLoader]) Tuple of length 2. The client train and validation loader.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in child class.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_data_loaders(self, config: Config) -&gt; tuple[DataLoader, DataLoader]:\n    \"\"\"\n    User defined method that returns a PyTorch Train ``DataLoader`` and a PyTorch Validation ``DataLoader``.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (tuple[DataLoader, DataLoader]) Tuple of length 2. The client train and validation loader.\n\n    Raises:\n        NotImplementedError: To be defined in child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_test_data_loader","title":"<code>get_test_data_loader(config)</code>","text":"<p>User defined method that returns a PyTorch Test DataLoader. By default, this function returns None, assuming that there is no test dataset to be used. If the user would like to load and evaluate a dataset, they need only override this function in their client class.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>DataLoader | None</code> <p>The optional client test loader.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_test_data_loader(self, config: Config) -&gt; DataLoader | None:\n    \"\"\"\n    User defined method that returns a PyTorch Test DataLoader. By default, this function returns None,\n    assuming that there is no test dataset to be used. If the user would like to load and evaluate a dataset,\n    they need only override this function in their client class.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (DataLoader | None): The optional client test loader.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.transform_target","title":"<code>transform_target(target)</code>","text":"<p>Method that users can extend to specify an arbitrary transformation to apply to the target prior to the loss being computed. Defaults to the identity transform.</p> <p>Overriding this method can be useful in a variety of scenarios such as Self Supervised Learning where the target is derived from the input sample itself. For example, the FedSimClr reference implementation overrides this method to extract features from the target, which is a transformed version of the input image itself.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>TorchTargetType</code> <p>The target or label used to compute the loss.</p> required <p>Returns:</p> Type Description <code>TorchTargetType</code> <p>Identical to target.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def transform_target(self, target: TorchTargetType) -&gt; TorchTargetType:\n    \"\"\"\n    Method that users can extend to specify an arbitrary transformation to apply to\n    the target prior to the loss being computed. Defaults to the identity transform.\n\n    Overriding this method can be useful in a variety of scenarios such as Self Supervised\n    Learning where the target is derived from the input sample itself. For example, the FedSimClr\n    reference implementation overrides this method to extract features from the target, which\n    is a transformed version of the input image itself.\n\n    Args:\n        target (TorchTargetType): The target or label used to compute the loss.\n\n    Returns:\n        (TorchTargetType): Identical to target.\n    \"\"\"\n    return target\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_criterion","title":"<code>get_criterion(config)</code>","text":"<p>User defined method that returns PyTorch loss to train model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in child class.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_criterion(self, config: Config) -&gt; _Loss:\n    \"\"\"\n    User defined method that returns PyTorch loss to train model.\n\n    Args:\n        config (Config): The config from the server.\n\n    Raises:\n        NotImplementedError: To be defined in child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_optimizer","title":"<code>get_optimizer(config)</code>","text":"<p>Method to be defined by user that returns the PyTorch optimizer used to train models locally Return value can be a single torch optimizer or a dictionary of string and torch optimizer. Returning multiple optimizers is useful in methods like APFL which has a different optimizer for the local and global models.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config sent from the server.</p> required <p>Returns:</p> Type Description <code>Optimizer | dict[str, Optimizer]</code> <p>An optimizer or dictionary of optimizers to train model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in child class.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_optimizer(self, config: Config) -&gt; Optimizer | dict[str, Optimizer]:\n    \"\"\"\n    Method to be defined by user that returns the PyTorch optimizer used to train models locally\n    Return value can be a single torch optimizer or a dictionary of string and torch optimizer.\n    Returning multiple optimizers is useful in methods like APFL which has a different optimizer\n    for the local and global models.\n\n    Args:\n        config (Config): The config sent from the server.\n\n    Returns:\n        (Optimizer | dict[str, Optimizer]): An optimizer or dictionary of optimizers to train model.\n\n    Raises:\n        NotImplementedError: To be defined in child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_model","title":"<code>get_model(config)</code>","text":"<p>User defined method that returns PyTorch model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The client model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in child class.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_model(self, config: Config) -&gt; nn.Module:\n    \"\"\"\n    User defined method that returns PyTorch model.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (nn.Module): The client model.\n\n    Raises:\n        NotImplementedError: To be defined in child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.get_lr_scheduler","title":"<code>get_lr_scheduler(optimizer_key, config)</code>","text":"<p>Optional user defined method that returns learning rate scheduler to be used throughout training for the given optimizer. Defaults to None.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_key</code> <code>str</code> <p>The key in the optimizer dict corresponding to the optimizer we are optionally defining a learning rate scheduler for.</p> required <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>LRScheduler | None</code> <p>Client learning rate schedulers.</p> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def get_lr_scheduler(self, optimizer_key: str, config: Config) -&gt; LRScheduler | None:\n    \"\"\"\n    Optional user defined method that returns learning rate scheduler\n    to be used throughout training for the given optimizer. Defaults to None.\n\n    Args:\n        optimizer_key (str): The key in the optimizer dict corresponding\n            to the optimizer we are optionally defining a learning rate\n            scheduler for.\n        config (Config): The config from the server.\n\n    Returns:\n        (LRScheduler | None): Client learning rate schedulers.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.update_lr_schedulers","title":"<code>update_lr_schedulers(step=None, epoch=None)</code>","text":"<p>Updates any schedulers that exist. Can be overridden to customize update logic based on client state (i.e <code>self.total_steps</code>).</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int | None</code> <p>If using <code>local_steps</code>, current step of this round. Otherwise None.</p> <code>None</code> <code>epoch</code> <code>int | None</code> <p>If using <code>local_epochs</code> current epoch of this round. Otherwise None.</p> <code>None</code> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def update_lr_schedulers(self, step: int | None = None, epoch: int | None = None) -&gt; None:\n    \"\"\"\n    Updates any schedulers that exist. Can be overridden to customize update logic based on client state\n    (i.e ``self.total_steps``).\n\n    Args:\n        step (int | None): If using ``local_steps``, current step of this round. Otherwise None.\n        epoch (int | None): If using ``local_epochs`` current epoch of this round. Otherwise None.\n    \"\"\"\n    assert (step is None) ^ (epoch is None)\n\n    for lr_scheduler in self.lr_schedulers.values():\n        lr_scheduler.step()  # Update LR\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.update_before_train","title":"<code>update_before_train(current_server_round)</code>","text":"<p>Hook method called before training with the number of current server rounds performed.</p> <p>NOTE: This method is called immediately AFTER the aggregated parameters are received from the server. For example, used by MOON and FENDA to save global modules after aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>current_server_round</code> <code>int</code> <p>The number of current server round.</p> required Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def update_before_train(self, current_server_round: int) -&gt; None:\n    \"\"\"\n    Hook method called before training with the number of current server rounds performed.\n\n    **NOTE**: This method is called immediately **AFTER** the aggregated parameters are received from the server.\n    For example, used by MOON and FENDA to save global modules after aggregation.\n\n    Args:\n        current_server_round (int): The number of current server round.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.update_after_train","title":"<code>update_after_train(local_steps, loss_dict, config)</code>","text":"<p>Hook method called after training with the number of <code>local_steps</code> performed over the FL round and the corresponding loss dictionary. For example, used by Scaffold to update the control variates after a local round of training. Also used by FedProx to update the current loss based on the loss returned during training. Also used by MOON and FENDA to save trained modules weights before aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>local_steps</code> <code>int</code> <p>The number of steps so far in the round in the local training.</p> required <code>loss_dict</code> <code>dict[str, float]</code> <p>A dictionary of losses from local training.</p> required <code>config</code> <code>Config</code> <p>The config from the server</p> required Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n    \"\"\"\n    Hook method called after training with the number of ``local_steps`` performed over the FL round and\n    the corresponding loss dictionary. For example, used by Scaffold to update the control variates\n    after a local round of training. Also used by FedProx to update the current loss based on the loss\n    returned during training. Also used by MOON and FENDA to save trained modules weights before\n    aggregation.\n\n    Args:\n        local_steps (int): The number of steps so far in the round in the local training.\n        loss_dict (dict[str, float]): A dictionary of losses from local training.\n        config (Config): The config from the server\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.update_before_step","title":"<code>update_before_step(step, current_round=None)</code>","text":"<p>Hook method called before local train step.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The local training step that was most recently completed. Resets only at the end of the round.</p> required <code>current_round</code> <code>int | None</code> <p>The current FL server round.</p> <code>None</code> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def update_before_step(self, step: int, current_round: int | None = None) -&gt; None:\n    \"\"\"\n    Hook method called before local train step.\n\n    Args:\n        step (int): The local training step that was most recently completed. Resets only at the end of the round.\n        current_round (int | None, optional): The current FL server round.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.update_after_step","title":"<code>update_after_step(step, current_round=None)</code>","text":"<p>Hook method called after local train step on client. Step is an integer that represents the local training step that was most recently completed. For example, used by the APFL method to update the alpha value after a training a step. Also used by the MOON, FENDA and Ditto to update optimized beta value for MK-MMD loss after n steps.</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>The step number in local training that was most recently completed. Resets only at the end of the round.</p> required <code>current_round</code> <code>int | None</code> <p>The current FL server round.</p> <code>None</code> Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def update_after_step(self, step: int, current_round: int | None = None) -&gt; None:\n    \"\"\"\n    Hook method called after local train step on client. Step is an integer that represents the local training\n    step that was most recently completed. For example, used by the APFL method to update the alpha value after a\n    training a step. Also used by the MOON, FENDA and Ditto to update optimized beta value for MK-MMD loss after\n    n steps.\n\n    Args:\n        step (int): The step number in local training that was most recently completed. Resets only at the end of\n            the round.\n        current_round (int | None, optional): The current FL server round.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.update_before_epoch","title":"<code>update_before_epoch(epoch)</code>","text":"<p>Hook method called before local epoch on client. Only called if client is being trained by epochs (i.e. using <code>local_epochs</code> key instead of local steps in the server config file).</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>Integer representing the epoch about to begin</p> required Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def update_before_epoch(self, epoch: int) -&gt; None:\n    \"\"\"\n    Hook method called before local epoch on client. Only called if client is being trained by epochs\n    (i.e. using ``local_epochs`` key instead of local steps in the server config file).\n\n    Args:\n        epoch (int): Integer representing the epoch about to begin\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.clients.basic_client.BasicClient.transform_gradients","title":"<code>transform_gradients(losses)</code>","text":"<p>Hook function for model training only called after backwards pass but before optimizer step. Useful for transforming the gradients (such as with gradient clipping) before they are applied to the model weights.</p> <p>Parameters:</p> Name Type Description Default <code>losses</code> <code>TrainingLosses</code> <p>The losses object from the train step</p> required Source code in <code>fl4health/clients/basic_client.py</code> <pre><code>def transform_gradients(self, losses: TrainingLosses) -&gt; None:\n    \"\"\"\n    Hook function for model training only called after backwards pass but before optimizer step. Useful for\n    transforming the gradients (such as with gradient clipping) before they are applied to the model weights.\n\n    Args:\n        losses (TrainingLosses): The losses object from the train step\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.clients.clipping_client","title":"<code>clipping_client</code>","text":""},{"location":"api/#fl4health.clients.clipping_client.NumpyClippingClient","title":"<code>NumpyClippingClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/clipping_client.py</code> <pre><code>class NumpyClippingClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Client that clips updates being sent to the server where noise is added. Used to obtain Client Level\n        Differential Privacy in FL setting.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.parameter_exchanger: FullParameterExchangerWithPacking[float]\n        self.clipping_bound: float | None = None\n        self.adaptive_clipping: bool | None = None\n\n    def calculate_parameters_norm(self, parameters: NDArrays) -&gt; float:\n        \"\"\"\n        Given a set of parameters, compute the l2-norm of the parameters. This is a matrix norm: squared sum of all of\n        the weights.\n\n        Args:\n            parameters (NDArrays): Tensor to measure with the norm\n\n        Returns:\n            (float): Squared sum of all values in the NDArrays\n        \"\"\"\n        layer_inner_products = [pow(linalg.norm(layer_weights), 2) for layer_weights in parameters]\n        # network Frobenius norm\n        return pow(sum(layer_inner_products), 0.5)\n\n    def clip_parameters(self, parameters: NDArrays) -&gt; tuple[NDArrays, float]:\n        \"\"\"\n        Performs \"flat clipping\" on the parameters as follows.\n\n        \\\\[\\\\text{parameters} \\\\cdot \\\\min \\\\left(1, \\\\frac{C}{\\\\Vert \\\\text{parameters} \\\\Vert_2} \\\\right)\\\\]\n\n        Args:\n            parameters (NDArrays): Parameters to clip\n\n        Returns:\n            (tuple[NDArrays, float]): Clipped parameters and the associated clipping bit indicating whether the norm\n                was below ``self.clipping_bound``. If ``self.adaptive_clipping`` is false, this bit is always 0.0\n        \"\"\"\n        assert self.clipping_bound is not None\n        assert self.adaptive_clipping is not None\n        # performs flat clipping (i.e. parameters * min(1, C/||parameters||_2))\n        network_frobenius_norm = self.calculate_parameters_norm(parameters)\n        log(INFO, f\"Update norm: {network_frobenius_norm}, Clipping Bound: {self.clipping_bound}\")\n        if network_frobenius_norm &lt;= self.clipping_bound:\n            # if we're not adaptively clipping then don't send true clipping bit info as this would potentially leak\n            # information\n            clipping_bit = 1.0 if self.adaptive_clipping else 0.0\n            return parameters, clipping_bit\n        clip_scalar = min(1.0, self.clipping_bound / network_frobenius_norm)\n        # parameters and clipping bit\n        return [layer_weights * clip_scalar for layer_weights in parameters], 0.0\n\n    def compute_weight_update_and_clip(self, parameters: NDArrays) -&gt; tuple[NDArrays, float]:\n        \"\"\"\n        Compute the weight delta (i.e. new weights - old weights) and clip according to ``self.clipping_bound``.\n\n        Args:\n            parameters (NDArrays): Updated parameters to compute the delta from and clip thereafter.\n\n        Returns:\n            (tuple[NDArrays, float]): Clipped weighted updates (weight deltas) and the associated clipping bit.\n        \"\"\"\n        assert self.initial_weights is not None\n        assert len(parameters) == len(self.initial_weights)\n        weight_update: NDArrays = [\n            new_layer_weights - old_layer_weights\n            for old_layer_weights, new_layer_weights in zip(self.initial_weights, parameters)\n        ]\n        # return clipped parameters and clipping bit\n        return self.clip_parameters(weight_update)\n\n    def get_parameters(self, config: Config) -&gt; NDArrays:\n        \"\"\"\n        This function performs clipping through ``compute_weight_update_and_clip`` and stores the clipping bit\n        as the last entry in the NDArrays.\n        \"\"\"\n        current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n\n        if not self.initialized or current_server_round == 0:\n            # If we haven't initialized the client we are being asked to return model parameters. So we send them all\n            return self.setup_client_and_return_all_model_parameters(config)\n\n        assert self.model is not None and self.parameter_exchanger is not None\n        model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n        clipped_weight_update, clipping_bit = self.compute_weight_update_and_clip(model_weights)\n        return self.parameter_exchanger.pack_parameters(clipped_weight_update, clipping_bit)\n\n    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n        \"\"\"\n        This function assumes that the parameters being passed contain model parameters followed by the last entry\n        of the list being the new clipping bound. They are unpacked for the clients to use in training. If it is\n        called in the first fitting round, we assume the full model is being initialized and use the\n        ``FullParameterExchanger()`` to set all model weights.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model and also the clipping bound.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning round\n                is a fitting round or an evaluation round.\n                This is used to help determine which parameter exchange should be used for pulling\n                parameters.\n                A full parameter exchanger is used if the current federated learning round is the very\n                first fitting round.\n        \"\"\"\n        assert self.model is not None and self.parameter_exchanger is not None\n        # The last entry in the parameters list is assumed to be a clipping bound (even if we're evaluating)\n        server_model_parameters, clipping_bound = self.parameter_exchanger.unpack_parameters(parameters)\n        self.clipping_bound = clipping_bound\n        current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n\n        if current_server_round == 1 and fitting_round:\n            # Initialize all model weights as this is the first time things have been set\n            self.initialize_all_model_weights(server_model_parameters, config)\n            # Extract only the initial weights that we care about clipping and exchanging\n            self.initial_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n        else:\n            # Store the starting parameters without clipping bound before client optimization steps\n            self.initial_weights = server_model_parameters\n            # Inject the server model parameters into the client model\n            self.parameter_exchanger.pull_parameters(server_model_parameters, self.model, config)\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        return FullParameterExchangerWithPacking(ParameterPackerWithClippingBit())\n\n    def setup_client(self, config: Config) -&gt; None:\n        self.adaptive_clipping = narrow_dict_type(config, \"adaptive_clipping\", bool)\n        super().setup_client(config)\n</code></pre>"},{"location":"api/#fl4health.clients.clipping_client.NumpyClippingClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>Client that clips updates being sent to the server where noise is added. Used to obtain Client Level Differential Privacy in FL setting.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/clipping_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Client that clips updates being sent to the server where noise is added. Used to obtain Client Level\n    Differential Privacy in FL setting.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.parameter_exchanger: FullParameterExchangerWithPacking[float]\n    self.clipping_bound: float | None = None\n    self.adaptive_clipping: bool | None = None\n</code></pre>"},{"location":"api/#fl4health.clients.clipping_client.NumpyClippingClient.calculate_parameters_norm","title":"<code>calculate_parameters_norm(parameters)</code>","text":"<p>Given a set of parameters, compute the l2-norm of the parameters. This is a matrix norm: squared sum of all of the weights.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Tensor to measure with the norm</p> required <p>Returns:</p> Type Description <code>float</code> <p>Squared sum of all values in the NDArrays</p> Source code in <code>fl4health/clients/clipping_client.py</code> <pre><code>def calculate_parameters_norm(self, parameters: NDArrays) -&gt; float:\n    \"\"\"\n    Given a set of parameters, compute the l2-norm of the parameters. This is a matrix norm: squared sum of all of\n    the weights.\n\n    Args:\n        parameters (NDArrays): Tensor to measure with the norm\n\n    Returns:\n        (float): Squared sum of all values in the NDArrays\n    \"\"\"\n    layer_inner_products = [pow(linalg.norm(layer_weights), 2) for layer_weights in parameters]\n    # network Frobenius norm\n    return pow(sum(layer_inner_products), 0.5)\n</code></pre>"},{"location":"api/#fl4health.clients.clipping_client.NumpyClippingClient.clip_parameters","title":"<code>clip_parameters(parameters)</code>","text":"<p>Performs \"flat clipping\" on the parameters as follows.</p> \\[\\text{parameters} \\cdot \\min \\left(1, \\frac{C}{\\Vert \\text{parameters} \\Vert_2} \\right)\\] <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters to clip</p> required <p>Returns:</p> Type Description <code>tuple[NDArrays, float]</code> <p>Clipped parameters and the associated clipping bit indicating whether the norm was below <code>self.clipping_bound</code>. If <code>self.adaptive_clipping</code> is false, this bit is always 0.0</p> Source code in <code>fl4health/clients/clipping_client.py</code> <pre><code>def clip_parameters(self, parameters: NDArrays) -&gt; tuple[NDArrays, float]:\n    \"\"\"\n    Performs \"flat clipping\" on the parameters as follows.\n\n    \\\\[\\\\text{parameters} \\\\cdot \\\\min \\\\left(1, \\\\frac{C}{\\\\Vert \\\\text{parameters} \\\\Vert_2} \\\\right)\\\\]\n\n    Args:\n        parameters (NDArrays): Parameters to clip\n\n    Returns:\n        (tuple[NDArrays, float]): Clipped parameters and the associated clipping bit indicating whether the norm\n            was below ``self.clipping_bound``. If ``self.adaptive_clipping`` is false, this bit is always 0.0\n    \"\"\"\n    assert self.clipping_bound is not None\n    assert self.adaptive_clipping is not None\n    # performs flat clipping (i.e. parameters * min(1, C/||parameters||_2))\n    network_frobenius_norm = self.calculate_parameters_norm(parameters)\n    log(INFO, f\"Update norm: {network_frobenius_norm}, Clipping Bound: {self.clipping_bound}\")\n    if network_frobenius_norm &lt;= self.clipping_bound:\n        # if we're not adaptively clipping then don't send true clipping bit info as this would potentially leak\n        # information\n        clipping_bit = 1.0 if self.adaptive_clipping else 0.0\n        return parameters, clipping_bit\n    clip_scalar = min(1.0, self.clipping_bound / network_frobenius_norm)\n    # parameters and clipping bit\n    return [layer_weights * clip_scalar for layer_weights in parameters], 0.0\n</code></pre>"},{"location":"api/#fl4health.clients.clipping_client.NumpyClippingClient.compute_weight_update_and_clip","title":"<code>compute_weight_update_and_clip(parameters)</code>","text":"<p>Compute the weight delta (i.e. new weights - old weights) and clip according to <code>self.clipping_bound</code>.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Updated parameters to compute the delta from and clip thereafter.</p> required <p>Returns:</p> Type Description <code>tuple[NDArrays, float]</code> <p>Clipped weighted updates (weight deltas) and the associated clipping bit.</p> Source code in <code>fl4health/clients/clipping_client.py</code> <pre><code>def compute_weight_update_and_clip(self, parameters: NDArrays) -&gt; tuple[NDArrays, float]:\n    \"\"\"\n    Compute the weight delta (i.e. new weights - old weights) and clip according to ``self.clipping_bound``.\n\n    Args:\n        parameters (NDArrays): Updated parameters to compute the delta from and clip thereafter.\n\n    Returns:\n        (tuple[NDArrays, float]): Clipped weighted updates (weight deltas) and the associated clipping bit.\n    \"\"\"\n    assert self.initial_weights is not None\n    assert len(parameters) == len(self.initial_weights)\n    weight_update: NDArrays = [\n        new_layer_weights - old_layer_weights\n        for old_layer_weights, new_layer_weights in zip(self.initial_weights, parameters)\n    ]\n    # return clipped parameters and clipping bit\n    return self.clip_parameters(weight_update)\n</code></pre>"},{"location":"api/#fl4health.clients.clipping_client.NumpyClippingClient.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>This function performs clipping through <code>compute_weight_update_and_clip</code> and stores the clipping bit as the last entry in the NDArrays.</p> Source code in <code>fl4health/clients/clipping_client.py</code> <pre><code>def get_parameters(self, config: Config) -&gt; NDArrays:\n    \"\"\"\n    This function performs clipping through ``compute_weight_update_and_clip`` and stores the clipping bit\n    as the last entry in the NDArrays.\n    \"\"\"\n    current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n\n    if not self.initialized or current_server_round == 0:\n        # If we haven't initialized the client we are being asked to return model parameters. So we send them all\n        return self.setup_client_and_return_all_model_parameters(config)\n\n    assert self.model is not None and self.parameter_exchanger is not None\n    model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n    clipped_weight_update, clipping_bit = self.compute_weight_update_and_clip(model_weights)\n    return self.parameter_exchanger.pack_parameters(clipped_weight_update, clipping_bit)\n</code></pre>"},{"location":"api/#fl4health.clients.clipping_client.NumpyClippingClient.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>This function assumes that the parameters being passed contain model parameters followed by the last entry of the list being the new clipping bound. They are unpacked for the clients to use in training. If it is called in the first fitting round, we assume the full model is being initialized and use the <code>FullParameterExchanger()</code> to set all model weights.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model and also the clipping bound.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. This is used to help determine which parameter exchange should be used for pulling parameters. A full parameter exchanger is used if the current federated learning round is the very first fitting round.</p> required Source code in <code>fl4health/clients/clipping_client.py</code> <pre><code>def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n    \"\"\"\n    This function assumes that the parameters being passed contain model parameters followed by the last entry\n    of the list being the new clipping bound. They are unpacked for the clients to use in training. If it is\n    called in the first fitting round, we assume the full model is being initialized and use the\n    ``FullParameterExchanger()`` to set all model weights.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model and also the clipping bound.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning round\n            is a fitting round or an evaluation round.\n            This is used to help determine which parameter exchange should be used for pulling\n            parameters.\n            A full parameter exchanger is used if the current federated learning round is the very\n            first fitting round.\n    \"\"\"\n    assert self.model is not None and self.parameter_exchanger is not None\n    # The last entry in the parameters list is assumed to be a clipping bound (even if we're evaluating)\n    server_model_parameters, clipping_bound = self.parameter_exchanger.unpack_parameters(parameters)\n    self.clipping_bound = clipping_bound\n    current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n\n    if current_server_round == 1 and fitting_round:\n        # Initialize all model weights as this is the first time things have been set\n        self.initialize_all_model_weights(server_model_parameters, config)\n        # Extract only the initial weights that we care about clipping and exchanging\n        self.initial_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n    else:\n        # Store the starting parameters without clipping bound before client optimization steps\n        self.initial_weights = server_model_parameters\n        # Inject the server model parameters into the client model\n        self.parameter_exchanger.pull_parameters(server_model_parameters, self.model, config)\n</code></pre>"},{"location":"api/#fl4health.clients.constrained_fenda_client","title":"<code>constrained_fenda_client</code>","text":""},{"location":"api/#fl4health.clients.constrained_fenda_client.ConstrainedFendaClient","title":"<code>ConstrainedFendaClient</code>","text":"<p>               Bases: <code>FendaClient</code></p> Source code in <code>fl4health/clients/constrained_fenda_client.py</code> <pre><code>class ConstrainedFendaClient(FendaClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n        loss_container: ConstrainedFendaLossContainer | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This class extends the functionality of FENDA training to include various kinds of constraints applied during\n        the client-side training of FENDA models.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            loss_container (ConstrainedFendaLossContainer | None, optional): Configuration that determines which\n                losses will be applied during FENDA training. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n\n        if loss_container:\n            self.loss_container = loss_container\n        else:\n            # If no loss configuration has been define, set everything to zero. This is equivalent to vanilla FENDA\n            log(\n                WARNING,\n                \"No loss container provided, defaulting to an empty container. \"\n                \"This is equivalent to running a vanilla FENDA client\",\n            )\n            self.loss_container = ConstrainedFendaLossContainer(None, None, None)\n\n        # Need to save previous local module, global module and aggregated global module at each communication round\n        # to compute contrastive loss.\n        self.old_local_module: nn.Module | None = None\n        self.old_global_module: nn.Module | None = None\n        self.initial_global_module: nn.Module | None = None\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        assert isinstance(self.model, FendaModelWithFeatureState)\n        return super().get_parameter_exchanger(config)\n\n    def _flatten(self, features: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Flatten the provided features ASSUMING they are provided in batch-first format.\n\n        Args:\n            features (torch.Tensor): features to be flattened\n\n        Returns:\n            (torch.Tensor): flattened feature vectors of shape (batch, -1)\n        \"\"\"\n        return features.reshape(len(features), -1)\n\n    def _perfcl_keys_present(self, features: dict[str, torch.Tensor]) -&gt; bool:\n        target_keys = {\n            \"old_local_features\",\n            \"old_global_features\",\n            \"initial_global_features\",\n        }\n        return target_keys.issubset(features.keys())\n\n    def predict(self, input: TorchInputType) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Computes the prediction(s) and features of the model(s) given the input.\n\n        Args:\n            input (TorchInputType): Inputs to be fed into the model. ``TorchInputType`` is simply an alias\n                for the union of ``torch.Tensor`` and ``dict[str, torch.Tensor]``.\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple in which the first element\n                contains predictions indexed by name and the second element contains intermediate activations\n                index by name. Specifically the features of the model, features of the global model and features of\n                the old model are returned. All predictions included in dictionary will be used to compute metrics.\n        \"\"\"\n        assert isinstance(input, torch.Tensor)\n        assert isinstance(self.model, FendaModelWithFeatureState)\n        preds, features = self.model(input)\n\n        if (\n            self.loss_container.has_contrastive_loss() or self.loss_container.has_perfcl_loss()\n        ) and self.old_local_module is not None:\n            # If we have defined a contrastive loss function or PerFCL loss function, we attempt to save old local\n            # features.\n            features[\"old_local_features\"] = self._flatten(self.old_local_module.forward(input))\n\n        if self.loss_container.has_perfcl_loss():\n            # If a PerFCL loss function has been defined, then we also save two additional feature components.\n            if self.old_global_module is not None:\n                features[\"old_global_features\"] = self._flatten(self.old_global_module.forward(input))\n            if self.initial_global_module is not None:\n                features[\"initial_global_features\"] = self._flatten(self.initial_global_module.forward(input))\n\n        return preds, features\n\n    def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n        \"\"\"\n        This function is called after client-side training concludes. If a contrastive or PerFCL loss function has\n        been defined, it is used to save the local and global feature extraction weights/modules to be used in the\n        next round of client-side training.\n\n        Args:\n            local_steps (int): Number of steps performed during training.\n            loss_dict (dict[str, float]): Losses computed during training.\n            config (Config): FL training configuration object.\n        \"\"\"\n        # Save the parameters of the old model\n        assert isinstance(self.model, FendaModelWithFeatureState)\n\n        if self.loss_container.has_contrastive_loss() or self.loss_container.has_perfcl_loss():\n            self.old_local_module = clone_and_freeze_model(self.model.first_feature_extractor)\n            self.old_global_module = clone_and_freeze_model(self.model.second_feature_extractor)\n\n        super().update_after_train(local_steps, loss_dict, config)\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        \"\"\"\n        This function is called prior to the start of client-side training, but after the server parameters have be\n        received and injected into the model. If a PerFCL loss function has been defined, it is used to save the\n        aggregated global feature extractor weights/module representing the initial state of this module **BEFORE**\n        this iteration of client-side training but **AFTER** server-side aggregation.\n\n        Args:\n            current_server_round (int): Current server round being performed.\n        \"\"\"\n        # Save the parameters of the aggregated global model\n        assert isinstance(self.model, FendaModelWithFeatureState)\n\n        if self.loss_container.has_perfcl_loss():\n            self.initial_global_module = clone_and_freeze_model(self.model.second_feature_extractor)\n\n        super().update_before_train(current_server_round)\n\n    def compute_loss_and_additional_losses(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Computes the loss and any additional losses given predictions of the model and ground truth data.\n        For FENDA, the loss is the total loss and the additional losses are the loss, total loss and, based on\n        client attributes set from server config, cosine similarity loss, contrastive loss and perfcl losses.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n                - The tensor for the total loss\n                - A dictionary with ``loss``, ``total_loss`` and, based on client attributes set from server\n                  config, also ``cos_sim_loss``, ``contrastive_loss``, ``contrastive_loss_minimize`` and\n                  ``contrastive_loss_minimize`` keys and their respective calculated values.\n        \"\"\"\n        loss = self.criterion(preds[\"prediction\"], target)\n        total_loss = loss.clone()\n        additional_losses = {\"loss\": loss}\n\n        if self.loss_container.has_cosine_similarity_loss():\n            cosine_similarity_loss = self.loss_container.compute_cosine_similarity_loss(\n                features[\"local_features\"], features[\"global_features\"]\n            )\n            total_loss += cosine_similarity_loss\n            additional_losses[\"cos_sim_loss\"] = cosine_similarity_loss\n\n        if self.loss_container.has_contrastive_loss() and \"old_local_features\" in features:\n            contrastive_loss = self.loss_container.compute_contrastive_loss(\n                features[\"local_features\"],\n                features[\"old_local_features\"].unsqueeze(0),\n                features[\"global_features\"].unsqueeze(0),\n            )\n            total_loss += contrastive_loss\n            additional_losses[\"contrastive_loss\"] = contrastive_loss\n\n        if self.loss_container.has_perfcl_loss() and self._perfcl_keys_present(features):\n            global_feature_contrastive_loss, local_feature_contrastive_loss = self.loss_container.compute_perfcl_loss(\n                features[\"local_features\"],\n                features[\"old_local_features\"],\n                features[\"global_features\"],\n                features[\"old_global_features\"],\n                features[\"initial_global_features\"],\n            )\n            total_loss += global_feature_contrastive_loss + local_feature_contrastive_loss\n            additional_losses[\"global_feature_contrastive_loss\"] = global_feature_contrastive_loss\n            additional_losses[\"local_feature_contrastive_loss\"] = local_feature_contrastive_loss\n\n        additional_losses[\"total_loss\"] = total_loss\n\n        return total_loss, additional_losses\n\n    def compute_evaluation_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses:\n        \"\"\"\n        Computes evaluation loss given predictions of the model and ground truth data. Optionally computes\n        additional loss components such as ``cosine_similarity_loss``, ``contrastive_loss`` and ``perfcl_loss`` based\n        on client attributes set from server config.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n                All predictions included in dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n                indexed by name. Additional losses may include ``cosine_similarity_loss``, ``contrastive_loss``\n                and ``perfcl_loss``.\n        \"\"\"\n        _, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n        return EvaluationLosses(checkpoint=additional_losses[\"loss\"], additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.constrained_fenda_client.ConstrainedFendaClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None, loss_container=None)</code>","text":"<p>This class extends the functionality of FENDA training to include various kinds of constraints applied during the client-side training of FENDA models.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>loss_container</code> <code>ConstrainedFendaLossContainer | None</code> <p>Configuration that determines which losses will be applied during FENDA training. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/constrained_fenda_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n    loss_container: ConstrainedFendaLossContainer | None = None,\n) -&gt; None:\n    \"\"\"\n    This class extends the functionality of FENDA training to include various kinds of constraints applied during\n    the client-side training of FENDA models.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        loss_container (ConstrainedFendaLossContainer | None, optional): Configuration that determines which\n            losses will be applied during FENDA training. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n\n    if loss_container:\n        self.loss_container = loss_container\n    else:\n        # If no loss configuration has been define, set everything to zero. This is equivalent to vanilla FENDA\n        log(\n            WARNING,\n            \"No loss container provided, defaulting to an empty container. \"\n            \"This is equivalent to running a vanilla FENDA client\",\n        )\n        self.loss_container = ConstrainedFendaLossContainer(None, None, None)\n\n    # Need to save previous local module, global module and aggregated global module at each communication round\n    # to compute contrastive loss.\n    self.old_local_module: nn.Module | None = None\n    self.old_global_module: nn.Module | None = None\n    self.initial_global_module: nn.Module | None = None\n</code></pre>"},{"location":"api/#fl4health.clients.constrained_fenda_client.ConstrainedFendaClient.predict","title":"<code>predict(input)</code>","text":"<p>Computes the prediction(s) and features of the model(s) given the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into the model. <code>TorchInputType</code> is simply an alias for the union of <code>torch.Tensor</code> and <code>dict[str, torch.Tensor]</code>.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>A tuple in which the first element contains predictions indexed by name and the second element contains intermediate activations index by name. Specifically the features of the model, features of the global model and features of the old model are returned. All predictions included in dictionary will be used to compute metrics.</p> Source code in <code>fl4health/clients/constrained_fenda_client.py</code> <pre><code>def predict(self, input: TorchInputType) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Computes the prediction(s) and features of the model(s) given the input.\n\n    Args:\n        input (TorchInputType): Inputs to be fed into the model. ``TorchInputType`` is simply an alias\n            for the union of ``torch.Tensor`` and ``dict[str, torch.Tensor]``.\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple in which the first element\n            contains predictions indexed by name and the second element contains intermediate activations\n            index by name. Specifically the features of the model, features of the global model and features of\n            the old model are returned. All predictions included in dictionary will be used to compute metrics.\n    \"\"\"\n    assert isinstance(input, torch.Tensor)\n    assert isinstance(self.model, FendaModelWithFeatureState)\n    preds, features = self.model(input)\n\n    if (\n        self.loss_container.has_contrastive_loss() or self.loss_container.has_perfcl_loss()\n    ) and self.old_local_module is not None:\n        # If we have defined a contrastive loss function or PerFCL loss function, we attempt to save old local\n        # features.\n        features[\"old_local_features\"] = self._flatten(self.old_local_module.forward(input))\n\n    if self.loss_container.has_perfcl_loss():\n        # If a PerFCL loss function has been defined, then we also save two additional feature components.\n        if self.old_global_module is not None:\n            features[\"old_global_features\"] = self._flatten(self.old_global_module.forward(input))\n        if self.initial_global_module is not None:\n            features[\"initial_global_features\"] = self._flatten(self.initial_global_module.forward(input))\n\n    return preds, features\n</code></pre>"},{"location":"api/#fl4health.clients.constrained_fenda_client.ConstrainedFendaClient.update_after_train","title":"<code>update_after_train(local_steps, loss_dict, config)</code>","text":"<p>This function is called after client-side training concludes. If a contrastive or PerFCL loss function has been defined, it is used to save the local and global feature extraction weights/modules to be used in the next round of client-side training.</p> <p>Parameters:</p> Name Type Description Default <code>local_steps</code> <code>int</code> <p>Number of steps performed during training.</p> required <code>loss_dict</code> <code>dict[str, float]</code> <p>Losses computed during training.</p> required <code>config</code> <code>Config</code> <p>FL training configuration object.</p> required Source code in <code>fl4health/clients/constrained_fenda_client.py</code> <pre><code>def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n    \"\"\"\n    This function is called after client-side training concludes. If a contrastive or PerFCL loss function has\n    been defined, it is used to save the local and global feature extraction weights/modules to be used in the\n    next round of client-side training.\n\n    Args:\n        local_steps (int): Number of steps performed during training.\n        loss_dict (dict[str, float]): Losses computed during training.\n        config (Config): FL training configuration object.\n    \"\"\"\n    # Save the parameters of the old model\n    assert isinstance(self.model, FendaModelWithFeatureState)\n\n    if self.loss_container.has_contrastive_loss() or self.loss_container.has_perfcl_loss():\n        self.old_local_module = clone_and_freeze_model(self.model.first_feature_extractor)\n        self.old_global_module = clone_and_freeze_model(self.model.second_feature_extractor)\n\n    super().update_after_train(local_steps, loss_dict, config)\n</code></pre>"},{"location":"api/#fl4health.clients.constrained_fenda_client.ConstrainedFendaClient.update_before_train","title":"<code>update_before_train(current_server_round)</code>","text":"<p>This function is called prior to the start of client-side training, but after the server parameters have be received and injected into the model. If a PerFCL loss function has been defined, it is used to save the aggregated global feature extractor weights/module representing the initial state of this module BEFORE this iteration of client-side training but AFTER server-side aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>current_server_round</code> <code>int</code> <p>Current server round being performed.</p> required Source code in <code>fl4health/clients/constrained_fenda_client.py</code> <pre><code>def update_before_train(self, current_server_round: int) -&gt; None:\n    \"\"\"\n    This function is called prior to the start of client-side training, but after the server parameters have be\n    received and injected into the model. If a PerFCL loss function has been defined, it is used to save the\n    aggregated global feature extractor weights/module representing the initial state of this module **BEFORE**\n    this iteration of client-side training but **AFTER** server-side aggregation.\n\n    Args:\n        current_server_round (int): Current server round being performed.\n    \"\"\"\n    # Save the parameters of the aggregated global model\n    assert isinstance(self.model, FendaModelWithFeatureState)\n\n    if self.loss_container.has_perfcl_loss():\n        self.initial_global_module = clone_and_freeze_model(self.model.second_feature_extractor)\n\n    super().update_before_train(current_server_round)\n</code></pre>"},{"location":"api/#fl4health.clients.constrained_fenda_client.ConstrainedFendaClient.compute_loss_and_additional_losses","title":"<code>compute_loss_and_additional_losses(preds, features, target)</code>","text":"<p>Computes the loss and any additional losses given predictions of the model and ground truth data. For FENDA, the loss is the total loss and the additional losses are the loss, total loss and, based on client attributes set from server config, cosine similarity loss, contrastive loss and perfcl losses.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor]]</code> <p>A tuple with:</p> <ul> <li>The tensor for the total loss</li> <li>A dictionary with <code>loss</code>, <code>total_loss</code> and, based on client attributes set from server   config, also <code>cos_sim_loss</code>, <code>contrastive_loss</code>, <code>contrastive_loss_minimize</code> and   <code>contrastive_loss_minimize</code> keys and their respective calculated values.</li> </ul> Source code in <code>fl4health/clients/constrained_fenda_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Computes the loss and any additional losses given predictions of the model and ground truth data.\n    For FENDA, the loss is the total loss and the additional losses are the loss, total loss and, based on\n    client attributes set from server config, cosine similarity loss, contrastive loss and perfcl losses.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n            - The tensor for the total loss\n            - A dictionary with ``loss``, ``total_loss`` and, based on client attributes set from server\n              config, also ``cos_sim_loss``, ``contrastive_loss``, ``contrastive_loss_minimize`` and\n              ``contrastive_loss_minimize`` keys and their respective calculated values.\n    \"\"\"\n    loss = self.criterion(preds[\"prediction\"], target)\n    total_loss = loss.clone()\n    additional_losses = {\"loss\": loss}\n\n    if self.loss_container.has_cosine_similarity_loss():\n        cosine_similarity_loss = self.loss_container.compute_cosine_similarity_loss(\n            features[\"local_features\"], features[\"global_features\"]\n        )\n        total_loss += cosine_similarity_loss\n        additional_losses[\"cos_sim_loss\"] = cosine_similarity_loss\n\n    if self.loss_container.has_contrastive_loss() and \"old_local_features\" in features:\n        contrastive_loss = self.loss_container.compute_contrastive_loss(\n            features[\"local_features\"],\n            features[\"old_local_features\"].unsqueeze(0),\n            features[\"global_features\"].unsqueeze(0),\n        )\n        total_loss += contrastive_loss\n        additional_losses[\"contrastive_loss\"] = contrastive_loss\n\n    if self.loss_container.has_perfcl_loss() and self._perfcl_keys_present(features):\n        global_feature_contrastive_loss, local_feature_contrastive_loss = self.loss_container.compute_perfcl_loss(\n            features[\"local_features\"],\n            features[\"old_local_features\"],\n            features[\"global_features\"],\n            features[\"old_global_features\"],\n            features[\"initial_global_features\"],\n        )\n        total_loss += global_feature_contrastive_loss + local_feature_contrastive_loss\n        additional_losses[\"global_feature_contrastive_loss\"] = global_feature_contrastive_loss\n        additional_losses[\"local_feature_contrastive_loss\"] = local_feature_contrastive_loss\n\n    additional_losses[\"total_loss\"] = total_loss\n\n    return total_loss, additional_losses\n</code></pre>"},{"location":"api/#fl4health.clients.constrained_fenda_client.ConstrainedFendaClient.compute_evaluation_loss","title":"<code>compute_evaluation_loss(preds, features, target)</code>","text":"<p>Computes evaluation loss given predictions of the model and ground truth data. Optionally computes additional loss components such as <code>cosine_similarity_loss</code>, <code>contrastive_loss</code> and <code>perfcl_loss</code> based on client attributes set from server config.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> containing checkpoint loss and additional losses indexed by name. Additional losses may include <code>cosine_similarity_loss</code>, <code>contrastive_loss</code> and <code>perfcl_loss</code>.</p> Source code in <code>fl4health/clients/constrained_fenda_client.py</code> <pre><code>def compute_evaluation_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; EvaluationLosses:\n    \"\"\"\n    Computes evaluation loss given predictions of the model and ground truth data. Optionally computes\n    additional loss components such as ``cosine_similarity_loss``, ``contrastive_loss`` and ``perfcl_loss`` based\n    on client attributes set from server config.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            All predictions included in dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n            indexed by name. Additional losses may include ``cosine_similarity_loss``, ``contrastive_loss``\n            and ``perfcl_loss``.\n    \"\"\"\n    _, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n    return EvaluationLosses(checkpoint=additional_losses[\"loss\"], additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.deep_mmd_clients","title":"<code>deep_mmd_clients</code>","text":""},{"location":"api/#fl4health.clients.deep_mmd_clients.ditto_deep_mmd_client","title":"<code>ditto_deep_mmd_client</code>","text":""},{"location":"api/#fl4health.clients.deep_mmd_clients.ditto_deep_mmd_client.DittoDeepMmdClient","title":"<code>DittoDeepMmdClient</code>","text":"<p>               Bases: <code>DittoClient</code></p> Source code in <code>fl4health/clients/deep_mmd_clients/ditto_deep_mmd_client.py</code> <pre><code>class DittoDeepMmdClient(DittoClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n        deep_mmd_loss_weight: float = 10.0,\n        feature_extraction_layers_with_size: dict[str, int] | None = None,\n        mmd_kernel_train_interval: int = 20,\n        num_accumulating_batches: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client implements the Deep MMD loss function in the Ditto framework. The Deep MMD loss is a measure of\n        the distance between the distributions of the features of the local model and initial global model of each\n        round. The Deep MMD loss is added to the local loss to penalize the local model for drifting away from the\n        global model.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            deep_mmd_loss_weight (float, optional): weight applied to the Deep MMD loss. Defaults to 10.0.\n            feature_extraction_layers_with_size (dict[str, int] | None, optional): Dictionary of layers to extract\n                features from them and their respective feature size. Defaults to None.\n            mmd_kernel_train_interval (int, optional): interval at which to train and update the Deep MMD kernel. If\n                set to above 0, the kernel will be train based on whole distribution of latent features of data with\n                the given train interval. If set to 0, the kernel will not be trained. If set to -1, the kernel will\n                be trained after each individual batch based on only that individual batch. Defaults to 20.\n            num_accumulating_batches (int, optional): Number of batches to accumulate features to approximate the whole\n                distribution of the latent features for updating Deep MMD kernel. This parameter is only used\n                if ``mmd_kernel_train_interval`` is set to larger than 0. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.deep_mmd_loss_weight = deep_mmd_loss_weight\n        if self.deep_mmd_loss_weight == 0:\n            log(\n                ERROR,\n                \"Deep MMD loss weight is set to 0. As Deep MMD loss will not be computed, \",\n                \"please use vanilla DittoClient instead.\",\n            )\n\n        if feature_extraction_layers_with_size is None:\n            feature_extraction_layers_with_size = {}\n        self.flatten_feature_extraction_layers = dict.fromkeys(feature_extraction_layers_with_size.keys(), True)\n        self.deep_mmd_losses: dict[str, DeepMmdLoss] = {}\n        # Save the random state to be restored after initializing the Deep MMD loss layers.\n        random_state, numpy_state, torch_state = save_random_state()\n        for layer, feature_size in feature_extraction_layers_with_size.items():\n            self.deep_mmd_losses[layer] = DeepMmdLoss(\n                device=self.device,\n                input_size=feature_size,\n            ).to(self.device)\n        # Restore the random state after initializing the Deep MMD loss layers. This is to ensure that the random state\n        # would not change after initializing the Deep MMD loss.\n        restore_random_state(random_state, numpy_state, torch_state)\n        self.initial_global_model: nn.Module\n        self.local_feature_extractor: FeatureExtractorBuffer\n        self.initial_global_feature_extractor: FeatureExtractorBuffer\n        self.num_accumulating_batches = num_accumulating_batches\n        self.mmd_kernel_train_interval = mmd_kernel_train_interval\n\n    def setup_client(self, config: Config) -&gt; None:\n        super().setup_client(config)\n        self.local_feature_extractor = FeatureExtractorBuffer(\n            model=self.model,\n            flatten_feature_extraction_layers=self.flatten_feature_extraction_layers,\n        )\n        # Register hooks to extract features from the local model if not already registered\n        self.local_feature_extractor._maybe_register_hooks()\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        super().update_before_train(current_server_round)\n        assert isinstance(self.global_model, nn.Module)\n        # Clone and freeze the initial weights GLOBAL MODEL. These are used to form the Ditto local\n        # update penalty term.\n        self.initial_global_model = clone_and_freeze_model(self.global_model)\n        self.initial_global_feature_extractor = FeatureExtractorBuffer(\n            model=self.initial_global_model,\n            flatten_feature_extraction_layers=self.flatten_feature_extraction_layers,\n        )\n        # Register hooks to extract features from the initial global model if not already registered\n        self.initial_global_feature_extractor._maybe_register_hooks()\n        # Enable training of Deep MMD loss layers if the mmd_kernel_train_interval is set to -1\n        # meaning that the kernel parameters will be trained after each individual batch based on only that\n        # individual batch\n        if self.mmd_kernel_train_interval == -1:\n            for layer in self.flatten_feature_extraction_layers:\n                self.deep_mmd_losses[layer].training = True\n\n    def _should_optimize_betas(self, step: int) -&gt; bool:\n        step_at_interval = (step - 1) % self.mmd_kernel_train_interval == 0\n        valid_components_present = self.initial_global_model is not None\n        # If the Deep MMD loss doesn't matter, we don't bother optimizing betas\n        weighted_deep_mmd_loss = self.deep_mmd_loss_weight != 0\n        return step_at_interval and valid_components_present and weighted_deep_mmd_loss\n\n    def update_after_step(self, step: int, current_round: int | None = None) -&gt; None:\n        if self.mmd_kernel_train_interval &gt; 0 and self._should_optimize_betas(step):\n            # Get the feature distribution of the local and initial global features with evaluation\n            # mode\n            local_distributions, initial_global_distributions = self.update_buffers(\n                self.model, self.initial_global_model\n            )\n            # As we set the training mode of the Deep MMD loss layers to True, we train the\n            # kernel of the Deep MMD loss based on gathered features in the buffer and compute the\n            # Deep MMD loss\n            for layer, layer_deep_mmd_loss in self.deep_mmd_losses.items():\n                layer_deep_mmd_loss.training = True\n                layer_deep_mmd_loss(local_distributions[layer], initial_global_distributions[layer])\n                layer_deep_mmd_loss.training = False\n        super().update_after_step(step)\n\n    def update_buffers(\n        self, local_model: torch.nn.Module, initial_global_model: torch.nn.Module\n    ) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n        \"\"\"\n        Update the feature buffer of the local and global features.\n\n        Args:\n            local_model (torch.nn.Module): Local model to extract features from.\n            initial_global_model (torch.nn.Module): Initial global model to extract features from.\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple containing the extracted\n                features using the local and initial global models.\n        \"\"\"\n        self.local_feature_extractor.clear_buffers()\n        self.initial_global_feature_extractor.clear_buffers()\n\n        self.local_feature_extractor.enable_accumulating_features()\n        self.initial_global_feature_extractor.enable_accumulating_features()\n\n        # Save the initial state of the local model to restore it after the buffer is populated,\n        # however as initial global model is already cloned and frozen, we don't need to save its state.\n        initial_state_local_model = local_model.training\n\n        # Set local model to evaluation mode, as we don't want to create a computational graph\n        # for the local model when populating the local buffer with features to train Deep MMD\n        # kernel\n        local_model.eval()\n\n        # Make sure the local model is in evaluation mode before populating the local buffer\n        assert not local_model.training\n\n        # Make sure the initial global model is in evaluation mode before populating the global buffer\n        # as it is already cloned and frozen from the global model\n        assert not initial_global_model.training\n\n        with torch.no_grad():\n            for i, (input, _) in enumerate(self.train_loader):\n                input = input.to(self.device)\n                # Pass the input through the local model to populate the local_feature_extractor buffer\n                local_model(input)\n                # Pass the input through the initial global model to populate the initial_global_feature_extractor\n                # buffer\n                initial_global_model(input)\n                # Break if the number of accumulating batches is reached to avoid memory issues\n                if i == self.num_accumulating_batches:\n                    break\n        local_distributions = self.local_feature_extractor.get_extracted_features()\n        initial_global_distributions = self.initial_global_feature_extractor.get_extracted_features()\n        # Restore the initial state of the local model\n        if initial_state_local_model:\n            local_model.train()\n\n        self.local_feature_extractor.disable_accumulating_features()\n        self.initial_global_feature_extractor.disable_accumulating_features()\n\n        self.local_feature_extractor.clear_buffers()\n        self.initial_global_feature_extractor.clear_buffers()\n\n        return local_distributions, initial_global_distributions\n\n    def predict(\n        self,\n        input: TorchInputType,\n    ) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Computes the predictions for both the **GLOBAL** and **LOCAL** models and pack them into the prediction\n        dictionary.\n\n        Args:\n            input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n                it is assumed that the keys of input match the names of the keyword arguments of\n                ``self.model.forward()``.\n\n        Returns:\n            (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n                predictions indexed by name and the second element contains intermediate activations indexed by name.\n                By passing features, we can compute all the losses. All predictions included in dictionary will by\n                default be used to compute metrics separately.\n        \"\"\"\n        # We use features from initial_global_model to compute the Deep MMD loss not the global_model\n        global_preds = self.global_model(input)\n        local_preds = self.model(input)\n        features = self.local_feature_extractor.get_extracted_features()\n        if self.deep_mmd_loss_weight != 0:\n            # Compute the features of the initial_global_model\n            self.initial_global_model(input)\n            initial_global_features = self.initial_global_feature_extractor.get_extracted_features()\n            for key, initial_global_feature in initial_global_features.items():\n                features[\" \".join([\"init_global\", key])] = initial_global_feature\n\n        return {\"global\": global_preds, \"local\": local_preds}, features\n\n    def _maybe_checkpoint(self, loss: float, metrics: dict[str, Scalar], checkpoint_mode: CheckpointMode) -&gt; None:\n        # Hooks need to be removed before checkpointing the model\n        self.local_feature_extractor.remove_hooks()\n        super()._maybe_checkpoint(loss=loss, metrics=metrics, checkpoint_mode=checkpoint_mode)\n        # As hooks have to be removed to checkpoint the model, so we check if they need to be re-registered\n        # each time.\n        self.local_feature_extractor._maybe_register_hooks()\n\n    def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Validate the current model on the entire validation dataset.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n        \"\"\"\n        for layer in self.flatten_feature_extraction_layers:\n            self.deep_mmd_losses[layer].training = False\n        return super().validate(include_losses_in_metrics)\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training losses given predictions of the global and local models and ground truth data.\n        For the local model we add to the vanilla loss function by including Ditto penalty loss which is the\n        \\\\(\\\\ell^2\\\\) inner product between the initial global model weights and weights of the local model. This is\n        stored in backward The loss to optimize the global model is stored in the additional losses dictionary\n        under \u201cglobal_loss\u201d.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n                dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n                by name. Additional losses includes each loss component and the global model loss tensor.\n        \"\"\"\n        for layer_loss_module in self.deep_mmd_losses.values():\n            if self.mmd_kernel_train_interval == -1:\n                assert layer_loss_module.training\n            else:\n                assert not layer_loss_module.training\n        # Check that both models are in training mode\n        assert self.global_model.training and self.model.training\n\n        # local loss is stored in loss, global model loss is stored in additional losses.\n        loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n        # Setting the adaptation loss to that of the local model, as its performance should dictate whether more or\n        # less weight is used to constrain it to the global model (as in FedProx)\n        additional_losses[\"loss_for_adaptation\"] = additional_losses[\"local_loss\"].clone()\n\n        # This is the Ditto penalty loss of the local model compared with the original Global model weights, scaled\n        # by drift_penalty_weight (or lambda in the original paper)\n        penalty_loss = self.compute_penalty_loss()\n        additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n        total_loss = loss + penalty_loss\n\n        # Add Deep MMD loss based on computed features during training\n        if self.deep_mmd_loss_weight != 0:\n            total_loss += additional_losses[\"deep_mmd_loss_total\"]\n\n        additional_losses[\"total_loss\"] = total_loss.clone()\n\n        return TrainingLosses(backward=total_loss, additional_losses=additional_losses)\n\n    def compute_evaluation_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses:\n        \"\"\"\n        Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored in preds will be\n                used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n                indexed by name.\n        \"\"\"\n        for layer_loss_module in self.deep_mmd_losses.values():\n            assert not layer_loss_module.training\n        return super().compute_evaluation_loss(preds, features, target)\n\n    def compute_loss_and_additional_losses(\n        self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Computes the loss and any additional losses given predictions of the model and ground truth data.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n                - The tensor for the loss\n                - A dictionary of additional losses with their names and values, or None if there are no additional\n                  losses.\n        \"\"\"\n        loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n\n        if self.deep_mmd_loss_weight != 0:\n            # Compute Deep MMD loss based on computed features during training\n            total_deep_mmd_loss = torch.tensor(0.0, device=self.device)\n            for layer, layer_deep_mmd_loss in self.deep_mmd_losses.items():\n                deep_mmd_loss = layer_deep_mmd_loss(features[layer], features[\" \".join([\"init_global\", layer])])\n                additional_losses[\"_\".join([\"deep_mmd_loss\", layer])] = deep_mmd_loss.clone()\n                total_deep_mmd_loss += deep_mmd_loss\n            additional_losses[\"deep_mmd_loss_total\"] = self.deep_mmd_loss_weight * total_deep_mmd_loss\n\n        return loss, additional_losses\n</code></pre> <code></code> <code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None, deep_mmd_loss_weight=10.0, feature_extraction_layers_with_size=None, mmd_kernel_train_interval=20, num_accumulating_batches=None)</code> \u00b6 <p>This client implements the Deep MMD loss function in the Ditto framework. The Deep MMD loss is a measure of the distance between the distributions of the features of the local model and initial global model of each round. The Deep MMD loss is added to the local loss to penalize the local model for drifting away from the global model.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>deep_mmd_loss_weight</code> <code>float</code> <p>weight applied to the Deep MMD loss. Defaults to 10.0.</p> <code>10.0</code> <code>feature_extraction_layers_with_size</code> <code>dict[str, int] | None</code> <p>Dictionary of layers to extract features from them and their respective feature size. Defaults to None.</p> <code>None</code> <code>mmd_kernel_train_interval</code> <code>int</code> <p>interval at which to train and update the Deep MMD kernel. If set to above 0, the kernel will be train based on whole distribution of latent features of data with the given train interval. If set to 0, the kernel will not be trained. If set to -1, the kernel will be trained after each individual batch based on only that individual batch. Defaults to 20.</p> <code>20</code> <code>num_accumulating_batches</code> <code>int</code> <p>Number of batches to accumulate features to approximate the whole distribution of the latent features for updating Deep MMD kernel. This parameter is only used if <code>mmd_kernel_train_interval</code> is set to larger than 0. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/deep_mmd_clients/ditto_deep_mmd_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n    deep_mmd_loss_weight: float = 10.0,\n    feature_extraction_layers_with_size: dict[str, int] | None = None,\n    mmd_kernel_train_interval: int = 20,\n    num_accumulating_batches: int | None = None,\n) -&gt; None:\n    \"\"\"\n    This client implements the Deep MMD loss function in the Ditto framework. The Deep MMD loss is a measure of\n    the distance between the distributions of the features of the local model and initial global model of each\n    round. The Deep MMD loss is added to the local loss to penalize the local model for drifting away from the\n    global model.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        deep_mmd_loss_weight (float, optional): weight applied to the Deep MMD loss. Defaults to 10.0.\n        feature_extraction_layers_with_size (dict[str, int] | None, optional): Dictionary of layers to extract\n            features from them and their respective feature size. Defaults to None.\n        mmd_kernel_train_interval (int, optional): interval at which to train and update the Deep MMD kernel. If\n            set to above 0, the kernel will be train based on whole distribution of latent features of data with\n            the given train interval. If set to 0, the kernel will not be trained. If set to -1, the kernel will\n            be trained after each individual batch based on only that individual batch. Defaults to 20.\n        num_accumulating_batches (int, optional): Number of batches to accumulate features to approximate the whole\n            distribution of the latent features for updating Deep MMD kernel. This parameter is only used\n            if ``mmd_kernel_train_interval`` is set to larger than 0. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.deep_mmd_loss_weight = deep_mmd_loss_weight\n    if self.deep_mmd_loss_weight == 0:\n        log(\n            ERROR,\n            \"Deep MMD loss weight is set to 0. As Deep MMD loss will not be computed, \",\n            \"please use vanilla DittoClient instead.\",\n        )\n\n    if feature_extraction_layers_with_size is None:\n        feature_extraction_layers_with_size = {}\n    self.flatten_feature_extraction_layers = dict.fromkeys(feature_extraction_layers_with_size.keys(), True)\n    self.deep_mmd_losses: dict[str, DeepMmdLoss] = {}\n    # Save the random state to be restored after initializing the Deep MMD loss layers.\n    random_state, numpy_state, torch_state = save_random_state()\n    for layer, feature_size in feature_extraction_layers_with_size.items():\n        self.deep_mmd_losses[layer] = DeepMmdLoss(\n            device=self.device,\n            input_size=feature_size,\n        ).to(self.device)\n    # Restore the random state after initializing the Deep MMD loss layers. This is to ensure that the random state\n    # would not change after initializing the Deep MMD loss.\n    restore_random_state(random_state, numpy_state, torch_state)\n    self.initial_global_model: nn.Module\n    self.local_feature_extractor: FeatureExtractorBuffer\n    self.initial_global_feature_extractor: FeatureExtractorBuffer\n    self.num_accumulating_batches = num_accumulating_batches\n    self.mmd_kernel_train_interval = mmd_kernel_train_interval\n</code></pre> <code></code> <code>update_buffers(local_model, initial_global_model)</code> \u00b6 <p>Update the feature buffer of the local and global features.</p> <p>Parameters:</p> Name Type Description Default <code>local_model</code> <code>Module</code> <p>Local model to extract features from.</p> required <code>initial_global_model</code> <code>Module</code> <p>Initial global model to extract features from.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>A tuple containing the extracted features using the local and initial global models.</p> Source code in <code>fl4health/clients/deep_mmd_clients/ditto_deep_mmd_client.py</code> <pre><code>def update_buffers(\n    self, local_model: torch.nn.Module, initial_global_model: torch.nn.Module\n) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n    \"\"\"\n    Update the feature buffer of the local and global features.\n\n    Args:\n        local_model (torch.nn.Module): Local model to extract features from.\n        initial_global_model (torch.nn.Module): Initial global model to extract features from.\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple containing the extracted\n            features using the local and initial global models.\n    \"\"\"\n    self.local_feature_extractor.clear_buffers()\n    self.initial_global_feature_extractor.clear_buffers()\n\n    self.local_feature_extractor.enable_accumulating_features()\n    self.initial_global_feature_extractor.enable_accumulating_features()\n\n    # Save the initial state of the local model to restore it after the buffer is populated,\n    # however as initial global model is already cloned and frozen, we don't need to save its state.\n    initial_state_local_model = local_model.training\n\n    # Set local model to evaluation mode, as we don't want to create a computational graph\n    # for the local model when populating the local buffer with features to train Deep MMD\n    # kernel\n    local_model.eval()\n\n    # Make sure the local model is in evaluation mode before populating the local buffer\n    assert not local_model.training\n\n    # Make sure the initial global model is in evaluation mode before populating the global buffer\n    # as it is already cloned and frozen from the global model\n    assert not initial_global_model.training\n\n    with torch.no_grad():\n        for i, (input, _) in enumerate(self.train_loader):\n            input = input.to(self.device)\n            # Pass the input through the local model to populate the local_feature_extractor buffer\n            local_model(input)\n            # Pass the input through the initial global model to populate the initial_global_feature_extractor\n            # buffer\n            initial_global_model(input)\n            # Break if the number of accumulating batches is reached to avoid memory issues\n            if i == self.num_accumulating_batches:\n                break\n    local_distributions = self.local_feature_extractor.get_extracted_features()\n    initial_global_distributions = self.initial_global_feature_extractor.get_extracted_features()\n    # Restore the initial state of the local model\n    if initial_state_local_model:\n        local_model.train()\n\n    self.local_feature_extractor.disable_accumulating_features()\n    self.initial_global_feature_extractor.disable_accumulating_features()\n\n    self.local_feature_extractor.clear_buffers()\n    self.initial_global_feature_extractor.clear_buffers()\n\n    return local_distributions, initial_global_distributions\n</code></pre> <code></code> <code>predict(input)</code> \u00b6 <p>Computes the predictions for both the GLOBAL and LOCAL models and pack them into the prediction dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into the model. If input is of type <code>dict[str, torch.Tensor]</code>, it is assumed that the keys of input match the names of the keyword arguments of <code>self.model.forward()</code>.</p> required <p>Returns:</p> Type Description <code>tuple[TorchPredType, TorchFeatureType]</code> <p>A tuple in which the first element contains a dictionary of predictions indexed by name and the second element contains intermediate activations indexed by name. By passing features, we can compute all the losses. All predictions included in dictionary will by default be used to compute metrics separately.</p> Source code in <code>fl4health/clients/deep_mmd_clients/ditto_deep_mmd_client.py</code> <pre><code>def predict(\n    self,\n    input: TorchInputType,\n) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Computes the predictions for both the **GLOBAL** and **LOCAL** models and pack them into the prediction\n    dictionary.\n\n    Args:\n        input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n            it is assumed that the keys of input match the names of the keyword arguments of\n            ``self.model.forward()``.\n\n    Returns:\n        (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n            predictions indexed by name and the second element contains intermediate activations indexed by name.\n            By passing features, we can compute all the losses. All predictions included in dictionary will by\n            default be used to compute metrics separately.\n    \"\"\"\n    # We use features from initial_global_model to compute the Deep MMD loss not the global_model\n    global_preds = self.global_model(input)\n    local_preds = self.model(input)\n    features = self.local_feature_extractor.get_extracted_features()\n    if self.deep_mmd_loss_weight != 0:\n        # Compute the features of the initial_global_model\n        self.initial_global_model(input)\n        initial_global_features = self.initial_global_feature_extractor.get_extracted_features()\n        for key, initial_global_feature in initial_global_features.items():\n            features[\" \".join([\"init_global\", key])] = initial_global_feature\n\n    return {\"global\": global_preds, \"local\": local_preds}, features\n</code></pre> <code></code> <code>validate(include_losses_in_metrics=False)</code> \u00b6 <p>Validate the current model on the entire validation dataset.</p> <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]]</code> <p>The validation loss and a dictionary of metrics from validation.</p> Source code in <code>fl4health/clients/deep_mmd_clients/ditto_deep_mmd_client.py</code> <pre><code>def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n    \"\"\"\n    Validate the current model on the entire validation dataset.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n    \"\"\"\n    for layer in self.flatten_feature_extraction_layers:\n        self.deep_mmd_losses[layer].training = False\n    return super().validate(include_losses_in_metrics)\n</code></pre> <code></code> <code>compute_training_loss(preds, features, target)</code> \u00b6 <p>Computes training losses given predictions of the global and local models and ground truth data. For the local model we add to the vanilla loss function by including Ditto penalty loss which is the \\(\\ell^2\\) inner product between the initial global model weights and weights of the local model. This is stored in backward The loss to optimize the global model is stored in the additional losses dictionary under \u201cglobal_loss\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name. Additional losses includes each loss component and the global model loss tensor.</p> Source code in <code>fl4health/clients/deep_mmd_clients/ditto_deep_mmd_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training losses given predictions of the global and local models and ground truth data.\n    For the local model we add to the vanilla loss function by including Ditto penalty loss which is the\n    \\\\(\\\\ell^2\\\\) inner product between the initial global model weights and weights of the local model. This is\n    stored in backward The loss to optimize the global model is stored in the additional losses dictionary\n    under \u201cglobal_loss\u201d.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n            dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n            by name. Additional losses includes each loss component and the global model loss tensor.\n    \"\"\"\n    for layer_loss_module in self.deep_mmd_losses.values():\n        if self.mmd_kernel_train_interval == -1:\n            assert layer_loss_module.training\n        else:\n            assert not layer_loss_module.training\n    # Check that both models are in training mode\n    assert self.global_model.training and self.model.training\n\n    # local loss is stored in loss, global model loss is stored in additional losses.\n    loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n    # Setting the adaptation loss to that of the local model, as its performance should dictate whether more or\n    # less weight is used to constrain it to the global model (as in FedProx)\n    additional_losses[\"loss_for_adaptation\"] = additional_losses[\"local_loss\"].clone()\n\n    # This is the Ditto penalty loss of the local model compared with the original Global model weights, scaled\n    # by drift_penalty_weight (or lambda in the original paper)\n    penalty_loss = self.compute_penalty_loss()\n    additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n    total_loss = loss + penalty_loss\n\n    # Add Deep MMD loss based on computed features during training\n    if self.deep_mmd_loss_weight != 0:\n        total_loss += additional_losses[\"deep_mmd_loss_total\"]\n\n    additional_losses[\"total_loss\"] = total_loss.clone()\n\n    return TrainingLosses(backward=total_loss, additional_losses=additional_losses)\n</code></pre> <code></code> <code>compute_evaluation_loss(preds, features, target)</code> \u00b6 <p>Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. Anything stored in preds will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> containing checkpoint loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/deep_mmd_clients/ditto_deep_mmd_client.py</code> <pre><code>def compute_evaluation_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; EvaluationLosses:\n    \"\"\"\n    Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored in preds will be\n            used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n            indexed by name.\n    \"\"\"\n    for layer_loss_module in self.deep_mmd_losses.values():\n        assert not layer_loss_module.training\n    return super().compute_evaluation_loss(preds, features, target)\n</code></pre> <code></code> <code>compute_loss_and_additional_losses(preds, features, target)</code> \u00b6 <p>Computes the loss and any additional losses given predictions of the model and ground truth data.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor]]</code> <p>A tuple with:</p> <ul> <li>The tensor for the loss</li> <li>A dictionary of additional losses with their names and values, or None if there are no additional   losses.</li> </ul> Source code in <code>fl4health/clients/deep_mmd_clients/ditto_deep_mmd_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Computes the loss and any additional losses given predictions of the model and ground truth data.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n            - The tensor for the loss\n            - A dictionary of additional losses with their names and values, or None if there are no additional\n              losses.\n    \"\"\"\n    loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n\n    if self.deep_mmd_loss_weight != 0:\n        # Compute Deep MMD loss based on computed features during training\n        total_deep_mmd_loss = torch.tensor(0.0, device=self.device)\n        for layer, layer_deep_mmd_loss in self.deep_mmd_losses.items():\n            deep_mmd_loss = layer_deep_mmd_loss(features[layer], features[\" \".join([\"init_global\", layer])])\n            additional_losses[\"_\".join([\"deep_mmd_loss\", layer])] = deep_mmd_loss.clone()\n            total_deep_mmd_loss += deep_mmd_loss\n        additional_losses[\"deep_mmd_loss_total\"] = self.deep_mmd_loss_weight * total_deep_mmd_loss\n\n    return loss, additional_losses\n</code></pre>"},{"location":"api/#fl4health.clients.deep_mmd_clients.mr_mtl_deep_mmd_client","title":"<code>mr_mtl_deep_mmd_client</code>","text":""},{"location":"api/#fl4health.clients.deep_mmd_clients.mr_mtl_deep_mmd_client.MrMtlDeepMmdClient","title":"<code>MrMtlDeepMmdClient</code>","text":"<p>               Bases: <code>MrMtlClient</code></p> Source code in <code>fl4health/clients/deep_mmd_clients/mr_mtl_deep_mmd_client.py</code> <pre><code>class MrMtlDeepMmdClient(MrMtlClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n        deep_mmd_loss_weight: float = 10.0,\n        feature_extraction_layers_with_size: dict[str, int] | None = None,\n        mmd_kernel_train_interval: int = 20,\n        num_accumulating_batches: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client implements the Deep MMD loss function in the MR-MTL framework. The Deep MMD loss is a measure of\n        the distance between the distributions of the features of the local model and averaged local models of each\n        round. The Deep MMD loss is added to the local loss to penalize the local model for drifting away from the\n        averaged local models.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            deep_mmd_loss_weight (float, optional): weight applied to the Deep MMD loss. Defaults to 10.0.\n            feature_extraction_layers_with_size (dict[str, int] | None, optional): Dictionary of layers to extract\n                features from them and their respective feature size. Defaults to None.\n            mmd_kernel_train_interval (int, optional): interval at which to train and update the Deep MMD kernel. If\n                set to above 0, the kernel will be train based on whole distribution of latent features of data with\n                the given train interval. If set to 0, the kernel will not be trained. If set to -1, the kernel will\n                be trained after each individual batch based on only that individual batch. Defaults to 20.\n            num_accumulating_batches (int, optional): Number of batches to accumulate features to approximate the whole\n                distribution of the latent features for updating Deep MMD kernel. This parameter is only used\n                if ``mmd_kernel_train_interval`` is set to larger than 0. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.deep_mmd_loss_weight = deep_mmd_loss_weight\n        if self.deep_mmd_loss_weight == 0:\n            log(\n                ERROR,\n                \"Deep MMD loss weight is set to 0. As Deep MMD loss will not be computed, \",\n                \"please use vanilla MrMtlClient instead.\",\n            )\n\n        if feature_extraction_layers_with_size is None:\n            feature_extraction_layers_with_size = {}\n        self.flatten_feature_extraction_layers = dict.fromkeys(feature_extraction_layers_with_size.keys(), True)\n        self.deep_mmd_losses: dict[str, DeepMmdLoss] = {}\n        # Save the random state to be restored after initializing the Deep MMD loss layers.\n        random_state, numpy_state, torch_state = save_random_state()\n        for layer, feature_size in feature_extraction_layers_with_size.items():\n            self.deep_mmd_losses[layer] = DeepMmdLoss(\n                device=self.device,\n                input_size=feature_size,\n            ).to(self.device)\n        # Restore the random state after initializing the Deep MMD loss layers. This is to ensure that the random state\n        # would not change after initializing the Deep MMD loss.\n        restore_random_state(random_state, numpy_state, torch_state)\n\n        self.local_feature_extractor: FeatureExtractorBuffer\n        self.initial_global_feature_extractor: FeatureExtractorBuffer\n        self.num_accumulating_batches = num_accumulating_batches\n        self.mmd_kernel_train_interval = mmd_kernel_train_interval\n\n    def setup_client(self, config: Config) -&gt; None:\n        super().setup_client(config)\n        self.local_feature_extractor = FeatureExtractorBuffer(\n            model=self.model,\n            flatten_feature_extraction_layers=self.flatten_feature_extraction_layers,\n        )\n        # Register hooks to extract features from the local model if not already registered\n        self.local_feature_extractor._maybe_register_hooks()\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        super().update_before_train(current_server_round)\n        self.initial_global_feature_extractor = FeatureExtractorBuffer(\n            model=self.initial_global_model,\n            flatten_feature_extraction_layers=self.flatten_feature_extraction_layers,\n        )\n        # Register hooks to extract features from the initial global model if not already registered\n        self.initial_global_feature_extractor._maybe_register_hooks()\n        # Enable training of Deep MMD loss layers if the mmd_kernel_train_interval is set to -1\n        # meaning that the kernel parameters will be trained after each individual batch based on only that\n        # individual batch\n        if self.mmd_kernel_train_interval == -1:\n            for layer in self.flatten_feature_extraction_layers:\n                self.deep_mmd_losses[layer].training = True\n\n    def _should_optimize_betas(self, step: int) -&gt; bool:\n        step_at_interval = (step - 1) % self.mmd_kernel_train_interval == 0\n        valid_components_present = self.initial_global_model is not None\n        # If the Deep MMD loss doesn't matter, we don't bother optimizing betas\n        weighted_deep_mmd_loss = self.deep_mmd_loss_weight != 0\n        return step_at_interval and valid_components_present and weighted_deep_mmd_loss\n\n    def update_after_step(self, step: int, current_round: int | None = None) -&gt; None:\n        if self.mmd_kernel_train_interval &gt; 0 and self._should_optimize_betas(step):\n            # Get the feature distribution of the local and initial global features with evaluation\n            # mode\n            local_distributions, initial_global_distributions = self.update_buffers(\n                self.model, self.initial_global_model\n            )\n            # As we set the training mode of the Deep MMD loss layers to True, we train the\n            # kernel of the Deep MMD loss based on gathered features in the buffer and compute the\n            # Deep MMD loss\n            for layer, layer_deep_mmd_loss in self.deep_mmd_losses.items():\n                layer_deep_mmd_loss.training = True\n                layer_deep_mmd_loss(local_distributions[layer], initial_global_distributions[layer])\n                layer_deep_mmd_loss.training = False\n        super().update_after_step(step)\n\n    def update_buffers(\n        self, local_model: torch.nn.Module, initial_global_model: torch.nn.Module\n    ) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n        \"\"\"\n        Update the feature buffer of the local and global features.\n\n        Args:\n            local_model (torch.nn.Module): Local model to extract features from.\n            initial_global_model (torch.nn.Module): Initial global model to extract features from.\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple containing the extracted\n                features using the local and initial global models.\n        \"\"\"\n        self.local_feature_extractor.clear_buffers()\n        self.initial_global_feature_extractor.clear_buffers()\n\n        self.local_feature_extractor.enable_accumulating_features()\n        self.initial_global_feature_extractor.enable_accumulating_features()\n\n        # Save the initial state of the local model to restore it after the buffer is populated,\n        # however as initial global model is already cloned and frozen, we don't need to save its state.\n        initial_state_local_model = local_model.training\n\n        # Set local model to evaluation mode, as we don't want to create a computational graph\n        # for the local model when populating the local buffer with features to train Deep MMD\n        # kernel\n        local_model.eval()\n\n        # Make sure the local model is in evaluation mode before populating the local buffer\n        assert not local_model.training\n\n        # Make sure the initial global model is in evaluation mode before populating the global buffer\n        # as it is already cloned and frozen from the global model\n        assert not initial_global_model.training\n\n        with torch.no_grad():\n            for i, (input, _) in enumerate(self.train_loader):\n                input = input.to(self.device)\n                # Pass the input through the local model to populate the local_feature_extractor buffer\n                local_model(input)\n                # Pass the input through the initial global model to populate the initial_global_feature_extractor\n                # buffer\n                initial_global_model(input)\n                # Break if the number of accumulating batches is reached to avoid memory issues\n                if i == self.num_accumulating_batches:\n                    break\n        local_distributions = self.local_feature_extractor.get_extracted_features()\n        initial_global_distributions = self.initial_global_feature_extractor.get_extracted_features()\n        # Restore the initial state of the local model\n        if initial_state_local_model:\n            local_model.train()\n\n        self.local_feature_extractor.disable_accumulating_features()\n        self.initial_global_feature_extractor.disable_accumulating_features()\n\n        self.local_feature_extractor.clear_buffers()\n        self.initial_global_feature_extractor.clear_buffers()\n\n        return local_distributions, initial_global_distributions\n\n    def predict(\n        self,\n        input: TorchInputType,\n    ) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Computes the predictions for both models and pack them into the prediction dictionary.\n\n        Args:\n            input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n                it is assumed that the keys of input match the names of the keyword arguments of\n                ``self.model.forward()``.\n\n        Returns:\n            (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n                predictions indexed by name and the second element contains intermediate activations indexed by name.\n                By passing features, we can compute all the losses. All predictions included in dictionary will by\n                default be used to compute metrics separately.\n        \"\"\"\n        # We use features from initial_global_model to compute the Deep MMD loss.\n        preds = self.model(input)\n        features = self.local_feature_extractor.get_extracted_features()\n        if self.deep_mmd_loss_weight != 0:\n            # Compute the features of the initial_global_model\n            self.initial_global_model(input)\n            initial_global_features = self.initial_global_feature_extractor.get_extracted_features()\n            for key, initial_global_feature in initial_global_features.items():\n                features[\" \".join([\"init_global\", key])] = initial_global_feature\n\n        return {\"prediction\": preds}, features\n\n    def _maybe_checkpoint(self, loss: float, metrics: dict[str, Scalar], checkpoint_mode: CheckpointMode) -&gt; None:\n        # Hooks need to be removed before checkpointing the model\n        self.local_feature_extractor.remove_hooks()\n        super()._maybe_checkpoint(loss=loss, metrics=metrics, checkpoint_mode=checkpoint_mode)\n        # As hooks have to be removed to checkpoint the model, so we check if they need to be re-registered\n        # each time.\n        self.local_feature_extractor._maybe_register_hooks()\n\n    def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Validate the current model on the entire validation dataset.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n        \"\"\"\n        for layer in self.flatten_feature_extraction_layers:\n            self.deep_mmd_losses[layer].training = False\n        return super().validate(include_losses_in_metrics)\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training losses given predictions of the client model and ground truth data.\n        For the local model we add to the vanilla loss function by including Mean Regularized (MR) penalty loss\n        which is the \\\\(\\\\ell^2\\\\) inner product between the initial global model weights and weights of the\n        current model.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n                dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n                by name.\n        \"\"\"\n        for layer_loss_module in self.deep_mmd_losses.values():\n            if self.mmd_kernel_train_interval == -1:\n                assert layer_loss_module.training\n            else:\n                assert not layer_loss_module.training\n        # Check that the initial global model isn't in training mode and that the local model is in training mode\n        assert not self.initial_global_model.training and self.model.training\n\n        loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n        # Setting the adaptation loss to that of the local model, as its performance should dictate whether more or\n        # less weight is used to constrain it to the global model (as in FedProx)\n        additional_losses[\"loss_for_adaptation\"] = additional_losses[\"loss\"].clone()\n\n        # This is the MR-MTL penalty loss of the local model compared with the original Global model weights, scaled\n        # by drift_penalty_weight (or lambda in the original paper)\n        penalty_loss = self.compute_penalty_loss()\n        additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n        total_loss = loss + penalty_loss\n\n        # Add Deep MMD loss based on computed features during training\n        if self.deep_mmd_loss_weight != 0:\n            total_loss += additional_losses[\"deep_mmd_loss_total\"]\n\n        additional_losses[\"total_loss\"] = total_loss.clone()\n\n        return TrainingLosses(backward=total_loss, additional_losses=additional_losses)\n\n    def compute_evaluation_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses:\n        \"\"\"\n        Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored in preds will be\n                used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n                indexed by name.\n        \"\"\"\n        for layer_loss_module in self.deep_mmd_losses.values():\n            assert not layer_loss_module.training\n        return super().compute_evaluation_loss(preds, features, target)\n\n    def compute_loss_and_additional_losses(\n        self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Computes the loss and any additional losses given predictions of the model and ground truth data.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n                - The tensor for the loss\n                - A dictionary of additional losses with their names and values, or None if there are no additional\n                  losses.\n        \"\"\"\n        assert \"prediction\" in preds\n        loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n\n        if additional_losses is None:\n            additional_losses = {\"loss\": loss}\n\n        if self.deep_mmd_loss_weight != 0:\n            # Compute Deep MMD loss based on computed features during training\n            total_deep_mmd_loss = torch.tensor(0.0, device=self.device)\n            for layer, layer_deep_mmd_loss in self.deep_mmd_losses.items():\n                deep_mmd_loss = layer_deep_mmd_loss(features[layer], features[\" \".join([\"init_global\", layer])])\n                additional_losses[\"_\".join([\"deep_mmd_loss\", layer])] = deep_mmd_loss.clone()\n                total_deep_mmd_loss += deep_mmd_loss\n            additional_losses[\"deep_mmd_loss_total\"] = self.deep_mmd_loss_weight * total_deep_mmd_loss\n\n        return loss, additional_losses\n</code></pre> <code></code> <code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None, deep_mmd_loss_weight=10.0, feature_extraction_layers_with_size=None, mmd_kernel_train_interval=20, num_accumulating_batches=None)</code> \u00b6 <p>This client implements the Deep MMD loss function in the MR-MTL framework. The Deep MMD loss is a measure of the distance between the distributions of the features of the local model and averaged local models of each round. The Deep MMD loss is added to the local loss to penalize the local model for drifting away from the averaged local models.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>deep_mmd_loss_weight</code> <code>float</code> <p>weight applied to the Deep MMD loss. Defaults to 10.0.</p> <code>10.0</code> <code>feature_extraction_layers_with_size</code> <code>dict[str, int] | None</code> <p>Dictionary of layers to extract features from them and their respective feature size. Defaults to None.</p> <code>None</code> <code>mmd_kernel_train_interval</code> <code>int</code> <p>interval at which to train and update the Deep MMD kernel. If set to above 0, the kernel will be train based on whole distribution of latent features of data with the given train interval. If set to 0, the kernel will not be trained. If set to -1, the kernel will be trained after each individual batch based on only that individual batch. Defaults to 20.</p> <code>20</code> <code>num_accumulating_batches</code> <code>int</code> <p>Number of batches to accumulate features to approximate the whole distribution of the latent features for updating Deep MMD kernel. This parameter is only used if <code>mmd_kernel_train_interval</code> is set to larger than 0. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/deep_mmd_clients/mr_mtl_deep_mmd_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n    deep_mmd_loss_weight: float = 10.0,\n    feature_extraction_layers_with_size: dict[str, int] | None = None,\n    mmd_kernel_train_interval: int = 20,\n    num_accumulating_batches: int | None = None,\n) -&gt; None:\n    \"\"\"\n    This client implements the Deep MMD loss function in the MR-MTL framework. The Deep MMD loss is a measure of\n    the distance between the distributions of the features of the local model and averaged local models of each\n    round. The Deep MMD loss is added to the local loss to penalize the local model for drifting away from the\n    averaged local models.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        deep_mmd_loss_weight (float, optional): weight applied to the Deep MMD loss. Defaults to 10.0.\n        feature_extraction_layers_with_size (dict[str, int] | None, optional): Dictionary of layers to extract\n            features from them and their respective feature size. Defaults to None.\n        mmd_kernel_train_interval (int, optional): interval at which to train and update the Deep MMD kernel. If\n            set to above 0, the kernel will be train based on whole distribution of latent features of data with\n            the given train interval. If set to 0, the kernel will not be trained. If set to -1, the kernel will\n            be trained after each individual batch based on only that individual batch. Defaults to 20.\n        num_accumulating_batches (int, optional): Number of batches to accumulate features to approximate the whole\n            distribution of the latent features for updating Deep MMD kernel. This parameter is only used\n            if ``mmd_kernel_train_interval`` is set to larger than 0. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.deep_mmd_loss_weight = deep_mmd_loss_weight\n    if self.deep_mmd_loss_weight == 0:\n        log(\n            ERROR,\n            \"Deep MMD loss weight is set to 0. As Deep MMD loss will not be computed, \",\n            \"please use vanilla MrMtlClient instead.\",\n        )\n\n    if feature_extraction_layers_with_size is None:\n        feature_extraction_layers_with_size = {}\n    self.flatten_feature_extraction_layers = dict.fromkeys(feature_extraction_layers_with_size.keys(), True)\n    self.deep_mmd_losses: dict[str, DeepMmdLoss] = {}\n    # Save the random state to be restored after initializing the Deep MMD loss layers.\n    random_state, numpy_state, torch_state = save_random_state()\n    for layer, feature_size in feature_extraction_layers_with_size.items():\n        self.deep_mmd_losses[layer] = DeepMmdLoss(\n            device=self.device,\n            input_size=feature_size,\n        ).to(self.device)\n    # Restore the random state after initializing the Deep MMD loss layers. This is to ensure that the random state\n    # would not change after initializing the Deep MMD loss.\n    restore_random_state(random_state, numpy_state, torch_state)\n\n    self.local_feature_extractor: FeatureExtractorBuffer\n    self.initial_global_feature_extractor: FeatureExtractorBuffer\n    self.num_accumulating_batches = num_accumulating_batches\n    self.mmd_kernel_train_interval = mmd_kernel_train_interval\n</code></pre> <code></code> <code>update_buffers(local_model, initial_global_model)</code> \u00b6 <p>Update the feature buffer of the local and global features.</p> <p>Parameters:</p> Name Type Description Default <code>local_model</code> <code>Module</code> <p>Local model to extract features from.</p> required <code>initial_global_model</code> <code>Module</code> <p>Initial global model to extract features from.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>A tuple containing the extracted features using the local and initial global models.</p> Source code in <code>fl4health/clients/deep_mmd_clients/mr_mtl_deep_mmd_client.py</code> <pre><code>def update_buffers(\n    self, local_model: torch.nn.Module, initial_global_model: torch.nn.Module\n) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n    \"\"\"\n    Update the feature buffer of the local and global features.\n\n    Args:\n        local_model (torch.nn.Module): Local model to extract features from.\n        initial_global_model (torch.nn.Module): Initial global model to extract features from.\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple containing the extracted\n            features using the local and initial global models.\n    \"\"\"\n    self.local_feature_extractor.clear_buffers()\n    self.initial_global_feature_extractor.clear_buffers()\n\n    self.local_feature_extractor.enable_accumulating_features()\n    self.initial_global_feature_extractor.enable_accumulating_features()\n\n    # Save the initial state of the local model to restore it after the buffer is populated,\n    # however as initial global model is already cloned and frozen, we don't need to save its state.\n    initial_state_local_model = local_model.training\n\n    # Set local model to evaluation mode, as we don't want to create a computational graph\n    # for the local model when populating the local buffer with features to train Deep MMD\n    # kernel\n    local_model.eval()\n\n    # Make sure the local model is in evaluation mode before populating the local buffer\n    assert not local_model.training\n\n    # Make sure the initial global model is in evaluation mode before populating the global buffer\n    # as it is already cloned and frozen from the global model\n    assert not initial_global_model.training\n\n    with torch.no_grad():\n        for i, (input, _) in enumerate(self.train_loader):\n            input = input.to(self.device)\n            # Pass the input through the local model to populate the local_feature_extractor buffer\n            local_model(input)\n            # Pass the input through the initial global model to populate the initial_global_feature_extractor\n            # buffer\n            initial_global_model(input)\n            # Break if the number of accumulating batches is reached to avoid memory issues\n            if i == self.num_accumulating_batches:\n                break\n    local_distributions = self.local_feature_extractor.get_extracted_features()\n    initial_global_distributions = self.initial_global_feature_extractor.get_extracted_features()\n    # Restore the initial state of the local model\n    if initial_state_local_model:\n        local_model.train()\n\n    self.local_feature_extractor.disable_accumulating_features()\n    self.initial_global_feature_extractor.disable_accumulating_features()\n\n    self.local_feature_extractor.clear_buffers()\n    self.initial_global_feature_extractor.clear_buffers()\n\n    return local_distributions, initial_global_distributions\n</code></pre> <code></code> <code>predict(input)</code> \u00b6 <p>Computes the predictions for both models and pack them into the prediction dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into the model. If input is of type <code>dict[str, torch.Tensor]</code>, it is assumed that the keys of input match the names of the keyword arguments of <code>self.model.forward()</code>.</p> required <p>Returns:</p> Type Description <code>tuple[TorchPredType, TorchFeatureType]</code> <p>A tuple in which the first element contains a dictionary of predictions indexed by name and the second element contains intermediate activations indexed by name. By passing features, we can compute all the losses. All predictions included in dictionary will by default be used to compute metrics separately.</p> Source code in <code>fl4health/clients/deep_mmd_clients/mr_mtl_deep_mmd_client.py</code> <pre><code>def predict(\n    self,\n    input: TorchInputType,\n) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Computes the predictions for both models and pack them into the prediction dictionary.\n\n    Args:\n        input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n            it is assumed that the keys of input match the names of the keyword arguments of\n            ``self.model.forward()``.\n\n    Returns:\n        (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n            predictions indexed by name and the second element contains intermediate activations indexed by name.\n            By passing features, we can compute all the losses. All predictions included in dictionary will by\n            default be used to compute metrics separately.\n    \"\"\"\n    # We use features from initial_global_model to compute the Deep MMD loss.\n    preds = self.model(input)\n    features = self.local_feature_extractor.get_extracted_features()\n    if self.deep_mmd_loss_weight != 0:\n        # Compute the features of the initial_global_model\n        self.initial_global_model(input)\n        initial_global_features = self.initial_global_feature_extractor.get_extracted_features()\n        for key, initial_global_feature in initial_global_features.items():\n            features[\" \".join([\"init_global\", key])] = initial_global_feature\n\n    return {\"prediction\": preds}, features\n</code></pre> <code></code> <code>validate(include_losses_in_metrics=False)</code> \u00b6 <p>Validate the current model on the entire validation dataset.</p> <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]]</code> <p>The validation loss and a dictionary of metrics from validation.</p> Source code in <code>fl4health/clients/deep_mmd_clients/mr_mtl_deep_mmd_client.py</code> <pre><code>def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n    \"\"\"\n    Validate the current model on the entire validation dataset.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n    \"\"\"\n    for layer in self.flatten_feature_extraction_layers:\n        self.deep_mmd_losses[layer].training = False\n    return super().validate(include_losses_in_metrics)\n</code></pre> <code></code> <code>compute_training_loss(preds, features, target)</code> \u00b6 <p>Computes training losses given predictions of the client model and ground truth data. For the local model we add to the vanilla loss function by including Mean Regularized (MR) penalty loss which is the \\(\\ell^2\\) inner product between the initial global model weights and weights of the current model.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/deep_mmd_clients/mr_mtl_deep_mmd_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training losses given predictions of the client model and ground truth data.\n    For the local model we add to the vanilla loss function by including Mean Regularized (MR) penalty loss\n    which is the \\\\(\\\\ell^2\\\\) inner product between the initial global model weights and weights of the\n    current model.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n            dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n            by name.\n    \"\"\"\n    for layer_loss_module in self.deep_mmd_losses.values():\n        if self.mmd_kernel_train_interval == -1:\n            assert layer_loss_module.training\n        else:\n            assert not layer_loss_module.training\n    # Check that the initial global model isn't in training mode and that the local model is in training mode\n    assert not self.initial_global_model.training and self.model.training\n\n    loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n    # Setting the adaptation loss to that of the local model, as its performance should dictate whether more or\n    # less weight is used to constrain it to the global model (as in FedProx)\n    additional_losses[\"loss_for_adaptation\"] = additional_losses[\"loss\"].clone()\n\n    # This is the MR-MTL penalty loss of the local model compared with the original Global model weights, scaled\n    # by drift_penalty_weight (or lambda in the original paper)\n    penalty_loss = self.compute_penalty_loss()\n    additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n    total_loss = loss + penalty_loss\n\n    # Add Deep MMD loss based on computed features during training\n    if self.deep_mmd_loss_weight != 0:\n        total_loss += additional_losses[\"deep_mmd_loss_total\"]\n\n    additional_losses[\"total_loss\"] = total_loss.clone()\n\n    return TrainingLosses(backward=total_loss, additional_losses=additional_losses)\n</code></pre> <code></code> <code>compute_evaluation_loss(preds, features, target)</code> \u00b6 <p>Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. Anything stored in preds will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> containing checkpoint loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/deep_mmd_clients/mr_mtl_deep_mmd_client.py</code> <pre><code>def compute_evaluation_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; EvaluationLosses:\n    \"\"\"\n    Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored in preds will be\n            used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n            indexed by name.\n    \"\"\"\n    for layer_loss_module in self.deep_mmd_losses.values():\n        assert not layer_loss_module.training\n    return super().compute_evaluation_loss(preds, features, target)\n</code></pre> <code></code> <code>compute_loss_and_additional_losses(preds, features, target)</code> \u00b6 <p>Computes the loss and any additional losses given predictions of the model and ground truth data.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor]]</code> <p>A tuple with:</p> <ul> <li>The tensor for the loss</li> <li>A dictionary of additional losses with their names and values, or None if there are no additional   losses.</li> </ul> Source code in <code>fl4health/clients/deep_mmd_clients/mr_mtl_deep_mmd_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Computes the loss and any additional losses given predictions of the model and ground truth data.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n            - The tensor for the loss\n            - A dictionary of additional losses with their names and values, or None if there are no additional\n              losses.\n    \"\"\"\n    assert \"prediction\" in preds\n    loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n\n    if additional_losses is None:\n        additional_losses = {\"loss\": loss}\n\n    if self.deep_mmd_loss_weight != 0:\n        # Compute Deep MMD loss based on computed features during training\n        total_deep_mmd_loss = torch.tensor(0.0, device=self.device)\n        for layer, layer_deep_mmd_loss in self.deep_mmd_losses.items():\n            deep_mmd_loss = layer_deep_mmd_loss(features[layer], features[\" \".join([\"init_global\", layer])])\n            additional_losses[\"_\".join([\"deep_mmd_loss\", layer])] = deep_mmd_loss.clone()\n            total_deep_mmd_loss += deep_mmd_loss\n        additional_losses[\"deep_mmd_loss_total\"] = self.deep_mmd_loss_weight * total_deep_mmd_loss\n\n    return loss, additional_losses\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client","title":"<code>ditto_client</code>","text":""},{"location":"api/#fl4health.clients.ditto_client.DittoClient","title":"<code>DittoClient</code>","text":"<p>               Bases: <code>AdaptiveDriftConstraintClient</code></p> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>class DittoClient(AdaptiveDriftConstraintClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client implements the Ditto algorithm from Ditto: Fair and Robust Federated Learning Through\n        Personalization. The idea is that we want to train personalized versions of the global model for each client.\n        So we simultaneously train a global model that is aggregated on the server-side and use those weights to also\n        constrain the training of a local model. The constraint for this local model is identical to the FedProx loss.\n\n        **NOTE**: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the\n        heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the\n        corresponding strategy used by the server\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.global_model: nn.Module\n\n    def get_optimizer(self, config: Config) -&gt; dict[str, Optimizer]:\n        \"\"\"\n        Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        raise NotImplementedError(\n            \"User Clients must define a function that returns a dict[str, Optimizer] with keys 'global' and 'local' \"\n            \"defining separate optimizers for the global and local models of Ditto.\"\n        )\n\n    def set_optimizer(self, config: Config) -&gt; None:\n        \"\"\"\n        Ditto requires an optimizer for the global model and one for the local model. This function simply ensures that\n        the optimizers setup by the user have the proper keys and that there are two optimizers.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        optimizers = self.get_optimizer(config)\n        assert isinstance(optimizers, dict) and {\"global\", \"local\"} == set(optimizers.keys())\n        self.optimizers = optimizers\n\n    def get_global_model(self, config: Config) -&gt; nn.Module:\n        \"\"\"\n        Returns the global model to be used during Ditto training and as a constraint for the local model.\n\n        The global model should be the same architecture as the local model so we reuse the ``get_model`` call. We\n        explicitly send the model to the desired device. This is idempotent.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (nn.Module): The PyTorch model serving as the global model for Ditto\n        \"\"\"\n        return self.get_model(config).to(self.device)\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n        Then set initialized attribute to True. In this class, this function simply adds the additional step of\n        setting up the global model.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        # Need to setup the global model here as well. It should be the same architecture as the local model.\n        self.global_model = self.get_global_model(config)\n        # The rest of the setup is the same\n        super().setup_client(config)\n\n    def get_parameters(self, config: Config) -&gt; NDArrays:\n        \"\"\"\n        For Ditto, we transfer the **GLOBAL** model weights to the server to be aggregated. The local model weights\n        stay with the client.\n\n        Args:\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n        Returns:\n            (NDArrays): **GLOBAL** model weights to be sent to the server for aggregation.\n        \"\"\"\n        if not self.initialized:\n            return self.setup_client_and_return_all_model_parameters(config)\n\n        assert self.global_model is not None and self.parameter_exchanger is not None\n        # NOTE: the global model weights are sent to the server here.\n        global_model_weights = self.parameter_exchanger.push_parameters(self.global_model, config=config)\n\n        # Weights and training loss sent to server for aggregation\n        # Training loss sent because server will decide to increase or decrease the penalty weight, if adaptivity\n        # is turned on\n        return self.parameter_exchanger.pack_parameters(global_model_weights, self.loss_for_adaptation)\n\n    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n        \"\"\"\n        Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n        unpacked for the clients to use in training. The parameters being passed are to be routed to the global model.\n        In the first fitting round, we assume the both the global and local models are being initialized and use\n        the ``FullParameterExchanger()`` to initialize both sets of model weights to the same parameters.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model (global model for all but the first step of Ditto). These should also include a penalty weight\n                from the server that needs to be unpacked.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning\n                round is a fitting round or an evaluation round. This is used to help determine which parameter\n                exchange should be used for pulling parameters. If the current federated learning round is the very\n                first fitting round, then we initialize both the global and local Ditto models with weights sent from\n                the server.\n        \"\"\"\n        # Make sure that the proper components exist.\n        assert self.global_model is not None and self.model is not None and self.parameter_exchanger is not None\n        server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n        log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n        current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n        if current_server_round == 1 and fitting_round:\n            log(INFO, \"Initializing the global and local models weights for the first time\")\n            self.initialize_all_model_weights(server_model_state, config)\n        else:\n            # Route the parameters to the GLOBAL model in Ditto after the initial stage\n            log(INFO, \"Setting the global model weights\")\n            self.parameter_exchanger.pull_parameters(server_model_state, self.global_model, config)\n\n    def initialize_all_model_weights(self, parameters: NDArrays, config: Config) -&gt; None:\n        \"\"\"\n        If this is the first time we're initializing the model weights, we initialize both the global and the local\n        weights together.\n\n        Args:\n            parameters (NDArrays): Model parameters to be injected into the client model.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        \"\"\"\n        self.parameter_exchanger.pull_parameters(parameters, self.model, config)\n        self.parameter_exchanger.pull_parameters(parameters, self.global_model, config)\n\n    def set_initial_global_tensors(self) -&gt; None:\n        \"\"\"\n        Saving the initial **GLOBAL MODEL** weights and detaching them so that we don't compute gradients with\n        respect to the tensors. These are used to form the Ditto local update penalty term.\n        \"\"\"\n        self.drift_penalty_tensors = [\n            initial_layer_weights.detach().clone() for initial_layer_weights in self.global_model.parameters()\n        ]\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        \"\"\"\n        Procedures that should occur before proceeding with the training loops for the models. In this case, we\n        save the global models parameters to be used in constraining training of the local model.\n\n        Args:\n            current_server_round (int): Indicates which server round we are currently executing.\n        \"\"\"\n        self.set_initial_global_tensors()\n\n        # Need to also set the global model to train mode before any training begins.\n        self.global_model.train()\n\n        super().update_before_train(current_server_round)\n\n    def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Mechanics of training loop follow from original Ditto implementation: https://github.com/litian96/ditto.\n\n        As in the implementation there, steps of the global and local models are done in tandem and for the same\n        number of steps.\n\n        Args:\n            input (TorchInputType): input tensor to be run through both the global and local models. Here,\n                ``TorchInputType`` is simply an alias for the union of ``torch.Tensor`` and\n                ``dict[str, torch.Tensor]``.\n            target (TorchTargetType): target tensor to be used to compute a loss given each models outputs.\n\n        Returns:\n            (tuple[TrainingLosses, TorchPredType]): Returns relevant loss values from both the global and local\n                model optimization steps. The prediction dictionary contains predictions indexed a \"global\" and \"local\"\n                corresponding to predictions from the global and local Ditto models for metric evaluations.\n        \"\"\"\n        # Clear gradients from optimizers if they exist\n        self.optimizers[\"global\"].zero_grad()\n        self.optimizers[\"local\"].zero_grad()\n\n        # Forward pass on both the global and local models\n        preds, features = self.predict(input)\n        target = self.transform_target(target)  # Apply transformation (Defaults to identity)\n\n        # Compute all relevant losses\n        losses = self.compute_training_loss(preds, features, target)\n\n        # Take a step with the global model vanilla loss\n        losses.additional_losses[\"global_loss\"].backward()\n        self.optimizers[\"global\"].step()\n\n        # Take a step with the local model using the local loss and Ditto constraint\n        losses.backward[\"backward\"].backward()\n        self.optimizers[\"local\"].step()\n\n        # Return dictionary of predictions where key is used to name respective MetricMeters\n        return losses, preds\n\n    def predict(\n        self,\n        input: TorchInputType,\n    ) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Computes the predictions for both the **GLOBAL** and **LOCAL** models and pack them into the prediction\n        dictionary.\n\n        Args:\n            input (TorchInputType): Inputs to be fed into both models.\n\n        Returns:\n            (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains predictions indexed\n                by name and the second element contains intermediate activations index by name. For Ditto, we only\n                need the predictions, so the second dictionary is simply empty.\n\n        Raises:\n            ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n                forward.\n        \"\"\"\n        if isinstance(input, torch.Tensor):\n            global_preds = self.global_model(input)\n            local_preds = self.model(input)\n        elif isinstance(input, dict):\n            # If input is a dictionary, then we unpack it before computing the forward pass.\n            # Note that this assumes the keys of the input match (exactly) the keyword args\n            # of the forward method.\n            global_preds = self.global_model(**input)\n            local_preds = self.model(**input)\n\n        # Here we assume that global and local preds are simply tensors\n        # TODO: Perhaps loosen this at a later date.\n        assert isinstance(global_preds, torch.Tensor)\n        assert isinstance(local_preds, torch.Tensor)\n        return {\"global\": global_preds, \"local\": local_preds}, {}\n\n    def compute_loss_and_additional_losses(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Computes the local model loss and the global Ditto model loss (stored in additional losses) for reporting and\n        training of the global model.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n                - The tensor for the model loss\n                - A dictionary with ``local_loss``, ``global_loss`` as additionally reported loss values.\n        \"\"\"\n        # Compute global model vanilla loss\n        assert \"global\" in preds\n        global_loss = self.criterion(preds[\"global\"], target)\n\n        # Compute local model loss + ditto constraint term\n        assert \"local\" in preds\n        local_loss = self.criterion(preds[\"local\"], target)\n\n        additional_losses = {\"local_loss\": local_loss.clone(), \"global_loss\": global_loss}\n\n        return local_loss, additional_losses\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training losses given predictions of the global and local models and ground truth data.\n        For the local model we add to the vanilla loss function by including Ditto penalty loss which is the\n        \\\\(\\\\ell^2\\\\) inner product between the initial global model weights and weights of the local model. This is\n        stored in backward The loss to optimize the global model is stored in the additional losses dictionary under\n        \u201cglobal_loss\u201d.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n                dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n                by name. Additional losses includes each loss component and the global model loss tensor.\n        \"\"\"\n        # Check that both models are in training mode\n        assert self.global_model.training and self.model.training\n\n        # local loss is stored in loss, global model loss is stored in additional losses.\n        loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n        # Setting the adaptation loss to that of the local model, as its performance should dictate whether more or\n        # less weight is used to constrain it to the global model (as in FedProx)\n        additional_losses[\"loss_for_adaptation\"] = additional_losses[\"local_loss\"].clone()\n\n        # This is the Ditto penalty loss of the local model compared with the original Global model weights, scaled\n        # by drift_penalty_weight (or lambda in the original paper)\n        penalty_loss = self.compute_penalty_loss()\n        additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n\n        return TrainingLosses(backward=loss + penalty_loss, additional_losses=additional_losses)\n\n    def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Validate the current model on the entire validation dataset.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n        \"\"\"\n        # Set the global model to evaluate mode\n        self.global_model.eval()\n        return super().validate(include_losses_in_metrics=include_losses_in_metrics)\n\n    def compute_evaluation_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses:\n        \"\"\"\n        Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n        For Ditto, we use the vanilla loss for the local model in checkpointing. However, during validation we also\n        compute the global model vanilla loss.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n                in preds will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n                indexed by name.\n        \"\"\"\n        # Check that both models are in eval mode\n        assert not self.global_model.training and not self.model.training\n        return super().compute_evaluation_loss(preds, features, target)\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>This client implements the Ditto algorithm from Ditto: Fair and Robust Federated Learning Through Personalization. The idea is that we want to train personalized versions of the global model for each client. So we simultaneously train a global model that is aggregated on the server-side and use those weights to also constrain the training of a local model. The constraint for this local model is identical to the FedProx loss.</p> <p>NOTE: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the corresponding strategy used by the server</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    This client implements the Ditto algorithm from Ditto: Fair and Robust Federated Learning Through\n    Personalization. The idea is that we want to train personalized versions of the global model for each client.\n    So we simultaneously train a global model that is aggregated on the server-side and use those weights to also\n    constrain the training of a local model. The constraint for this local model is identical to the FedProx loss.\n\n    **NOTE**: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the\n    heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the\n    corresponding strategy used by the server\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.global_model: nn.Module\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.get_optimizer","title":"<code>get_optimizer(config)</code>","text":"<p>Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def get_optimizer(self, config: Config) -&gt; dict[str, Optimizer]:\n    \"\"\"\n    Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    raise NotImplementedError(\n        \"User Clients must define a function that returns a dict[str, Optimizer] with keys 'global' and 'local' \"\n        \"defining separate optimizers for the global and local models of Ditto.\"\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.set_optimizer","title":"<code>set_optimizer(config)</code>","text":"<p>Ditto requires an optimizer for the global model and one for the local model. This function simply ensures that the optimizers setup by the user have the proper keys and that there are two optimizers.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def set_optimizer(self, config: Config) -&gt; None:\n    \"\"\"\n    Ditto requires an optimizer for the global model and one for the local model. This function simply ensures that\n    the optimizers setup by the user have the proper keys and that there are two optimizers.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    optimizers = self.get_optimizer(config)\n    assert isinstance(optimizers, dict) and {\"global\", \"local\"} == set(optimizers.keys())\n    self.optimizers = optimizers\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.get_global_model","title":"<code>get_global_model(config)</code>","text":"<p>Returns the global model to be used during Ditto training and as a constraint for the local model.</p> <p>The global model should be the same architecture as the local model so we reuse the <code>get_model</code> call. We explicitly send the model to the desired device. This is idempotent.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The PyTorch model serving as the global model for Ditto</p> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def get_global_model(self, config: Config) -&gt; nn.Module:\n    \"\"\"\n    Returns the global model to be used during Ditto training and as a constraint for the local model.\n\n    The global model should be the same architecture as the local model so we reuse the ``get_model`` call. We\n    explicitly send the model to the desired device. This is idempotent.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (nn.Module): The PyTorch model serving as the global model for Ditto\n    \"\"\"\n    return self.get_model(config).to(self.device)\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set initialized attribute to True. In this class, this function simply adds the additional step of setting up the global model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n    Then set initialized attribute to True. In this class, this function simply adds the additional step of\n    setting up the global model.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    # Need to setup the global model here as well. It should be the same architecture as the local model.\n    self.global_model = self.get_global_model(config)\n    # The rest of the setup is the same\n    super().setup_client(config)\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>For Ditto, we transfer the GLOBAL model weights to the server to be aggregated. The local model weights stay with the client.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>GLOBAL model weights to be sent to the server for aggregation.</p> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def get_parameters(self, config: Config) -&gt; NDArrays:\n    \"\"\"\n    For Ditto, we transfer the **GLOBAL** model weights to the server to be aggregated. The local model weights\n    stay with the client.\n\n    Args:\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n    Returns:\n        (NDArrays): **GLOBAL** model weights to be sent to the server for aggregation.\n    \"\"\"\n    if not self.initialized:\n        return self.setup_client_and_return_all_model_parameters(config)\n\n    assert self.global_model is not None and self.parameter_exchanger is not None\n    # NOTE: the global model weights are sent to the server here.\n    global_model_weights = self.parameter_exchanger.push_parameters(self.global_model, config=config)\n\n    # Weights and training loss sent to server for aggregation\n    # Training loss sent because server will decide to increase or decrease the penalty weight, if adaptivity\n    # is turned on\n    return self.parameter_exchanger.pack_parameters(global_model_weights, self.loss_for_adaptation)\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are unpacked for the clients to use in training. The parameters being passed are to be routed to the global model. In the first fitting round, we assume the both the global and local models are being initialized and use the <code>FullParameterExchanger()</code> to initialize both sets of model weights to the same parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model (global model for all but the first step of Ditto). These should also include a penalty weight from the server that needs to be unpacked.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. This is used to help determine which parameter exchange should be used for pulling parameters. If the current federated learning round is the very first fitting round, then we initialize both the global and local Ditto models with weights sent from the server.</p> required Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n    \"\"\"\n    Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n    unpacked for the clients to use in training. The parameters being passed are to be routed to the global model.\n    In the first fitting round, we assume the both the global and local models are being initialized and use\n    the ``FullParameterExchanger()`` to initialize both sets of model weights to the same parameters.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model (global model for all but the first step of Ditto). These should also include a penalty weight\n            from the server that needs to be unpacked.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning\n            round is a fitting round or an evaluation round. This is used to help determine which parameter\n            exchange should be used for pulling parameters. If the current federated learning round is the very\n            first fitting round, then we initialize both the global and local Ditto models with weights sent from\n            the server.\n    \"\"\"\n    # Make sure that the proper components exist.\n    assert self.global_model is not None and self.model is not None and self.parameter_exchanger is not None\n    server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n    log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n    current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n    if current_server_round == 1 and fitting_round:\n        log(INFO, \"Initializing the global and local models weights for the first time\")\n        self.initialize_all_model_weights(server_model_state, config)\n    else:\n        # Route the parameters to the GLOBAL model in Ditto after the initial stage\n        log(INFO, \"Setting the global model weights\")\n        self.parameter_exchanger.pull_parameters(server_model_state, self.global_model, config)\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.initialize_all_model_weights","title":"<code>initialize_all_model_weights(parameters, config)</code>","text":"<p>If this is the first time we're initializing the model weights, we initialize both the global and the local weights together.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Model parameters to be injected into the client model.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def initialize_all_model_weights(self, parameters: NDArrays, config: Config) -&gt; None:\n    \"\"\"\n    If this is the first time we're initializing the model weights, we initialize both the global and the local\n    weights together.\n\n    Args:\n        parameters (NDArrays): Model parameters to be injected into the client model.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n    \"\"\"\n    self.parameter_exchanger.pull_parameters(parameters, self.model, config)\n    self.parameter_exchanger.pull_parameters(parameters, self.global_model, config)\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.set_initial_global_tensors","title":"<code>set_initial_global_tensors()</code>","text":"<p>Saving the initial GLOBAL MODEL weights and detaching them so that we don't compute gradients with respect to the tensors. These are used to form the Ditto local update penalty term.</p> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def set_initial_global_tensors(self) -&gt; None:\n    \"\"\"\n    Saving the initial **GLOBAL MODEL** weights and detaching them so that we don't compute gradients with\n    respect to the tensors. These are used to form the Ditto local update penalty term.\n    \"\"\"\n    self.drift_penalty_tensors = [\n        initial_layer_weights.detach().clone() for initial_layer_weights in self.global_model.parameters()\n    ]\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.update_before_train","title":"<code>update_before_train(current_server_round)</code>","text":"<p>Procedures that should occur before proceeding with the training loops for the models. In this case, we save the global models parameters to be used in constraining training of the local model.</p> <p>Parameters:</p> Name Type Description Default <code>current_server_round</code> <code>int</code> <p>Indicates which server round we are currently executing.</p> required Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def update_before_train(self, current_server_round: int) -&gt; None:\n    \"\"\"\n    Procedures that should occur before proceeding with the training loops for the models. In this case, we\n    save the global models parameters to be used in constraining training of the local model.\n\n    Args:\n        current_server_round (int): Indicates which server round we are currently executing.\n    \"\"\"\n    self.set_initial_global_tensors()\n\n    # Need to also set the global model to train mode before any training begins.\n    self.global_model.train()\n\n    super().update_before_train(current_server_round)\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.train_step","title":"<code>train_step(input, target)</code>","text":"<p>Mechanics of training loop follow from original Ditto implementation: https://github.com/litian96/ditto.</p> <p>As in the implementation there, steps of the global and local models are done in tandem and for the same number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>input tensor to be run through both the global and local models. Here, <code>TorchInputType</code> is simply an alias for the union of <code>torch.Tensor</code> and <code>dict[str, torch.Tensor]</code>.</p> required <code>target</code> <code>TorchTargetType</code> <p>target tensor to be used to compute a loss given each models outputs.</p> required <p>Returns:</p> Type Description <code>tuple[TrainingLosses, TorchPredType]</code> <p>Returns relevant loss values from both the global and local model optimization steps. The prediction dictionary contains predictions indexed a \"global\" and \"local\" corresponding to predictions from the global and local Ditto models for metric evaluations.</p> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n    \"\"\"\n    Mechanics of training loop follow from original Ditto implementation: https://github.com/litian96/ditto.\n\n    As in the implementation there, steps of the global and local models are done in tandem and for the same\n    number of steps.\n\n    Args:\n        input (TorchInputType): input tensor to be run through both the global and local models. Here,\n            ``TorchInputType`` is simply an alias for the union of ``torch.Tensor`` and\n            ``dict[str, torch.Tensor]``.\n        target (TorchTargetType): target tensor to be used to compute a loss given each models outputs.\n\n    Returns:\n        (tuple[TrainingLosses, TorchPredType]): Returns relevant loss values from both the global and local\n            model optimization steps. The prediction dictionary contains predictions indexed a \"global\" and \"local\"\n            corresponding to predictions from the global and local Ditto models for metric evaluations.\n    \"\"\"\n    # Clear gradients from optimizers if they exist\n    self.optimizers[\"global\"].zero_grad()\n    self.optimizers[\"local\"].zero_grad()\n\n    # Forward pass on both the global and local models\n    preds, features = self.predict(input)\n    target = self.transform_target(target)  # Apply transformation (Defaults to identity)\n\n    # Compute all relevant losses\n    losses = self.compute_training_loss(preds, features, target)\n\n    # Take a step with the global model vanilla loss\n    losses.additional_losses[\"global_loss\"].backward()\n    self.optimizers[\"global\"].step()\n\n    # Take a step with the local model using the local loss and Ditto constraint\n    losses.backward[\"backward\"].backward()\n    self.optimizers[\"local\"].step()\n\n    # Return dictionary of predictions where key is used to name respective MetricMeters\n    return losses, preds\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.predict","title":"<code>predict(input)</code>","text":"<p>Computes the predictions for both the GLOBAL and LOCAL models and pack them into the prediction dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into both models.</p> required <p>Returns:</p> Type Description <code>tuple[TorchPredType, TorchFeatureType]</code> <p>A tuple in which the first element contains predictions indexed by name and the second element contains intermediate activations index by name. For Ditto, we only need the predictions, so the second dictionary is simply empty.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Occurs when something other than a tensor or dict of tensors is returned by the model forward.</p> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def predict(\n    self,\n    input: TorchInputType,\n) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Computes the predictions for both the **GLOBAL** and **LOCAL** models and pack them into the prediction\n    dictionary.\n\n    Args:\n        input (TorchInputType): Inputs to be fed into both models.\n\n    Returns:\n        (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains predictions indexed\n            by name and the second element contains intermediate activations index by name. For Ditto, we only\n            need the predictions, so the second dictionary is simply empty.\n\n    Raises:\n        ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n            forward.\n    \"\"\"\n    if isinstance(input, torch.Tensor):\n        global_preds = self.global_model(input)\n        local_preds = self.model(input)\n    elif isinstance(input, dict):\n        # If input is a dictionary, then we unpack it before computing the forward pass.\n        # Note that this assumes the keys of the input match (exactly) the keyword args\n        # of the forward method.\n        global_preds = self.global_model(**input)\n        local_preds = self.model(**input)\n\n    # Here we assume that global and local preds are simply tensors\n    # TODO: Perhaps loosen this at a later date.\n    assert isinstance(global_preds, torch.Tensor)\n    assert isinstance(local_preds, torch.Tensor)\n    return {\"global\": global_preds, \"local\": local_preds}, {}\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.compute_loss_and_additional_losses","title":"<code>compute_loss_and_additional_losses(preds, features, target)</code>","text":"<p>Computes the local model loss and the global Ditto model loss (stored in additional losses) for reporting and training of the global model.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor]]</code> <p>A tuple with:</p> <ul> <li>The tensor for the model loss</li> <li>A dictionary with <code>local_loss</code>, <code>global_loss</code> as additionally reported loss values.</li> </ul> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Computes the local model loss and the global Ditto model loss (stored in additional losses) for reporting and\n    training of the global model.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n            - The tensor for the model loss\n            - A dictionary with ``local_loss``, ``global_loss`` as additionally reported loss values.\n    \"\"\"\n    # Compute global model vanilla loss\n    assert \"global\" in preds\n    global_loss = self.criterion(preds[\"global\"], target)\n\n    # Compute local model loss + ditto constraint term\n    assert \"local\" in preds\n    local_loss = self.criterion(preds[\"local\"], target)\n\n    additional_losses = {\"local_loss\": local_loss.clone(), \"global_loss\": global_loss}\n\n    return local_loss, additional_losses\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.compute_training_loss","title":"<code>compute_training_loss(preds, features, target)</code>","text":"<p>Computes training losses given predictions of the global and local models and ground truth data. For the local model we add to the vanilla loss function by including Ditto penalty loss which is the \\(\\ell^2\\) inner product between the initial global model weights and weights of the local model. This is stored in backward The loss to optimize the global model is stored in the additional losses dictionary under \u201cglobal_loss\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name. Additional losses includes each loss component and the global model loss tensor.</p> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training losses given predictions of the global and local models and ground truth data.\n    For the local model we add to the vanilla loss function by including Ditto penalty loss which is the\n    \\\\(\\\\ell^2\\\\) inner product between the initial global model weights and weights of the local model. This is\n    stored in backward The loss to optimize the global model is stored in the additional losses dictionary under\n    \u201cglobal_loss\u201d.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n            dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n            by name. Additional losses includes each loss component and the global model loss tensor.\n    \"\"\"\n    # Check that both models are in training mode\n    assert self.global_model.training and self.model.training\n\n    # local loss is stored in loss, global model loss is stored in additional losses.\n    loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n    # Setting the adaptation loss to that of the local model, as its performance should dictate whether more or\n    # less weight is used to constrain it to the global model (as in FedProx)\n    additional_losses[\"loss_for_adaptation\"] = additional_losses[\"local_loss\"].clone()\n\n    # This is the Ditto penalty loss of the local model compared with the original Global model weights, scaled\n    # by drift_penalty_weight (or lambda in the original paper)\n    penalty_loss = self.compute_penalty_loss()\n    additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n\n    return TrainingLosses(backward=loss + penalty_loss, additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.validate","title":"<code>validate(include_losses_in_metrics=False)</code>","text":"<p>Validate the current model on the entire validation dataset.</p> <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]]</code> <p>The validation loss and a dictionary of metrics from validation.</p> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n    \"\"\"\n    Validate the current model on the entire validation dataset.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n    \"\"\"\n    # Set the global model to evaluate mode\n    self.global_model.eval()\n    return super().validate(include_losses_in_metrics=include_losses_in_metrics)\n</code></pre>"},{"location":"api/#fl4health.clients.ditto_client.DittoClient.compute_evaluation_loss","title":"<code>compute_evaluation_loss(preds, features, target)</code>","text":"<p>Computes evaluation loss given predictions (and potentially features) of the model and ground truth data. For Ditto, we use the vanilla loss for the local model in checkpointing. However, during validation we also compute the global model vanilla loss.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. Anything stored in preds will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> containing checkpoint loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/ditto_client.py</code> <pre><code>def compute_evaluation_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; EvaluationLosses:\n    \"\"\"\n    Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n    For Ditto, we use the vanilla loss for the local model in checkpointing. However, during validation we also\n    compute the global model vanilla loss.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n            in preds will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n            indexed by name.\n    \"\"\"\n    # Check that both models are in eval mode\n    assert not self.global_model.training and not self.model.training\n    return super().compute_evaluation_loss(preds, features, target)\n</code></pre>"},{"location":"api/#fl4health.clients.ensemble_client","title":"<code>ensemble_client</code>","text":""},{"location":"api/#fl4health.clients.ensemble_client.EnsembleClient","title":"<code>EnsembleClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/ensemble_client.py</code> <pre><code>class EnsembleClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client enables the training of ensemble models in a federated manner.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n\n        self.model: EnsembleModel\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n        Then set initialized attribute to True. Also perform some checks to ensure the keys of the\n        optimizer dictionary are consistent with the model keys.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        super().setup_client(config)\n\n        assert len(self.optimizers) == len(self.model.ensemble_models)\n        assert all(\n            opt_key == model_key\n            for opt_key, model_key in zip(sorted(self.optimizers.keys()), sorted(self.model.ensemble_models.keys()))\n        )\n\n    def set_optimizer(self, config: Config) -&gt; None:\n        \"\"\"\n        Method called in the the ``setup_client`` method to set optimizer attribute returned by used-defined\n        ``get_optimizer``. Ensures that the return value of ``get_optimizer`` is a dictionary since that is required\n        for the ensemble client.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        optimizers = self.get_optimizer(config)\n        assert isinstance(optimizers, dict)\n        self.optimizers = optimizers\n\n    def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Given a single batch of input and target data, generate predictions (both individual models and ensemble\n        prediction), compute loss, update parameters and optionally update metrics if they exist. (i.e.\n        backpropagation on a single batch of data). Assumes ``self.model`` is in train mode already. Differs from\n        parent  method in that, there are multiple losses that we have to do backward passes on and multiple\n        optimizers to  update parameters each train step.\n\n        Args:\n            input (TorchInputType): The input to be fed into the model. ``TorchInputType`` is simply an alias for the\n                union of ``torch.Tensor`` and ``dict[str, torch.Tensor]``.\n            target (TorchTargetType): The target corresponding to the input.\n\n        Returns:\n            (tuple[TrainingLosses, dict[str, torch.Tensor]]): The losses object from the train step along with\n                a dictionary of any predictions produced by the model.\n        \"\"\"\n        assert isinstance(input, torch.Tensor)\n        for optimizer in self.optimizers.values():\n            optimizer.zero_grad()\n\n        preds, features = self.predict(input)\n        target = self.transform_target(target)  # Apply transformation (Defaults to identity)\n\n        losses = self.compute_training_loss(preds, features, target)\n\n        for loss in losses.backward.values():\n            loss.backward()\n\n        for optimizer in self.optimizers.values():\n            optimizer.step()\n\n        return losses, preds\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training loss given predictions (and potentially features) of the model and ground truth data.\n        Since the ensemble client has more than one model, there are multiple backward losses that exist.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n                in preds will be used to compute metrics.\n            features (dict[str, torch.Tensor]): Feature(s) of the model(s) indexed by name.\n            target (torch.Tensor): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses\n                indexed by name.\n        \"\"\"\n        loss_dict = {}\n        for key, pred in preds.items():\n            loss_dict[key] = self.criterion(pred.float(), target)\n\n        individual_model_losses = {key: loss for key, loss in loss_dict.items() if key != \"ensemble-pred\"}\n        return TrainingLosses(backward=individual_model_losses)\n\n    def compute_evaluation_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses:\n        \"\"\"\n        Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n        Since the ensemble client has more than one model, there are multiple backward losses that exist.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n                in preds will be used to compute metrics.\n            features (dict[str, torch.Tensor]): Feature(s) of the model(s) indexed by name.\n            target (torch.Tensor): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n                indexed by name.\n        \"\"\"\n        loss_dict = {}\n        for key, pred in preds.items():\n            loss_dict[key] = self.criterion(pred.float(), target)\n\n        checkpoint_loss = loss_dict[\"ensemble-pred\"]\n        return EvaluationLosses(checkpoint=checkpoint_loss)\n\n    def get_optimizer(self, config: Config) -&gt; dict[str, Optimizer]:\n        \"\"\"\n        Method to be defined by user that returns dictionary of optimizers with keys corresponding to the keys of the\n        models in ``EnsembleModel`` that the optimizer applies too.\n\n        Args:\n            config (Config): The config sent from the server.\n\n        Returns:\n            (dict[str, Optimizer]): An optimizer or dictionary of optimizers to train model.\n\n        Raises:\n            NotImplementedError: To be defined in child class.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.ensemble_client.EnsembleClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>This client enables the training of ensemble models in a federated manner.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/ensemble_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    This client enables the training of ensemble models in a federated manner.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n\n    self.model: EnsembleModel\n</code></pre>"},{"location":"api/#fl4health.clients.ensemble_client.EnsembleClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set initialized attribute to True. Also perform some checks to ensure the keys of the optimizer dictionary are consistent with the model keys.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/ensemble_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n    Then set initialized attribute to True. Also perform some checks to ensure the keys of the\n    optimizer dictionary are consistent with the model keys.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    super().setup_client(config)\n\n    assert len(self.optimizers) == len(self.model.ensemble_models)\n    assert all(\n        opt_key == model_key\n        for opt_key, model_key in zip(sorted(self.optimizers.keys()), sorted(self.model.ensemble_models.keys()))\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.ensemble_client.EnsembleClient.set_optimizer","title":"<code>set_optimizer(config)</code>","text":"<p>Method called in the the <code>setup_client</code> method to set optimizer attribute returned by used-defined <code>get_optimizer</code>. Ensures that the return value of <code>get_optimizer</code> is a dictionary since that is required for the ensemble client.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/ensemble_client.py</code> <pre><code>def set_optimizer(self, config: Config) -&gt; None:\n    \"\"\"\n    Method called in the the ``setup_client`` method to set optimizer attribute returned by used-defined\n    ``get_optimizer``. Ensures that the return value of ``get_optimizer`` is a dictionary since that is required\n    for the ensemble client.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    optimizers = self.get_optimizer(config)\n    assert isinstance(optimizers, dict)\n    self.optimizers = optimizers\n</code></pre>"},{"location":"api/#fl4health.clients.ensemble_client.EnsembleClient.train_step","title":"<code>train_step(input, target)</code>","text":"<p>Given a single batch of input and target data, generate predictions (both individual models and ensemble prediction), compute loss, update parameters and optionally update metrics if they exist. (i.e. backpropagation on a single batch of data). Assumes <code>self.model</code> is in train mode already. Differs from parent  method in that, there are multiple losses that we have to do backward passes on and multiple optimizers to  update parameters each train step.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>The input to be fed into the model. <code>TorchInputType</code> is simply an alias for the union of <code>torch.Tensor</code> and <code>dict[str, torch.Tensor]</code>.</p> required <code>target</code> <code>TorchTargetType</code> <p>The target corresponding to the input.</p> required <p>Returns:</p> Type Description <code>tuple[TrainingLosses, dict[str, Tensor]]</code> <p>The losses object from the train step along with a dictionary of any predictions produced by the model.</p> Source code in <code>fl4health/clients/ensemble_client.py</code> <pre><code>def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n    \"\"\"\n    Given a single batch of input and target data, generate predictions (both individual models and ensemble\n    prediction), compute loss, update parameters and optionally update metrics if they exist. (i.e.\n    backpropagation on a single batch of data). Assumes ``self.model`` is in train mode already. Differs from\n    parent  method in that, there are multiple losses that we have to do backward passes on and multiple\n    optimizers to  update parameters each train step.\n\n    Args:\n        input (TorchInputType): The input to be fed into the model. ``TorchInputType`` is simply an alias for the\n            union of ``torch.Tensor`` and ``dict[str, torch.Tensor]``.\n        target (TorchTargetType): The target corresponding to the input.\n\n    Returns:\n        (tuple[TrainingLosses, dict[str, torch.Tensor]]): The losses object from the train step along with\n            a dictionary of any predictions produced by the model.\n    \"\"\"\n    assert isinstance(input, torch.Tensor)\n    for optimizer in self.optimizers.values():\n        optimizer.zero_grad()\n\n    preds, features = self.predict(input)\n    target = self.transform_target(target)  # Apply transformation (Defaults to identity)\n\n    losses = self.compute_training_loss(preds, features, target)\n\n    for loss in losses.backward.values():\n        loss.backward()\n\n    for optimizer in self.optimizers.values():\n        optimizer.step()\n\n    return losses, preds\n</code></pre>"},{"location":"api/#fl4health.clients.ensemble_client.EnsembleClient.compute_training_loss","title":"<code>compute_training_loss(preds, features, target)</code>","text":"<p>Computes training loss given predictions (and potentially features) of the model and ground truth data. Since the ensemble client has more than one model, there are multiple backward losses that exist.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. Anything stored in preds will be used to compute metrics.</p> required <code>features</code> <code>dict[str, Tensor]</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>Tensor</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/ensemble_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training loss given predictions (and potentially features) of the model and ground truth data.\n    Since the ensemble client has more than one model, there are multiple backward losses that exist.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n            in preds will be used to compute metrics.\n        features (dict[str, torch.Tensor]): Feature(s) of the model(s) indexed by name.\n        target (torch.Tensor): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses\n            indexed by name.\n    \"\"\"\n    loss_dict = {}\n    for key, pred in preds.items():\n        loss_dict[key] = self.criterion(pred.float(), target)\n\n    individual_model_losses = {key: loss for key, loss in loss_dict.items() if key != \"ensemble-pred\"}\n    return TrainingLosses(backward=individual_model_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.ensemble_client.EnsembleClient.compute_evaluation_loss","title":"<code>compute_evaluation_loss(preds, features, target)</code>","text":"<p>Computes evaluation loss given predictions (and potentially features) of the model and ground truth data. Since the ensemble client has more than one model, there are multiple backward losses that exist.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. Anything stored in preds will be used to compute metrics.</p> required <code>features</code> <code>dict[str, Tensor]</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>Tensor</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> containing checkpoint loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/ensemble_client.py</code> <pre><code>def compute_evaluation_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; EvaluationLosses:\n    \"\"\"\n    Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n    Since the ensemble client has more than one model, there are multiple backward losses that exist.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n            in preds will be used to compute metrics.\n        features (dict[str, torch.Tensor]): Feature(s) of the model(s) indexed by name.\n        target (torch.Tensor): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n            indexed by name.\n    \"\"\"\n    loss_dict = {}\n    for key, pred in preds.items():\n        loss_dict[key] = self.criterion(pred.float(), target)\n\n    checkpoint_loss = loss_dict[\"ensemble-pred\"]\n    return EvaluationLosses(checkpoint=checkpoint_loss)\n</code></pre>"},{"location":"api/#fl4health.clients.ensemble_client.EnsembleClient.get_optimizer","title":"<code>get_optimizer(config)</code>","text":"<p>Method to be defined by user that returns dictionary of optimizers with keys corresponding to the keys of the models in <code>EnsembleModel</code> that the optimizer applies too.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config sent from the server.</p> required <p>Returns:</p> Type Description <code>dict[str, Optimizer]</code> <p>An optimizer or dictionary of optimizers to train model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in child class.</p> Source code in <code>fl4health/clients/ensemble_client.py</code> <pre><code>def get_optimizer(self, config: Config) -&gt; dict[str, Optimizer]:\n    \"\"\"\n    Method to be defined by user that returns dictionary of optimizers with keys corresponding to the keys of the\n    models in ``EnsembleModel`` that the optimizer applies too.\n\n    Args:\n        config (Config): The config sent from the server.\n\n    Returns:\n        (dict[str, Optimizer]): An optimizer or dictionary of optimizers to train model.\n\n    Raises:\n        NotImplementedError: To be defined in child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.evaluate_client","title":"<code>evaluate_client</code>","text":""},{"location":"api/#fl4health.clients.evaluate_client.EvaluateClient","title":"<code>EvaluateClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/evaluate_client.py</code> <pre><code>class EvaluateClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        model_checkpoint_path: Path | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client implements an evaluation only flow. That is, there is no expectation of parameter exchange with\n        the server past the model initialization stage. The implementing client should instantiate a global model if\n        one is expected from the server, which will be loaded using the passed parameters. If a model checkpoint path\n        is provided the client attempts to load a local model from the specified path.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            model_checkpoint_path (Path | None, optional): Path to which the model should be checkpointed. Defaults to\n                None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Defaults to None.\n        \"\"\"\n        # EvaluateClient does not call BasicClient constructor and sets attributes\n        # in a custom way to account for the fact it does not involve any training\n        self.client_name = generate_hash() if client_name is None else client_name\n        self.data_path = data_path\n        self.device = device\n        self.model_checkpoint_path = model_checkpoint_path\n        self.metrics = metrics\n        self.initialized = False\n\n        # Initialize reporters with client information.\n        self.reports_manager = ReportsManager(reporters)\n        self.reports_manager.initialize(id=self.client_name)\n\n        # This data loader should be instantiated as the one on which to run evaluation\n        self.global_loss_meter = LossMeter[EvaluationLosses](loss_meter_type, EvaluationLosses)\n        self.global_metric_manager = MetricManager(self.metrics, \"global_eval_manager\")\n        self.local_loss_meter = LossMeter[EvaluationLosses](loss_meter_type, EvaluationLosses)\n        self.local_metric_manager = MetricManager(self.metrics, \"local_eval_manager\")\n\n        # The attributes to be set in setup_client\n        # Models corresponding to client-side and server-side checkpoints,\n        # if they exist, to be evaluated on the client's dataset.\n        self.data_loader: DataLoader\n        self.criterion: _Loss\n        self.local_model: nn.Module | None = None\n        self.global_model: nn.Module | None = None\n\n    def get_parameters(self, config: dict[str, Scalar]) -&gt; NDArrays:\n        raise ValueError(\"Get Parameters is not implemented for an Evaluation-Only Client\")\n\n    def fit(self, parameters: NDArrays, config: Config) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n        raise ValueError(\"Fit is not implemented for an Evaluation-Only Client\")\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"Set dataloaders, parameter exchangers and other attributes for the client.\"\"\"\n        (data_loader,) = self.get_data_loader(config)\n        self.data_loader = data_loader\n        self.global_model = self.initialize_global_model(config)\n        self.local_model = self.get_local_model(config)\n\n        # The following lines are type ignored because torch datasets are not \"Sized\"\n        # IE __len__ is considered optionally defined. In practice, it is almost always defined\n        # and as such, we will make that assumption.\n        self.num_samples = len(self.data_loader.dataset)  # type: ignore\n\n        self.criterion = self.get_criterion(config)\n        self.parameter_exchanger = self.get_parameter_exchanger(config)\n\n        self.reports_manager.report({\"host_type\": \"client\", \"initialized\": str(datetime.datetime.now())})\n\n        self.initialized = True\n\n    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n        assert not fitting_round\n        # Sets the global model parameters transferred from the server using a parameter exchanger to coordinate how\n        # parameters are set\n        if len(parameters) &gt; 0:\n            # If a non-empty set of parameters are passed, then they are inserted into a global model to be evaluated.\n            # If none are provided or a global model is not instantiated, then we only evaluate a local model\n            assert self.global_model is not None and self.parameter_exchanger is not None\n            self.parameter_exchanger.pull_parameters(parameters, self.global_model, config)\n        else:\n            # If no global parameters are passed then we kill a global model (if instantiated) as it is not going to\n            # be initialized with trained weights.\n            self.global_model = None\n\n    def evaluate(self, parameters: NDArrays, config: Config) -&gt; tuple[float, int, dict[str, Scalar]]:\n        if not self.initialized:\n            self.setup_client(config)\n\n        start_time = datetime.datetime.now()\n        self.set_parameters(parameters, config, fitting_round=False)\n        # Make sure at least one of local or global model is not none (i.e. there is something to evaluate)\n        assert self.local_model or self.global_model\n\n        loss, metric_values = self.validate()\n        end_time = datetime.datetime.now()\n        elapsed = end_time - start_time\n\n        self.reports_manager.report(\n            {\n                \"eval_metrics\": metric_values,\n                \"eval_loss\": loss,\n                \"eval_start\": str(start_time),\n                \"eval_time_elapsed\": str(elapsed),\n                \"eval_end\": str(end_time),\n            },\n            0,\n        )\n\n        # EvaluateRes should return the loss, number of examples on client, and a dictionary holding metrics\n        # calculation results.\n        return (\n            loss,\n            self.num_samples,\n            metric_values,\n        )\n\n    def _handle_logging(  # type: ignore\n        self, losses: EvaluationLosses, metrics_dict: dict[str, Scalar], is_global: bool\n    ) -&gt; None:\n        metric_string = \"\\t\".join([f\"{key}: {str(val)}\" for key, val in metrics_dict.items()])\n        loss_string = \"\\t\".join([f\"{key}: {str(val)}\" for key, val in losses.as_dict().items()])\n        eval_prefix = \"Global Model\" if is_global else \"Local Model\"\n        log(\n            INFO,\n            f\"Client Evaluation {eval_prefix} Losses: {loss_string} \\n\"\n            f\"Client Evaluation {eval_prefix} Metrics: {metric_string}\",\n        )\n\n    def validate_on_model(\n        self,\n        model: nn.Module,\n        metric_meter: MetricManager,\n        loss_meter: LossMeter,\n        is_global: bool,\n    ) -&gt; tuple[EvaluationLosses, dict[str, Scalar]]:\n        model.eval()\n        metric_meter.clear()\n        loss_meter.clear()\n        model.to(self.device)\n\n        with torch.no_grad():\n            for inputs, targets in self.data_loader:\n                inputs, targets = inputs.to(self.device), targets.to(self.device)\n                preds = {\"prediction\": model(inputs)}\n                losses = self.compute_evaluation_loss(preds, {}, targets)\n\n                metric_meter.update(preds, targets)\n                loss_meter.update(losses)\n\n        metrics = metric_meter.compute()\n        losses = loss_meter.compute()\n        self._handle_logging(losses, metrics, is_global)\n        return losses, metrics\n\n    def validate(self, include_loss_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n        local_loss: EvaluationLosses | None = None\n        local_metrics: dict[str, Scalar] | None = None\n\n        global_loss: EvaluationLosses | None = None\n        global_metrics: dict[str, Scalar] | None = None\n\n        if self.local_model:\n            log(INFO, \"Performing evaluation on local model\")\n            local_loss, local_metrics = self.validate_on_model(\n                self.local_model,\n                self.local_metric_manager,\n                self.local_loss_meter,\n                is_global=False,\n            )\n\n        if self.global_model:\n            log(INFO, \"Performing evaluation on global model\")\n            global_loss, global_metrics = self.validate_on_model(\n                self.global_model,\n                self.global_metric_manager,\n                self.global_loss_meter,\n                is_global=True,\n            )\n\n        # Store the losses in the metrics, since we can't return more than one loss.\n        metrics = EvaluateClient.merge_metrics(global_metrics, local_metrics)\n        if global_loss:\n            metrics.update({f\"global_loss_{key}\": val for key, val in global_loss.as_dict().items()})\n        if local_loss:\n            metrics.update({f\"local_loss_{key}\": val for key, val in local_loss.as_dict().items()})\n\n        # Dummy loss is returned, global and local loss values are stored in the metrics dictionary\n        return float(\"nan\"), metrics\n\n    @staticmethod\n    def merge_metrics(\n        global_metrics: dict[str, Scalar] | None,\n        local_metrics: dict[str, Scalar] | None,\n    ) -&gt; dict[str, Scalar]:\n        # Merge metrics if necessary\n        if global_metrics:\n            metrics = global_metrics\n            if local_metrics:\n                for metric_name, metric_value in local_metrics.items():\n                    if metric_name in metrics:\n                        log(\n                            WARNING,\n                            f\"metric_name: {metric_name} already exists in dictionary. \"\n                            \"Please ensure that this is intended behavior\",\n                        )\n                    metrics[metric_name] = metric_value\n        elif local_metrics:\n            metrics = local_metrics\n        else:\n            raise ValueError(\n                \"Both metric dictionaries are None. At least one global or local model should be present.\"\n            )\n        return metrics\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        \"\"\"\n        Parameter exchange is assumed to always be full for evaluation only clients. If there are partial weights\n        exchanged during training, we assume that the checkpoint has been saved locally. However, this functionality\n        may be overridden if a different exchanger is needed.\n        \"\"\"\n        return FullParameterExchanger()\n\n    def get_data_loader(self, config: Config) -&gt; tuple[DataLoader]:\n        \"\"\"User defined method that returns a PyTorch DataLoader for validation.\"\"\"\n        raise NotImplementedError\n\n    def initialize_global_model(self, config: Config) -&gt; nn.Module | None:\n        \"\"\"\n        User defined method that to initializes a global model to potentially be hydrated by parameters sent by the\n        server, by default, no global model is assumed to exist unless specified by the user.\n        \"\"\"\n        return None\n\n    def get_local_model(self, config: Config) -&gt; nn.Module | None:\n        \"\"\"\n        Functionality for initializing a model from a local checkpoint. This can be overridden for custom\n        behavior.\n        \"\"\"\n        # If a model checkpoint is provided, we load the checkpoint into the local model to be evaluated.\n        if self.model_checkpoint_path:\n            log(\n                INFO,\n                f\"Loading model checkpoint at: {str(self.model_checkpoint_path)}\",\n            )\n            return torch.load(self.model_checkpoint_path, weights_only=False)\n        return None\n</code></pre>"},{"location":"api/#fl4health.clients.evaluate_client.EvaluateClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, model_checkpoint_path=None, reporters=None, client_name=None)</code>","text":"<p>This client implements an evaluation only flow. That is, there is no expectation of parameter exchange with the server past the model initialization stage. The implementing client should instantiate a global model if one is expected from the server, which will be loaded using the passed parameters. If a model checkpoint path is provided the client attempts to load a local model from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>model_checkpoint_path</code> <code>Path | None</code> <p>Path to which the model should be checkpointed. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/evaluate_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    model_checkpoint_path: Path | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    This client implements an evaluation only flow. That is, there is no expectation of parameter exchange with\n    the server past the model initialization stage. The implementing client should instantiate a global model if\n    one is expected from the server, which will be loaded using the passed parameters. If a model checkpoint path\n    is provided the client attempts to load a local model from the specified path.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        model_checkpoint_path (Path | None, optional): Path to which the model should be checkpointed. Defaults to\n            None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Defaults to None.\n    \"\"\"\n    # EvaluateClient does not call BasicClient constructor and sets attributes\n    # in a custom way to account for the fact it does not involve any training\n    self.client_name = generate_hash() if client_name is None else client_name\n    self.data_path = data_path\n    self.device = device\n    self.model_checkpoint_path = model_checkpoint_path\n    self.metrics = metrics\n    self.initialized = False\n\n    # Initialize reporters with client information.\n    self.reports_manager = ReportsManager(reporters)\n    self.reports_manager.initialize(id=self.client_name)\n\n    # This data loader should be instantiated as the one on which to run evaluation\n    self.global_loss_meter = LossMeter[EvaluationLosses](loss_meter_type, EvaluationLosses)\n    self.global_metric_manager = MetricManager(self.metrics, \"global_eval_manager\")\n    self.local_loss_meter = LossMeter[EvaluationLosses](loss_meter_type, EvaluationLosses)\n    self.local_metric_manager = MetricManager(self.metrics, \"local_eval_manager\")\n\n    # The attributes to be set in setup_client\n    # Models corresponding to client-side and server-side checkpoints,\n    # if they exist, to be evaluated on the client's dataset.\n    self.data_loader: DataLoader\n    self.criterion: _Loss\n    self.local_model: nn.Module | None = None\n    self.global_model: nn.Module | None = None\n</code></pre>"},{"location":"api/#fl4health.clients.evaluate_client.EvaluateClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Set dataloaders, parameter exchangers and other attributes for the client.</p> Source code in <code>fl4health/clients/evaluate_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"Set dataloaders, parameter exchangers and other attributes for the client.\"\"\"\n    (data_loader,) = self.get_data_loader(config)\n    self.data_loader = data_loader\n    self.global_model = self.initialize_global_model(config)\n    self.local_model = self.get_local_model(config)\n\n    # The following lines are type ignored because torch datasets are not \"Sized\"\n    # IE __len__ is considered optionally defined. In practice, it is almost always defined\n    # and as such, we will make that assumption.\n    self.num_samples = len(self.data_loader.dataset)  # type: ignore\n\n    self.criterion = self.get_criterion(config)\n    self.parameter_exchanger = self.get_parameter_exchanger(config)\n\n    self.reports_manager.report({\"host_type\": \"client\", \"initialized\": str(datetime.datetime.now())})\n\n    self.initialized = True\n</code></pre>"},{"location":"api/#fl4health.clients.evaluate_client.EvaluateClient.get_parameter_exchanger","title":"<code>get_parameter_exchanger(config)</code>","text":"<p>Parameter exchange is assumed to always be full for evaluation only clients. If there are partial weights exchanged during training, we assume that the checkpoint has been saved locally. However, this functionality may be overridden if a different exchanger is needed.</p> Source code in <code>fl4health/clients/evaluate_client.py</code> <pre><code>def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n    \"\"\"\n    Parameter exchange is assumed to always be full for evaluation only clients. If there are partial weights\n    exchanged during training, we assume that the checkpoint has been saved locally. However, this functionality\n    may be overridden if a different exchanger is needed.\n    \"\"\"\n    return FullParameterExchanger()\n</code></pre>"},{"location":"api/#fl4health.clients.evaluate_client.EvaluateClient.get_data_loader","title":"<code>get_data_loader(config)</code>","text":"<p>User defined method that returns a PyTorch DataLoader for validation.</p> Source code in <code>fl4health/clients/evaluate_client.py</code> <pre><code>def get_data_loader(self, config: Config) -&gt; tuple[DataLoader]:\n    \"\"\"User defined method that returns a PyTorch DataLoader for validation.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.evaluate_client.EvaluateClient.initialize_global_model","title":"<code>initialize_global_model(config)</code>","text":"<p>User defined method that to initializes a global model to potentially be hydrated by parameters sent by the server, by default, no global model is assumed to exist unless specified by the user.</p> Source code in <code>fl4health/clients/evaluate_client.py</code> <pre><code>def initialize_global_model(self, config: Config) -&gt; nn.Module | None:\n    \"\"\"\n    User defined method that to initializes a global model to potentially be hydrated by parameters sent by the\n    server, by default, no global model is assumed to exist unless specified by the user.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/#fl4health.clients.evaluate_client.EvaluateClient.get_local_model","title":"<code>get_local_model(config)</code>","text":"<p>Functionality for initializing a model from a local checkpoint. This can be overridden for custom behavior.</p> Source code in <code>fl4health/clients/evaluate_client.py</code> <pre><code>def get_local_model(self, config: Config) -&gt; nn.Module | None:\n    \"\"\"\n    Functionality for initializing a model from a local checkpoint. This can be overridden for custom\n    behavior.\n    \"\"\"\n    # If a model checkpoint is provided, we load the checkpoint into the local model to be evaluated.\n    if self.model_checkpoint_path:\n        log(\n            INFO,\n            f\"Loading model checkpoint at: {str(self.model_checkpoint_path)}\",\n        )\n        return torch.load(self.model_checkpoint_path, weights_only=False)\n    return None\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client","title":"<code>fed_pca_client</code>","text":""},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient","title":"<code>FedPCAClient</code>","text":"<p>               Bases: <code>NumPyClient</code></p> Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>class FedPCAClient(NumPyClient):\n    def __init__(\n        self, data_path: Path, device: torch.device, model_save_dir: Path, client_name: str | None = None\n    ) -&gt; None:\n        \"\"\"\n        Client that facilitates the execution of federated PCA.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            model_save_dir (Path): Dir to save the PCA components for use later, perhaps in dimensionality reduction\n            client_name (str | None, optional): client name, mainly used for saving components. Defaults to None.\n        \"\"\"\n        self.client_name = self.generate_hash() if client_name is None else client_name\n        self.model: PcaModule\n        self.initialized = False\n        self.data_path = data_path\n        self.model_save_dir = model_save_dir\n        self.device = device\n        self.train_data_tensor: Tensor\n        self.val_data_tensor: Tensor\n        self.num_train_samples: int\n        self.num_val_samples: int\n        self.parameter_exchanger = FullParameterExchanger()\n\n    def generate_hash(self, length: int = 8) -&gt; str:\n        \"\"\"\n        Generates unique hash used as id for client.\n\n        Args:\n            length (int, optional): Length of the generated hash. Defaults to 8.\n\n        Returns:\n            (str): Generated hash of length ``length``\n        \"\"\"\n        return \"\".join(random.choice(string.ascii_lowercase) for _ in range(length))\n\n    def get_parameters(self, config: Config) -&gt; NDArrays:\n        \"\"\"\n        Sends all of the model components back to the server. The model in this case represents the principal\n        components that have been computed.\n\n        Args:\n            config (Config): Configurations to allow for customization of this functions behavior\n\n        Returns:\n            (NDArrays): Parameters representing the principal components computed by the client that need to be\n                aggregated in some way.\n        \"\"\"\n        if not self.initialized:\n            log(INFO, \"Setting up client and providing full model parameters to the server for initialization\")\n            if not config:\n                log(\n                    WARNING,\n                    (\n                        \"This client has not yet been initialized and the config is empty. This may cause unexpected \"\n                        \"failures, as setting up a client typically requires several configuration parameters, \"\n                        \"including batch_size and current_server_round.\"\n                    ),\n                )\n\n            # If initialized is False, the server is requesting model parameters from which to initialize all other\n            # clients. As such get_parameters is being called before fit or evaluate, so we must call\n            # setup_client first.\n            self.setup_client(config)\n\n            # Need all parameters even if normally exchanging partial\n            return FullParameterExchanger().push_parameters(self.model, config=config)\n\n        assert self.model is not None and self.parameter_exchanger is not None\n        return self.parameter_exchanger.push_parameters(self.model, config=config)\n\n    def set_parameters(self, parameters: NDArrays, config: Config) -&gt; None:\n        \"\"\"\n        Sets the merged principal components transferred from the server. Since federated PCA only runs for one round,\n        the principal components obtained here are in fact the final result, so they are saved locally by each client\n        for downstream tasks.\n\n        Args:\n            parameters (NDArrays): Aggregated principal components from the server. These are **FINAL** in the sense\n                that FedPCA only runs for one round.\n            config (Config): Configurations to allow for customization of this functions behavior\n        \"\"\"\n        self.parameter_exchanger.pull_parameters(parameters, self.model, config)\n        self.save_model()\n\n    def get_data_loaders(self, config: Config) -&gt; tuple[DataLoader, DataLoader]:\n        \"\"\"\n        User defined method that returns a PyTorch Train ``DataLoader`` and a PyTorch Validation ``DataLoader``.\n\n        Args:\n            config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n\n        Returns:\n            (tuple[DataLoader, DataLoader]): Tuple of length 2. The client train and validation loader.\n\n        Raises:\n            NotImplementedError: To be defined in child class.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_model(self, config: Config) -&gt; PcaModule:\n        \"\"\"\n        Returns an instance of the ``PCAModule``. This module is used to facilitate FedPCA training on the client side.\n\n        Args:\n            config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n\n        Returns:\n            (PcaModule): Module that determines how local FedPCA optimization will be performed.\n        \"\"\"\n        low_rank = narrow_dict_type(config, \"low_rank\", bool)\n        full_svd = narrow_dict_type(config, \"full_svd\", bool)\n        rank_estimation = narrow_dict_type(config, \"rank_estimation\", int)\n        return PcaModule(low_rank, full_svd, rank_estimation).to(self.device)\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Used to setup all of the components necessary to run ``FedPCA``.\n\n        Args:\n            config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n        \"\"\"\n        self.model = self.get_model(config).to(self.device)\n\n        train_loader, val_loader = self.get_data_loaders(config)\n        self.train_data_tensor = self.get_data_tensor(train_loader).to(self.device)\n        self.val_data_tensor = self.get_data_tensor(val_loader).to(self.device)\n\n        # The following lines are type ignored because torch datasets are not \"Sized\"\n        # IE __len__ is considered optionally defined. In practice, it is almost always defined\n        # and as such, we will make that assumption.\n        self.num_train_samples = len(train_loader.dataset)  # type: ignore\n        self.num_val_samples = len(val_loader.dataset)  # type: ignore\n\n        self.initialized = True\n\n    def get_data_tensor(self, data_loader: DataLoader) -&gt; Tensor:\n        \"\"\"\n        This function should be used to \"collate\" each of the dataloader batches into a single monolithic tensor\n        representing all of the data in the loader.\n\n        Args:\n            data_loader (DataLoader): The dataloader that can be used to iterate through a dataset\n\n        Raises:\n            NotImplementedError: Should be defined by the child class\n\n        Returns:\n            (Tensor): Single torch tensor representing all of the data stacked together.\n        \"\"\"\n        raise NotImplementedError\n\n    def fit(self, parameters: NDArrays, config: Config) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n        \"\"\"\n        Function to perform the local side of ``FedPCA``. We don't use any parameters sent by the server. Hence\n        ``parameters`` is ignored. We need only the ``train_data_tensor`` to do the work.\n\n        Args:\n            parameters (NDArrays): ignored\n            config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n\n        Returns:\n            (tuple[NDArrays, int, dict[str, Scalar]]): The local principal components following the local training\n                along with the number of samples in the local training dataset and the computed metrics throughout the\n                fit.\n        \"\"\"\n        if not self.initialized:\n            self.setup_client(config)\n        center_data = narrow_dict_type(config, \"center_data\", bool)\n\n        principal_components, singular_values = self.model(self.train_data_tensor, center_data)\n        self.model.set_principal_components(principal_components, singular_values)\n\n        cumulative_explained_variance = self.model.compute_cumulative_explained_variance()\n        explained_variance_ratios = self.model.compute_explained_variance_ratios()\n        metrics: dict[str, Scalar] = {\n            \"cumulative_explained_variance\": cumulative_explained_variance,\n            \"top_explained_variance_ratio\": explained_variance_ratios[0].item(),\n        }\n        return (self.get_parameters(config), self.num_train_samples, metrics)\n\n    def evaluate(self, parameters: NDArrays, config: Config) -&gt; tuple[float, int, dict[str, Scalar]]:\n        \"\"\"\n        Evaluate merged principal components on the local validation set.\n\n        Args:\n            parameters (NDArrays): Server-merged principal components.\n            config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n\n        Returns:\n            (tuple[float, int, dict[str, Scalar]]): A loss associated with the evaluation, the number of samples in the\n                validation/test set and the ``metric_values`` associated with evaluation.\n        \"\"\"\n        if not self.initialized:\n            self.setup_client(config)\n            self.model.set_data_mean(self.model.maybe_reshape(self.train_data_tensor))\n        self.set_parameters(parameters, config)\n        num_components_eval = (\n            narrow_dict_type(config, \"num_components_eval\", int) if \"num_components_eval\" in config else None\n        )\n        val_data_tensor_prepared = self.model.center_data(self.model.maybe_reshape(self.val_data_tensor)).to(\n            self.device\n        )\n        reconstruction_loss = self.model.compute_reconstruction_error(val_data_tensor_prepared, num_components_eval)\n        projection_variance = self.model.compute_projection_variance(val_data_tensor_prepared, num_components_eval)\n        metrics: dict[str, Scalar] = {\"projection_variance\": projection_variance}\n        return (reconstruction_loss, self.num_val_samples, metrics)\n\n    def save_model(self) -&gt; None:\n        \"\"\"\n        Method to save the FedPCA computed principal components to disk. These can be reloaded to allow for\n        dimensionality reduction in subsequent FL training.\n        \"\"\"\n        final_model_save_path = self.model_save_dir / f\"client_{self.client_name}_pca.pt\"\n        torch.save(self.model, final_model_save_path)\n        log(INFO, f\"Model parameters saved to {final_model_save_path}.\")\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.__init__","title":"<code>__init__(data_path, device, model_save_dir, client_name=None)</code>","text":"<p>Client that facilitates the execution of federated PCA.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>model_save_dir</code> <code>Path</code> <p>Dir to save the PCA components for use later, perhaps in dimensionality reduction</p> required <code>client_name</code> <code>str | None</code> <p>client name, mainly used for saving components. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def __init__(\n    self, data_path: Path, device: torch.device, model_save_dir: Path, client_name: str | None = None\n) -&gt; None:\n    \"\"\"\n    Client that facilitates the execution of federated PCA.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        model_save_dir (Path): Dir to save the PCA components for use later, perhaps in dimensionality reduction\n        client_name (str | None, optional): client name, mainly used for saving components. Defaults to None.\n    \"\"\"\n    self.client_name = self.generate_hash() if client_name is None else client_name\n    self.model: PcaModule\n    self.initialized = False\n    self.data_path = data_path\n    self.model_save_dir = model_save_dir\n    self.device = device\n    self.train_data_tensor: Tensor\n    self.val_data_tensor: Tensor\n    self.num_train_samples: int\n    self.num_val_samples: int\n    self.parameter_exchanger = FullParameterExchanger()\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.generate_hash","title":"<code>generate_hash(length=8)</code>","text":"<p>Generates unique hash used as id for client.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>Length of the generated hash. Defaults to 8.</p> <code>8</code> <p>Returns:</p> Type Description <code>str</code> <p>Generated hash of length <code>length</code></p> Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def generate_hash(self, length: int = 8) -&gt; str:\n    \"\"\"\n    Generates unique hash used as id for client.\n\n    Args:\n        length (int, optional): Length of the generated hash. Defaults to 8.\n\n    Returns:\n        (str): Generated hash of length ``length``\n    \"\"\"\n    return \"\".join(random.choice(string.ascii_lowercase) for _ in range(length))\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>Sends all of the model components back to the server. The model in this case represents the principal components that have been computed.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configurations to allow for customization of this functions behavior</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Parameters representing the principal components computed by the client that need to be aggregated in some way.</p> Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def get_parameters(self, config: Config) -&gt; NDArrays:\n    \"\"\"\n    Sends all of the model components back to the server. The model in this case represents the principal\n    components that have been computed.\n\n    Args:\n        config (Config): Configurations to allow for customization of this functions behavior\n\n    Returns:\n        (NDArrays): Parameters representing the principal components computed by the client that need to be\n            aggregated in some way.\n    \"\"\"\n    if not self.initialized:\n        log(INFO, \"Setting up client and providing full model parameters to the server for initialization\")\n        if not config:\n            log(\n                WARNING,\n                (\n                    \"This client has not yet been initialized and the config is empty. This may cause unexpected \"\n                    \"failures, as setting up a client typically requires several configuration parameters, \"\n                    \"including batch_size and current_server_round.\"\n                ),\n            )\n\n        # If initialized is False, the server is requesting model parameters from which to initialize all other\n        # clients. As such get_parameters is being called before fit or evaluate, so we must call\n        # setup_client first.\n        self.setup_client(config)\n\n        # Need all parameters even if normally exchanging partial\n        return FullParameterExchanger().push_parameters(self.model, config=config)\n\n    assert self.model is not None and self.parameter_exchanger is not None\n    return self.parameter_exchanger.push_parameters(self.model, config=config)\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.set_parameters","title":"<code>set_parameters(parameters, config)</code>","text":"<p>Sets the merged principal components transferred from the server. Since federated PCA only runs for one round, the principal components obtained here are in fact the final result, so they are saved locally by each client for downstream tasks.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Aggregated principal components from the server. These are FINAL in the sense that FedPCA only runs for one round.</p> required <code>config</code> <code>Config</code> <p>Configurations to allow for customization of this functions behavior</p> required Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def set_parameters(self, parameters: NDArrays, config: Config) -&gt; None:\n    \"\"\"\n    Sets the merged principal components transferred from the server. Since federated PCA only runs for one round,\n    the principal components obtained here are in fact the final result, so they are saved locally by each client\n    for downstream tasks.\n\n    Args:\n        parameters (NDArrays): Aggregated principal components from the server. These are **FINAL** in the sense\n            that FedPCA only runs for one round.\n        config (Config): Configurations to allow for customization of this functions behavior\n    \"\"\"\n    self.parameter_exchanger.pull_parameters(parameters, self.model, config)\n    self.save_model()\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.get_data_loaders","title":"<code>get_data_loaders(config)</code>","text":"<p>User defined method that returns a PyTorch Train <code>DataLoader</code> and a PyTorch Validation <code>DataLoader</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configurations sent by the server to allow for customization of this functions behavior.</p> required <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader]</code> <p>Tuple of length 2. The client train and validation loader.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in child class.</p> Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def get_data_loaders(self, config: Config) -&gt; tuple[DataLoader, DataLoader]:\n    \"\"\"\n    User defined method that returns a PyTorch Train ``DataLoader`` and a PyTorch Validation ``DataLoader``.\n\n    Args:\n        config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n\n    Returns:\n        (tuple[DataLoader, DataLoader]): Tuple of length 2. The client train and validation loader.\n\n    Raises:\n        NotImplementedError: To be defined in child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.get_model","title":"<code>get_model(config)</code>","text":"<p>Returns an instance of the <code>PCAModule</code>. This module is used to facilitate FedPCA training on the client side.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configurations sent by the server to allow for customization of this functions behavior.</p> required <p>Returns:</p> Type Description <code>PcaModule</code> <p>Module that determines how local FedPCA optimization will be performed.</p> Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def get_model(self, config: Config) -&gt; PcaModule:\n    \"\"\"\n    Returns an instance of the ``PCAModule``. This module is used to facilitate FedPCA training on the client side.\n\n    Args:\n        config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n\n    Returns:\n        (PcaModule): Module that determines how local FedPCA optimization will be performed.\n    \"\"\"\n    low_rank = narrow_dict_type(config, \"low_rank\", bool)\n    full_svd = narrow_dict_type(config, \"full_svd\", bool)\n    rank_estimation = narrow_dict_type(config, \"rank_estimation\", int)\n    return PcaModule(low_rank, full_svd, rank_estimation).to(self.device)\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Used to setup all of the components necessary to run <code>FedPCA</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configurations sent by the server to allow for customization of this functions behavior.</p> required Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Used to setup all of the components necessary to run ``FedPCA``.\n\n    Args:\n        config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n    \"\"\"\n    self.model = self.get_model(config).to(self.device)\n\n    train_loader, val_loader = self.get_data_loaders(config)\n    self.train_data_tensor = self.get_data_tensor(train_loader).to(self.device)\n    self.val_data_tensor = self.get_data_tensor(val_loader).to(self.device)\n\n    # The following lines are type ignored because torch datasets are not \"Sized\"\n    # IE __len__ is considered optionally defined. In practice, it is almost always defined\n    # and as such, we will make that assumption.\n    self.num_train_samples = len(train_loader.dataset)  # type: ignore\n    self.num_val_samples = len(val_loader.dataset)  # type: ignore\n\n    self.initialized = True\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.get_data_tensor","title":"<code>get_data_tensor(data_loader)</code>","text":"<p>This function should be used to \"collate\" each of the dataloader batches into a single monolithic tensor representing all of the data in the loader.</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <code>DataLoader</code> <p>The dataloader that can be used to iterate through a dataset</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Should be defined by the child class</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Single torch tensor representing all of the data stacked together.</p> Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def get_data_tensor(self, data_loader: DataLoader) -&gt; Tensor:\n    \"\"\"\n    This function should be used to \"collate\" each of the dataloader batches into a single monolithic tensor\n    representing all of the data in the loader.\n\n    Args:\n        data_loader (DataLoader): The dataloader that can be used to iterate through a dataset\n\n    Raises:\n        NotImplementedError: Should be defined by the child class\n\n    Returns:\n        (Tensor): Single torch tensor representing all of the data stacked together.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.fit","title":"<code>fit(parameters, config)</code>","text":"<p>Function to perform the local side of <code>FedPCA</code>. We don't use any parameters sent by the server. Hence <code>parameters</code> is ignored. We need only the <code>train_data_tensor</code> to do the work.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>ignored</p> required <code>config</code> <code>Config</code> <p>Configurations sent by the server to allow for customization of this functions behavior.</p> required <p>Returns:</p> Type Description <code>tuple[NDArrays, int, dict[str, Scalar]]</code> <p>The local principal components following the local training along with the number of samples in the local training dataset and the computed metrics throughout the fit.</p> Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def fit(self, parameters: NDArrays, config: Config) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n    \"\"\"\n    Function to perform the local side of ``FedPCA``. We don't use any parameters sent by the server. Hence\n    ``parameters`` is ignored. We need only the ``train_data_tensor`` to do the work.\n\n    Args:\n        parameters (NDArrays): ignored\n        config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n\n    Returns:\n        (tuple[NDArrays, int, dict[str, Scalar]]): The local principal components following the local training\n            along with the number of samples in the local training dataset and the computed metrics throughout the\n            fit.\n    \"\"\"\n    if not self.initialized:\n        self.setup_client(config)\n    center_data = narrow_dict_type(config, \"center_data\", bool)\n\n    principal_components, singular_values = self.model(self.train_data_tensor, center_data)\n    self.model.set_principal_components(principal_components, singular_values)\n\n    cumulative_explained_variance = self.model.compute_cumulative_explained_variance()\n    explained_variance_ratios = self.model.compute_explained_variance_ratios()\n    metrics: dict[str, Scalar] = {\n        \"cumulative_explained_variance\": cumulative_explained_variance,\n        \"top_explained_variance_ratio\": explained_variance_ratios[0].item(),\n    }\n    return (self.get_parameters(config), self.num_train_samples, metrics)\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.evaluate","title":"<code>evaluate(parameters, config)</code>","text":"<p>Evaluate merged principal components on the local validation set.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Server-merged principal components.</p> required <code>config</code> <code>Config</code> <p>Configurations sent by the server to allow for customization of this functions behavior.</p> required <p>Returns:</p> Type Description <code>tuple[float, int, dict[str, Scalar]]</code> <p>A loss associated with the evaluation, the number of samples in the validation/test set and the <code>metric_values</code> associated with evaluation.</p> Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def evaluate(self, parameters: NDArrays, config: Config) -&gt; tuple[float, int, dict[str, Scalar]]:\n    \"\"\"\n    Evaluate merged principal components on the local validation set.\n\n    Args:\n        parameters (NDArrays): Server-merged principal components.\n        config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n\n    Returns:\n        (tuple[float, int, dict[str, Scalar]]): A loss associated with the evaluation, the number of samples in the\n            validation/test set and the ``metric_values`` associated with evaluation.\n    \"\"\"\n    if not self.initialized:\n        self.setup_client(config)\n        self.model.set_data_mean(self.model.maybe_reshape(self.train_data_tensor))\n    self.set_parameters(parameters, config)\n    num_components_eval = (\n        narrow_dict_type(config, \"num_components_eval\", int) if \"num_components_eval\" in config else None\n    )\n    val_data_tensor_prepared = self.model.center_data(self.model.maybe_reshape(self.val_data_tensor)).to(\n        self.device\n    )\n    reconstruction_loss = self.model.compute_reconstruction_error(val_data_tensor_prepared, num_components_eval)\n    projection_variance = self.model.compute_projection_variance(val_data_tensor_prepared, num_components_eval)\n    metrics: dict[str, Scalar] = {\"projection_variance\": projection_variance}\n    return (reconstruction_loss, self.num_val_samples, metrics)\n</code></pre>"},{"location":"api/#fl4health.clients.fed_pca_client.FedPCAClient.save_model","title":"<code>save_model()</code>","text":"<p>Method to save the FedPCA computed principal components to disk. These can be reloaded to allow for dimensionality reduction in subsequent FL training.</p> Source code in <code>fl4health/clients/fed_pca_client.py</code> <pre><code>def save_model(self) -&gt; None:\n    \"\"\"\n    Method to save the FedPCA computed principal components to disk. These can be reloaded to allow for\n    dimensionality reduction in subsequent FL training.\n    \"\"\"\n    final_model_save_path = self.model_save_dir / f\"client_{self.client_name}_pca.pt\"\n    torch.save(self.model, final_model_save_path)\n    log(INFO, f\"Model parameters saved to {final_model_save_path}.\")\n</code></pre>"},{"location":"api/#fl4health.clients.fed_prox_client","title":"<code>fed_prox_client</code>","text":""},{"location":"api/#fl4health.clients.fed_prox_client.FedProxClient","title":"<code>FedProxClient</code>","text":"<p>               Bases: <code>AdaptiveDriftConstraintClient</code></p> <p>This client implements the FedProx algorithm from Federated Optimization in Heterogeneous Networks. The idea is fairly straightforward. The local loss for each client is augmented with a norm on the difference between the local client weights during training \\(\\mathbf{w}\\) and the initial globally shared weights \\(\\mathbf{w}^t\\).</p> <p>NOTE: The initial value for mu (the drift penalty weight) is set on the server side and passed to each client through parameter exchange. It is stored as the more generally named <code>drift_penalty_weight</code>.</p> Source code in <code>fl4health/clients/fed_prox_client.py</code> <pre><code>class FedProxClient(AdaptiveDriftConstraintClient):\n    \"\"\"\n    This client implements the FedProx algorithm from Federated Optimization in Heterogeneous Networks. The idea is\n    fairly straightforward. The local loss for each client is augmented with a norm on the difference between the\n    local client weights during training \\\\(\\\\mathbf{w}\\\\) and the initial globally shared weights\n    \\\\(\\\\mathbf{w}^t\\\\).\n\n    **NOTE**: The initial value for mu (the drift penalty weight) is set on the server side and passed to each client\n    through parameter exchange. It is stored as the more generally named ``drift_penalty_weight``.\n    \"\"\"\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        # Saving the initial weights and detaching them so that we don't compute gradients with respect to the\n        # tensors. These are used to form the FedProx loss.\n        self.drift_penalty_tensors = [\n            initial_layer_weights.detach().clone() for initial_layer_weights in self.model.parameters()\n        ]\n\n        return super().update_before_train(current_server_round)\n</code></pre>"},{"location":"api/#fl4health.clients.fedbn_client","title":"<code>fedbn_client</code>","text":""},{"location":"api/#fl4health.clients.fedbn_client.FedBnClient","title":"<code>FedBnClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> <p>This class serves as a sparse interface for clients aiming to leverage the FedBN method (https://arxiv.org/abs/2102.07623) or any other approach that excludes specific types of model layers during parameter exchange. This class simply ensures that the user has overridden the <code>get_parameter_exchanger</code> properly.</p> <p>For example, in FedBN, batch normalization layers are excluded from exchange with the server but all other layers flow through and are aggregated via whatever strategy the server is implementing. An example of this where one wants to exclude 2D batch normalization layers during exchange is <code>LayerExchangerWithExclusions(self.model, {nn.BatchNorm2d})</code>, where the model is provided so that the exchanger can identify the appropriate layers to leave out.</p> Source code in <code>fl4health/clients/fedbn_client.py</code> <pre><code>class FedBnClient(BasicClient):\n    \"\"\"\n    This class serves as a sparse interface for clients aiming to leverage the FedBN method\n    (https://arxiv.org/abs/2102.07623) or any other approach that excludes specific types of model layers during\n    parameter exchange. This class simply ensures that the user has overridden the ``get_parameter_exchanger``\n    properly.\n\n    For example, in FedBN, batch normalization layers are excluded from exchange with the server\n    but all other layers flow through and are aggregated via whatever strategy the server is implementing. An example\n    of this where one wants to exclude 2D batch normalization layers during exchange is\n    ``LayerExchangerWithExclusions(self.model, {nn.BatchNorm2d})``, where the model is provided so that the exchanger\n    can identify the appropriate layers to leave out.\n    \"\"\"\n\n    def setup_client(self, config: Config) -&gt; None:\n        super().setup_client(config=config)\n        assert isinstance(self.parameter_exchanger, LayerExchangerWithExclusions), (\n            \"For FedBnClients the parameter exchanger must be of type LayerExchangerWithExclusions \"\n            f\"but got {type(self.parameter_exchanger)}. If you haven't already, override the get_parameter_exchanger \"\n            \"function in your class.\"\n        )\n        return super().setup_client(config)\n</code></pre>"},{"location":"api/#fl4health.clients.fedper_client","title":"<code>fedper_client</code>","text":""},{"location":"api/#fl4health.clients.fedper_client.FedPerClient","title":"<code>FedPerClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> <p>Client to implement the FedPer method (https://arxiv.org/abs/1912.00818).</p> <p>Trains a global feature extractor shared by all clients through FedAvg and a private classifier that is unique to each client. The training is nearly identical to the <code>BasicClient</code> with the exception that our parameter exchanger needs to be a fixed layer exchanger that only exchanges the feature extraction base, which relies on the model being of type <code>SequentiallySplitExchangeBaseModel</code>.</p> Source code in <code>fl4health/clients/fedper_client.py</code> <pre><code>class FedPerClient(BasicClient):\n    \"\"\"\n    Client to implement the FedPer method (https://arxiv.org/abs/1912.00818).\n\n    Trains a global feature extractor shared by all clients through FedAvg and a private classifier that is unique to\n    each client. The training is nearly identical to the ``BasicClient`` with the exception that our parameter\n    exchanger needs to be a fixed layer exchanger that only exchanges the feature extraction base, which relies on the\n    model being of type ``SequentiallySplitExchangeBaseModel``.\n    \"\"\"\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        assert isinstance(self.model, SequentiallySplitExchangeBaseModel), (\n            \"Models for FedPer must be of type SequentiallySplitExchangeBaseModel to facilitate partial weight \"\n            f\"exchange. The current model is of type {type(self.model)}.\"\n        )\n        return FixedLayerExchanger(self.model.layers_to_exchange())\n</code></pre>"},{"location":"api/#fl4health.clients.fedpm_client","title":"<code>fedpm_client</code>","text":""},{"location":"api/#fl4health.clients.fedpm_client.FedPmClient","title":"<code>FedPmClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/fedpm_client.py</code> <pre><code>class FedPmClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Client implementing the FedPM algorithm (https://arxiv.org/pdf/2209.15328).\n\n        FedPM is a recent sparse, communication efficient approach to federated learning. The method has been shown to\n        have exceptional information compression while maintaining good performance. Interestingly, it is also\n        connected to the Lottery Ticket Hypothesis. Training on the client-side is effectively the same as\n        ``BasicClient``. The two components that change are ensuring that the model to be training is a Masked Model\n        compatible with FedPM (or to convert it to one). Second, we use the FedPM exchanger to facilitate exchange\n        with the server.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Performs the same setup as ``BasicClient``, but adds on the possibility of converting an ordinary model to a\n        masked model compatible with ``FedPM``.\n\n        Args:\n            config (Config): Configuration specifying all of the required parameters for training.\n        \"\"\"\n        super().setup_client(config)\n        # Convert self.model to a masked model unless it is specified in the config\n        # file that the model is already a masked model.\n        is_masked_model = narrow_dict_type(config, \"is_masked_model\", bool)\n        if not is_masked_model:\n            self.model = convert_to_masked_model(self.model).to(self.device)\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        \"\"\"\n        Forces the client to use the ``FedPmExchanger``.\n\n        Args:\n            config (Config): Configuration specifying all of the required parameters for training.\n\n        Returns:\n            (ParameterExchanger): returns a ``FedPmExchanger`` to facilitate exchange properly\n        \"\"\"\n        return FedPmExchanger()\n</code></pre>"},{"location":"api/#fl4health.clients.fedpm_client.FedPmClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>Client implementing the FedPM algorithm (https://arxiv.org/pdf/2209.15328).</p> <p>FedPM is a recent sparse, communication efficient approach to federated learning. The method has been shown to have exceptional information compression while maintaining good performance. Interestingly, it is also connected to the Lottery Ticket Hypothesis. Training on the client-side is effectively the same as <code>BasicClient</code>. The two components that change are ensuring that the model to be training is a Masked Model compatible with FedPM (or to convert it to one). Second, we use the FedPM exchanger to facilitate exchange with the server.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/fedpm_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Client implementing the FedPM algorithm (https://arxiv.org/pdf/2209.15328).\n\n    FedPM is a recent sparse, communication efficient approach to federated learning. The method has been shown to\n    have exceptional information compression while maintaining good performance. Interestingly, it is also\n    connected to the Lottery Ticket Hypothesis. Training on the client-side is effectively the same as\n    ``BasicClient``. The two components that change are ensuring that the model to be training is a Masked Model\n    compatible with FedPM (or to convert it to one). Second, we use the FedPM exchanger to facilitate exchange\n    with the server.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.fedpm_client.FedPmClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Performs the same setup as <code>BasicClient</code>, but adds on the possibility of converting an ordinary model to a masked model compatible with <code>FedPM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration specifying all of the required parameters for training.</p> required Source code in <code>fl4health/clients/fedpm_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Performs the same setup as ``BasicClient``, but adds on the possibility of converting an ordinary model to a\n    masked model compatible with ``FedPM``.\n\n    Args:\n        config (Config): Configuration specifying all of the required parameters for training.\n    \"\"\"\n    super().setup_client(config)\n    # Convert self.model to a masked model unless it is specified in the config\n    # file that the model is already a masked model.\n    is_masked_model = narrow_dict_type(config, \"is_masked_model\", bool)\n    if not is_masked_model:\n        self.model = convert_to_masked_model(self.model).to(self.device)\n</code></pre>"},{"location":"api/#fl4health.clients.fedpm_client.FedPmClient.get_parameter_exchanger","title":"<code>get_parameter_exchanger(config)</code>","text":"<p>Forces the client to use the <code>FedPmExchanger</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration specifying all of the required parameters for training.</p> required <p>Returns:</p> Type Description <code>ParameterExchanger</code> <p>returns a <code>FedPmExchanger</code> to facilitate exchange properly</p> Source code in <code>fl4health/clients/fedpm_client.py</code> <pre><code>def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n    \"\"\"\n    Forces the client to use the ``FedPmExchanger``.\n\n    Args:\n        config (Config): Configuration specifying all of the required parameters for training.\n\n    Returns:\n        (ParameterExchanger): returns a ``FedPmExchanger`` to facilitate exchange properly\n    \"\"\"\n    return FedPmExchanger()\n</code></pre>"},{"location":"api/#fl4health.clients.fedrep_client","title":"<code>fedrep_client</code>","text":""},{"location":"api/#fl4health.clients.fedrep_client.FedRepClient","title":"<code>FedRepClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/fedrep_client.py</code> <pre><code>class FedRepClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Client implementing the training of FedRep (https://arxiv.org/abs/2303.05206).\n\n        Similar to FedPer, FedRep trains a global feature extractor shared by all clients through FedAvg and a\n        private classifier that is unique to each client. However, FedRep breaks up the client-side training of\n        these components into two phases. First the local classifier is trained with the feature extractor frozen.\n        Next, the classifier is frozen and the feature extractor is trained.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.fedrep_train_mode = FedRepTrainMode.HEAD\n\n    def _prepare_train_representations(self) -&gt; None:\n        \"\"\"\n        Handles the components switching needed to train the representation submodule as required by FedRep.\n\n        This includes:\n\n        1. Setting the training mode enum to know which optimizer should be stepping during training\n        2. Unfreezing the base module, which represents the feature extractor (if frozen)\n        3. Freezing the weights of the head module representing the classification layers.\n        \"\"\"\n        assert isinstance(self.model, FedRepModel)\n        self.fedrep_train_mode = FedRepTrainMode.REPRESENTATION\n        self.model.unfreeze_base_module()\n        self.model.freeze_head_module()\n\n    def _prepare_train_head(self) -&gt; None:\n        \"\"\"\n        Handles the components switching needed to train the classification submodule as required by FedRep.\n\n        This includes:\n\n        1. Setting the training mode enum to know which optimizer should be stepping during training\n        2. Freezing the base module, which represents the feature extractor.\n        3. Unfreezing the weights of the head module representing the classification layers (if frozen).\n        \"\"\"\n        assert isinstance(self.model, FedRepModel)\n        self.fedrep_train_mode = FedRepTrainMode.HEAD\n        self.model.unfreeze_head_module()\n        self.model.freeze_base_module()\n\n    def _prefix_loss_and_metrics_dictionaries(\n        self, prefix: str, loss_dict: dict[str, float], metrics_dict: dict[str, Scalar]\n    ) -&gt; None:\n        \"\"\"\n        This method is used to added the provided prefix string to the keys of both the loss_dict and the metrics_dict\n        This function is used to separate the losses and metrics values obtained during local training of the head and\n        feature extraction modules of FedRep, which occur separately and sequentially for the approach.\n\n        Args:\n            prefix (str): Prefix to be attached to all keys of the provided dictionaries.\n            loss_dict (dict[str, float]): Dictionary of loss values obtained during training.\n            metrics_dict (dict[str, Scalar]): Dictionary of metrics values measured during training.\n        \"\"\"\n        for loss_key in list(loss_dict):\n            loss_dict[f\"{prefix}_{loss_key}\"] = loss_dict.pop(loss_key)\n        for metrics_key in list(metrics_dict):\n            metrics_dict[f\"{prefix}_{metrics_key}\"] = metrics_dict.pop(metrics_key)\n\n    def _extract_epochs_or_steps_specified(self, config: Config) -&gt; EpochsAndStepsTuple:\n        \"\"\"\n        Function parses the configuration specified and extracts the epochs or step based training values necessary\n        to train a FedRep model. Note that we do not allow for mixed epoch and step based training. You must specify\n        either epochs or steps for both the head and representation modules. The keys should be either\n        ``{local_head_epochs, local_rep_epochs}`` or ``{local_head_steps, local_rep_steps}``.\n\n        Args:\n            config (Config): Configuration specifying all of the required parameters for training.\n\n        Raises:\n            ValueError: This function raises a value error in two scenarios. The first is when both steps and epochs\n                have been specified for training the head and representation modules. The second is when epochs or\n                steps values have not been specified for **BOTH** modules. This could also mean that the keys are\n                wrong.\n\n        Returns:\n            (EpochsAndStepsTuple): Returns a tuple of epochs and steps for which to train the head and representation\n                modules. Only two of the four possible values will be defined, depending on whether we're doing\n                epoch-based or step based training.\n        \"\"\"\n        epochs_specified = (\"local_head_epochs\" in config) and (\"local_rep_epochs\" in config)\n        steps_specified = (\"local_head_steps\" in config) and (\"local_rep_steps\" in config)\n        if epochs_specified and not steps_specified:\n            log(\n                INFO,\n                \"Epochs for head and representation module specified. Proceeding with epoch-based training\",\n            )\n            return (\n                narrow_dict_type(config, \"local_head_epochs\", int),\n                narrow_dict_type(config, \"local_rep_epochs\", int),\n                None,\n                None,\n            )\n        if steps_specified and not epochs_specified:\n            log(\n                INFO,\n                \"Steps for head and representation module specified. Proceeding with step-based training\",\n            )\n            return (\n                None,\n                None,\n                narrow_dict_type(config, \"local_head_steps\", int),\n                narrow_dict_type(config, \"local_rep_steps\", int),\n            )\n        if epochs_specified and steps_specified:\n            raise ValueError(\"Cannot specify both epochs and steps based training values in the config\")\n        raise ValueError(\n            \"Either configuration keys not properly present or a mix of steps and epochs based training was \"\n            \"specified and is not admissible. Keys should be one of {local_head_epochs, local_rep_epochs} or \"\n            \"{local_head_steps, local_rep_steps}\"\n        )\n\n    def process_fed_rep_config(self, config: Config) -&gt; tuple[EpochsAndStepsTuple, int, bool]:\n        \"\"\"\n        Method to ensure the required keys are present in config and extracts values to be returned. We override this\n        functionality from the ``BasicClient``, because ``FedRep`` has slightly different requirements. That is, one\n        needs to specify a number of epochs or steps to do for BOTH the head module **AND** the representation module.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (tuple[int | None, int | None, int | None, int | None, int, bool]): Returns the ``local_epochs``,\n                ``local_steps``, ``current_server_round`` and ``evaluate_after_fit``. Ensures only one of\n                ``local_epochs`` and ``local_steps`` is defined in the config and sets the one that is not to None.\n\n        Raises:\n            ValueError: If the config contains both local_steps and local epochs or if ``local_steps``,\n                ``local_epochs`` or ``current_server_round`` is of the wrong type (int).\n        \"\"\"\n        current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n        steps_or_epochs_tuple = self._extract_epochs_or_steps_specified(config)\n\n        try:\n            evaluate_after_fit = narrow_dict_type(config, \"evaluate_after_fit\", bool)\n        except ValueError:\n            evaluate_after_fit = False\n\n        # Either local epochs or local steps is none based on what key is passed in the config\n        return steps_or_epochs_tuple, current_server_round, evaluate_after_fit\n\n    def get_optimizer(self, config: Config) -&gt; dict[str, Optimizer]:\n        \"\"\"\n        Returns a dictionary with global and local optimizers with string keys \"representation\" and \"head,\"\n        respectively.\n        \"\"\"\n        raise NotImplementedError\n\n    def set_optimizer(self, config: Config) -&gt; None:\n        \"\"\"\n        FedRep requires an optimizer for the representations optimization and one for the model head. This function\n        simply ensures that the optimizers setup by the user have the proper keys and that there are two optimizers.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        optimizers = self.get_optimizer(config)\n        assert isinstance(optimizers, dict) and {\"representation\", \"head\"} == set(optimizers.keys()), (\n            'Optimizer keys must be \"representation\" and \"head\" to use FedRep'\n        )\n        self.optimizers = optimizers\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        # Ensure that the model has the right type and setup the exchanger accordingly\n        assert isinstance(self.model, SequentiallySplitExchangeBaseModel)\n        return FixedLayerExchanger(self.model.layers_to_exchange())\n\n    def fit(self, parameters: NDArrays, config: Config) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n        \"\"\"\n        Processes config, initializes client (if first round) and performs training based on the passed config.\n        For FedRep, this coordinates calling the right training functions based on the passed steps. We need to\n        override the functionality of the ``BasicClient`` to allow for two distinct training passes of the model, as\n        required by FedRep.\n\n        Args:\n            parameters (NDArrays): The parameters of the model to be used in fit.\n            config (NDArrays): The config from the server.\n\n        Returns:\n            (tuple[NDArrays, int, dict[str, Scalar]]): The parameters following the local training along with the\n                number of samples in the local training dataset and the computed metrics throughout the fit.\n\n        Raises:\n            ValueError: If the steps or epochs for the representation and head module training processes are are\n                correctly specified.\n        \"\"\"\n        round_start_time = datetime.datetime.now()\n        (\n            (local_head_epochs, local_rep_epochs, local_head_steps, local_rep_steps),\n            current_server_round,\n            evaluate_after_fit,\n        ) = self.process_fed_rep_config(config)\n\n        if not self.initialized:\n            self.setup_client(config)\n\n        self.set_parameters(parameters, config, fitting_round=True)\n\n        self.update_before_train(current_server_round)\n\n        fit_start_time = datetime.datetime.now()\n        if local_head_epochs and local_rep_epochs:\n            loss_dict, metrics = self.train_fedrep_by_epochs(local_head_epochs, local_rep_epochs, current_server_round)\n        elif local_head_steps and local_rep_steps:\n            loss_dict, metrics = self.train_fedrep_by_steps(local_head_steps, local_rep_steps, current_server_round)\n        else:\n            raise ValueError(\n                \"Local epochs or local steps have not been correctly specified. They have values \"\n                f\"{local_head_epochs}, {local_rep_epochs}, {local_head_steps}, {local_rep_steps}\"\n            )\n        fit_time = datetime.datetime.now() - fit_start_time\n\n        # Check if we should run an evaluation with validation data after fit\n        # (for example, this is used by FedDGGA)\n        if self._should_evaluate_after_fit(evaluate_after_fit):\n            validation_loss, validation_metrics = self.validate()\n            metrics.update(validation_metrics)\n            # We perform a pre-aggregation checkpoint if applicable\n            self._maybe_checkpoint(validation_loss, validation_metrics, CheckpointMode.PRE_AGGREGATION)\n\n        # Report data at end of round\n        self.reports_manager.report(\n            {\n                \"fit_metrics\": metrics,\n                \"fit_losses\": loss_dict,\n                \"round\": current_server_round,\n                \"round_start\": str(round_start_time),\n                \"fit_time_elapsed\": str(fit_time),\n            },\n            current_server_round,\n        )\n\n        # FitRes should contain local parameters, number of examples on client, and a dictionary holding metrics\n        # calculation results.\n        return (\n            self.get_parameters(config),\n            self.num_train_samples,\n            metrics,\n        )\n\n    def train_fedrep_by_epochs(\n        self, head_epochs: int, rep_epochs: int, current_round: int | None = None\n    ) -&gt; tuple[dict[str, float], dict[str, Scalar]]:\n        \"\"\"\n        Train locally for the specified number of epochs.\n\n        Args:\n            head_epochs (int): The number of epochs for local training of the head module.\n            rep_epochs (int): The number of epochs for local training of the representation module\n            current_round (int | None, optional): The current FL round. Defaults to None.\n\n        Returns:\n            (tuple[dict[str, float], dict[str, Scalar]]): The loss and metrics dictionary from the local training.\n                Loss is a dictionary of one or more losses that represent the different components of the loss.\n        \"\"\"\n        # First we train the head module for head_epochs with the representations frozen in place\n        self._prepare_train_head()\n        log(INFO, f\"Beginning FedRep Head Training Phase for {head_epochs} Epochs\")\n        loss_dict_head, metrics_dict_head = self.train_by_epochs(head_epochs, current_round)\n        log(INFO, \"Converting the loss and metrics dictionary keys for head training\")\n        # The loss and metrics coming from train_by_epochs are generically keyed, for example \"backward.\" To avoid\n        # clashing or being overwritten by the rep module training below, we prefix these keys.\n        self._prefix_loss_and_metrics_dictionaries(\"head\", loss_dict_head, metrics_dict_head)\n\n        # Second we train the representation module for rep_epochs with the head module frozen in place\n        self._prepare_train_representations()\n        log(\n            INFO,\n            f\"Beginning FedRep Representation Training Phase for {rep_epochs} Epochs\",\n        )\n        loss_dict_rep, metrics_dict_rep = self.train_by_epochs(rep_epochs, current_round)\n        log(INFO, \"Converting the loss and metrics dictionary keys for Rep training\")\n        # The loss and metrics coming from train_by_epochs are generically keyed, for example \"backward.\" To avoid\n        # clashing or being overwritten by the head module training above, we prefix these keys.\n        self._prefix_loss_and_metrics_dictionaries(\"rep\", loss_dict_rep, metrics_dict_rep)\n        log(INFO, \"Merging the loss and training dictionaries\")\n        loss_dict_head.update(loss_dict_rep)\n        metrics_dict_head.update(metrics_dict_rep)\n        return loss_dict_head, metrics_dict_head\n\n    def train_fedrep_by_steps(\n        self, head_steps: int, rep_steps: int, current_round: int | None = None\n    ) -&gt; tuple[dict[str, float], dict[str, Scalar]]:\n        \"\"\"\n        Train locally for the specified number of steps.\n\n        Args:\n            head_steps (int): The number of steps to train locally for the head model.\n            rep_steps (int): The number of steps to train locally for the representation model\n            current_round (int | None, optional): What round of FL training we're currently on. Defaults to None.\n\n        Returns:\n            (tuple[dict[str, float], dict[str, Scalar]]): The loss and metrics dictionary from the local training.\n                Loss is a dictionary of one or more losses that represent the different components of the loss.\n        \"\"\"\n        assert isinstance(self.model, FedRepModel)\n        # First we train the head module for head_steps with the representations frozen in place\n        self._prepare_train_head()\n        log(INFO, f\"Beginning FedRep Head Training Phase for {head_steps} Steps\")\n        loss_dict_head, metrics_dict_head = self.train_by_steps(head_steps, current_round)\n        log(INFO, \"Converting the loss and metrics dictionary keys for head training\")\n        # The loss and metrics coming from train_by_steps are generically keyed, for example \"backward.\" To avoid\n        # clashing or being overwritten by the rep module training below, we prefix these keys.\n        self._prefix_loss_and_metrics_dictionaries(\"head\", loss_dict_head, metrics_dict_head)\n\n        # Second we train the representation module for rep_steps with the head module frozen in place\n        self._prepare_train_representations()\n        log(\n            INFO,\n            f\"Beginning FedRep Representation Training Phase for {rep_steps} Steps\",\n        )\n        loss_dict_rep, metrics_dict_rep = self.train_by_steps(rep_steps, current_round)\n        log(INFO, \"Converting the loss and metrics dictionary keys for Rep training\")\n        # The loss and metrics coming from train_by_steps are generically keyed, for example \"backward.\" To avoid\n        # clashing or being overwritten by the head module training above, we prefix these keys.\n        self._prefix_loss_and_metrics_dictionaries(\"rep\", loss_dict_rep, metrics_dict_rep)\n        log(INFO, \"Merging the loss and training dictionaries\")\n        loss_dict_head.update(loss_dict_rep)\n        metrics_dict_head.update(metrics_dict_rep)\n        return loss_dict_head, metrics_dict_head\n\n    def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Mechanics of training loop follow the FedRep paper: https://arxiv.org/pdf/2102.07078.pdf. In order to reuse\n        the ``train_step`` functionality, we switch between the appropriate optimizers depending on the\n        clients training mode (HEAD vs. REPRESENTATION).\n\n        Args:\n            input (TorchInputType): input tensor to be run through the model. Here, ``TorchInputType`` is simply an\n                alias for the union of ``torch.Tensor`` and ``dict[str, torch.Tensor]``.\n            target (TorchTargetType): target tensor to be used to compute a loss given the model's outputs.\n\n        Returns:\n            (tuple[TrainingLosses, dict[str, torch.Tensor]]): The losses object from the train step along with\n                a dictionary of any predictions produced by the model.\n        \"\"\"\n        # Clear gradients from the optimizers if they exits. We do both regardless of the client mode.\n        self.optimizers[\"representation\"].zero_grad()\n        self.optimizers[\"head\"].zero_grad()\n\n        # Perform forward pass on the full model\n        preds, features = self.predict(input)\n        target = self.transform_target(target)  # Apply transformation (Defaults to identity)\n\n        # Compute all relevant losses\n        losses = self.compute_training_loss(preds, features, target)\n        losses.backward[\"backward\"].backward()\n\n        if self.fedrep_train_mode == FedRepTrainMode.HEAD:\n            self.optimizers[\"head\"].step()\n        elif self.fedrep_train_mode == FedRepTrainMode.REPRESENTATION:\n            self.optimizers[\"representation\"].step()\n        else:\n            raise ValueError(\"Training Mode in an invalid state\")\n\n        # Return dictionary of predictions where key is used to name respective MetricMeters\n        return losses, preds\n</code></pre>"},{"location":"api/#fl4health.clients.fedrep_client.FedRepClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>Client implementing the training of FedRep (https://arxiv.org/abs/2303.05206).</p> <p>Similar to FedPer, FedRep trains a global feature extractor shared by all clients through FedAvg and a private classifier that is unique to each client. However, FedRep breaks up the client-side training of these components into two phases. First the local classifier is trained with the feature extractor frozen. Next, the classifier is frozen and the feature extractor is trained.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/fedrep_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Client implementing the training of FedRep (https://arxiv.org/abs/2303.05206).\n\n    Similar to FedPer, FedRep trains a global feature extractor shared by all clients through FedAvg and a\n    private classifier that is unique to each client. However, FedRep breaks up the client-side training of\n    these components into two phases. First the local classifier is trained with the feature extractor frozen.\n    Next, the classifier is frozen and the feature extractor is trained.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.fedrep_train_mode = FedRepTrainMode.HEAD\n</code></pre>"},{"location":"api/#fl4health.clients.fedrep_client.FedRepClient.process_fed_rep_config","title":"<code>process_fed_rep_config(config)</code>","text":"<p>Method to ensure the required keys are present in config and extracts values to be returned. We override this functionality from the <code>BasicClient</code>, because <code>FedRep</code> has slightly different requirements. That is, one needs to specify a number of epochs or steps to do for BOTH the head module AND the representation module.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>tuple[int | None, int | None, int | None, int | None, int, bool]</code> <p>Returns the <code>local_epochs</code>, <code>local_steps</code>, <code>current_server_round</code> and <code>evaluate_after_fit</code>. Ensures only one of <code>local_epochs</code> and <code>local_steps</code> is defined in the config and sets the one that is not to None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the config contains both local_steps and local epochs or if <code>local_steps</code>, <code>local_epochs</code> or <code>current_server_round</code> is of the wrong type (int).</p> Source code in <code>fl4health/clients/fedrep_client.py</code> <pre><code>def process_fed_rep_config(self, config: Config) -&gt; tuple[EpochsAndStepsTuple, int, bool]:\n    \"\"\"\n    Method to ensure the required keys are present in config and extracts values to be returned. We override this\n    functionality from the ``BasicClient``, because ``FedRep`` has slightly different requirements. That is, one\n    needs to specify a number of epochs or steps to do for BOTH the head module **AND** the representation module.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (tuple[int | None, int | None, int | None, int | None, int, bool]): Returns the ``local_epochs``,\n            ``local_steps``, ``current_server_round`` and ``evaluate_after_fit``. Ensures only one of\n            ``local_epochs`` and ``local_steps`` is defined in the config and sets the one that is not to None.\n\n    Raises:\n        ValueError: If the config contains both local_steps and local epochs or if ``local_steps``,\n            ``local_epochs`` or ``current_server_round`` is of the wrong type (int).\n    \"\"\"\n    current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n    steps_or_epochs_tuple = self._extract_epochs_or_steps_specified(config)\n\n    try:\n        evaluate_after_fit = narrow_dict_type(config, \"evaluate_after_fit\", bool)\n    except ValueError:\n        evaluate_after_fit = False\n\n    # Either local epochs or local steps is none based on what key is passed in the config\n    return steps_or_epochs_tuple, current_server_round, evaluate_after_fit\n</code></pre>"},{"location":"api/#fl4health.clients.fedrep_client.FedRepClient.get_optimizer","title":"<code>get_optimizer(config)</code>","text":"<p>Returns a dictionary with global and local optimizers with string keys \"representation\" and \"head,\" respectively.</p> Source code in <code>fl4health/clients/fedrep_client.py</code> <pre><code>def get_optimizer(self, config: Config) -&gt; dict[str, Optimizer]:\n    \"\"\"\n    Returns a dictionary with global and local optimizers with string keys \"representation\" and \"head,\"\n    respectively.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.fedrep_client.FedRepClient.set_optimizer","title":"<code>set_optimizer(config)</code>","text":"<p>FedRep requires an optimizer for the representations optimization and one for the model head. This function simply ensures that the optimizers setup by the user have the proper keys and that there are two optimizers.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/fedrep_client.py</code> <pre><code>def set_optimizer(self, config: Config) -&gt; None:\n    \"\"\"\n    FedRep requires an optimizer for the representations optimization and one for the model head. This function\n    simply ensures that the optimizers setup by the user have the proper keys and that there are two optimizers.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    optimizers = self.get_optimizer(config)\n    assert isinstance(optimizers, dict) and {\"representation\", \"head\"} == set(optimizers.keys()), (\n        'Optimizer keys must be \"representation\" and \"head\" to use FedRep'\n    )\n    self.optimizers = optimizers\n</code></pre>"},{"location":"api/#fl4health.clients.fedrep_client.FedRepClient.fit","title":"<code>fit(parameters, config)</code>","text":"<p>Processes config, initializes client (if first round) and performs training based on the passed config. For FedRep, this coordinates calling the right training functions based on the passed steps. We need to override the functionality of the <code>BasicClient</code> to allow for two distinct training passes of the model, as required by FedRep.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>The parameters of the model to be used in fit.</p> required <code>config</code> <code>NDArrays</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>tuple[NDArrays, int, dict[str, Scalar]]</code> <p>The parameters following the local training along with the number of samples in the local training dataset and the computed metrics throughout the fit.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the steps or epochs for the representation and head module training processes are are correctly specified.</p> Source code in <code>fl4health/clients/fedrep_client.py</code> <pre><code>def fit(self, parameters: NDArrays, config: Config) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n    \"\"\"\n    Processes config, initializes client (if first round) and performs training based on the passed config.\n    For FedRep, this coordinates calling the right training functions based on the passed steps. We need to\n    override the functionality of the ``BasicClient`` to allow for two distinct training passes of the model, as\n    required by FedRep.\n\n    Args:\n        parameters (NDArrays): The parameters of the model to be used in fit.\n        config (NDArrays): The config from the server.\n\n    Returns:\n        (tuple[NDArrays, int, dict[str, Scalar]]): The parameters following the local training along with the\n            number of samples in the local training dataset and the computed metrics throughout the fit.\n\n    Raises:\n        ValueError: If the steps or epochs for the representation and head module training processes are are\n            correctly specified.\n    \"\"\"\n    round_start_time = datetime.datetime.now()\n    (\n        (local_head_epochs, local_rep_epochs, local_head_steps, local_rep_steps),\n        current_server_round,\n        evaluate_after_fit,\n    ) = self.process_fed_rep_config(config)\n\n    if not self.initialized:\n        self.setup_client(config)\n\n    self.set_parameters(parameters, config, fitting_round=True)\n\n    self.update_before_train(current_server_round)\n\n    fit_start_time = datetime.datetime.now()\n    if local_head_epochs and local_rep_epochs:\n        loss_dict, metrics = self.train_fedrep_by_epochs(local_head_epochs, local_rep_epochs, current_server_round)\n    elif local_head_steps and local_rep_steps:\n        loss_dict, metrics = self.train_fedrep_by_steps(local_head_steps, local_rep_steps, current_server_round)\n    else:\n        raise ValueError(\n            \"Local epochs or local steps have not been correctly specified. They have values \"\n            f\"{local_head_epochs}, {local_rep_epochs}, {local_head_steps}, {local_rep_steps}\"\n        )\n    fit_time = datetime.datetime.now() - fit_start_time\n\n    # Check if we should run an evaluation with validation data after fit\n    # (for example, this is used by FedDGGA)\n    if self._should_evaluate_after_fit(evaluate_after_fit):\n        validation_loss, validation_metrics = self.validate()\n        metrics.update(validation_metrics)\n        # We perform a pre-aggregation checkpoint if applicable\n        self._maybe_checkpoint(validation_loss, validation_metrics, CheckpointMode.PRE_AGGREGATION)\n\n    # Report data at end of round\n    self.reports_manager.report(\n        {\n            \"fit_metrics\": metrics,\n            \"fit_losses\": loss_dict,\n            \"round\": current_server_round,\n            \"round_start\": str(round_start_time),\n            \"fit_time_elapsed\": str(fit_time),\n        },\n        current_server_round,\n    )\n\n    # FitRes should contain local parameters, number of examples on client, and a dictionary holding metrics\n    # calculation results.\n    return (\n        self.get_parameters(config),\n        self.num_train_samples,\n        metrics,\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.fedrep_client.FedRepClient.train_fedrep_by_epochs","title":"<code>train_fedrep_by_epochs(head_epochs, rep_epochs, current_round=None)</code>","text":"<p>Train locally for the specified number of epochs.</p> <p>Parameters:</p> Name Type Description Default <code>head_epochs</code> <code>int</code> <p>The number of epochs for local training of the head module.</p> required <code>rep_epochs</code> <code>int</code> <p>The number of epochs for local training of the representation module</p> required <code>current_round</code> <code>int | None</code> <p>The current FL round. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict[str, float], dict[str, Scalar]]</code> <p>The loss and metrics dictionary from the local training. Loss is a dictionary of one or more losses that represent the different components of the loss.</p> Source code in <code>fl4health/clients/fedrep_client.py</code> <pre><code>def train_fedrep_by_epochs(\n    self, head_epochs: int, rep_epochs: int, current_round: int | None = None\n) -&gt; tuple[dict[str, float], dict[str, Scalar]]:\n    \"\"\"\n    Train locally for the specified number of epochs.\n\n    Args:\n        head_epochs (int): The number of epochs for local training of the head module.\n        rep_epochs (int): The number of epochs for local training of the representation module\n        current_round (int | None, optional): The current FL round. Defaults to None.\n\n    Returns:\n        (tuple[dict[str, float], dict[str, Scalar]]): The loss and metrics dictionary from the local training.\n            Loss is a dictionary of one or more losses that represent the different components of the loss.\n    \"\"\"\n    # First we train the head module for head_epochs with the representations frozen in place\n    self._prepare_train_head()\n    log(INFO, f\"Beginning FedRep Head Training Phase for {head_epochs} Epochs\")\n    loss_dict_head, metrics_dict_head = self.train_by_epochs(head_epochs, current_round)\n    log(INFO, \"Converting the loss and metrics dictionary keys for head training\")\n    # The loss and metrics coming from train_by_epochs are generically keyed, for example \"backward.\" To avoid\n    # clashing or being overwritten by the rep module training below, we prefix these keys.\n    self._prefix_loss_and_metrics_dictionaries(\"head\", loss_dict_head, metrics_dict_head)\n\n    # Second we train the representation module for rep_epochs with the head module frozen in place\n    self._prepare_train_representations()\n    log(\n        INFO,\n        f\"Beginning FedRep Representation Training Phase for {rep_epochs} Epochs\",\n    )\n    loss_dict_rep, metrics_dict_rep = self.train_by_epochs(rep_epochs, current_round)\n    log(INFO, \"Converting the loss and metrics dictionary keys for Rep training\")\n    # The loss and metrics coming from train_by_epochs are generically keyed, for example \"backward.\" To avoid\n    # clashing or being overwritten by the head module training above, we prefix these keys.\n    self._prefix_loss_and_metrics_dictionaries(\"rep\", loss_dict_rep, metrics_dict_rep)\n    log(INFO, \"Merging the loss and training dictionaries\")\n    loss_dict_head.update(loss_dict_rep)\n    metrics_dict_head.update(metrics_dict_rep)\n    return loss_dict_head, metrics_dict_head\n</code></pre>"},{"location":"api/#fl4health.clients.fedrep_client.FedRepClient.train_fedrep_by_steps","title":"<code>train_fedrep_by_steps(head_steps, rep_steps, current_round=None)</code>","text":"<p>Train locally for the specified number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>head_steps</code> <code>int</code> <p>The number of steps to train locally for the head model.</p> required <code>rep_steps</code> <code>int</code> <p>The number of steps to train locally for the representation model</p> required <code>current_round</code> <code>int | None</code> <p>What round of FL training we're currently on. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict[str, float], dict[str, Scalar]]</code> <p>The loss and metrics dictionary from the local training. Loss is a dictionary of one or more losses that represent the different components of the loss.</p> Source code in <code>fl4health/clients/fedrep_client.py</code> <pre><code>def train_fedrep_by_steps(\n    self, head_steps: int, rep_steps: int, current_round: int | None = None\n) -&gt; tuple[dict[str, float], dict[str, Scalar]]:\n    \"\"\"\n    Train locally for the specified number of steps.\n\n    Args:\n        head_steps (int): The number of steps to train locally for the head model.\n        rep_steps (int): The number of steps to train locally for the representation model\n        current_round (int | None, optional): What round of FL training we're currently on. Defaults to None.\n\n    Returns:\n        (tuple[dict[str, float], dict[str, Scalar]]): The loss and metrics dictionary from the local training.\n            Loss is a dictionary of one or more losses that represent the different components of the loss.\n    \"\"\"\n    assert isinstance(self.model, FedRepModel)\n    # First we train the head module for head_steps with the representations frozen in place\n    self._prepare_train_head()\n    log(INFO, f\"Beginning FedRep Head Training Phase for {head_steps} Steps\")\n    loss_dict_head, metrics_dict_head = self.train_by_steps(head_steps, current_round)\n    log(INFO, \"Converting the loss and metrics dictionary keys for head training\")\n    # The loss and metrics coming from train_by_steps are generically keyed, for example \"backward.\" To avoid\n    # clashing or being overwritten by the rep module training below, we prefix these keys.\n    self._prefix_loss_and_metrics_dictionaries(\"head\", loss_dict_head, metrics_dict_head)\n\n    # Second we train the representation module for rep_steps with the head module frozen in place\n    self._prepare_train_representations()\n    log(\n        INFO,\n        f\"Beginning FedRep Representation Training Phase for {rep_steps} Steps\",\n    )\n    loss_dict_rep, metrics_dict_rep = self.train_by_steps(rep_steps, current_round)\n    log(INFO, \"Converting the loss and metrics dictionary keys for Rep training\")\n    # The loss and metrics coming from train_by_steps are generically keyed, for example \"backward.\" To avoid\n    # clashing or being overwritten by the head module training above, we prefix these keys.\n    self._prefix_loss_and_metrics_dictionaries(\"rep\", loss_dict_rep, metrics_dict_rep)\n    log(INFO, \"Merging the loss and training dictionaries\")\n    loss_dict_head.update(loss_dict_rep)\n    metrics_dict_head.update(metrics_dict_rep)\n    return loss_dict_head, metrics_dict_head\n</code></pre>"},{"location":"api/#fl4health.clients.fedrep_client.FedRepClient.train_step","title":"<code>train_step(input, target)</code>","text":"<p>Mechanics of training loop follow the FedRep paper: https://arxiv.org/pdf/2102.07078.pdf. In order to reuse the <code>train_step</code> functionality, we switch between the appropriate optimizers depending on the clients training mode (HEAD vs. REPRESENTATION).</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>input tensor to be run through the model. Here, <code>TorchInputType</code> is simply an alias for the union of <code>torch.Tensor</code> and <code>dict[str, torch.Tensor]</code>.</p> required <code>target</code> <code>TorchTargetType</code> <p>target tensor to be used to compute a loss given the model's outputs.</p> required <p>Returns:</p> Type Description <code>tuple[TrainingLosses, dict[str, Tensor]]</code> <p>The losses object from the train step along with a dictionary of any predictions produced by the model.</p> Source code in <code>fl4health/clients/fedrep_client.py</code> <pre><code>def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n    \"\"\"\n    Mechanics of training loop follow the FedRep paper: https://arxiv.org/pdf/2102.07078.pdf. In order to reuse\n    the ``train_step`` functionality, we switch between the appropriate optimizers depending on the\n    clients training mode (HEAD vs. REPRESENTATION).\n\n    Args:\n        input (TorchInputType): input tensor to be run through the model. Here, ``TorchInputType`` is simply an\n            alias for the union of ``torch.Tensor`` and ``dict[str, torch.Tensor]``.\n        target (TorchTargetType): target tensor to be used to compute a loss given the model's outputs.\n\n    Returns:\n        (tuple[TrainingLosses, dict[str, torch.Tensor]]): The losses object from the train step along with\n            a dictionary of any predictions produced by the model.\n    \"\"\"\n    # Clear gradients from the optimizers if they exits. We do both regardless of the client mode.\n    self.optimizers[\"representation\"].zero_grad()\n    self.optimizers[\"head\"].zero_grad()\n\n    # Perform forward pass on the full model\n    preds, features = self.predict(input)\n    target = self.transform_target(target)  # Apply transformation (Defaults to identity)\n\n    # Compute all relevant losses\n    losses = self.compute_training_loss(preds, features, target)\n    losses.backward[\"backward\"].backward()\n\n    if self.fedrep_train_mode == FedRepTrainMode.HEAD:\n        self.optimizers[\"head\"].step()\n    elif self.fedrep_train_mode == FedRepTrainMode.REPRESENTATION:\n        self.optimizers[\"representation\"].step()\n    else:\n        raise ValueError(\"Training Mode in an invalid state\")\n\n    # Return dictionary of predictions where key is used to name respective MetricMeters\n    return losses, preds\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_client","title":"<code>fenda_client</code>","text":""},{"location":"api/#fl4health.clients.fenda_client.FendaClient","title":"<code>FendaClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/fenda_client.py</code> <pre><code>class FendaClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client is used to perform client-side training associated with the FENDA method described in\n        https://arxiv.org/pdf/2309.16825.\n\n        The approach splits a model being trained into parallel feature extractors whose latent feature spaces are\n        then further processed by a classification head. The global feature extractor is federally trained with FedAvg\n        and the local feature extractor and classification head are exclusively trained locally. This is closely\n        related (and is essentially an ablation of) the PerFCL method.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        assert isinstance(self.model, FendaModel)\n        return FixedLayerExchanger(self.model.layers_to_exchange())\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_client.FendaClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>This client is used to perform client-side training associated with the FENDA method described in https://arxiv.org/pdf/2309.16825.</p> <p>The approach splits a model being trained into parallel feature extractors whose latent feature spaces are then further processed by a classification head. The global feature extractor is federally trained with FedAvg and the local feature extractor and classification head are exclusively trained locally. This is closely related (and is essentially an ablation of) the PerFCL method.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/fenda_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    This client is used to perform client-side training associated with the FENDA method described in\n    https://arxiv.org/pdf/2309.16825.\n\n    The approach splits a model being trained into parallel feature extractors whose latent feature spaces are\n    then further processed by a classification head. The global feature extractor is federally trained with FedAvg\n    and the local feature extractor and classification head are exclusively trained locally. This is closely\n    related (and is essentially an ablation of) the PerFCL method.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client","title":"<code>fenda_ditto_client</code>","text":""},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient","title":"<code>FendaDittoClient</code>","text":"<p>               Bases: <code>DittoClient</code></p> Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>class FendaDittoClient(DittoClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n        freeze_global_feature_extractor: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        This client implements a combination of the Ditto algorithm from Ditto: Fair and Robust Federated Learning\n        Through Personalization with FENDA-FL models. In this implementation, the global Ditto model consists of a\n        feature extractor and classification head, where the feature extractor architecture is identical to that of\n        the global and local feature extractors of the FENDA model being trained. The idea is that we want to train a\n        local FENDA model along with the global model for each client. We simultaneously train a global model that is\n        aggregated on the server-side and use those weights to also constrain the training of a local\n        FENDA model. At the beginning of each server round, the feature extractor from globally aggregated model is\n        injected into the global feature extractor of the FENDA model.\n\n        There are two distinct modes of operation:\n\n        - If ``freeze_global_feature_extractor`` is True. The global Ditto model feature extractor\n          **SETS AND FREEZES** weights of global FENDA feature extractor. The local components of the FENDA model are\n          trained and an additional drift loss is computed between the local and global feature extractors of the\n          FENDA model.\n\n        - If ``freeze_global_feature_extractor`` is False. The global Ditto model feature extractor **INITIALIZES**\n          weights of the FENDA model's global feature extractor, both local and global components of FENDA are\n          trained and a drift loss is calculated between Ditto global feature extractor and FENDA global feature\n          extractor.\n\n        The constraint for the FENDA model feature extractors discussed above uses a weight drift loss on its\n        feature extraction modules.\n\n        **NOTE**: Unlike FENDA, the global feature extractor of the FENDA model is NOT exchanged with the server.\n        Rather, the global Ditto model is exchanged and injected at each round into the global feature extractor. If\n        the global feature extractor is frozen, then only the local components of the FENDA network are trained.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            freeze_global_feature_extractor (bool, optional): Determines whether we freeze the FENDA global feature\n                extractor during training. If ``freeze_global_feature_extractor`` is False, both the global and the\n                local feature extractor in the local FENDA model will be trained. Otherwise, the global feature\n                extractor submodule is frozen. If ``freeze_global_feature_extractor`` is True, the Ditto loss will be\n                calculated using the local FENDA feature extractor and the global model. Otherwise, the loss is\n                calculated using the global FENDA feature extractor and the global model. Defaults to False.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.global_model: SequentiallySplitModel\n        self.model: FendaModel\n        self.freeze_global_feature_extractor = freeze_global_feature_extractor\n\n    def get_model(self, config: Config) -&gt; FendaModel:\n        \"\"\"\n        User defined method that returns FENDA model.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (FendaModel): The client FENDA model.\n\n        Raises:\n            NotImplementedError: To be defined in child class.\n        \"\"\"\n        raise NotImplementedError(\"This function must be defined in the inheriting class to use this client\")\n\n    def get_global_model(self, config: Config) -&gt; SequentiallySplitModel:\n        \"\"\"\n        User defined method that returns a Global Sequential Model that is compatible with the local FENDA model.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (SequentiallySplitModel): The global (Ditto) model.\n\n        Raises:\n            NotImplementedError: To be defined in child class.\n        \"\"\"\n        raise NotImplementedError(\"This function must be defined in the inheriting class to use this client\")\n\n    def _check_shape_match(self) -&gt; None:\n        \"\"\"\n        Checks that the defined Ditto model is compatible with the sub-components of the FENDA model and that the\n        feature extractors of the FENDA model are also compatible.\n        \"\"\"\n        # Check if shapes of global_model feature_extractor and self.model.second_feature_extractor match\n        check_shape_match(\n            self.global_model.base_module.parameters(),\n            self.model.second_feature_extractor.parameters(),\n            \"Shapes of self.global_model.feature_extractor and self.model.second_feature_extractor do not match.\\\n                For FENDA+Ditto, these components much match exactly.\",\n        )\n\n        # Check if shapes of self.model.second_feature_extractor and self.model.first_feature_extractor match\n        check_shape_match(\n            self.model.second_feature_extractor.parameters(),\n            self.model.first_feature_extractor.parameters(),\n            \"Shapes of self.model.second_feature_extractor and self.model.first_feature_extractor do not match.\\\n                For FENDA+Ditto, these components much match exactly.\",\n        )\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n        Then set initialized attribute to True. This function simply straps on the compatibility of the models.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        super().setup_client(config)\n        self._check_shape_match()\n\n    def get_parameters(self, config: Config) -&gt; NDArrays:\n        \"\"\"\n        For FendaDitto, we transfer the **GLOBAL** Ditto model weights to the server to be aggregated. The local FENDA\n        model weights stay with the client. The local FENDA model has a different architecture than the **GLOBAL**\n        model. So if the client is being asked for initialization parameters, we just send the GLOBAL model to sync\n        all **GLOBAL** models across clients AND the local FENDA model's global feature extractor.\n\n        Args:\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n        Returns:\n            (NDArrays): **GLOBAL** model weights to be sent to the server for aggregation.\n        \"\"\"\n        if not self.initialized:\n            log(INFO, \"Setting up client\")\n            if not config:\n                log(\n                    WARNING,\n                    (\n                        \"This client has not yet been initialized and the config is empty. This may cause unexpected \"\n                        \"failures, as setting up a client typically requires several configuration parameters, \"\n                        \"including batch_size and current_server_round.\"\n                    ),\n                )\n            self.setup_client(config)\n\n        assert (\n            self.global_model is not None\n            and self.parameter_exchanger is not None\n            and self.loss_for_adaptation is not None\n        )\n\n        model_weights = self.parameter_exchanger.push_parameters(self.global_model, config=config)\n        # Weights and training loss sent to server for aggregation\n        # Training loss sent because server will decide to increase or decrease the penalty weight, if adaptivity\n        # is turned on\n        return self.parameter_exchanger.pack_parameters(model_weights, self.loss_for_adaptation)\n\n    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n        \"\"\"\n        The parameters being passed are to be routed to the global (ditto) model and copied to the global feature\n        extractor of the local FENDA model and saved as the initial global model tensors to be used in a penalty term\n        in training the local model. We assume the both the global and local models are being initialized and use\n        a ``FullParameterExchanger()`` to set the model weights for the global model, the global model feature\n        extractor weights will be then copied to the global feature extractor of local FENDA model.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n                round or an evaluation round. This is used to help determine which parameter exchange should be used\n                for pulling parameters. A full parameter exchanger is only used if the current federated learning\n                round is the very first fitting round.\n        \"\"\"\n        # Make sure that the proper components exist.\n        assert self.global_model is not None and self.model is not None\n        assert self.parameter_exchanger is not None and isinstance(\n            self.parameter_exchanger, FullParameterExchangerWithPacking\n        )\n\n        server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n        log(INFO, f\"Penalty weight received from the server: {self.drift_penalty_weight}\")\n\n        self.parameter_exchanger.pull_parameters(server_model_state, self.global_model, config)\n        # GLOBAL MODEL feature extractor is given to local FENDA model\n        self.model.second_feature_extractor.load_state_dict(self.global_model.base_module.state_dict())\n\n    def set_initial_global_tensors(self) -&gt; None:\n        \"\"\"\n        Saves the initial **GLOBAL** (DITTO) MODEL weights and detaching them so that we don't compute gradients with\n        respect to the tensors. These are used to form the Ditto local update penalty term.\n\n        **NOTE**: We are only saving the base model parameters, as these will be used to constraint a feature extractor\n        in the local FENDA model (not the full stack)\n        \"\"\"\n        self.drift_penalty_tensors = [\n            initial_layer_weights.detach().clone()\n            for initial_layer_weights in self.global_model.base_module.parameters()\n        ]\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        \"\"\"\n        Follows the same flow as the ``DittoClient`` parent module, but adds the ability to freeze the global feature\n        extractor during training if ``self.freeze_global_feature_extractor`` is True.\n\n        Args:\n            current_server_round (int): Which round we're currently on\n        \"\"\"\n        # freeze the global feature extractor during training updates if desired.\n        if self.freeze_global_feature_extractor:\n            for param in self.model.second_feature_extractor.parameters():\n                param.requires_grad = False\n        return super().update_before_train(current_server_round)\n\n    def predict(\n        self,\n        input: TorchInputType,\n    ) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Computes the predictions for both the **GLOBAL** and **LOCAL** models and pack them into the prediction\n        dictionary.\n\n        Args:\n            input (TorchInputType): Inputs to be fed into both models.\n\n        Returns:\n            (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains predictions indexed\n                by name and the second element contains intermediate activations index by name. For Ditto+FENDA, we\n                only need the predictions, so the second dictionary is simply empty.\n\n        Raises:\n            ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n                forward pass.\n        \"\"\"\n        if isinstance(input, torch.Tensor):\n            global_preds, _ = self.global_model(input)\n            local_preds, _ = self.model(input)\n        elif isinstance(input, dict):\n            # If input is a dictionary, then we unpack it before computing the forward pass.\n            # Note that this assumes the keys of the input match (exactly) the keyword args\n            # of the forward method.\n            global_preds, _ = self.global_model(**input)\n            local_preds, _ = self.model(**input)\n\n        global_preds = global_preds[\"prediction\"]\n        local_preds = local_preds[\"prediction\"]\n        # Here we assume that global and local preds are simply tensors\n        # TODO: Perhaps loosen this at a later date.\n        assert isinstance(global_preds, torch.Tensor)\n        assert isinstance(local_preds, torch.Tensor)\n        return {\"global\": global_preds, \"local\": local_preds}, {}\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training losses given predictions of the global and local models and ground truth data.\n        For the local model, we add to the vanilla loss function by including a Ditto penalty loss. This penalty\n        is the \\\\(\\\\ell^2\\\\) inner product between the initial global model feature extractor weights and the\n        feature extractor weights of the local model. If the global feature extractor is not frozen, the penalty is\n        computed using the global feature extractor of the local model. If it is frozen, the penalty is computed using\n        the local feature extractor of the local model. This allows for flexibility in training scenarios where the\n        feature extractors may differ between the global and local models. The penalty is stored in \"backward\". The\n        loss to optimize the global model is stored in the additional losses dictionary under \u201cglobal_loss\u201d.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included\n                in the dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing the backward loss and additional losses\n                indexed by name. Additional losses include each loss component and the global model loss tensor.\n        \"\"\"\n        # Check that both models are in training mode\n        assert self.global_model.training and self.model.training\n\n        loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n        if additional_losses is None:\n            additional_losses = {}\n\n        # adding the vanilla loss to the additional losses to be used by update_after_train for potential adaptation\n        additional_losses[\"loss_for_adaptation\"] = loss.clone()\n\n        # Compute the appropriate Ditto drift loss\n        if self.freeze_global_feature_extractor:\n            penalty_loss = self.penalty_loss_function(\n                self.model.first_feature_extractor, self.drift_penalty_tensors, self.drift_penalty_weight\n            )\n        else:\n            penalty_loss = self.penalty_loss_function(\n                self.model.second_feature_extractor, self.drift_penalty_tensors, self.drift_penalty_weight\n            )\n        additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n\n        return TrainingLosses(backward=loss + penalty_loss, additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None, freeze_global_feature_extractor=False)</code>","text":"<p>This client implements a combination of the Ditto algorithm from Ditto: Fair and Robust Federated Learning Through Personalization with FENDA-FL models. In this implementation, the global Ditto model consists of a feature extractor and classification head, where the feature extractor architecture is identical to that of the global and local feature extractors of the FENDA model being trained. The idea is that we want to train a local FENDA model along with the global model for each client. We simultaneously train a global model that is aggregated on the server-side and use those weights to also constrain the training of a local FENDA model. At the beginning of each server round, the feature extractor from globally aggregated model is injected into the global feature extractor of the FENDA model.</p> <p>There are two distinct modes of operation:</p> <ul> <li> <p>If <code>freeze_global_feature_extractor</code> is True. The global Ditto model feature extractor   SETS AND FREEZES weights of global FENDA feature extractor. The local components of the FENDA model are   trained and an additional drift loss is computed between the local and global feature extractors of the   FENDA model.</p> </li> <li> <p>If <code>freeze_global_feature_extractor</code> is False. The global Ditto model feature extractor INITIALIZES   weights of the FENDA model's global feature extractor, both local and global components of FENDA are   trained and a drift loss is calculated between Ditto global feature extractor and FENDA global feature   extractor.</p> </li> </ul> <p>The constraint for the FENDA model feature extractors discussed above uses a weight drift loss on its feature extraction modules.</p> <p>NOTE: Unlike FENDA, the global feature extractor of the FENDA model is NOT exchanged with the server. Rather, the global Ditto model is exchanged and injected at each round into the global feature extractor. If the global feature extractor is frozen, then only the local components of the FENDA network are trained.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>freeze_global_feature_extractor</code> <code>bool</code> <p>Determines whether we freeze the FENDA global feature extractor during training. If <code>freeze_global_feature_extractor</code> is False, both the global and the local feature extractor in the local FENDA model will be trained. Otherwise, the global feature extractor submodule is frozen. If <code>freeze_global_feature_extractor</code> is True, the Ditto loss will be calculated using the local FENDA feature extractor and the global model. Otherwise, the loss is calculated using the global FENDA feature extractor and the global model. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n    freeze_global_feature_extractor: bool = False,\n) -&gt; None:\n    \"\"\"\n    This client implements a combination of the Ditto algorithm from Ditto: Fair and Robust Federated Learning\n    Through Personalization with FENDA-FL models. In this implementation, the global Ditto model consists of a\n    feature extractor and classification head, where the feature extractor architecture is identical to that of\n    the global and local feature extractors of the FENDA model being trained. The idea is that we want to train a\n    local FENDA model along with the global model for each client. We simultaneously train a global model that is\n    aggregated on the server-side and use those weights to also constrain the training of a local\n    FENDA model. At the beginning of each server round, the feature extractor from globally aggregated model is\n    injected into the global feature extractor of the FENDA model.\n\n    There are two distinct modes of operation:\n\n    - If ``freeze_global_feature_extractor`` is True. The global Ditto model feature extractor\n      **SETS AND FREEZES** weights of global FENDA feature extractor. The local components of the FENDA model are\n      trained and an additional drift loss is computed between the local and global feature extractors of the\n      FENDA model.\n\n    - If ``freeze_global_feature_extractor`` is False. The global Ditto model feature extractor **INITIALIZES**\n      weights of the FENDA model's global feature extractor, both local and global components of FENDA are\n      trained and a drift loss is calculated between Ditto global feature extractor and FENDA global feature\n      extractor.\n\n    The constraint for the FENDA model feature extractors discussed above uses a weight drift loss on its\n    feature extraction modules.\n\n    **NOTE**: Unlike FENDA, the global feature extractor of the FENDA model is NOT exchanged with the server.\n    Rather, the global Ditto model is exchanged and injected at each round into the global feature extractor. If\n    the global feature extractor is frozen, then only the local components of the FENDA network are trained.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        freeze_global_feature_extractor (bool, optional): Determines whether we freeze the FENDA global feature\n            extractor during training. If ``freeze_global_feature_extractor`` is False, both the global and the\n            local feature extractor in the local FENDA model will be trained. Otherwise, the global feature\n            extractor submodule is frozen. If ``freeze_global_feature_extractor`` is True, the Ditto loss will be\n            calculated using the local FENDA feature extractor and the global model. Otherwise, the loss is\n            calculated using the global FENDA feature extractor and the global model. Defaults to False.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.global_model: SequentiallySplitModel\n    self.model: FendaModel\n    self.freeze_global_feature_extractor = freeze_global_feature_extractor\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient.get_model","title":"<code>get_model(config)</code>","text":"<p>User defined method that returns FENDA model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>FendaModel</code> <p>The client FENDA model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in child class.</p> Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>def get_model(self, config: Config) -&gt; FendaModel:\n    \"\"\"\n    User defined method that returns FENDA model.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (FendaModel): The client FENDA model.\n\n    Raises:\n        NotImplementedError: To be defined in child class.\n    \"\"\"\n    raise NotImplementedError(\"This function must be defined in the inheriting class to use this client\")\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient.get_global_model","title":"<code>get_global_model(config)</code>","text":"<p>User defined method that returns a Global Sequential Model that is compatible with the local FENDA model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>SequentiallySplitModel</code> <p>The global (Ditto) model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in child class.</p> Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>def get_global_model(self, config: Config) -&gt; SequentiallySplitModel:\n    \"\"\"\n    User defined method that returns a Global Sequential Model that is compatible with the local FENDA model.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (SequentiallySplitModel): The global (Ditto) model.\n\n    Raises:\n        NotImplementedError: To be defined in child class.\n    \"\"\"\n    raise NotImplementedError(\"This function must be defined in the inheriting class to use this client\")\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set initialized attribute to True. This function simply straps on the compatibility of the models.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n    Then set initialized attribute to True. This function simply straps on the compatibility of the models.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    super().setup_client(config)\n    self._check_shape_match()\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>For FendaDitto, we transfer the GLOBAL Ditto model weights to the server to be aggregated. The local FENDA model weights stay with the client. The local FENDA model has a different architecture than the GLOBAL model. So if the client is being asked for initialization parameters, we just send the GLOBAL model to sync all GLOBAL models across clients AND the local FENDA model's global feature extractor.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>GLOBAL model weights to be sent to the server for aggregation.</p> Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>def get_parameters(self, config: Config) -&gt; NDArrays:\n    \"\"\"\n    For FendaDitto, we transfer the **GLOBAL** Ditto model weights to the server to be aggregated. The local FENDA\n    model weights stay with the client. The local FENDA model has a different architecture than the **GLOBAL**\n    model. So if the client is being asked for initialization parameters, we just send the GLOBAL model to sync\n    all **GLOBAL** models across clients AND the local FENDA model's global feature extractor.\n\n    Args:\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n    Returns:\n        (NDArrays): **GLOBAL** model weights to be sent to the server for aggregation.\n    \"\"\"\n    if not self.initialized:\n        log(INFO, \"Setting up client\")\n        if not config:\n            log(\n                WARNING,\n                (\n                    \"This client has not yet been initialized and the config is empty. This may cause unexpected \"\n                    \"failures, as setting up a client typically requires several configuration parameters, \"\n                    \"including batch_size and current_server_round.\"\n                ),\n            )\n        self.setup_client(config)\n\n    assert (\n        self.global_model is not None\n        and self.parameter_exchanger is not None\n        and self.loss_for_adaptation is not None\n    )\n\n    model_weights = self.parameter_exchanger.push_parameters(self.global_model, config=config)\n    # Weights and training loss sent to server for aggregation\n    # Training loss sent because server will decide to increase or decrease the penalty weight, if adaptivity\n    # is turned on\n    return self.parameter_exchanger.pack_parameters(model_weights, self.loss_for_adaptation)\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>The parameters being passed are to be routed to the global (ditto) model and copied to the global feature extractor of the local FENDA model and saved as the initial global model tensors to be used in a penalty term in training the local model. We assume the both the global and local models are being initialized and use a <code>FullParameterExchanger()</code> to set the model weights for the global model, the global model feature extractor weights will be then copied to the global feature extractor of local FENDA model.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. This is used to help determine which parameter exchange should be used for pulling parameters. A full parameter exchanger is only used if the current federated learning round is the very first fitting round.</p> required Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n    \"\"\"\n    The parameters being passed are to be routed to the global (ditto) model and copied to the global feature\n    extractor of the local FENDA model and saved as the initial global model tensors to be used in a penalty term\n    in training the local model. We assume the both the global and local models are being initialized and use\n    a ``FullParameterExchanger()`` to set the model weights for the global model, the global model feature\n    extractor weights will be then copied to the global feature extractor of local FENDA model.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n            round or an evaluation round. This is used to help determine which parameter exchange should be used\n            for pulling parameters. A full parameter exchanger is only used if the current federated learning\n            round is the very first fitting round.\n    \"\"\"\n    # Make sure that the proper components exist.\n    assert self.global_model is not None and self.model is not None\n    assert self.parameter_exchanger is not None and isinstance(\n        self.parameter_exchanger, FullParameterExchangerWithPacking\n    )\n\n    server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n    log(INFO, f\"Penalty weight received from the server: {self.drift_penalty_weight}\")\n\n    self.parameter_exchanger.pull_parameters(server_model_state, self.global_model, config)\n    # GLOBAL MODEL feature extractor is given to local FENDA model\n    self.model.second_feature_extractor.load_state_dict(self.global_model.base_module.state_dict())\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient.set_initial_global_tensors","title":"<code>set_initial_global_tensors()</code>","text":"<p>Saves the initial GLOBAL (DITTO) MODEL weights and detaching them so that we don't compute gradients with respect to the tensors. These are used to form the Ditto local update penalty term.</p> <p>NOTE: We are only saving the base model parameters, as these will be used to constraint a feature extractor in the local FENDA model (not the full stack)</p> Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>def set_initial_global_tensors(self) -&gt; None:\n    \"\"\"\n    Saves the initial **GLOBAL** (DITTO) MODEL weights and detaching them so that we don't compute gradients with\n    respect to the tensors. These are used to form the Ditto local update penalty term.\n\n    **NOTE**: We are only saving the base model parameters, as these will be used to constraint a feature extractor\n    in the local FENDA model (not the full stack)\n    \"\"\"\n    self.drift_penalty_tensors = [\n        initial_layer_weights.detach().clone()\n        for initial_layer_weights in self.global_model.base_module.parameters()\n    ]\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient.update_before_train","title":"<code>update_before_train(current_server_round)</code>","text":"<p>Follows the same flow as the <code>DittoClient</code> parent module, but adds the ability to freeze the global feature extractor during training if <code>self.freeze_global_feature_extractor</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>current_server_round</code> <code>int</code> <p>Which round we're currently on</p> required Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>def update_before_train(self, current_server_round: int) -&gt; None:\n    \"\"\"\n    Follows the same flow as the ``DittoClient`` parent module, but adds the ability to freeze the global feature\n    extractor during training if ``self.freeze_global_feature_extractor`` is True.\n\n    Args:\n        current_server_round (int): Which round we're currently on\n    \"\"\"\n    # freeze the global feature extractor during training updates if desired.\n    if self.freeze_global_feature_extractor:\n        for param in self.model.second_feature_extractor.parameters():\n            param.requires_grad = False\n    return super().update_before_train(current_server_round)\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient.predict","title":"<code>predict(input)</code>","text":"<p>Computes the predictions for both the GLOBAL and LOCAL models and pack them into the prediction dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into both models.</p> required <p>Returns:</p> Type Description <code>tuple[TorchPredType, TorchFeatureType]</code> <p>A tuple in which the first element contains predictions indexed by name and the second element contains intermediate activations index by name. For Ditto+FENDA, we only need the predictions, so the second dictionary is simply empty.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Occurs when something other than a tensor or dict of tensors is returned by the model forward pass.</p> Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>def predict(\n    self,\n    input: TorchInputType,\n) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Computes the predictions for both the **GLOBAL** and **LOCAL** models and pack them into the prediction\n    dictionary.\n\n    Args:\n        input (TorchInputType): Inputs to be fed into both models.\n\n    Returns:\n        (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains predictions indexed\n            by name and the second element contains intermediate activations index by name. For Ditto+FENDA, we\n            only need the predictions, so the second dictionary is simply empty.\n\n    Raises:\n        ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n            forward pass.\n    \"\"\"\n    if isinstance(input, torch.Tensor):\n        global_preds, _ = self.global_model(input)\n        local_preds, _ = self.model(input)\n    elif isinstance(input, dict):\n        # If input is a dictionary, then we unpack it before computing the forward pass.\n        # Note that this assumes the keys of the input match (exactly) the keyword args\n        # of the forward method.\n        global_preds, _ = self.global_model(**input)\n        local_preds, _ = self.model(**input)\n\n    global_preds = global_preds[\"prediction\"]\n    local_preds = local_preds[\"prediction\"]\n    # Here we assume that global and local preds are simply tensors\n    # TODO: Perhaps loosen this at a later date.\n    assert isinstance(global_preds, torch.Tensor)\n    assert isinstance(local_preds, torch.Tensor)\n    return {\"global\": global_preds, \"local\": local_preds}, {}\n</code></pre>"},{"location":"api/#fl4health.clients.fenda_ditto_client.FendaDittoClient.compute_training_loss","title":"<code>compute_training_loss(preds, features, target)</code>","text":"<p>Computes training losses given predictions of the global and local models and ground truth data. For the local model, we add to the vanilla loss function by including a Ditto penalty loss. This penalty is the \\(\\ell^2\\) inner product between the initial global model feature extractor weights and the feature extractor weights of the local model. If the global feature extractor is not frozen, the penalty is computed using the global feature extractor of the local model. If it is frozen, the penalty is computed using the local feature extractor of the local model. This allows for flexibility in training scenarios where the feature extractors may differ between the global and local models. The penalty is stored in \"backward\". The loss to optimize the global model is stored in the additional losses dictionary under \u201cglobal_loss\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in the dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing the backward loss and additional losses indexed by name. Additional losses include each loss component and the global model loss tensor.</p> Source code in <code>fl4health/clients/fenda_ditto_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training losses given predictions of the global and local models and ground truth data.\n    For the local model, we add to the vanilla loss function by including a Ditto penalty loss. This penalty\n    is the \\\\(\\\\ell^2\\\\) inner product between the initial global model feature extractor weights and the\n    feature extractor weights of the local model. If the global feature extractor is not frozen, the penalty is\n    computed using the global feature extractor of the local model. If it is frozen, the penalty is computed using\n    the local feature extractor of the local model. This allows for flexibility in training scenarios where the\n    feature extractors may differ between the global and local models. The penalty is stored in \"backward\". The\n    loss to optimize the global model is stored in the additional losses dictionary under \u201cglobal_loss\u201d.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included\n            in the dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing the backward loss and additional losses\n            indexed by name. Additional losses include each loss component and the global model loss tensor.\n    \"\"\"\n    # Check that both models are in training mode\n    assert self.global_model.training and self.model.training\n\n    loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n    if additional_losses is None:\n        additional_losses = {}\n\n    # adding the vanilla loss to the additional losses to be used by update_after_train for potential adaptation\n    additional_losses[\"loss_for_adaptation\"] = loss.clone()\n\n    # Compute the appropriate Ditto drift loss\n    if self.freeze_global_feature_extractor:\n        penalty_loss = self.penalty_loss_function(\n            self.model.first_feature_extractor, self.drift_penalty_tensors, self.drift_penalty_weight\n        )\n    else:\n        penalty_loss = self.penalty_loss_function(\n            self.model.second_feature_extractor, self.drift_penalty_tensors, self.drift_penalty_weight\n        )\n    additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n\n    return TrainingLosses(backward=loss + penalty_loss, additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.flash_client","title":"<code>flash_client</code>","text":""},{"location":"api/#fl4health.clients.flash_client.FlashClient","title":"<code>FlashClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/flash_client.py</code> <pre><code>class FlashClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client is used to perform client-side training associated with the Flash method described in\n        https://proceedings.mlr.press/v202/panchal23a/panchal23a.pdf.\n\n        Flash is designed to handle statistical heterogeneity and concept drift in federated learning through\n        client-side early stopping and server-side drift-aware adaptive optimization.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        # gamma: Threshold for early stopping based on the change in validation loss. Set through the config\n        self.gamma: float | None = None\n\n    def process_config(self, config: Config) -&gt; tuple[int | None, int | None, int, bool, bool]:\n        \"\"\"\n        Performs the necessary processing of the config from the server. FLASH is not defined for step-wise training.\n        So this method straps on a check to ensure that we aren't trying to do step-wise training.\n\n        Args:\n            config (Config): The config object from the server.\n\n        Raises:\n            ValueError: Throws if the user is attempting to train by steps instead of epochs for this method.\n\n        Returns:\n            (tuple[int | None, int | None, int, bool, bool]): Returns the ``local_epochs``, ``local_steps``,\n                ``current_server_round``, ``evaluate_after_fit`` and ``pack_losses_with_val_metrics``. Ensures only\n                one of ``local_epochs`` and ``local_steps`` is defined in the config and sets the one that is not to\n                None.\n        \"\"\"\n        local_epochs, local_steps, current_server_round, evaluate_after_fit, pack_losses_with_val_metrics = (\n            super().process_config(config)\n        )\n        if local_steps is not None:\n            raise ValueError(\n                \"Training by steps is not applicable for FLASH clients. Please define 'local_epochs' in your\"\n                \" config instead\"\n            )\n        return local_epochs, local_steps, current_server_round, evaluate_after_fit, pack_losses_with_val_metrics\n\n    def train_by_epochs(\n        self, epochs: int, current_round: int | None = None\n    ) -&gt; tuple[dict[str, float], dict[str, Scalar]]:\n        \"\"\"\n        Implements a custom train_by_epochs for this client to allow for the FLASH adaptations on the client side.\n        If gamma is None, then this function works exactly as the ``BasicClient``. Otherwise, we perform epochs and\n        possibly stop early using gamma as a threshold.\n\n        Args:\n            epochs (int): Number of epochs to train\n            current_round (int | None, optional): Current server round being performed. Defaults to None.\n\n        Returns:\n            (tuple[dict[str, float], dict[str, Scalar]]): The loss and metrics dictionary from the local training.\n                Loss is a dictionary of one or more losses that represent the different components of the loss.\n        \"\"\"\n        if self.gamma is None:\n            return super().train_by_epochs(epochs, current_round)\n\n        self.model.train()\n        local_step = 0\n        previous_loss = float(\"inf\")\n        report_data: dict = {\"round\": current_round}\n        for local_epoch in range(epochs):\n            self.train_metric_manager.clear()\n            self.train_loss_meter.clear()\n            self._log_header_str(current_round, local_epoch)\n            report_data.update({\"fit_epoch\": local_epoch})\n            for input, target in self.train_loader:\n                if check_if_batch_is_empty_and_verify_input(input):\n                    log(INFO, \"Empty batch generated by data loader. Skipping step.\")\n                    continue\n\n                input = move_data_to_device(input, self.device)\n                target = move_data_to_device(target, self.device)\n                losses, preds = self.train_step(input, target)\n                self.train_loss_meter.update(losses)\n                self.train_metric_manager.update(preds, target)\n                self.update_after_step(local_step, current_round)\n                report_data.update({\"fit_losses\": losses.as_dict(), \"fit_step\": self.total_steps})\n                report_data.update(self.get_client_specific_reports())\n                self.reports_manager.report(report_data, current_round, local_epoch, self.total_steps)\n                self.total_steps += 1\n                local_step += 1\n\n            metrics = self.train_metric_manager.compute()\n            loss_dict = self.train_loss_meter.compute().as_dict()\n            current_loss, _ = self.validate()\n\n            self._log_results(\n                loss_dict,\n                metrics,\n                current_round=current_round,\n                current_epoch=local_epoch,\n            )\n\n            if self.gamma is not None and previous_loss - current_loss &lt; self.gamma / (local_epoch + 1):\n                log(\n                    INFO,\n                    f\"Early stopping at epoch {local_epoch} with loss change {abs(previous_loss - current_loss)}\\\n                        and gamma {self.gamma}\",\n                )\n                break\n\n            previous_loss = current_loss\n\n        return loss_dict, metrics\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Follows the same flow as ``BasicClient`` for setting up the client. This function simply performs an additional\n        step to process whether the gamma parameter is in the configuration.\n\n        Args:\n            config (Config): The config object from the server.\n        \"\"\"\n        super().setup_client(config)\n        if \"gamma\" in config:\n            self.gamma = narrow_dict_type(config, \"gamma\", float)\n        else:\n            log(INFO, \"Gamma not present in config. Early stopping is disabled.\")\n</code></pre>"},{"location":"api/#fl4health.clients.flash_client.FlashClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>This client is used to perform client-side training associated with the Flash method described in https://proceedings.mlr.press/v202/panchal23a/panchal23a.pdf.</p> <p>Flash is designed to handle statistical heterogeneity and concept drift in federated learning through client-side early stopping and server-side drift-aware adaptive optimization.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/flash_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    This client is used to perform client-side training associated with the Flash method described in\n    https://proceedings.mlr.press/v202/panchal23a/panchal23a.pdf.\n\n    Flash is designed to handle statistical heterogeneity and concept drift in federated learning through\n    client-side early stopping and server-side drift-aware adaptive optimization.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    # gamma: Threshold for early stopping based on the change in validation loss. Set through the config\n    self.gamma: float | None = None\n</code></pre>"},{"location":"api/#fl4health.clients.flash_client.FlashClient.process_config","title":"<code>process_config(config)</code>","text":"<p>Performs the necessary processing of the config from the server. FLASH is not defined for step-wise training. So this method straps on a check to ensure that we aren't trying to do step-wise training.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config object from the server.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Throws if the user is attempting to train by steps instead of epochs for this method.</p> <p>Returns:</p> Type Description <code>tuple[int | None, int | None, int, bool, bool]</code> <p>Returns the <code>local_epochs</code>, <code>local_steps</code>, <code>current_server_round</code>, <code>evaluate_after_fit</code> and <code>pack_losses_with_val_metrics</code>. Ensures only one of <code>local_epochs</code> and <code>local_steps</code> is defined in the config and sets the one that is not to None.</p> Source code in <code>fl4health/clients/flash_client.py</code> <pre><code>def process_config(self, config: Config) -&gt; tuple[int | None, int | None, int, bool, bool]:\n    \"\"\"\n    Performs the necessary processing of the config from the server. FLASH is not defined for step-wise training.\n    So this method straps on a check to ensure that we aren't trying to do step-wise training.\n\n    Args:\n        config (Config): The config object from the server.\n\n    Raises:\n        ValueError: Throws if the user is attempting to train by steps instead of epochs for this method.\n\n    Returns:\n        (tuple[int | None, int | None, int, bool, bool]): Returns the ``local_epochs``, ``local_steps``,\n            ``current_server_round``, ``evaluate_after_fit`` and ``pack_losses_with_val_metrics``. Ensures only\n            one of ``local_epochs`` and ``local_steps`` is defined in the config and sets the one that is not to\n            None.\n    \"\"\"\n    local_epochs, local_steps, current_server_round, evaluate_after_fit, pack_losses_with_val_metrics = (\n        super().process_config(config)\n    )\n    if local_steps is not None:\n        raise ValueError(\n            \"Training by steps is not applicable for FLASH clients. Please define 'local_epochs' in your\"\n            \" config instead\"\n        )\n    return local_epochs, local_steps, current_server_round, evaluate_after_fit, pack_losses_with_val_metrics\n</code></pre>"},{"location":"api/#fl4health.clients.flash_client.FlashClient.train_by_epochs","title":"<code>train_by_epochs(epochs, current_round=None)</code>","text":"<p>Implements a custom train_by_epochs for this client to allow for the FLASH adaptations on the client side. If gamma is None, then this function works exactly as the <code>BasicClient</code>. Otherwise, we perform epochs and possibly stop early using gamma as a threshold.</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>int</code> <p>Number of epochs to train</p> required <code>current_round</code> <code>int | None</code> <p>Current server round being performed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict[str, float], dict[str, Scalar]]</code> <p>The loss and metrics dictionary from the local training. Loss is a dictionary of one or more losses that represent the different components of the loss.</p> Source code in <code>fl4health/clients/flash_client.py</code> <pre><code>def train_by_epochs(\n    self, epochs: int, current_round: int | None = None\n) -&gt; tuple[dict[str, float], dict[str, Scalar]]:\n    \"\"\"\n    Implements a custom train_by_epochs for this client to allow for the FLASH adaptations on the client side.\n    If gamma is None, then this function works exactly as the ``BasicClient``. Otherwise, we perform epochs and\n    possibly stop early using gamma as a threshold.\n\n    Args:\n        epochs (int): Number of epochs to train\n        current_round (int | None, optional): Current server round being performed. Defaults to None.\n\n    Returns:\n        (tuple[dict[str, float], dict[str, Scalar]]): The loss and metrics dictionary from the local training.\n            Loss is a dictionary of one or more losses that represent the different components of the loss.\n    \"\"\"\n    if self.gamma is None:\n        return super().train_by_epochs(epochs, current_round)\n\n    self.model.train()\n    local_step = 0\n    previous_loss = float(\"inf\")\n    report_data: dict = {\"round\": current_round}\n    for local_epoch in range(epochs):\n        self.train_metric_manager.clear()\n        self.train_loss_meter.clear()\n        self._log_header_str(current_round, local_epoch)\n        report_data.update({\"fit_epoch\": local_epoch})\n        for input, target in self.train_loader:\n            if check_if_batch_is_empty_and_verify_input(input):\n                log(INFO, \"Empty batch generated by data loader. Skipping step.\")\n                continue\n\n            input = move_data_to_device(input, self.device)\n            target = move_data_to_device(target, self.device)\n            losses, preds = self.train_step(input, target)\n            self.train_loss_meter.update(losses)\n            self.train_metric_manager.update(preds, target)\n            self.update_after_step(local_step, current_round)\n            report_data.update({\"fit_losses\": losses.as_dict(), \"fit_step\": self.total_steps})\n            report_data.update(self.get_client_specific_reports())\n            self.reports_manager.report(report_data, current_round, local_epoch, self.total_steps)\n            self.total_steps += 1\n            local_step += 1\n\n        metrics = self.train_metric_manager.compute()\n        loss_dict = self.train_loss_meter.compute().as_dict()\n        current_loss, _ = self.validate()\n\n        self._log_results(\n            loss_dict,\n            metrics,\n            current_round=current_round,\n            current_epoch=local_epoch,\n        )\n\n        if self.gamma is not None and previous_loss - current_loss &lt; self.gamma / (local_epoch + 1):\n            log(\n                INFO,\n                f\"Early stopping at epoch {local_epoch} with loss change {abs(previous_loss - current_loss)}\\\n                    and gamma {self.gamma}\",\n            )\n            break\n\n        previous_loss = current_loss\n\n    return loss_dict, metrics\n</code></pre>"},{"location":"api/#fl4health.clients.flash_client.FlashClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Follows the same flow as <code>BasicClient</code> for setting up the client. This function simply performs an additional step to process whether the gamma parameter is in the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config object from the server.</p> required Source code in <code>fl4health/clients/flash_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Follows the same flow as ``BasicClient`` for setting up the client. This function simply performs an additional\n    step to process whether the gamma parameter is in the configuration.\n\n    Args:\n        config (Config): The config object from the server.\n    \"\"\"\n    super().setup_client(config)\n    if \"gamma\" in config:\n        self.gamma = narrow_dict_type(config, \"gamma\", float)\n    else:\n        log(INFO, \"Gamma not present in config. Early stopping is disabled.\")\n</code></pre>"},{"location":"api/#fl4health.clients.flexible","title":"<code>flexible</code>","text":""},{"location":"api/#fl4health.clients.flexible.base","title":"<code>base</code>","text":""},{"location":"api/#fl4health.clients.flexible.base.FlexibleClient","title":"<code>FlexibleClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/flexible/base.py</code> <pre><code>class FlexibleClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Flexible FL Client with functionality to train, evaluate, log, report and checkpoint.\n\n        ``FlexibleClient`` is similar to ``BasicClient`` but provides added flexibility through the\n        ability to inject models and optimizers in the methods responsible for making predictions\n        and performing both train and validation steps.\n\n        This added flexibility allows for ``FlexibleClient`` to be automatically adapted with our\n        personalized methods: ``~fl4health.mixins.personalized``.\n\n        As with ``BasicClient``, users are responsible for implementing methods:\n\n            - ``get_model``\n            - ``get_optimizer``\n            - ``get_data_loaders``,\n            - ``get_criterion``\n\n        However, unlike ``BasicClient``, users looking to specialize logic for making predictions,\n        and performing train and validation steps, should instead override:\n\n            - ``predict_with_model``\n            - ``_train_step_with_model_and_optimizer`` (and its delegated helpers)\n            - ``_val_step_with_model``\n\n        Other methods can be overridden to achieve custom functionality.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path,\n            metrics,\n            device,\n            loss_meter_type,\n            checkpoint_and_state_module,\n            reporters,\n            progress_bar,\n            client_name,\n        )\n\n    def __init_subclass__(cls, **kwargs: Any) -&gt; None:\n        \"\"\"Perform some validations on subclasses of FlexibleClient.\"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # check that specific methods are not overridden, otherwise throw warning\n        methods_should_not_be_overridden = [\n            (\n                \"predict\",\n                (\n                    f\"`{cls.__name__}` overrides `predict()`, but this method should no longer be overridden. \"\n                    \"Please use `predict_with_model()` instead.\"\n                ),\n            ),\n            (\n                \"val_step\",\n                (\n                    f\"`{cls.__name__}` overrides `val_step()`, but this method should no longer be overridden. \"\n                    \"Please use `_val_step_with_model()` instead.\"\n                ),\n            ),\n            (\n                \"train_step\",\n                (\n                    f\"`{cls.__name__}` overrides `train_step()`, but this method should no longer be overridden. \"\n                    \"Please use `_train_step_with_model_and_optimizer()` and its helper methods instead \"\n                    \"for proper customization.\"\n                ),\n            ),\n        ]\n\n        for method_name, msg in methods_should_not_be_overridden:\n            if method_name in cls.__dict__:  # method was overridden by subclass\n                log(WARN, msg)\n                warnings.warn(msg, RuntimeWarning, stacklevel=2)\n\n    def _compute_preds_and_losses(\n        self,\n        model: nn.Module,\n        optimizer: Optimizer,\n        input: TorchInputType,\n        target: TorchTargetType,\n    ) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Helper method within the train step for computing preds and losses.\n\n        **NOTE**: Subclasses should implement this helper method if there is a need\n        to specialize this part of the overall train step.\n\n        Args:\n            model (nn.Module): the model used to make predictions\n            optimizer (Optimizer): the associated optimizer\n            input (TorchInputType): The input to be fed into the model.\n            target (TorchTargetType): The target corresponding to the input.\n\n        Returns:\n            (tuple[TrainingLosses, TorchPredType]): The losses object from the train step along with a dictionary of\n                any predictions produced by the model prior to the application of the backwards phase\n        \"\"\"\n        # Clear gradients from optimizer if they exist\n        optimizer.zero_grad()\n\n        # Call user defined methods to get predictions and compute loss\n        preds, features = self.predict_with_model(model, input)\n        target = self.transform_target(target)\n        losses = self.compute_training_loss(preds, features, target)\n\n        return losses, preds\n\n    def _apply_backwards_on_losses_and_take_step(\n        self, model: nn.Module, optimizer: Optimizer, losses: TrainingLosses\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Helper method within the train step for applying backwards on losses and taking step with optimizer.\n\n        **NOTE**: Subclasses should implement this helper method if there is a need\n        to specialize this part of the overall train step.\n\n        Args:\n            model (nn.Module): the model used for making predictions. Passed here in case subclasses need it.\n            optimizer (Optimizer): the optimizer with which we take the step\n            losses (TrainingLosses): the losses to apply backwards on\n\n        Returns:\n            (TrainingLosses): The losses object post backwards application\n        \"\"\"\n        # Compute backward pass and update parameters with optimizer\n        losses.backward[\"backward\"].backward()\n        self._transform_gradients_with_model(model, losses)\n        optimizer.step()\n\n        return losses\n\n    def _train_step_with_model_and_optimizer(\n        self,\n        model: torch.nn.Module,\n        optimizer: Optimizer,\n        input: TorchInputType,\n        target: TorchTargetType,\n    ) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Helper train step method that allows for injection of model and optimizer.\n\n        **NOTE**: Subclasses should implement this method if there is a need to specialize\n        the train_step logic.\n\n        Args:\n            model (nn.Module): the model used for making predictions. Passed here in case subclasses need it.\n            optimizer (Optimizer): the optimizer with which we take the step\n            input (TorchInputType): The input to be fed into the model.\n            target (TorchTargetType): The target corresponding to the input.\n\n        Returns:\n            (tuple[TrainingLosses, TorchPredType]): The losses object from the train step along with\n                a dictionary of any predictions produced by the model.\n        \"\"\"\n        losses, preds = self._compute_preds_and_losses(model, optimizer, input, target)\n        losses = self._apply_backwards_on_losses_and_take_step(model, optimizer, losses)\n\n        return losses, preds\n\n    def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Given a single batch of input and target data, generate predictions, compute loss, update parameters and\n        optionally update metrics if they exist. (i.e. backprop on a single batch of data).\n        Assumes ``self.model`` is in train mode already.\n\n        Args:\n            input (TorchInputType): The input to be fed into the model.\n            target (TorchTargetType): The target corresponding to the input.\n\n        Returns:\n            (tuple[TrainingLosses, TorchPredType]): The losses object from the train step along with\n                a dictionary of any predictions produced by the model.\n        \"\"\"\n        return self._train_step_with_model_and_optimizer(self.model, self.optimizers[\"global\"], input, target)\n\n    def _val_step_with_model(\n        self, model: nn.Module, input: TorchInputType, target: TorchTargetType\n    ) -&gt; tuple[EvaluationLosses, TorchPredType]:\n        \"\"\"\n        Helper method for val_step that allows for injection of model.\n\n        **NOTE**: Subclasses should implement this method if there is a need to\n        specialize the val_step logic.\n\n        Args:\n            model (nn.Module): the model used for making predictions. Passed here in case subclasses need it.\n            input (TorchInputType): The input to be fed into the model.\n            target (TorchTargetType): The target corresponding to the input.\n\n        Returns:\n            (tuple[EvaluationLosses, TorchPredType]): The losses object from the val step along with a dictionary of\n                the predictions produced by the model.\n        \"\"\"\n        # Get preds and compute loss\n        with torch.no_grad():\n            preds, features = self.predict_with_model(model, input)\n            target = self.transform_target(target)\n            losses = self.compute_evaluation_loss(preds, features, target)\n\n        return losses, preds\n\n    def val_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[EvaluationLosses, TorchPredType]:\n        \"\"\"\n        Given input and target, compute loss, update loss and metrics. Assumes ``self.model`` is in eval mode already.\n\n        Args:\n            input (TorchInputType): The input to be fed into the model.\n            target (TorchTargetType): The target corresponding to the input.\n\n        Returns:\n            (tuple[EvaluationLosses, TorchPredType]): The losses object from the val step along with a dictionary of\n                the predictions produced by the model.\n        \"\"\"\n        return self._val_step_with_model(self.model, input, target)\n\n    def predict_with_model(\n        self, model: torch.nn.Module, input: TorchInputType\n    ) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Helper predict method that allows for injection of model.\n\n        **NOTE**: Subclasses should implement this method if there is need to specialize\n        the predict logic of the client.\n\n        Args:\n            model (torch.nn.Module): the model with which to make predictions\n            input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n                it is assumed that the keys of input match the names of the keyword arguments of\n                ``self.model.forward().``\n\n        Returns:\n            (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n                predictions indexed by name and the second element contains intermediate activations indexed by name.\n                By passing features, we can compute losses such as the contrastive loss in MOON. All predictions\n                included in dictionary will by default be used to compute metrics separately.\n\n        Raises:\n            TypeError: Occurs when something other than a tensor or dict of tensors is passed in to the model's\n                forward method.\n            ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n                forward.\n        \"\"\"\n        if isinstance(input, torch.Tensor):\n            output = model(input)\n        elif isinstance(input, dict):\n            # If input is a dictionary, then we unpack it before computing the forward pass.\n            # Note that this assumes the keys of the input match (exactly) the keyword args\n            # of self.model.forward().\n            output = model(**input)\n        else:\n            raise TypeError(\"'input' must be of type torch.Tensor or dict[str, torch.Tensor].\")\n\n        if isinstance(output, dict):\n            return output, {}\n        if isinstance(output, torch.Tensor):\n            return {\"prediction\": output}, {}\n        if isinstance(output, tuple):\n            if len(output) != EXPECTED_OUTPUT_TUPLE_SIZE:\n                raise ValueError(f\"Output tuple should have length 2 but has length {len(output)}\")\n            preds, features = output\n            return preds, features\n        raise ValueError(\"Model forward did not return a tensor, dictionary of tensors, or tuple of tensors\")\n\n    def _transform_gradients_with_model(self, model: torch.nn.Module, losses: TrainingLosses) -&gt; None:\n        \"\"\"\n        Helper transform gradients method that allows for injection of model.\n\n        **NOTE**: Subclasses should implement this helper should there be a need to specialize the logic\n        for transforming gradients.\n\n        Args:\n            model (torch.nn.Module): the model used to generate predictions to compute losses\n            losses (TrainingLosses): The losses object from the train step\n        \"\"\"\n        pass\n\n    def transform_gradients(self, losses: TrainingLosses) -&gt; None:\n        \"\"\"\n        Hook function for model training only called after backwards pass but before optimizer step. Useful for\n        transforming the gradients (such as with gradient clipping) before they are applied to the model weights.\n\n        Args:\n            losses (TrainingLosses): The losses object from the train step\n        \"\"\"\n        return self._transform_gradients_with_model(self.model, losses)\n</code></pre> <code></code> <code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code> \u00b6 <p>Flexible FL Client with functionality to train, evaluate, log, report and checkpoint.</p> <p><code>FlexibleClient</code> is similar to <code>BasicClient</code> but provides added flexibility through the ability to inject models and optimizers in the methods responsible for making predictions and performing both train and validation steps.</p> <p>This added flexibility allows for <code>FlexibleClient</code> to be automatically adapted with our personalized methods: <code>~fl4health.mixins.personalized</code>.</p> <p>As with <code>BasicClient</code>, users are responsible for implementing methods:</p> <pre><code>- ``get_model``\n- ``get_optimizer``\n- ``get_data_loaders``,\n- ``get_criterion``\n</code></pre> <p>However, unlike <code>BasicClient</code>, users looking to specialize logic for making predictions, and performing train and validation steps, should instead override:</p> <pre><code>- ``predict_with_model``\n- ``_train_step_with_model_and_optimizer`` (and its delegated helpers)\n- ``_val_step_with_model``\n</code></pre> <p>Other methods can be overridden to achieve custom functionality.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/flexible/base.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Flexible FL Client with functionality to train, evaluate, log, report and checkpoint.\n\n    ``FlexibleClient`` is similar to ``BasicClient`` but provides added flexibility through the\n    ability to inject models and optimizers in the methods responsible for making predictions\n    and performing both train and validation steps.\n\n    This added flexibility allows for ``FlexibleClient`` to be automatically adapted with our\n    personalized methods: ``~fl4health.mixins.personalized``.\n\n    As with ``BasicClient``, users are responsible for implementing methods:\n\n        - ``get_model``\n        - ``get_optimizer``\n        - ``get_data_loaders``,\n        - ``get_criterion``\n\n    However, unlike ``BasicClient``, users looking to specialize logic for making predictions,\n    and performing train and validation steps, should instead override:\n\n        - ``predict_with_model``\n        - ``_train_step_with_model_and_optimizer`` (and its delegated helpers)\n        - ``_val_step_with_model``\n\n    Other methods can be overridden to achieve custom functionality.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path,\n        metrics,\n        device,\n        loss_meter_type,\n        checkpoint_and_state_module,\n        reporters,\n        progress_bar,\n        client_name,\n    )\n</code></pre> <code></code> <code>__init_subclass__(**kwargs)</code> \u00b6 <p>Perform some validations on subclasses of FlexibleClient.</p> Source code in <code>fl4health/clients/flexible/base.py</code> <pre><code>def __init_subclass__(cls, **kwargs: Any) -&gt; None:\n    \"\"\"Perform some validations on subclasses of FlexibleClient.\"\"\"\n    super().__init_subclass__(**kwargs)\n\n    # check that specific methods are not overridden, otherwise throw warning\n    methods_should_not_be_overridden = [\n        (\n            \"predict\",\n            (\n                f\"`{cls.__name__}` overrides `predict()`, but this method should no longer be overridden. \"\n                \"Please use `predict_with_model()` instead.\"\n            ),\n        ),\n        (\n            \"val_step\",\n            (\n                f\"`{cls.__name__}` overrides `val_step()`, but this method should no longer be overridden. \"\n                \"Please use `_val_step_with_model()` instead.\"\n            ),\n        ),\n        (\n            \"train_step\",\n            (\n                f\"`{cls.__name__}` overrides `train_step()`, but this method should no longer be overridden. \"\n                \"Please use `_train_step_with_model_and_optimizer()` and its helper methods instead \"\n                \"for proper customization.\"\n            ),\n        ),\n    ]\n\n    for method_name, msg in methods_should_not_be_overridden:\n        if method_name in cls.__dict__:  # method was overridden by subclass\n            log(WARN, msg)\n            warnings.warn(msg, RuntimeWarning, stacklevel=2)\n</code></pre> <code></code> <code>train_step(input, target)</code> \u00b6 <p>Given a single batch of input and target data, generate predictions, compute loss, update parameters and optionally update metrics if they exist. (i.e. backprop on a single batch of data). Assumes <code>self.model</code> is in train mode already.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>The input to be fed into the model.</p> required <code>target</code> <code>TorchTargetType</code> <p>The target corresponding to the input.</p> required <p>Returns:</p> Type Description <code>tuple[TrainingLosses, TorchPredType]</code> <p>The losses object from the train step along with a dictionary of any predictions produced by the model.</p> Source code in <code>fl4health/clients/flexible/base.py</code> <pre><code>def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n    \"\"\"\n    Given a single batch of input and target data, generate predictions, compute loss, update parameters and\n    optionally update metrics if they exist. (i.e. backprop on a single batch of data).\n    Assumes ``self.model`` is in train mode already.\n\n    Args:\n        input (TorchInputType): The input to be fed into the model.\n        target (TorchTargetType): The target corresponding to the input.\n\n    Returns:\n        (tuple[TrainingLosses, TorchPredType]): The losses object from the train step along with\n            a dictionary of any predictions produced by the model.\n    \"\"\"\n    return self._train_step_with_model_and_optimizer(self.model, self.optimizers[\"global\"], input, target)\n</code></pre> <code></code> <code>val_step(input, target)</code> \u00b6 <p>Given input and target, compute loss, update loss and metrics. Assumes <code>self.model</code> is in eval mode already.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>The input to be fed into the model.</p> required <code>target</code> <code>TorchTargetType</code> <p>The target corresponding to the input.</p> required <p>Returns:</p> Type Description <code>tuple[EvaluationLosses, TorchPredType]</code> <p>The losses object from the val step along with a dictionary of the predictions produced by the model.</p> Source code in <code>fl4health/clients/flexible/base.py</code> <pre><code>def val_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[EvaluationLosses, TorchPredType]:\n    \"\"\"\n    Given input and target, compute loss, update loss and metrics. Assumes ``self.model`` is in eval mode already.\n\n    Args:\n        input (TorchInputType): The input to be fed into the model.\n        target (TorchTargetType): The target corresponding to the input.\n\n    Returns:\n        (tuple[EvaluationLosses, TorchPredType]): The losses object from the val step along with a dictionary of\n            the predictions produced by the model.\n    \"\"\"\n    return self._val_step_with_model(self.model, input, target)\n</code></pre> <code></code> <code>predict_with_model(model, input)</code> \u00b6 <p>Helper predict method that allows for injection of model.</p> <p>NOTE: Subclasses should implement this method if there is need to specialize the predict logic of the client.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>the model with which to make predictions</p> required <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into the model. If input is of type <code>dict[str, torch.Tensor]</code>, it is assumed that the keys of input match the names of the keyword arguments of <code>self.model.forward().</code></p> required <p>Returns:</p> Type Description <code>tuple[TorchPredType, TorchFeatureType]</code> <p>A tuple in which the first element contains a dictionary of predictions indexed by name and the second element contains intermediate activations indexed by name. By passing features, we can compute losses such as the contrastive loss in MOON. All predictions included in dictionary will by default be used to compute metrics separately.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Occurs when something other than a tensor or dict of tensors is passed in to the model's forward method.</p> <code>ValueError</code> <p>Occurs when something other than a tensor or dict of tensors is returned by the model forward.</p> Source code in <code>fl4health/clients/flexible/base.py</code> <pre><code>def predict_with_model(\n    self, model: torch.nn.Module, input: TorchInputType\n) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Helper predict method that allows for injection of model.\n\n    **NOTE**: Subclasses should implement this method if there is need to specialize\n    the predict logic of the client.\n\n    Args:\n        model (torch.nn.Module): the model with which to make predictions\n        input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n            it is assumed that the keys of input match the names of the keyword arguments of\n            ``self.model.forward().``\n\n    Returns:\n        (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n            predictions indexed by name and the second element contains intermediate activations indexed by name.\n            By passing features, we can compute losses such as the contrastive loss in MOON. All predictions\n            included in dictionary will by default be used to compute metrics separately.\n\n    Raises:\n        TypeError: Occurs when something other than a tensor or dict of tensors is passed in to the model's\n            forward method.\n        ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n            forward.\n    \"\"\"\n    if isinstance(input, torch.Tensor):\n        output = model(input)\n    elif isinstance(input, dict):\n        # If input is a dictionary, then we unpack it before computing the forward pass.\n        # Note that this assumes the keys of the input match (exactly) the keyword args\n        # of self.model.forward().\n        output = model(**input)\n    else:\n        raise TypeError(\"'input' must be of type torch.Tensor or dict[str, torch.Tensor].\")\n\n    if isinstance(output, dict):\n        return output, {}\n    if isinstance(output, torch.Tensor):\n        return {\"prediction\": output}, {}\n    if isinstance(output, tuple):\n        if len(output) != EXPECTED_OUTPUT_TUPLE_SIZE:\n            raise ValueError(f\"Output tuple should have length 2 but has length {len(output)}\")\n        preds, features = output\n        return preds, features\n    raise ValueError(\"Model forward did not return a tensor, dictionary of tensors, or tuple of tensors\")\n</code></pre> <code></code> <code>transform_gradients(losses)</code> \u00b6 <p>Hook function for model training only called after backwards pass but before optimizer step. Useful for transforming the gradients (such as with gradient clipping) before they are applied to the model weights.</p> <p>Parameters:</p> Name Type Description Default <code>losses</code> <code>TrainingLosses</code> <p>The losses object from the train step</p> required Source code in <code>fl4health/clients/flexible/base.py</code> <pre><code>def transform_gradients(self, losses: TrainingLosses) -&gt; None:\n    \"\"\"\n    Hook function for model training only called after backwards pass but before optimizer step. Useful for\n    transforming the gradients (such as with gradient clipping) before they are applied to the model weights.\n\n    Args:\n        losses (TrainingLosses): The losses object from the train step\n    \"\"\"\n    return self._transform_gradients_with_model(self.model, losses)\n</code></pre>"},{"location":"api/#fl4health.clients.flexible.nnunet","title":"<code>nnunet</code>","text":""},{"location":"api/#fl4health.clients.flexible.nnunet.FlexibleNnunetClient","title":"<code>FlexibleNnunetClient</code>","text":"<p>               Bases: <code>FlexibleClient</code></p> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>class FlexibleNnunetClient(FlexibleClient):\n    def __init__(\n        self,\n        device: torch.device,\n        dataset_id: int,\n        fold: int | str,\n        data_identifier: str | None = None,\n        plans_identifier: str | None = None,\n        compile: bool = True,\n        always_preprocess: bool = False,\n        max_grad_norm: float = 12,\n        n_dataload_processes: int | None = None,\n        verbose: bool = True,\n        metrics: Sequence[Metric] | None = None,\n        progress_bar: bool = False,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        client_name: str | None = None,\n        nnunet_trainer_class: type[nnUNetTrainer] = nnUNetTrainer,\n        nnunet_trainer_class_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        A client for training nnunet models. Requires the nnunet environment variables to be set. Also requires the\n        following additional keys in the config sent from the server as follows.\n\n        - \"nnunet_plans\": (serialized dict)\n        - \"nnunet_config\": (str)\n\n        Args:\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\" or \"mps\"\n            dataset_id (int): The nnunet dataset id for the local client dataset to use for training and validation.\n            fold (int | str): Which fold of the local client dataset to use for validation. nnunet defaults to\n                5 folds (0 to 4). Can also be set to \"all\" to use all the data for both training and validation.\n            data_identifier (str | None, optional): The nnunet data identifier prefix to use. The final data\n                identifier will be ``{data_identifier}_config`` where \"config\" is the nnunet config (e.g. 2d,\n                ``3d_fullres``, etc.). If preprocessed data already exists can be used to specify which preprocessed\n                data to use. By default, the ``plans_identifier`` is used as the ``data_identifier``.\n            plans_identifier (str | None, optional): Specify what to save the client's local copy of the plans file\n                as. The client makes a local modified copy of the global source plans file sent by the server. If left\n                as default None, the plans identifier will be set as \"FL-plansname-000local\" where 000 is the\n                ``dataset_id`` and plansname is the \"plans_name\" value of the source plans file. The original plans\n                will be saved under the ``source_plans_name`` key in the modified plans file.\n            compile (bool, optional): If set to True, the client will Just-In-Time (JIT) compile the nnUNet model and\n                perform optimizations at the start of training. This process significantly reduces the runtime for\n                nnUNet models, especially for larger models or long-running jobs. However, it introduces some overhead\n                time and computation during the initial step. It is recommended to keep this option enabled. The\n                default value is True.\n            always_preprocess (bool, optional): If True, will preprocess the local client dataset even if the\n                preprocessed data already seems to exist. The existence of the preprocessed data is determined by\n                matching the provided ``data_identifier`` with that of the preprocessed data already  on the client.\n                Defaults to False.\n            max_grad_norm (float, optional): The maximum gradient norm to use for gradient clipping. Defaults to 12\n                which is the nnunetv2 2.5.1 default.\n            n_dataload_processes (int | None, optional): The number of processes to spawn for each nnunet\n                dataloader. If left as None we use the nnunetv2 version 2.5.1 defaults for each config\n            verbose (bool, optional): If True the client will log some extra INFO logs. Defaults to False unless\n                the log level is DEBUG or lower.\n            metrics (Sequence[Metric], optional): Metrics to be computed based on the labels and predictions of the\n                client model. Defaults to None.\n            progress_bar (bool, optional): Whether or not to print a progress bar to stdout for training. Defaults\n                to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over each\n                batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the client should\n                send data to.\n            nnunet_trainer_class (type[nnUNetTrainer]): A ``nnUNetTrainer`` constructor. Useful for passing custom\n                ``nnUNetTrainer``. Defaults to the standard nnUNetTrainer class. Must match the\n                ``nnunet_trainer_class`` passed to the ``NnunetServer``.\n            nnunet_trainer_class_kwargs (dict[str, Any]): Additional kwargs to pass to ``nnunet_trainer_class``.\n                Defaults to empty dictionary.\n        \"\"\"\n        metrics = metrics if metrics else []\n\n        # Parent method sets up several class attributes\n        super().__init__(\n            data_path=Path(\"dummy/path\"),  # data_path not used by NnunetClient\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n\n        # Some nnunet client specific attributes\n        self.dataset_id: int = dataset_id\n        self.dataset_name = convert_id_to_dataset_name(self.dataset_id)\n        self.fold = fold\n        self.data_identifier = data_identifier\n        self.always_preprocess = always_preprocess\n        self.plans_name = plans_identifier\n        self.fingerprint_extracted = False\n        self.grad_scaler = GradScaler()\n        self.max_grad_norm = max_grad_norm\n        self.n_dataload_proc = n_dataload_processes\n\n        try:\n            self.dataset_json = load_json(join(nnUNet_raw, self.dataset_name, \"dataset.json\"))\n        except Exception:\n            try:\n                self.dataset_json = load_json(join(nnUNet_preprocessed, self.dataset_name, \"dataset.json\"))\n            except Exception as e:\n                log(\n                    ERROR,\n                    \"Could not load the nnunet dataset json from nnUNet_raw or nnUNet_preprocessed.\",\n                )\n                raise e  # Raising e will raise both exceptions since it is nested.\n\n        # Auto set verbose to True if console handler is on DEBUG mode\n        self.verbose = verbose if console_handler.level &gt;= INFO else True\n\n        # Used to redirect stdout to logger\n        self.stream2debug = StreamToLogger(FLOWER_LOGGER, DEBUG)\n\n        # nnunet specific attributes to be initialized in setup_client\n        self.nnunet_trainer_class = nnunet_trainer_class\n        self.nnunet_trainer_class_kwargs = nnunet_trainer_class_kwargs or {}\n        self.nnunet_trainer: nnUNetTrainer\n        self.nnunet_config: NnunetConfig\n        self.plans: dict[str, Any] | None = None\n        self.steps_per_round: int  # N steps per server round\n        self.max_steps: int  # N_rounds x steps_per_round\n\n        # Turn on/off model optimizations for runtime efficiency.\n        if compile:\n            log(\n                INFO,\n                (\n                    \"Model will be compiled to support training efficiency. NOTE: torch.backends.cudnn.benchmark will \"\n                    \"be set to True disregarding any previous values.\"\n                ),\n            )\n            # Turning on cudnn.benchmark reduces nnUNet runtimes by 2-3x in our experiments.\n            # Limit of 0 tells cudnn to benchmark all available conv algorithms (default is 10)\n            torch.backends.cudnn.benchmark = True\n            torch.backends.cudnn.benchmark_limit = 0\n            os.environ[\"nnUNet_compile\"] = str(\"true\")  # noqa: SIM112\n        else:\n            log(\n                INFO,\n                (\n                    \"Model will be not be compiled NOTE: torch.backends.cudnn.benchmark will be set to False \"\n                    \"disregarding any previous values.\"\n                ),\n            )\n            torch.backends.cudnn.benchmark = False\n            os.environ[\"nnUNet_compile\"] = str(\"false\")  # noqa: SIM112\n            if self.verbose:\n                log(\n                    INFO,\n                    \"Disabling model optimizations and JIT compilation. This may impact runtime performance.\",\n                )\n\n    @override\n    def _apply_backwards_on_losses_and_take_step(\n        self, model: nn.Module, optimizer: Optimizer, losses: TrainingLosses\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Overrides parent to include mixed precision training (autocasting and corresponding gradient scaling) as per\n        the original ``nnUNetTrainer``.\n\n        Args:\n            model (nn.Module): the model used for making predictions. Passed here in case subclasses need it.\n            optimizer (Optimizer): the optimizer with which we take the step\n            losses (TrainingLosses): the losses to apply backwards on\n\n        Returns:\n            (TrainingLosses): The losses object after backwards application\n        \"\"\"\n        # If the device type is not cuda, we don't use mixed precision training and therefore can use parent method.\n        if self.device.type != \"cuda\":\n            return super()._apply_backwards_on_losses_and_take_step(model, optimizer, losses)\n\n        # Compute scaled loss and perform backward pass\n        scaled_backward_loss = self.grad_scaler.scale(losses.backward[\"backward\"])\n        scaled_backward_loss.backward()\n\n        # Rescale gradients then clip based on specified norm\n        self.grad_scaler.unscale_(optimizer)\n        self._transform_gradients_with_model(model, losses)\n\n        # Update parameters and scaler\n        self.grad_scaler.step(optimizer)\n        self.grad_scaler.update()\n\n        return losses\n\n    @use_default_signal_handlers  # Dataloaders use multiprocessing\n    @override\n    def get_data_loaders(self, config: Config) -&gt; tuple[DataLoader, DataLoader]:\n        \"\"\"\n        Gets the nnunet dataloaders and wraps them in another class to make them pytorch iterators.\n\n        Args:\n            config (Config): The config file from the server\n\n        Returns:\n            (tuple[DataLoader, DataLoader]): A tuple of length two. The client train and validation dataloaders as\n                pytorch dataloaders\n        \"\"\"\n        start_time = time.time()\n        # Set the number of processes for each dataloader.\n        if self.n_dataload_proc is None:\n            # Nnunet default is 12 or max cpu's. We decrease max by 1 just in case\n            # NOTE: The type: ignore here is to skip issues where a local operating system is not compatible\n            # with sched_getaffinity (older versions of MacOS, for example). The code still won't run but mypy won't\n            # complain. Workarounds like using os.cpu_count(), while not exactly the same, are possible.\n            try:\n                self.n_dataload_proc = min(12, len(os.sched_getaffinity(0)) - 1)  # type: ignore\n            except AttributeError:\n                # TODO: this is pretty brittle\n                if cpu_count := os.cpu_count():\n                    self.n_dataload_proc = min(12, cpu_count - 2)\n                else:\n                    self.n_dataload_proc = 1\n        os.environ[\"nnUNet_n_proc_DA\"] = str(self.n_dataload_proc)  # noqa: SIM112\n\n        # The batchgenerators package used under the hood by the dataloaders creates an additional stream handler for\n        # the root logger Therefore all logs get printed twice. First stop flwr logger from propagating logs to root.\n        # Issue: https://github.com/MIC-DKFZ/batchgenerators/issues/123\n        # PR: https://github.com/MIC-DKFZ/batchgenerators/pull/124\n        FLOWER_LOGGER.propagate = False\n\n        # Redirect nnunet output to flwr logger at DEBUG level.\n        with redirect_stdout(self.stream2debug):\n            # Get the nnunet dataloader iterators. (Technically augmenter classes)\n            train_loader, val_loader = self.nnunet_trainer.get_dataloaders()\n\n        # Now clear root handler that was created when get_dataloaders was called and reset flwr logger propagate\n        root_logger = logging.getLogger()\n        root_logger.handlers.clear()\n        FLOWER_LOGGER.propagate = True\n\n        # Get accurate estimate of image shape so that we can get accurate dataloader length\n        if self.plans is None:\n            self.plans = self.create_plans(config)  # Local plans will have metadata we need\n        fullres_cfg = \"3d_fullres\" if \"3d_fullres\" in self.plans[\"configurations\"] else \"2d\"\n        shape = self.plans[\"configurations\"][fullres_cfg][\"median_image_size_in_voxels\"]\n\n        # Wrap nnunet dataloaders to make them compatible with fl4health\n        train_loader = NnUNetDataLoaderWrapper(\n            nnunet_augmenter=train_loader,\n            nnunet_config=self.nnunet_config,\n            ref_image_shape=shape,\n        )\n        val_loader = NnUNetDataLoaderWrapper(\n            nnunet_augmenter=val_loader,\n            nnunet_config=self.nnunet_config,\n            ref_image_shape=shape,\n        )\n        log(\n            INFO,\n            f\"{len(val_loader)}, {len(val_loader.dataset)}, {val_loader.nnunet_dataloader.batch_size}\",\n        )\n\n        if self.verbose:\n            log(INFO, f\"\\tDataloaders initialized in {time.time() - start_time:.1f}s\")\n\n        return train_loader, val_loader\n\n    @override\n    def get_model(self, config: Config) -&gt; nn.Module:\n        return self.nnunet_trainer.network\n\n    @override\n    def get_criterion(self, config: Config) -&gt; _Loss:\n        if isinstance(self.nnunet_trainer.loss, DeepSupervisionWrapper):\n            self.reports_manager.report({\"Criterion\": self.nnunet_trainer.loss.loss.__class__.__name__})\n        else:\n            self.reports_manager.report({\"Criterion\": self.nnunet_trainer.loss.__class__.__name__})\n\n        return Module2LossWrapper(self.nnunet_trainer.loss)\n\n    @override\n    def get_optimizer(self, config: Config) -&gt; Optimizer:\n        self.reports_manager.report({\"Optimizer\": self.nnunet_trainer.optimizer.__class__.__name__})\n        return self.nnunet_trainer.optimizer\n\n    @override\n    def get_lr_scheduler(self, optimizer_key: str, config: Config) -&gt; _LRScheduler:\n        \"\"\"\n        Creates an LR Scheduler similar to the nnunet default except we set max steps to the total number of steps\n        and update every step. Initial and final LR are the same as nnunet, difference is nnunet sets max steps to\n        num \"epochs\", but they define an \"epoch\" as exactly 250 steps. Therefore they update every 250 steps. Override\n        this method to set your own LR scheduler.\n\n        Args:\n            optimizer_key (str): Key of the optimizer to which the scheduler will be applied.\n            config (Config): The server config. This method will look for the\n        Returns:\n            (_LRScheduler): The default nnunet LR Scheduler for nnunetv2 2.5.1\n        \"\"\"\n        if not isinstance(self.nnunet_trainer.lr_scheduler, PolyLRScheduler):\n            log(\n                WARNING,\n                (\n                    \"Nnunet seems to have changed their default LR scheduler to \"\n                    f\"type: {type(self.nnunet_trainer.lr_scheduler)}. \"\n                    \"Using PolyLRScheduler instead. Override or update the \"\n                    \"get_lr_scheduler method of nnUNetClient to change this\"\n                ),\n            )\n\n        # Determine total number of steps throughout all FL rounds\n        local_epochs, local_steps, _, _, _ = self.process_config(config)\n        if local_steps is not None:\n            steps_per_round = local_steps\n        elif local_epochs is not None:\n            steps_per_round = local_epochs * len(self.train_loader)\n        else:\n            raise ValueError(\"One of local steps or local epochs must be specified\")\n\n        total_steps = int(config[\"n_server_rounds\"]) * steps_per_round\n\n        # Create and return LR Scheduler Wrapper for the PolyLRScheduler so that it is\n        # compatible with Torch LRScheduler\n        # Create and return LR Scheduler. This is nnunet default for version 2.5.1\n        self.reports_manager.report({\"LR Scheduler\": \"PolyLRScheduler\"})\n        return PolyLRSchedulerWrapper(\n            self.optimizers[optimizer_key],\n            initial_lr=self.nnunet_trainer.initial_lr,\n            max_steps=total_steps,\n        )\n\n    def create_plans(self, config: Config) -&gt; dict[str, Any]:\n        \"\"\"\n        Modifies the provided plans file to work with the local client dataset and then saves it to disk. Requires the\n        local ``dataset_fingerprint.json`` to exist, the local ``dataset_name``, ``plans_name``, ``data_identifier``\n        and ``dataset_json``.\n\n        The following fields are modified:\n\n        - plans_name\n        - dataset_name\n        - original_median_shape_after_transp\n        - original_median_spacing_after_transp\n        - configurations.{config}.data_identifier\n        - configurations.{config}.batch_size\n        - configurations.{config}.median_image_size_in_voxels\n        - foreground_intensity_properties_per_channel\n\n        Args:\n            config (Config): The config provided by the server. Expects the \"nnunet_plans\" key with a pickled\n                dictionary as the value\n\n        Returns:\n            (dict[str, Any]): The modified nnunet plans for the client\n        \"\"\"\n        # TODO: Make this an external function or part of another class and explicitly accept the required arguments\n        # rather than using class attributes.\n\n        # Get the source nnunet plans specified by the server\n        plans = pickle.loads(narrow_dict_type(config, \"nnunet_plans\", bytes))\n\n        # Change plans name.\n        if self.plans_name is None:\n            self.plans_name = f\"FL-{plans['plans_name']}-{self.dataset_id}local\"\n\n        plans[\"source_plans_name\"] = plans[\"plans_name\"]\n        plans[\"plans_name\"] = self.plans_name\n\n        # Change dataset name\n        plans[\"dataset_name\"] = self.dataset_name\n\n        # Load the dataset fingerprint for the local client dataset\n        fp_path = Path(nnUNet_preprocessed) / self.dataset_name / \"dataset_fingerprint.json\"\n        assert fp_path.exists(), \"Could not find the dataset fingerprint file\"\n        fp = load_json(fp_path)\n\n        # Change the foreground intensity properties per channel\n        plans[\"foreground_intensity_properties_per_channel\"] = fp[\"foreground_intensity_properties_per_channel\"]\n\n        # Compute the median image size and spacing of the local client dataset\n        median_shape = np.median(fp[\"shapes_after_crop\"], axis=0)[plans[\"transpose_forward\"]]\n        median_spacing = np.median(fp[\"spacings\"], axis=0)[plans[\"transpose_forward\"]]\n        plans[\"original_median_shape_after_transp\"] = [int(round(i)) for i in median_shape]\n        plans[\"original_median_spacing_after_transp\"] = [float(i) for i in median_spacing]\n\n        # Get the spacing that the network will resample to. Need to check if samples are 2d or 3d\n        fullres_cfg = \"3d_fullres\" if \"3d_fullres\" in plans[\"configurations\"] else \"2d\"\n        target_spacing = plans[\"configurations\"][fullres_cfg][\"spacing\"]\n\n        # Get the median shape after resampling to the desired/input voxel spacing\n        resampled_shapes = [\n            compute_new_shape(i, j, target_spacing) for i, j in zip(fp[\"shapes_after_crop\"], fp[\"spacings\"])\n        ]\n        resampled_median_shape = np.median(resampled_shapes, axis=0)[plans[\"transpose_forward\"]].tolist()\n\n        # Change data identifier\n        if self.data_identifier is None:\n            self.data_identifier = self.plans_name\n\n        # To be consistent with nnunet, a batch cannot contain more than 5% of the voxels in the dataset.\n        max_voxels = np.prod(resampled_median_shape, dtype=np.float64) * self.dataset_json[\"numTraining\"] * 0.05\n\n        # Iterate through nnunet configs in plans file\n        for c in plans[\"configurations\"]:\n            # Change the data identifier\n            plans[\"configurations\"][c][\"data_identifier\"] = self.data_identifier + \"_\" + c\n\n            # Ensure batch size is at least 2 and at most 5 percent of dataset\n            # Otherwise we keep it the same as it affects the target VRAM consumption\n            if \"batch_size\" in plans[\"configurations\"][c]:\n                old_bs = plans[\"configurations\"][c][\"batch_size\"]\n                bs_5percent = round(max_voxels / np.prod(plans[\"configurations\"][c][\"patch_size\"], dtype=np.float64))\n                new_bs = max(min(old_bs, bs_5percent), 2)\n                plans[\"configurations\"][c][\"batch_size\"] = new_bs\n            else:\n                log(\n                    WARNING,\n                    f\"Did not find a 'batch_size' key in the nnunet plans dict for nnunet config: {c}\",\n                )\n\n            # Update median shape of resampled input images\n            if str(c).startswith(\"2d\"):\n                plans[\"configurations\"][c][\"median_image_size_in_voxels\"] = resampled_median_shape[1:]\n            else:\n                plans[\"configurations\"][c][\"median_image_size_in_voxels\"] = resampled_median_shape\n\n        # Save local plans file\n        os.makedirs(join(nnUNet_preprocessed, self.dataset_name), exist_ok=True)\n        plans_save_path = join(nnUNet_preprocessed, self.dataset_name, self.plans_name + \".json\")\n        save_json(plans, plans_save_path, sort_keys=False)\n        return plans\n\n    @use_default_signal_handlers  # Preprocessing spawns subprocesses\n    def maybe_preprocess(self, nnunet_config: NnunetConfig) -&gt; None:\n        \"\"\"\n        Checks if preprocessed data for current plans exists and if not preprocesses the nnunet_raw dataset. The\n        preprocessed data is saved in '{nnUNet_preprocessed}/{dataset_name}/{data_identifier} as follows.\n\n        - ``nnUNet_preprocessed`` is the directory specified by the ``nnUNet_preprocessed`` environment variable.\n        - ``dataset_name`` is the nnunet dataset name (e.g. Dataset123_MyDataset)\n        - ``data_identifier`` is ``{self.data_identifier}_{self.nnunet_config}``\n\n        Args:\n            nnunet_config (NnunetConfig): The nnunet config as a ``NnunetConfig`` Enum. Enum type ensures nnunet\n                config is valid\n        \"\"\"\n        assert self.data_identifier is not None, \"Was expecting data identifier to be initialized in self.create_plans\"\n\n        # Preprocess data if it's not already there\n        if self.always_preprocess or not exists(self.nnunet_trainer.preprocessed_dataset_folder):\n            if self.verbose:\n                log(INFO, f\"\\tPreprocessing local client dataset: {self.dataset_name}\")\n            # Unless log level is debugging or lower, hide nnunet output\n            with redirect_stdout(self.stream2debug):\n                preprocess_dataset(\n                    dataset_id=self.dataset_id,\n                    plans_identifier=self.plans_name,\n                    num_processes=[NNUNET_DEFAULT_NP[nnunet_config]],\n                    configurations=[nnunet_config.value],\n                )\n        elif self.verbose:\n            log(\n                INFO,\n                \"\\tnnunet preprocessed data seems to already exist. Skipping preprocessing\",\n            )\n\n    @use_default_signal_handlers  # Fingerprint extraction spawns subprocess\n    def maybe_extract_fingerprint(self) -&gt; None:\n        \"\"\"Checks if nnunet dataset fingerprint already exists and if not extracts one from the dataset.\"\"\"\n        # Check first whether this client instance has already extracted a dataset fp\n        # Possible if the client was asked to generate the nnunet plans for the server\n        if not self.fingerprint_extracted:\n            fp_path = join(nnUNet_preprocessed, self.dataset_name, \"dataset_fingerprint.json\")\n            # Check if fp already exists or if we want to redo fp extraction\n            if self.always_preprocess or not exists(fp_path):\n                start = time.time()\n                # Unless log level is DEBUG or lower hide nnunet output\n                with redirect_stdout(self.stream2debug):\n                    extract_fingerprints(dataset_ids=[self.dataset_id])\n                if self.verbose:\n                    log(\n                        INFO,\n                        f\"\\tExtracted dataset fingerprint in {time.time() - start:.1f}s\",\n                    )\n            elif self.verbose:\n                log(\n                    INFO,\n                    \"\\tnnunet dataset fingerprint already exists. Skipping fingerprint extraction\",\n                )\n        elif self.verbose:\n            log(\n                INFO,\n                \"\\tThis client has already extracted the dataset fingerprint during this session. Skipping.\",\n            )\n        # Avoid extracting fingerprint multiple times when always_preprocess is true\n        self.fingerprint_extracted = True\n\n    # Several subprocesses spawned in setup during torch.compile and dataset unpacking\n    @use_default_signal_handlers\n    @override\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Ensures the necessary files for training are on disk and initializes several class attributes that depend on\n        values in the config from the server. This is called once when the client is sampled by the server for the\n        first time.\n\n        Args:\n            config (Config): The config file from the server. The ``nnUNetClient`` expects the keys 'nnunet_config'\n                and 'nnunet_plans' in addition to those required by ``BasicClient``\n        \"\"\"\n        log(INFO, \"Setting up the nnUNetClient\")\n\n        # Empty gpu cache because nnunet does it\n        self.empty_cache()\n\n        # Get nnunet config\n        self.nnunet_config = NnunetConfig(config[\"nnunet_config\"])\n\n        # Check if dataset fingerprint has already been extracted\n        self.maybe_extract_fingerprint()\n\n        # Create the nnunet plans for the local client\n        if self.plans is None:\n            self.plans = self.create_plans(config=config)\n\n        # Unless log level is DEBUG or lower hide nnunet output\n        with redirect_stdout(self.stream2debug):\n            # Create the nnunet trainer\n            self.nnunet_trainer = self.nnunet_trainer_class(\n                plans=self.plans,\n                configuration=self.nnunet_config.value,\n                fold=self.fold,\n                dataset_json=self.dataset_json,\n                device=self.device,\n                **self.nnunet_trainer_class_kwargs,\n            )\n            # nnunet_trainer initialization\n            self.nnunet_trainer.initialize()\n            # This is done by nnunet_trainer in self.on_train_start, we\n            # do it manually since nnunet_trainer not being used for training\n            self.nnunet_trainer.set_deep_supervision_enabled(self.nnunet_trainer.enable_deep_supervision)\n\n        # Prevent nnunet from generating log files and delete empty output directories\n        os.remove(self.nnunet_trainer.log_file)\n        self.nnunet_trainer.log_file = os.devnull\n        output_folder = Path(self.nnunet_trainer.output_folder)\n        while len(os.listdir(output_folder)) == 0:\n            os.rmdir(output_folder)\n            output_folder = output_folder.parent\n\n        # Preprocess nnunet_raw data if needed\n        self.maybe_preprocess(self.nnunet_config)\n        start = time.time()\n        unpack_dataset(  # Reduces load on CPU and RAM during training\n            folder=self.nnunet_trainer.preprocessed_dataset_folder,\n            unpack_segmentation=self.nnunet_trainer.unpack_dataset,\n            overwrite_existing=self.always_preprocess,\n            verify_npy=True,\n        )\n        if self.verbose:\n            log(INFO, f\"\\tUnpacked dataset in {time.time() - start:.1f}s\")\n\n        # We have to call parent method after setting up nnunet trainer\n        super().setup_client(config)\n\n    @override\n    def predict_with_model(\n        self, model: torch.nn.Module, input: TorchInputType\n    ) -&gt; tuple[TorchPredType, dict[str, torch.Tensor]]:\n        \"\"\"\n        Generate model outputs. Overridden because nnunets outputs lists when deep supervision is on so we have to\n        reformat the output into dicts.\n\n        Additionally if device type is cuda, loss computed in mixed precision.\n\n        Args:\n            model (nn.Module): The model used to make predictions\n            input (TorchInputType): The model inputs\n\n        Returns:\n            (tuple[TorchPredType, dict[str, torch.Tensor]]): A tuple in which the first element model outputs indexed\n                by name. The second element is unused by this subclass and therefore is always an empty dict\n        \"\"\"\n        if isinstance(input, torch.Tensor):\n            # If device type is cuda, nnUNet defaults to mixed precision forward pass\n            if self.device.type == \"cuda\":\n                with torch.autocast(self.device.type, enabled=True):\n                    output = model(input)\n            else:\n                output = model(input)\n        else:\n            raise TypeError('\"input\" must be of type torch.Tensor for nnUNetClient')\n\n        if isinstance(output, torch.Tensor):\n            return {\"prediction\": output}, {}\n        # If output is a list or tuple then deep supervision is on and we need to convert preds into a dict\n        if isinstance(output, (list, tuple)):\n            num_spatial_dims = NNUNET_N_SPATIAL_DIMS[self.nnunet_config]\n            preds = convert_deep_supervision_list_to_dict(output, num_spatial_dims)\n            return preds, {}\n        raise TypeError(\n            \"Was expecting nnunet model output to be either a torch.Tensor or a list/tuple of torch.Tensors\"\n        )\n\n    @override\n    def compute_loss_and_additional_losses(\n        self,\n        preds: TorchPredType,\n        features: dict[str, torch.Tensor],\n        target: TorchTargetType,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor] | None]:\n        \"\"\"\n        Checks the pred and target types and computes the loss. If device type is cuda, loss computed in mixed\n        precision.\n\n        Args:\n            preds (TorchPredType): Dictionary of model output tensors indexed by name\n            features (dict[str, torch.Tensor]): Not used by this subclass\n            target (TorchTargetType): The targets to evaluate the predictions with. If multiple prediction tensors\n                are given, target must be a dictionary with the same number of tensors\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor] | None]): A tuple where the first element is the loss and the\n                second element is an optional additional loss\n        \"\"\"\n        # If deep supervision is turned on we must convert loss and target dicts into lists\n        loss_preds = prepare_loss_arg(preds)\n        loss_targets = prepare_loss_arg(target)\n\n        # Ensure we have the same number of predictions and targets\n        assert isinstance(loss_preds, type(loss_targets)), (\n            f\"Got unexpected types for preds and targets: {type(loss_preds)} and {type(loss_targets)}\"\n        )\n\n        if isinstance(loss_preds, list):\n            assert len(loss_preds) == len(loss_targets), (\n                \"Was expecting the number of predictions and targets to be the same. \"\n                f\"Got {len(loss_preds)} predictions and {len(loss_targets)} targets.\"\n            )\n\n        # If device type is cuda, nnUNet defaults to compute loss in mixed precision\n        if self.device.type == \"cuda\":\n            with torch.autocast(self.device.type, enabled=True):\n                loss = self.criterion(loss_preds, loss_targets), None\n        else:\n            loss = self.criterion(loss_preds, loss_targets), None\n\n        return loss\n\n    def mask_data(self, pred: torch.Tensor, target: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Masks the pred and target tensors according to nnunet ``ignore_label``. The number of classes in the input\n        tensors should be at least 3 corresponding to 2 classes for binary segmentation and 1 class which is\n        the ignore class specified by ignore label.\n\n        https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/ignore_label.md\n\n        Args:\n            pred (torch.Tensor): The one hot encoded predicted segmentation maps with shape\n                ``(batch, classes, x, y(, z))``\n            target (TorchTargetType): The ground truth segmentation map with shape ``(batch, classes, x, y(, z))``\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Tuple of:\n\n                - torch.Tensor: The masked one hot encoded predicted segmentation maps\n                - torch.Tensor: The masked target segmentation maps\n        \"\"\"\n        # create mask where 1 is where pixels in target are not ignore label\n        # Modify target to remove the last class which is the ignore_label class\n        new_target = target\n        if self.nnunet_trainer.label_manager.has_regions:  # nnunet returns a ohe target if has_regions is true\n            mask = ~target[:, -1:] if target.dtype == torch.bool else 1 - target[:, -1:]\n            new_target = new_target[:, :-1]  # Remove final ignore_label class from target\n        else:  # target is not one hot encoded\n            mask = (target != self.nnunet_trainer.label_manager.ignore_label).float()\n            # Set ignore label to background essentially removing it as a class\n            new_target[new_target == self.nnunet_trainer.label_manager.ignore_label] = 0\n\n        # Tile the mask to be one hot encoded\n        mask_here = torch.tile(mask, (1, pred.shape[1], *[1 for _ in range(2, pred.ndim)]))\n\n        return (\n            pred * mask_here,\n            new_target,\n        )  # Mask the input tensor and return the modified target\n\n    @override\n    def update_metric_manager(\n        self,\n        preds: TorchPredType,\n        target: TorchTargetType,\n        metric_manager: MetricManager,\n    ) -&gt; None:\n        \"\"\"\n        Update the metrics with preds and target. Overridden because we might need to manipulate inputs due to deep\n        supervision.\n\n        Args:\n            preds (TorchPredType): dictionary of model outputs\n            target (TorchTargetType): the targets generated by the dataloader to evaluate the preds with\n            metric_manager (MetricManager): the metric manager to update\n        \"\"\"\n        log(DEBUG, f\"preds: {preds.keys()}\")\n\n        # If personalized preds will have keys prefixed with \"global-\" and \"local-\",\n        # but we only want to keep the local ones here. NOTE: this will still work\n        # even in the non-personalized case as those preds don't get prefixed with \"global-\"\n        preds = {k: v for k, v in preds.items() if not k.startswith(\"global\")}\n        preds = {k.replace(\"local-\", \"\"): v for k, v in preds.items()}  # no-op in non-personalized case\n\n        if len(preds) &gt; 1:\n            # for nnunet the first pred in the output list is the main one\n            m_pred = convert_deep_supervision_dict_to_list(preds)[0]\n\n        if isinstance(target, torch.Tensor):\n            m_target = target\n        elif isinstance(target, dict):\n            if len(target) &gt; 1:\n                # If deep supervision is in use, we drop the additional targets\n                # when calculating the metrics as we only care about the\n                # original target which by default in nnunet is at index 0\n                m_target = convert_deep_supervision_dict_to_list(target)[0]\n            else:\n                m_target = list(target.values())[0]\n        else:\n            raise TypeError(\"Was expecting target to be type dict[str, torch.Tensor] or torch.Tensor\")\n\n        # Check if target is one hot encoded. Prediction always is for nnunet\n        # Add channel dimension if there isn't one\n        if m_pred.ndim != m_target.ndim:\n            m_target = m_target.view(m_target.shape[0], 1, *m_target.shape[1:])\n\n        # One hot encode targets if needed\n        if m_pred.shape != m_target.shape:\n            m_target_one_hot = torch.zeros(m_pred.shape, device=self.device, dtype=torch.bool)\n            # This is how nnunet does ohe in their functions\n            # Its a weird function that is not intuitive\n            # CAREFUL: Notice the underscore at the end of the scatter function.\n            # It makes a difference, was a hard bug to find!\n            m_target_one_hot.scatter_(1, m_target.long(), 1)\n        else:\n            m_target_one_hot = m_target\n\n        # Check if ignore label is in use. The nnunet loss figures this out on\n        # it's own, but we do it manually here for the metrics\n        if self.nnunet_trainer.label_manager.ignore_label is not None:\n            m_pred, m_target_one_hot = self.mask_data(m_pred, m_target_one_hot)\n\n        # m_pred is one hot encoded (OHE) output logits. Maybe masked by ignore label\n        # m_target_one_hot is OHE boolean label. Maybe masked by ignore label\n        metric_manager.update({\"prediction\": m_pred}, m_target_one_hot)\n\n    def empty_cache(self) -&gt; None:\n        \"\"\"Checks torch device and empties cache before training to optimize VRAM usage.\"\"\"\n        if self.device.type == \"cuda\":\n            torch.cuda.empty_cache()\n        elif self.device.type == \"mps\":\n            torch.mps.empty_cache()\n\n    @override\n    def get_client_specific_logs(\n        self,\n        current_round: int | None,\n        current_epoch: int | None,\n        logging_mode: LoggingMode,\n    ) -&gt; tuple[str, list[tuple[LogLevel, str]]]:\n        if logging_mode == LoggingMode.TRAIN:\n            lr = float(self.optimizers[\"global\"].param_groups[0][\"lr\"])\n            if current_epoch is None:\n                # Assume training by steps\n                return f\"Initial LR {lr}\", []\n            return f\" Current LR: {lr}\", []\n        return \"\", []\n\n    @override\n    def get_client_specific_reports(self) -&gt; dict[str, Any]:\n        return {\"learning_rate\": float(self.optimizers[\"global\"].param_groups[0][\"lr\"])}\n\n    @use_default_signal_handlers  # Experiment planner spawns a process I think\n    @override\n    def get_properties(self, config: Config) -&gt; dict[str, Scalar]:\n        \"\"\"\n        Return properties (sample counts and nnunet plans) of client.\n\n        If nnunet plans are not provided by the server, creates a new set of nnunet plans from the local client\n        dataset. These plans are intended to be used for initializing global nnunet plans when they are not\n        provided.\n\n        Args:\n            config (Config): The config from the server\n\n        Returns:\n            (dict[str, Scalar]): A dictionary containing the train and validation sample counts as well as the\n                serialized nnunet plans.\n        \"\"\"\n        # Check if nnunet plans have already been initialized\n        if \"nnunet_plans\" not in config:\n            log(INFO, \"Initializing the global plans using local dataset\")\n            # Local client will initialize global nnunet plans\n            # Check if local nnunet dataset fingerprint needs to be extracted\n            self.maybe_extract_fingerprint()\n\n            # Create experiment planner and plans.\n            # Plans name must be temp_plans so that we can safely delete the generated plans file\n            planner = ExperimentPlanner(dataset_name_or_id=self.dataset_id, plans_name=\"temp_plans\")\n\n            # Unless log level is DEBUG or lower, hide nnunet output\n            with redirect_stdout(self.stream2debug):\n                plans = planner.plan_experiment()\n\n            # Set plans name to local dataset so we know the source\n            plans[\"plans_name\"] = self.dataset_name + \"_plans\"\n            plans_bytes = pickle.dumps(plans)\n\n            # Remove plans file . A new one will be generated in self.setup_client\n            plans_path = join(\n                nnUNet_preprocessed,\n                self.dataset_name,\n                planner.plans_identifier + \".json\",\n            )\n            if exists(plans_path):\n                os.remove(plans_path)\n\n            # Update local config with plans\n            config[\"nnunet_plans\"] = plans_bytes\n\n        # Get client properties. We are now sure that config contains plans\n        properties = super().get_properties(config)\n        properties[\"nnunet_plans\"] = config[\"nnunet_plans\"]\n\n        # super.get_properties should setup the client anyways, but we can add a check here as a precaution.\n        if not self.initialized:\n            self.setup_client(config)  # Client must be setup in order to initialize nnunet_trainer\n\n        # Add additional properties from nnunet trainer to properties dict. We may want to add more keys later\n        properties[\"num_input_channels\"] = self.nnunet_trainer.num_input_channels\n        properties[\"num_segmentation_heads\"] = self.nnunet_trainer.label_manager.num_segmentation_heads\n        properties[\"enable_deep_supervision\"] = self.nnunet_trainer.enable_deep_supervision\n\n        return properties\n\n    def shutdown_dataloader(self, dataloader: DataLoader | None, dl_name: str | None = None) -&gt; None:\n        \"\"\"\n        The nnunet dataloader/augmenter uses multiprocessing under the hood, so the shutdown method terminates the\n        child processes gracefully.\n\n        Args:\n            dataloader (DataLoader): The dataloader to shutdown\n            dl_name (str | None): A string that identifies the dataloader to shutdown. Used for logging purposes.\n                Defaults to None\n        \"\"\"\n        if dataloader is not None and isinstance(dataloader, NnUNetDataLoaderWrapper):\n            if self.verbose:\n                log(INFO, f\"\\tShutting down nnunet dataloader: {dl_name}\")\n            dataloader.shutdown()\n\n        del dataloader\n\n    @override\n    def shutdown(self) -&gt; None:\n        # Unfreeze and collect memory that was frozen during training\n        # See self.update_before_train()\n        gc.unfreeze()\n        gc.collect()\n\n        # Shutdown dataloader subprocesses gracefully\n        self.shutdown_dataloader(self.train_loader, \"train_loader\")\n        self.shutdown_dataloader(self.val_loader, \"val_loader\")\n        self.shutdown_dataloader(self.test_loader, \"test_loader\")\n\n        # Parent shutdown\n        super().shutdown()\n\n    @override\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        # Was getting OOM errors that could only be fixed by manually cleaning up RAM\n        # https://github.com/pytorch/pytorch/issues/95462\n        # The above issue seems to be the situation I was in.\n        gc.collect()  # Cleans up unused variables\n        # As the linked issue above points out, calling gc.freeze() greatly reduces the\n        # overhead of garbage collection. (from 1.5s to 0.005s)\n        if current_server_round == 2:  # noqa: PLR2004\n            # Collect runs even faster if we freeze after the end of the first iteration\n            # Likely because a lot of variables are created in the first pass. If we\n            # freeze before the first pass, gc.collect has to check all those variables\n            gc.freeze()\n\n    @override\n    def _transform_gradients_with_model(self, model: torch.nn.Module, losses: TrainingLosses) -&gt; None:\n        \"\"\"\n        Apply the gradient clipping performed by the default nnunet trainer. This is the default behavior for\n        nnunet 2.5.1.\n\n        Args:\n            model (torch.nn.Module): the model used to generate predictions to compute losses\n            losses (TrainingLosses): The losses object from the train step\n        \"\"\"\n        nn.utils.clip_grad_norm_(model.parameters(), self.max_grad_norm)\n</code></pre> <code></code> <code>__init__(device, dataset_id, fold, data_identifier=None, plans_identifier=None, compile=True, always_preprocess=False, max_grad_norm=12, n_dataload_processes=None, verbose=True, metrics=None, progress_bar=False, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, client_name=None, nnunet_trainer_class=nnUNetTrainer, nnunet_trainer_class_kwargs=None)</code> \u00b6 <p>A client for training nnunet models. Requires the nnunet environment variables to be set. Also requires the following additional keys in the config sent from the server as follows.</p> <ul> <li>\"nnunet_plans\": (serialized dict)</li> <li>\"nnunet_config\": (str)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\" or \"mps\"</p> required <code>dataset_id</code> <code>int</code> <p>The nnunet dataset id for the local client dataset to use for training and validation.</p> required <code>fold</code> <code>int | str</code> <p>Which fold of the local client dataset to use for validation. nnunet defaults to 5 folds (0 to 4). Can also be set to \"all\" to use all the data for both training and validation.</p> required <code>data_identifier</code> <code>str | None</code> <p>The nnunet data identifier prefix to use. The final data identifier will be <code>{data_identifier}_config</code> where \"config\" is the nnunet config (e.g. 2d, <code>3d_fullres</code>, etc.). If preprocessed data already exists can be used to specify which preprocessed data to use. By default, the <code>plans_identifier</code> is used as the <code>data_identifier</code>.</p> <code>None</code> <code>plans_identifier</code> <code>str | None</code> <p>Specify what to save the client's local copy of the plans file as. The client makes a local modified copy of the global source plans file sent by the server. If left as default None, the plans identifier will be set as \"FL-plansname-000local\" where 000 is the <code>dataset_id</code> and plansname is the \"plans_name\" value of the source plans file. The original plans will be saved under the <code>source_plans_name</code> key in the modified plans file.</p> <code>None</code> <code>compile</code> <code>bool</code> <p>If set to True, the client will Just-In-Time (JIT) compile the nnUNet model and perform optimizations at the start of training. This process significantly reduces the runtime for nnUNet models, especially for larger models or long-running jobs. However, it introduces some overhead time and computation during the initial step. It is recommended to keep this option enabled. The default value is True.</p> <code>True</code> <code>always_preprocess</code> <code>bool</code> <p>If True, will preprocess the local client dataset even if the preprocessed data already seems to exist. The existence of the preprocessed data is determined by matching the provided <code>data_identifier</code> with that of the preprocessed data already  on the client. Defaults to False.</p> <code>False</code> <code>max_grad_norm</code> <code>float</code> <p>The maximum gradient norm to use for gradient clipping. Defaults to 12 which is the nnunetv2 2.5.1 default.</p> <code>12</code> <code>n_dataload_processes</code> <code>int | None</code> <p>The number of processes to spawn for each nnunet dataloader. If left as None we use the nnunetv2 version 2.5.1 defaults for each config</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True the client will log some extra INFO logs. Defaults to False unless the log level is DEBUG or lower.</p> <code>True</code> <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to print a progress bar to stdout for training. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter]</code> <p>A sequence of FL4Health reporters which the client should send data to.</p> <code>None</code> <code>nnunet_trainer_class</code> <code>type[nnUNetTrainer]</code> <p>A <code>nnUNetTrainer</code> constructor. Useful for passing custom <code>nnUNetTrainer</code>. Defaults to the standard nnUNetTrainer class. Must match the <code>nnunet_trainer_class</code> passed to the <code>NnunetServer</code>.</p> <code>nnUNetTrainer</code> <code>nnunet_trainer_class_kwargs</code> <code>dict[str, Any]</code> <p>Additional kwargs to pass to <code>nnunet_trainer_class</code>. Defaults to empty dictionary.</p> <code>None</code> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>def __init__(\n    self,\n    device: torch.device,\n    dataset_id: int,\n    fold: int | str,\n    data_identifier: str | None = None,\n    plans_identifier: str | None = None,\n    compile: bool = True,\n    always_preprocess: bool = False,\n    max_grad_norm: float = 12,\n    n_dataload_processes: int | None = None,\n    verbose: bool = True,\n    metrics: Sequence[Metric] | None = None,\n    progress_bar: bool = False,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    client_name: str | None = None,\n    nnunet_trainer_class: type[nnUNetTrainer] = nnUNetTrainer,\n    nnunet_trainer_class_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    A client for training nnunet models. Requires the nnunet environment variables to be set. Also requires the\n    following additional keys in the config sent from the server as follows.\n\n    - \"nnunet_plans\": (serialized dict)\n    - \"nnunet_config\": (str)\n\n    Args:\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\" or \"mps\"\n        dataset_id (int): The nnunet dataset id for the local client dataset to use for training and validation.\n        fold (int | str): Which fold of the local client dataset to use for validation. nnunet defaults to\n            5 folds (0 to 4). Can also be set to \"all\" to use all the data for both training and validation.\n        data_identifier (str | None, optional): The nnunet data identifier prefix to use. The final data\n            identifier will be ``{data_identifier}_config`` where \"config\" is the nnunet config (e.g. 2d,\n            ``3d_fullres``, etc.). If preprocessed data already exists can be used to specify which preprocessed\n            data to use. By default, the ``plans_identifier`` is used as the ``data_identifier``.\n        plans_identifier (str | None, optional): Specify what to save the client's local copy of the plans file\n            as. The client makes a local modified copy of the global source plans file sent by the server. If left\n            as default None, the plans identifier will be set as \"FL-plansname-000local\" where 000 is the\n            ``dataset_id`` and plansname is the \"plans_name\" value of the source plans file. The original plans\n            will be saved under the ``source_plans_name`` key in the modified plans file.\n        compile (bool, optional): If set to True, the client will Just-In-Time (JIT) compile the nnUNet model and\n            perform optimizations at the start of training. This process significantly reduces the runtime for\n            nnUNet models, especially for larger models or long-running jobs. However, it introduces some overhead\n            time and computation during the initial step. It is recommended to keep this option enabled. The\n            default value is True.\n        always_preprocess (bool, optional): If True, will preprocess the local client dataset even if the\n            preprocessed data already seems to exist. The existence of the preprocessed data is determined by\n            matching the provided ``data_identifier`` with that of the preprocessed data already  on the client.\n            Defaults to False.\n        max_grad_norm (float, optional): The maximum gradient norm to use for gradient clipping. Defaults to 12\n            which is the nnunetv2 2.5.1 default.\n        n_dataload_processes (int | None, optional): The number of processes to spawn for each nnunet\n            dataloader. If left as None we use the nnunetv2 version 2.5.1 defaults for each config\n        verbose (bool, optional): If True the client will log some extra INFO logs. Defaults to False unless\n            the log level is DEBUG or lower.\n        metrics (Sequence[Metric], optional): Metrics to be computed based on the labels and predictions of the\n            client model. Defaults to None.\n        progress_bar (bool, optional): Whether or not to print a progress bar to stdout for training. Defaults\n            to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over each\n            batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the client should\n            send data to.\n        nnunet_trainer_class (type[nnUNetTrainer]): A ``nnUNetTrainer`` constructor. Useful for passing custom\n            ``nnUNetTrainer``. Defaults to the standard nnUNetTrainer class. Must match the\n            ``nnunet_trainer_class`` passed to the ``NnunetServer``.\n        nnunet_trainer_class_kwargs (dict[str, Any]): Additional kwargs to pass to ``nnunet_trainer_class``.\n            Defaults to empty dictionary.\n    \"\"\"\n    metrics = metrics if metrics else []\n\n    # Parent method sets up several class attributes\n    super().__init__(\n        data_path=Path(\"dummy/path\"),  # data_path not used by NnunetClient\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n\n    # Some nnunet client specific attributes\n    self.dataset_id: int = dataset_id\n    self.dataset_name = convert_id_to_dataset_name(self.dataset_id)\n    self.fold = fold\n    self.data_identifier = data_identifier\n    self.always_preprocess = always_preprocess\n    self.plans_name = plans_identifier\n    self.fingerprint_extracted = False\n    self.grad_scaler = GradScaler()\n    self.max_grad_norm = max_grad_norm\n    self.n_dataload_proc = n_dataload_processes\n\n    try:\n        self.dataset_json = load_json(join(nnUNet_raw, self.dataset_name, \"dataset.json\"))\n    except Exception:\n        try:\n            self.dataset_json = load_json(join(nnUNet_preprocessed, self.dataset_name, \"dataset.json\"))\n        except Exception as e:\n            log(\n                ERROR,\n                \"Could not load the nnunet dataset json from nnUNet_raw or nnUNet_preprocessed.\",\n            )\n            raise e  # Raising e will raise both exceptions since it is nested.\n\n    # Auto set verbose to True if console handler is on DEBUG mode\n    self.verbose = verbose if console_handler.level &gt;= INFO else True\n\n    # Used to redirect stdout to logger\n    self.stream2debug = StreamToLogger(FLOWER_LOGGER, DEBUG)\n\n    # nnunet specific attributes to be initialized in setup_client\n    self.nnunet_trainer_class = nnunet_trainer_class\n    self.nnunet_trainer_class_kwargs = nnunet_trainer_class_kwargs or {}\n    self.nnunet_trainer: nnUNetTrainer\n    self.nnunet_config: NnunetConfig\n    self.plans: dict[str, Any] | None = None\n    self.steps_per_round: int  # N steps per server round\n    self.max_steps: int  # N_rounds x steps_per_round\n\n    # Turn on/off model optimizations for runtime efficiency.\n    if compile:\n        log(\n            INFO,\n            (\n                \"Model will be compiled to support training efficiency. NOTE: torch.backends.cudnn.benchmark will \"\n                \"be set to True disregarding any previous values.\"\n            ),\n        )\n        # Turning on cudnn.benchmark reduces nnUNet runtimes by 2-3x in our experiments.\n        # Limit of 0 tells cudnn to benchmark all available conv algorithms (default is 10)\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.benchmark_limit = 0\n        os.environ[\"nnUNet_compile\"] = str(\"true\")  # noqa: SIM112\n    else:\n        log(\n            INFO,\n            (\n                \"Model will be not be compiled NOTE: torch.backends.cudnn.benchmark will be set to False \"\n                \"disregarding any previous values.\"\n            ),\n        )\n        torch.backends.cudnn.benchmark = False\n        os.environ[\"nnUNet_compile\"] = str(\"false\")  # noqa: SIM112\n        if self.verbose:\n            log(\n                INFO,\n                \"Disabling model optimizations and JIT compilation. This may impact runtime performance.\",\n            )\n</code></pre> <code></code> <code>get_data_loaders(config)</code> \u00b6 <p>Gets the nnunet dataloaders and wraps them in another class to make them pytorch iterators.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config file from the server</p> required <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader]</code> <p>A tuple of length two. The client train and validation dataloaders as pytorch dataloaders</p> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>@use_default_signal_handlers  # Dataloaders use multiprocessing\n@override\ndef get_data_loaders(self, config: Config) -&gt; tuple[DataLoader, DataLoader]:\n    \"\"\"\n    Gets the nnunet dataloaders and wraps them in another class to make them pytorch iterators.\n\n    Args:\n        config (Config): The config file from the server\n\n    Returns:\n        (tuple[DataLoader, DataLoader]): A tuple of length two. The client train and validation dataloaders as\n            pytorch dataloaders\n    \"\"\"\n    start_time = time.time()\n    # Set the number of processes for each dataloader.\n    if self.n_dataload_proc is None:\n        # Nnunet default is 12 or max cpu's. We decrease max by 1 just in case\n        # NOTE: The type: ignore here is to skip issues where a local operating system is not compatible\n        # with sched_getaffinity (older versions of MacOS, for example). The code still won't run but mypy won't\n        # complain. Workarounds like using os.cpu_count(), while not exactly the same, are possible.\n        try:\n            self.n_dataload_proc = min(12, len(os.sched_getaffinity(0)) - 1)  # type: ignore\n        except AttributeError:\n            # TODO: this is pretty brittle\n            if cpu_count := os.cpu_count():\n                self.n_dataload_proc = min(12, cpu_count - 2)\n            else:\n                self.n_dataload_proc = 1\n    os.environ[\"nnUNet_n_proc_DA\"] = str(self.n_dataload_proc)  # noqa: SIM112\n\n    # The batchgenerators package used under the hood by the dataloaders creates an additional stream handler for\n    # the root logger Therefore all logs get printed twice. First stop flwr logger from propagating logs to root.\n    # Issue: https://github.com/MIC-DKFZ/batchgenerators/issues/123\n    # PR: https://github.com/MIC-DKFZ/batchgenerators/pull/124\n    FLOWER_LOGGER.propagate = False\n\n    # Redirect nnunet output to flwr logger at DEBUG level.\n    with redirect_stdout(self.stream2debug):\n        # Get the nnunet dataloader iterators. (Technically augmenter classes)\n        train_loader, val_loader = self.nnunet_trainer.get_dataloaders()\n\n    # Now clear root handler that was created when get_dataloaders was called and reset flwr logger propagate\n    root_logger = logging.getLogger()\n    root_logger.handlers.clear()\n    FLOWER_LOGGER.propagate = True\n\n    # Get accurate estimate of image shape so that we can get accurate dataloader length\n    if self.plans is None:\n        self.plans = self.create_plans(config)  # Local plans will have metadata we need\n    fullres_cfg = \"3d_fullres\" if \"3d_fullres\" in self.plans[\"configurations\"] else \"2d\"\n    shape = self.plans[\"configurations\"][fullres_cfg][\"median_image_size_in_voxels\"]\n\n    # Wrap nnunet dataloaders to make them compatible with fl4health\n    train_loader = NnUNetDataLoaderWrapper(\n        nnunet_augmenter=train_loader,\n        nnunet_config=self.nnunet_config,\n        ref_image_shape=shape,\n    )\n    val_loader = NnUNetDataLoaderWrapper(\n        nnunet_augmenter=val_loader,\n        nnunet_config=self.nnunet_config,\n        ref_image_shape=shape,\n    )\n    log(\n        INFO,\n        f\"{len(val_loader)}, {len(val_loader.dataset)}, {val_loader.nnunet_dataloader.batch_size}\",\n    )\n\n    if self.verbose:\n        log(INFO, f\"\\tDataloaders initialized in {time.time() - start_time:.1f}s\")\n\n    return train_loader, val_loader\n</code></pre> <code></code> <code>get_lr_scheduler(optimizer_key, config)</code> \u00b6 <p>Creates an LR Scheduler similar to the nnunet default except we set max steps to the total number of steps and update every step. Initial and final LR are the same as nnunet, difference is nnunet sets max steps to num \"epochs\", but they define an \"epoch\" as exactly 250 steps. Therefore they update every 250 steps. Override this method to set your own LR scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_key</code> <code>str</code> <p>Key of the optimizer to which the scheduler will be applied.</p> required <code>config</code> <code>Config</code> <p>The server config. This method will look for the</p> required <p>Returns:     (_LRScheduler): The default nnunet LR Scheduler for nnunetv2 2.5.1</p> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>@override\ndef get_lr_scheduler(self, optimizer_key: str, config: Config) -&gt; _LRScheduler:\n    \"\"\"\n    Creates an LR Scheduler similar to the nnunet default except we set max steps to the total number of steps\n    and update every step. Initial and final LR are the same as nnunet, difference is nnunet sets max steps to\n    num \"epochs\", but they define an \"epoch\" as exactly 250 steps. Therefore they update every 250 steps. Override\n    this method to set your own LR scheduler.\n\n    Args:\n        optimizer_key (str): Key of the optimizer to which the scheduler will be applied.\n        config (Config): The server config. This method will look for the\n    Returns:\n        (_LRScheduler): The default nnunet LR Scheduler for nnunetv2 2.5.1\n    \"\"\"\n    if not isinstance(self.nnunet_trainer.lr_scheduler, PolyLRScheduler):\n        log(\n            WARNING,\n            (\n                \"Nnunet seems to have changed their default LR scheduler to \"\n                f\"type: {type(self.nnunet_trainer.lr_scheduler)}. \"\n                \"Using PolyLRScheduler instead. Override or update the \"\n                \"get_lr_scheduler method of nnUNetClient to change this\"\n            ),\n        )\n\n    # Determine total number of steps throughout all FL rounds\n    local_epochs, local_steps, _, _, _ = self.process_config(config)\n    if local_steps is not None:\n        steps_per_round = local_steps\n    elif local_epochs is not None:\n        steps_per_round = local_epochs * len(self.train_loader)\n    else:\n        raise ValueError(\"One of local steps or local epochs must be specified\")\n\n    total_steps = int(config[\"n_server_rounds\"]) * steps_per_round\n\n    # Create and return LR Scheduler Wrapper for the PolyLRScheduler so that it is\n    # compatible with Torch LRScheduler\n    # Create and return LR Scheduler. This is nnunet default for version 2.5.1\n    self.reports_manager.report({\"LR Scheduler\": \"PolyLRScheduler\"})\n    return PolyLRSchedulerWrapper(\n        self.optimizers[optimizer_key],\n        initial_lr=self.nnunet_trainer.initial_lr,\n        max_steps=total_steps,\n    )\n</code></pre> <code></code> <code>create_plans(config)</code> \u00b6 <p>Modifies the provided plans file to work with the local client dataset and then saves it to disk. Requires the local <code>dataset_fingerprint.json</code> to exist, the local <code>dataset_name</code>, <code>plans_name</code>, <code>data_identifier</code> and <code>dataset_json</code>.</p> <p>The following fields are modified:</p> <ul> <li>plans_name</li> <li>dataset_name</li> <li>original_median_shape_after_transp</li> <li>original_median_spacing_after_transp</li> <li>configurations.{config}.data_identifier</li> <li>configurations.{config}.batch_size</li> <li>configurations.{config}.median_image_size_in_voxels</li> <li>foreground_intensity_properties_per_channel</li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config provided by the server. Expects the \"nnunet_plans\" key with a pickled dictionary as the value</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The modified nnunet plans for the client</p> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>def create_plans(self, config: Config) -&gt; dict[str, Any]:\n    \"\"\"\n    Modifies the provided plans file to work with the local client dataset and then saves it to disk. Requires the\n    local ``dataset_fingerprint.json`` to exist, the local ``dataset_name``, ``plans_name``, ``data_identifier``\n    and ``dataset_json``.\n\n    The following fields are modified:\n\n    - plans_name\n    - dataset_name\n    - original_median_shape_after_transp\n    - original_median_spacing_after_transp\n    - configurations.{config}.data_identifier\n    - configurations.{config}.batch_size\n    - configurations.{config}.median_image_size_in_voxels\n    - foreground_intensity_properties_per_channel\n\n    Args:\n        config (Config): The config provided by the server. Expects the \"nnunet_plans\" key with a pickled\n            dictionary as the value\n\n    Returns:\n        (dict[str, Any]): The modified nnunet plans for the client\n    \"\"\"\n    # TODO: Make this an external function or part of another class and explicitly accept the required arguments\n    # rather than using class attributes.\n\n    # Get the source nnunet plans specified by the server\n    plans = pickle.loads(narrow_dict_type(config, \"nnunet_plans\", bytes))\n\n    # Change plans name.\n    if self.plans_name is None:\n        self.plans_name = f\"FL-{plans['plans_name']}-{self.dataset_id}local\"\n\n    plans[\"source_plans_name\"] = plans[\"plans_name\"]\n    plans[\"plans_name\"] = self.plans_name\n\n    # Change dataset name\n    plans[\"dataset_name\"] = self.dataset_name\n\n    # Load the dataset fingerprint for the local client dataset\n    fp_path = Path(nnUNet_preprocessed) / self.dataset_name / \"dataset_fingerprint.json\"\n    assert fp_path.exists(), \"Could not find the dataset fingerprint file\"\n    fp = load_json(fp_path)\n\n    # Change the foreground intensity properties per channel\n    plans[\"foreground_intensity_properties_per_channel\"] = fp[\"foreground_intensity_properties_per_channel\"]\n\n    # Compute the median image size and spacing of the local client dataset\n    median_shape = np.median(fp[\"shapes_after_crop\"], axis=0)[plans[\"transpose_forward\"]]\n    median_spacing = np.median(fp[\"spacings\"], axis=0)[plans[\"transpose_forward\"]]\n    plans[\"original_median_shape_after_transp\"] = [int(round(i)) for i in median_shape]\n    plans[\"original_median_spacing_after_transp\"] = [float(i) for i in median_spacing]\n\n    # Get the spacing that the network will resample to. Need to check if samples are 2d or 3d\n    fullres_cfg = \"3d_fullres\" if \"3d_fullres\" in plans[\"configurations\"] else \"2d\"\n    target_spacing = plans[\"configurations\"][fullres_cfg][\"spacing\"]\n\n    # Get the median shape after resampling to the desired/input voxel spacing\n    resampled_shapes = [\n        compute_new_shape(i, j, target_spacing) for i, j in zip(fp[\"shapes_after_crop\"], fp[\"spacings\"])\n    ]\n    resampled_median_shape = np.median(resampled_shapes, axis=0)[plans[\"transpose_forward\"]].tolist()\n\n    # Change data identifier\n    if self.data_identifier is None:\n        self.data_identifier = self.plans_name\n\n    # To be consistent with nnunet, a batch cannot contain more than 5% of the voxels in the dataset.\n    max_voxels = np.prod(resampled_median_shape, dtype=np.float64) * self.dataset_json[\"numTraining\"] * 0.05\n\n    # Iterate through nnunet configs in plans file\n    for c in plans[\"configurations\"]:\n        # Change the data identifier\n        plans[\"configurations\"][c][\"data_identifier\"] = self.data_identifier + \"_\" + c\n\n        # Ensure batch size is at least 2 and at most 5 percent of dataset\n        # Otherwise we keep it the same as it affects the target VRAM consumption\n        if \"batch_size\" in plans[\"configurations\"][c]:\n            old_bs = plans[\"configurations\"][c][\"batch_size\"]\n            bs_5percent = round(max_voxels / np.prod(plans[\"configurations\"][c][\"patch_size\"], dtype=np.float64))\n            new_bs = max(min(old_bs, bs_5percent), 2)\n            plans[\"configurations\"][c][\"batch_size\"] = new_bs\n        else:\n            log(\n                WARNING,\n                f\"Did not find a 'batch_size' key in the nnunet plans dict for nnunet config: {c}\",\n            )\n\n        # Update median shape of resampled input images\n        if str(c).startswith(\"2d\"):\n            plans[\"configurations\"][c][\"median_image_size_in_voxels\"] = resampled_median_shape[1:]\n        else:\n            plans[\"configurations\"][c][\"median_image_size_in_voxels\"] = resampled_median_shape\n\n    # Save local plans file\n    os.makedirs(join(nnUNet_preprocessed, self.dataset_name), exist_ok=True)\n    plans_save_path = join(nnUNet_preprocessed, self.dataset_name, self.plans_name + \".json\")\n    save_json(plans, plans_save_path, sort_keys=False)\n    return plans\n</code></pre> <code></code> <code>maybe_preprocess(nnunet_config)</code> \u00b6 <p>Checks if preprocessed data for current plans exists and if not preprocesses the nnunet_raw dataset. The preprocessed data is saved in '{nnUNet_preprocessed}/{dataset_name}/{data_identifier} as follows.</p> <ul> <li><code>nnUNet_preprocessed</code> is the directory specified by the <code>nnUNet_preprocessed</code> environment variable.</li> <li><code>dataset_name</code> is the nnunet dataset name (e.g. Dataset123_MyDataset)</li> <li><code>data_identifier</code> is <code>{self.data_identifier}_{self.nnunet_config}</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>nnunet_config</code> <code>NnunetConfig</code> <p>The nnunet config as a <code>NnunetConfig</code> Enum. Enum type ensures nnunet config is valid</p> required Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>@use_default_signal_handlers  # Preprocessing spawns subprocesses\ndef maybe_preprocess(self, nnunet_config: NnunetConfig) -&gt; None:\n    \"\"\"\n    Checks if preprocessed data for current plans exists and if not preprocesses the nnunet_raw dataset. The\n    preprocessed data is saved in '{nnUNet_preprocessed}/{dataset_name}/{data_identifier} as follows.\n\n    - ``nnUNet_preprocessed`` is the directory specified by the ``nnUNet_preprocessed`` environment variable.\n    - ``dataset_name`` is the nnunet dataset name (e.g. Dataset123_MyDataset)\n    - ``data_identifier`` is ``{self.data_identifier}_{self.nnunet_config}``\n\n    Args:\n        nnunet_config (NnunetConfig): The nnunet config as a ``NnunetConfig`` Enum. Enum type ensures nnunet\n            config is valid\n    \"\"\"\n    assert self.data_identifier is not None, \"Was expecting data identifier to be initialized in self.create_plans\"\n\n    # Preprocess data if it's not already there\n    if self.always_preprocess or not exists(self.nnunet_trainer.preprocessed_dataset_folder):\n        if self.verbose:\n            log(INFO, f\"\\tPreprocessing local client dataset: {self.dataset_name}\")\n        # Unless log level is debugging or lower, hide nnunet output\n        with redirect_stdout(self.stream2debug):\n            preprocess_dataset(\n                dataset_id=self.dataset_id,\n                plans_identifier=self.plans_name,\n                num_processes=[NNUNET_DEFAULT_NP[nnunet_config]],\n                configurations=[nnunet_config.value],\n            )\n    elif self.verbose:\n        log(\n            INFO,\n            \"\\tnnunet preprocessed data seems to already exist. Skipping preprocessing\",\n        )\n</code></pre> <code></code> <code>maybe_extract_fingerprint()</code> \u00b6 <p>Checks if nnunet dataset fingerprint already exists and if not extracts one from the dataset.</p> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>@use_default_signal_handlers  # Fingerprint extraction spawns subprocess\ndef maybe_extract_fingerprint(self) -&gt; None:\n    \"\"\"Checks if nnunet dataset fingerprint already exists and if not extracts one from the dataset.\"\"\"\n    # Check first whether this client instance has already extracted a dataset fp\n    # Possible if the client was asked to generate the nnunet plans for the server\n    if not self.fingerprint_extracted:\n        fp_path = join(nnUNet_preprocessed, self.dataset_name, \"dataset_fingerprint.json\")\n        # Check if fp already exists or if we want to redo fp extraction\n        if self.always_preprocess or not exists(fp_path):\n            start = time.time()\n            # Unless log level is DEBUG or lower hide nnunet output\n            with redirect_stdout(self.stream2debug):\n                extract_fingerprints(dataset_ids=[self.dataset_id])\n            if self.verbose:\n                log(\n                    INFO,\n                    f\"\\tExtracted dataset fingerprint in {time.time() - start:.1f}s\",\n                )\n        elif self.verbose:\n            log(\n                INFO,\n                \"\\tnnunet dataset fingerprint already exists. Skipping fingerprint extraction\",\n            )\n    elif self.verbose:\n        log(\n            INFO,\n            \"\\tThis client has already extracted the dataset fingerprint during this session. Skipping.\",\n        )\n    # Avoid extracting fingerprint multiple times when always_preprocess is true\n    self.fingerprint_extracted = True\n</code></pre> <code></code> <code>setup_client(config)</code> \u00b6 <p>Ensures the necessary files for training are on disk and initializes several class attributes that depend on values in the config from the server. This is called once when the client is sampled by the server for the first time.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config file from the server. The <code>nnUNetClient</code> expects the keys 'nnunet_config' and 'nnunet_plans' in addition to those required by <code>BasicClient</code></p> required Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>@use_default_signal_handlers\n@override\ndef setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Ensures the necessary files for training are on disk and initializes several class attributes that depend on\n    values in the config from the server. This is called once when the client is sampled by the server for the\n    first time.\n\n    Args:\n        config (Config): The config file from the server. The ``nnUNetClient`` expects the keys 'nnunet_config'\n            and 'nnunet_plans' in addition to those required by ``BasicClient``\n    \"\"\"\n    log(INFO, \"Setting up the nnUNetClient\")\n\n    # Empty gpu cache because nnunet does it\n    self.empty_cache()\n\n    # Get nnunet config\n    self.nnunet_config = NnunetConfig(config[\"nnunet_config\"])\n\n    # Check if dataset fingerprint has already been extracted\n    self.maybe_extract_fingerprint()\n\n    # Create the nnunet plans for the local client\n    if self.plans is None:\n        self.plans = self.create_plans(config=config)\n\n    # Unless log level is DEBUG or lower hide nnunet output\n    with redirect_stdout(self.stream2debug):\n        # Create the nnunet trainer\n        self.nnunet_trainer = self.nnunet_trainer_class(\n            plans=self.plans,\n            configuration=self.nnunet_config.value,\n            fold=self.fold,\n            dataset_json=self.dataset_json,\n            device=self.device,\n            **self.nnunet_trainer_class_kwargs,\n        )\n        # nnunet_trainer initialization\n        self.nnunet_trainer.initialize()\n        # This is done by nnunet_trainer in self.on_train_start, we\n        # do it manually since nnunet_trainer not being used for training\n        self.nnunet_trainer.set_deep_supervision_enabled(self.nnunet_trainer.enable_deep_supervision)\n\n    # Prevent nnunet from generating log files and delete empty output directories\n    os.remove(self.nnunet_trainer.log_file)\n    self.nnunet_trainer.log_file = os.devnull\n    output_folder = Path(self.nnunet_trainer.output_folder)\n    while len(os.listdir(output_folder)) == 0:\n        os.rmdir(output_folder)\n        output_folder = output_folder.parent\n\n    # Preprocess nnunet_raw data if needed\n    self.maybe_preprocess(self.nnunet_config)\n    start = time.time()\n    unpack_dataset(  # Reduces load on CPU and RAM during training\n        folder=self.nnunet_trainer.preprocessed_dataset_folder,\n        unpack_segmentation=self.nnunet_trainer.unpack_dataset,\n        overwrite_existing=self.always_preprocess,\n        verify_npy=True,\n    )\n    if self.verbose:\n        log(INFO, f\"\\tUnpacked dataset in {time.time() - start:.1f}s\")\n\n    # We have to call parent method after setting up nnunet trainer\n    super().setup_client(config)\n</code></pre> <code></code> <code>predict_with_model(model, input)</code> \u00b6 <p>Generate model outputs. Overridden because nnunets outputs lists when deep supervision is on so we have to reformat the output into dicts.</p> <p>Additionally if device type is cuda, loss computed in mixed precision.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model used to make predictions</p> required <code>input</code> <code>TorchInputType</code> <p>The model inputs</p> required <p>Returns:</p> Type Description <code>tuple[TorchPredType, dict[str, Tensor]]</code> <p>A tuple in which the first element model outputs indexed by name. The second element is unused by this subclass and therefore is always an empty dict</p> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>@override\ndef predict_with_model(\n    self, model: torch.nn.Module, input: TorchInputType\n) -&gt; tuple[TorchPredType, dict[str, torch.Tensor]]:\n    \"\"\"\n    Generate model outputs. Overridden because nnunets outputs lists when deep supervision is on so we have to\n    reformat the output into dicts.\n\n    Additionally if device type is cuda, loss computed in mixed precision.\n\n    Args:\n        model (nn.Module): The model used to make predictions\n        input (TorchInputType): The model inputs\n\n    Returns:\n        (tuple[TorchPredType, dict[str, torch.Tensor]]): A tuple in which the first element model outputs indexed\n            by name. The second element is unused by this subclass and therefore is always an empty dict\n    \"\"\"\n    if isinstance(input, torch.Tensor):\n        # If device type is cuda, nnUNet defaults to mixed precision forward pass\n        if self.device.type == \"cuda\":\n            with torch.autocast(self.device.type, enabled=True):\n                output = model(input)\n        else:\n            output = model(input)\n    else:\n        raise TypeError('\"input\" must be of type torch.Tensor for nnUNetClient')\n\n    if isinstance(output, torch.Tensor):\n        return {\"prediction\": output}, {}\n    # If output is a list or tuple then deep supervision is on and we need to convert preds into a dict\n    if isinstance(output, (list, tuple)):\n        num_spatial_dims = NNUNET_N_SPATIAL_DIMS[self.nnunet_config]\n        preds = convert_deep_supervision_list_to_dict(output, num_spatial_dims)\n        return preds, {}\n    raise TypeError(\n        \"Was expecting nnunet model output to be either a torch.Tensor or a list/tuple of torch.Tensors\"\n    )\n</code></pre> <code></code> <code>compute_loss_and_additional_losses(preds, features, target)</code> \u00b6 <p>Checks the pred and target types and computes the loss. If device type is cuda, loss computed in mixed precision.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Dictionary of model output tensors indexed by name</p> required <code>features</code> <code>dict[str, Tensor]</code> <p>Not used by this subclass</p> required <code>target</code> <code>TorchTargetType</code> <p>The targets to evaluate the predictions with. If multiple prediction tensors are given, target must be a dictionary with the same number of tensors</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor] | None]</code> <p>A tuple where the first element is the loss and the second element is an optional additional loss</p> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>@override\ndef compute_loss_and_additional_losses(\n    self,\n    preds: TorchPredType,\n    features: dict[str, torch.Tensor],\n    target: TorchTargetType,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor] | None]:\n    \"\"\"\n    Checks the pred and target types and computes the loss. If device type is cuda, loss computed in mixed\n    precision.\n\n    Args:\n        preds (TorchPredType): Dictionary of model output tensors indexed by name\n        features (dict[str, torch.Tensor]): Not used by this subclass\n        target (TorchTargetType): The targets to evaluate the predictions with. If multiple prediction tensors\n            are given, target must be a dictionary with the same number of tensors\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor] | None]): A tuple where the first element is the loss and the\n            second element is an optional additional loss\n    \"\"\"\n    # If deep supervision is turned on we must convert loss and target dicts into lists\n    loss_preds = prepare_loss_arg(preds)\n    loss_targets = prepare_loss_arg(target)\n\n    # Ensure we have the same number of predictions and targets\n    assert isinstance(loss_preds, type(loss_targets)), (\n        f\"Got unexpected types for preds and targets: {type(loss_preds)} and {type(loss_targets)}\"\n    )\n\n    if isinstance(loss_preds, list):\n        assert len(loss_preds) == len(loss_targets), (\n            \"Was expecting the number of predictions and targets to be the same. \"\n            f\"Got {len(loss_preds)} predictions and {len(loss_targets)} targets.\"\n        )\n\n    # If device type is cuda, nnUNet defaults to compute loss in mixed precision\n    if self.device.type == \"cuda\":\n        with torch.autocast(self.device.type, enabled=True):\n            loss = self.criterion(loss_preds, loss_targets), None\n    else:\n        loss = self.criterion(loss_preds, loss_targets), None\n\n    return loss\n</code></pre> <code></code> <code>mask_data(pred, target)</code> \u00b6 <p>Masks the pred and target tensors according to nnunet <code>ignore_label</code>. The number of classes in the input tensors should be at least 3 corresponding to 2 classes for binary segmentation and 1 class which is the ignore class specified by ignore label.</p> <p>https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/ignore_label.md</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>The one hot encoded predicted segmentation maps with shape <code>(batch, classes, x, y(, z))</code></p> required <code>target</code> <code>TorchTargetType</code> <p>The ground truth segmentation map with shape <code>(batch, classes, x, y(, z))</code></p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple of:</p> <ul> <li>torch.Tensor: The masked one hot encoded predicted segmentation maps</li> <li>torch.Tensor: The masked target segmentation maps</li> </ul> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>def mask_data(self, pred: torch.Tensor, target: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Masks the pred and target tensors according to nnunet ``ignore_label``. The number of classes in the input\n    tensors should be at least 3 corresponding to 2 classes for binary segmentation and 1 class which is\n    the ignore class specified by ignore label.\n\n    https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/ignore_label.md\n\n    Args:\n        pred (torch.Tensor): The one hot encoded predicted segmentation maps with shape\n            ``(batch, classes, x, y(, z))``\n        target (TorchTargetType): The ground truth segmentation map with shape ``(batch, classes, x, y(, z))``\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Tuple of:\n\n            - torch.Tensor: The masked one hot encoded predicted segmentation maps\n            - torch.Tensor: The masked target segmentation maps\n    \"\"\"\n    # create mask where 1 is where pixels in target are not ignore label\n    # Modify target to remove the last class which is the ignore_label class\n    new_target = target\n    if self.nnunet_trainer.label_manager.has_regions:  # nnunet returns a ohe target if has_regions is true\n        mask = ~target[:, -1:] if target.dtype == torch.bool else 1 - target[:, -1:]\n        new_target = new_target[:, :-1]  # Remove final ignore_label class from target\n    else:  # target is not one hot encoded\n        mask = (target != self.nnunet_trainer.label_manager.ignore_label).float()\n        # Set ignore label to background essentially removing it as a class\n        new_target[new_target == self.nnunet_trainer.label_manager.ignore_label] = 0\n\n    # Tile the mask to be one hot encoded\n    mask_here = torch.tile(mask, (1, pred.shape[1], *[1 for _ in range(2, pred.ndim)]))\n\n    return (\n        pred * mask_here,\n        new_target,\n    )  # Mask the input tensor and return the modified target\n</code></pre> <code></code> <code>update_metric_manager(preds, target, metric_manager)</code> \u00b6 <p>Update the metrics with preds and target. Overridden because we might need to manipulate inputs due to deep supervision.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>dictionary of model outputs</p> required <code>target</code> <code>TorchTargetType</code> <p>the targets generated by the dataloader to evaluate the preds with</p> required <code>metric_manager</code> <code>MetricManager</code> <p>the metric manager to update</p> required Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>@override\ndef update_metric_manager(\n    self,\n    preds: TorchPredType,\n    target: TorchTargetType,\n    metric_manager: MetricManager,\n) -&gt; None:\n    \"\"\"\n    Update the metrics with preds and target. Overridden because we might need to manipulate inputs due to deep\n    supervision.\n\n    Args:\n        preds (TorchPredType): dictionary of model outputs\n        target (TorchTargetType): the targets generated by the dataloader to evaluate the preds with\n        metric_manager (MetricManager): the metric manager to update\n    \"\"\"\n    log(DEBUG, f\"preds: {preds.keys()}\")\n\n    # If personalized preds will have keys prefixed with \"global-\" and \"local-\",\n    # but we only want to keep the local ones here. NOTE: this will still work\n    # even in the non-personalized case as those preds don't get prefixed with \"global-\"\n    preds = {k: v for k, v in preds.items() if not k.startswith(\"global\")}\n    preds = {k.replace(\"local-\", \"\"): v for k, v in preds.items()}  # no-op in non-personalized case\n\n    if len(preds) &gt; 1:\n        # for nnunet the first pred in the output list is the main one\n        m_pred = convert_deep_supervision_dict_to_list(preds)[0]\n\n    if isinstance(target, torch.Tensor):\n        m_target = target\n    elif isinstance(target, dict):\n        if len(target) &gt; 1:\n            # If deep supervision is in use, we drop the additional targets\n            # when calculating the metrics as we only care about the\n            # original target which by default in nnunet is at index 0\n            m_target = convert_deep_supervision_dict_to_list(target)[0]\n        else:\n            m_target = list(target.values())[0]\n    else:\n        raise TypeError(\"Was expecting target to be type dict[str, torch.Tensor] or torch.Tensor\")\n\n    # Check if target is one hot encoded. Prediction always is for nnunet\n    # Add channel dimension if there isn't one\n    if m_pred.ndim != m_target.ndim:\n        m_target = m_target.view(m_target.shape[0], 1, *m_target.shape[1:])\n\n    # One hot encode targets if needed\n    if m_pred.shape != m_target.shape:\n        m_target_one_hot = torch.zeros(m_pred.shape, device=self.device, dtype=torch.bool)\n        # This is how nnunet does ohe in their functions\n        # Its a weird function that is not intuitive\n        # CAREFUL: Notice the underscore at the end of the scatter function.\n        # It makes a difference, was a hard bug to find!\n        m_target_one_hot.scatter_(1, m_target.long(), 1)\n    else:\n        m_target_one_hot = m_target\n\n    # Check if ignore label is in use. The nnunet loss figures this out on\n    # it's own, but we do it manually here for the metrics\n    if self.nnunet_trainer.label_manager.ignore_label is not None:\n        m_pred, m_target_one_hot = self.mask_data(m_pred, m_target_one_hot)\n\n    # m_pred is one hot encoded (OHE) output logits. Maybe masked by ignore label\n    # m_target_one_hot is OHE boolean label. Maybe masked by ignore label\n    metric_manager.update({\"prediction\": m_pred}, m_target_one_hot)\n</code></pre> <code></code> <code>empty_cache()</code> \u00b6 <p>Checks torch device and empties cache before training to optimize VRAM usage.</p> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>def empty_cache(self) -&gt; None:\n    \"\"\"Checks torch device and empties cache before training to optimize VRAM usage.\"\"\"\n    if self.device.type == \"cuda\":\n        torch.cuda.empty_cache()\n    elif self.device.type == \"mps\":\n        torch.mps.empty_cache()\n</code></pre> <code></code> <code>get_properties(config)</code> \u00b6 <p>Return properties (sample counts and nnunet plans) of client.</p> <p>If nnunet plans are not provided by the server, creates a new set of nnunet plans from the local client dataset. These plans are intended to be used for initializing global nnunet plans when they are not provided.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server</p> required <p>Returns:</p> Type Description <code>dict[str, Scalar]</code> <p>A dictionary containing the train and validation sample counts as well as the serialized nnunet plans.</p> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>@use_default_signal_handlers  # Experiment planner spawns a process I think\n@override\ndef get_properties(self, config: Config) -&gt; dict[str, Scalar]:\n    \"\"\"\n    Return properties (sample counts and nnunet plans) of client.\n\n    If nnunet plans are not provided by the server, creates a new set of nnunet plans from the local client\n    dataset. These plans are intended to be used for initializing global nnunet plans when they are not\n    provided.\n\n    Args:\n        config (Config): The config from the server\n\n    Returns:\n        (dict[str, Scalar]): A dictionary containing the train and validation sample counts as well as the\n            serialized nnunet plans.\n    \"\"\"\n    # Check if nnunet plans have already been initialized\n    if \"nnunet_plans\" not in config:\n        log(INFO, \"Initializing the global plans using local dataset\")\n        # Local client will initialize global nnunet plans\n        # Check if local nnunet dataset fingerprint needs to be extracted\n        self.maybe_extract_fingerprint()\n\n        # Create experiment planner and plans.\n        # Plans name must be temp_plans so that we can safely delete the generated plans file\n        planner = ExperimentPlanner(dataset_name_or_id=self.dataset_id, plans_name=\"temp_plans\")\n\n        # Unless log level is DEBUG or lower, hide nnunet output\n        with redirect_stdout(self.stream2debug):\n            plans = planner.plan_experiment()\n\n        # Set plans name to local dataset so we know the source\n        plans[\"plans_name\"] = self.dataset_name + \"_plans\"\n        plans_bytes = pickle.dumps(plans)\n\n        # Remove plans file . A new one will be generated in self.setup_client\n        plans_path = join(\n            nnUNet_preprocessed,\n            self.dataset_name,\n            planner.plans_identifier + \".json\",\n        )\n        if exists(plans_path):\n            os.remove(plans_path)\n\n        # Update local config with plans\n        config[\"nnunet_plans\"] = plans_bytes\n\n    # Get client properties. We are now sure that config contains plans\n    properties = super().get_properties(config)\n    properties[\"nnunet_plans\"] = config[\"nnunet_plans\"]\n\n    # super.get_properties should setup the client anyways, but we can add a check here as a precaution.\n    if not self.initialized:\n        self.setup_client(config)  # Client must be setup in order to initialize nnunet_trainer\n\n    # Add additional properties from nnunet trainer to properties dict. We may want to add more keys later\n    properties[\"num_input_channels\"] = self.nnunet_trainer.num_input_channels\n    properties[\"num_segmentation_heads\"] = self.nnunet_trainer.label_manager.num_segmentation_heads\n    properties[\"enable_deep_supervision\"] = self.nnunet_trainer.enable_deep_supervision\n\n    return properties\n</code></pre> <code></code> <code>shutdown_dataloader(dataloader, dl_name=None)</code> \u00b6 <p>The nnunet dataloader/augmenter uses multiprocessing under the hood, so the shutdown method terminates the child processes gracefully.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>The dataloader to shutdown</p> required <code>dl_name</code> <code>str | None</code> <p>A string that identifies the dataloader to shutdown. Used for logging purposes. Defaults to None</p> <code>None</code> Source code in <code>fl4health/clients/flexible/nnunet.py</code> <pre><code>def shutdown_dataloader(self, dataloader: DataLoader | None, dl_name: str | None = None) -&gt; None:\n    \"\"\"\n    The nnunet dataloader/augmenter uses multiprocessing under the hood, so the shutdown method terminates the\n    child processes gracefully.\n\n    Args:\n        dataloader (DataLoader): The dataloader to shutdown\n        dl_name (str | None): A string that identifies the dataloader to shutdown. Used for logging purposes.\n            Defaults to None\n    \"\"\"\n    if dataloader is not None and isinstance(dataloader, NnUNetDataLoaderWrapper):\n        if self.verbose:\n            log(INFO, f\"\\tShutting down nnunet dataloader: {dl_name}\")\n        dataloader.shutdown()\n\n    del dataloader\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client","title":"<code>gpfl_client</code>","text":""},{"location":"api/#fl4health.clients.gpfl_client.GpflClient","title":"<code>GpflClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>class GpflClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n        lam: float = 0.01,\n        mu: float = 0.01,\n    ) -&gt; None:\n        \"\"\"\n        This client is used to perform client-side training associated with the GPFL method described in\n        https://arxiv.org/abs/2308.10279.\n\n        In this approach, the client's model is sequentially split into a feature extractor and a head module.\n        The client also has two extra modules that are trained alongside the main model: a CoV (Conditional Value),\n        and a GCE (Global Category Embedding) module. These sub-modules are trained in the client and shared\n        with the server alongside the feature extractor. In simple words, CoV takes in the output of the\n        feature extractor (feature_tensor) and maps it into two feature tensors (personal f_p and general f_g)\n        computed through affine mapping. `f_p`is fed into the head module for classification, while `f_g` is used\n        to train the GCE module. GCE is a lookup table that stores a global representative embedding for each class.\n        The GCE module is used to generate two conditional tensors: ``global_conditional_input`` and\n        ``personalized_conditional_input`` referred to in the paper as g and p_i, respectively.\n        These conditional inputs are then used in the CoV module. All the components are trained simultaneously via\n        a combined loss.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\"\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            lam (float, optional): A hyperparameter that controls the weight of the GCE magnitude-level\n                global loss. Defaults to 0.01.\n            mu (float, optional): A hyperparameter that acts as the weight of the L2 regularization on the GCE and CoV\n                modules. This value is used as the optimizers' weight decay parameter. This can be set in\n                ``get_optimizer`` function defined by the client user, or if it is not set by the user, it will be\n                set in ``set_optimizer`` method. Defaults to 0.01.\n        \"\"\"\n        self.model: GpflModel\n        self.gce_frozen: Gce\n\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.lam = lam\n        self.mu = mu\n        if self.lam == 0.0:\n            log(\n                WARNING,\n                \"Lambda parameter is set to 0.0, which means that the magnitude-level global loss will not be used.\",\n            )\n        # If self.mu is set to 0.0, it means user does not want to use L2 regularization.\n        if self.mu == 0.0:\n            log(\n                WARNING,\n                \"Mu parameter is set to 0.0, which means that the GCE and CoV modules will not be regularized.\",\n            )\n\n    def get_optimizer(self, config: Config) -&gt; dict[str, Optimizer]:\n        \"\"\"\n        Returns a dictionary with model, gce, and cov optimizers with string keys \"model\", \"gce\",\n        and \"cov\" respectively.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (dict[str, Optimizer]): A dictionary of optimizers defined by the user\n        \"\"\"\n        raise NotImplementedError(\n            \"User Clients must define a function that returns a dict[str, Optimizer] with keys 'model',\"\n            \" 'gce', and 'cov',\"\n            \"defining separate optimizers for different modules of the client.\"\n        )\n\n    def set_optimizer(self, config: Config) -&gt; None:\n        \"\"\"\n        This function simply ensures that the optimizers setup by the user have the proper keys\n        and that there are three optimizers.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        optimizers = self.get_optimizer(config)\n        assert isinstance(optimizers, dict) and {\"model\", \"gce\", \"cov\"} == set(optimizers.keys()), (\n            \"Three optimizers must be defined with keys 'model', 'gce', and 'cov'. Now, only \"\n            f\"{optimizers.keys()} optimizers are defined.\"\n        )\n        # If user has specified weight decay for the GCE or CoV optimizers,\n        # we will log a warning before overwriting these values with mu.\n        user_gce_weight_decay: float = optimizers[\"gce\"].param_groups[0].get(\"weight_decay\", 0.0)\n        user_cov_weight_decay: float = optimizers[\"cov\"].param_groups[0].get(\"weight_decay\", 0.0)\n        if user_gce_weight_decay != 0.0 or user_cov_weight_decay != 0.0:\n            log(\n                WARNING,\n                \"Your gce or cov optimizer weight decay will be overwritten by the mu parameter.\",\n            )\n        # Set the weight decay for the GCE and CoV optimizers to self.mu to enable\n        # L2 regularization in the loss.\n        log(INFO, f\"Setting the GCE optimizer's weight decay to mu = {self.mu}\")\n        for param_group in optimizers[\"gce\"].param_groups:\n            param_group[\"weight_decay\"] = self.mu\n\n        log(INFO, f\"Setting the CoV optimizer's weight decay to my = {self.mu}\")\n        for param_group in optimizers[\"cov\"].param_groups:\n            param_group[\"weight_decay\"] = self.mu\n        self.optimizers = optimizers\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        \"\"\"\n        GPFL client uses a fixed layer exchanger to exchange layers in three sub-modules.\n        Sub-modules to be exchanged are defined in the ``GpflModel`` class.\n\n        Args:\n            config (Config): Config from the server..\n\n        Returns:\n            (ParameterExchanger): FixedLayerExchanger used to exchange a set of fixed and specific layers.\n        \"\"\"\n        assert isinstance(self.model, GpflModel)\n        return FixedLayerExchanger(self.model.layers_to_exchange())\n\n    def calculate_class_sample_proportions(self) -&gt; torch.Tensor:\n        \"\"\"\n        This method is used to compute the class sample proportions based on the training data.\n        It computes the proportion of samples for each class in the training dataset.\n\n        Returns:\n            (torch.Tensor): A tensor containing the proportion of samples for each class.\n        \"\"\"\n        class_sample_proportion = torch.zeros(self.num_classes, device=self.device)\n        one_hot_n_dim = 2  # To avoid having magic numbers\n        for _, target in self.train_loader:\n            if target.dim() == one_hot_n_dim:  # Target is one-hot encoded\n                assert target.shape[1] == self.num_classes, (\n                    \"Shape of the one-hot encoded labels should be (batch_size, num_classes).\"\n                )\n            else:  # Target is not one-hot encoded\n                target = one_hot(target, num_classes=self.num_classes).to(self.device)\n\n            # Compute the proportion of samples for each class by summing the one-hot encoded targets along each column\n            # which gives the count of samples per class.\n            class_sample_proportion += target.sum(0)\n\n        # Divide the number of samples per class by the total number of samples (sum of all ones).\n        class_sample_proportion /= class_sample_proportion.sum()\n        return class_sample_proportion\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        In addition to dataloaders, optimizers, parameter exchangers, a few GPFL specific parameters\n        are set up in this method. This includes the number of classes, feature dimension,\n        and the sample per class tensor. The global and personalized conditional inputs are also initialized.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        super().setup_client(config)\n\n        # Initiate some parameters related to GPFL.\n        # ``num_class`` and ``feature_dim`` are essential parts of the GPFL model construction.\n        self.num_classes = self.model.num_classes\n        self.feature_dim = self.model.feature_dim\n        # class_sample_proportion tensor is used to compute personalized conditional input.\n        self.class_sample_proportion = self.calculate_class_sample_proportions()\n\n    def compute_conditional_inputs(self) -&gt; None:\n        \"\"\"\n        Calculates the conditional inputs (p_i and g) for the CoV module based on the new GCE from the server.\n        The ``self.global_conditional_input`` and ``self.personalized_conditional_input`` tensors are computed\n        based on a frozen GCE model and the sample per class tensor. These tensors are fixed in each client round,\n        and are recomputed when a new GCE module is shared by the server in every client round.\n        \"\"\"\n        # Initiate g(global_conditional_input) and p_i(personalized_conditional_input) tensors to zeros.\n        self.global_conditional_input = torch.zeros(self.feature_dim).to(self.device)\n        self.personalized_conditional_input = torch.zeros(self.feature_dim).to(self.device)\n\n        embeddings = self.gce_frozen.embedding.weight\n        for i, embedding in enumerate(embeddings):\n            self.global_conditional_input += embedding\n            self.personalized_conditional_input += embedding * self.class_sample_proportion[i]\n\n        self.global_conditional_input = embeddings.sum(0) / self.num_classes\n        self.personalized_conditional_input = (\n            torch.matmul(embeddings.T, self.class_sample_proportion) / self.num_classes\n        )\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        \"\"\"\n        Updates the frozen GCE model and computes the conditional inputs before training starts.\n\n        Args:\n            current_server_round (int): The number of current server round.\n        \"\"\"\n        # Update the frozen GCE\n        cloned_model = clone_and_freeze_model(self.model.gce)\n        assert isinstance(cloned_model, Gce)\n        self.gce_frozen = cloned_model\n        # Update conditional inputs before training\n        self.compute_conditional_inputs()\n\n        return super().update_before_train(current_server_round)\n\n    def transform_input(self, input: TorchInputType) -&gt; TorchInputType:\n        \"\"\"\n        Extend the input dictionary with ``global_conditional_input`` and ``personalized_conditional_input``\n        tensors. This let's use provide these additional tensor to the GPFL model .\n\n        Args:\n            input (TorchInputType): Input tensor.\n\n        Returns:\n            (TorchInputType): Transformed input tensor.\n        \"\"\"\n        # Attach the global and personalized conditional inputs to the input\n        if isinstance(input, torch.Tensor):\n            return {\n                \"input\": input,\n                \"global_conditional_input\": self.global_conditional_input.detach(),\n                \"personalized_conditional_input\": self.personalized_conditional_input.detach(),\n            }\n        assert isinstance(input, dict)\n        input.update(\n            {\n                \"global_conditional_input\": self.global_conditional_input.detach(),\n                \"personalized_conditional_input\": self.personalized_conditional_input.detach(),\n            }\n        )\n        return input\n\n    def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Given a single batch of input and target data, generate predictions, compute loss, update parameters and\n        optionally update metrics if they exist. (i.e. backprop on a single batch of data).\n        Assumes ``self.model`` is in train mode already.\n\n        Args:\n            input (TorchInputType): The input to be fed into the model.\n            target (TorchTargetType): The target corresponding to the input.\n\n        Returns:\n            (tuple[TrainingLosses, TorchPredType]): The losses object from the train step along with\n        `            a dictionary of any predictions produced by the model.\n        \"\"\"\n        # Clear gradients from the optimizers if they exist\n        self.optimizers[\"model\"].zero_grad()\n        self.optimizers[\"gce\"].zero_grad()\n        self.optimizers[\"cov\"].zero_grad()\n\n        # Call user defined methods to get predictions and compute loss\n        input = self.transform_input(input)\n        preds, features = self.predict(input)\n        target = self.transform_target(target)\n        losses = self.compute_training_loss(preds, features, target)\n\n        # Compute backward pass and update parameters with optimizer\n        losses.backward[\"backward\"].backward()\n        self.transform_gradients(losses)\n        self.optimizers[\"model\"].step()\n        self.optimizers[\"gce\"].step()\n        self.optimizers[\"cov\"].step()\n\n        return losses, preds\n\n    def compute_magnitude_level_loss(\n        self,\n        global_features: torch.Tensor,\n        target: TorchTargetType,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Computes magnitude level loss corresponds to \\\\(\\\\mathcal{L}_i^{\\text{mlg}}\\\\) in the paper.\n\n        Args:\n            global_features (torch.Tensor): global features computed in this client.\n            target (TorchTargetType): Either a tensor of class indices or one-hot encoded tensors.\n\n        Returns:\n            (torch.Tensor): L2 norm loss between the global features and the frozen GCE's global features.\n        \"\"\"\n        # In magnitude level loss, GCE's embedding table is frozen, and the goal is to train\n        # the model to generate good global features by making the generated embeddings closer to\n        # frozen GCE's global embeddings.\n        assert isinstance(target, torch.Tensor), \"GPFL clients take only tensor targets.\"\n        return torch.norm(global_features - self.gce_frozen.lookup(target).detach(), 2)\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes the combined training loss given predictions, global features of the model, and ground truth data.\n        GPFL loss is a combined loss and is defined as ``prediction_loss + gce_softmax_loss + magnitude_level_loss``.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n                in preds will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses\n                indexed by name.\n        \"\"\"\n        # The loss used during training is a combination of the prediction loss (CrossEntropy used in the paper),\n        # angel-level (GCE loss) and magnitude-level global losses.\n        prediction_loss, _ = self.compute_loss_and_additional_losses(preds, features, target)\n        # ``gce_softmax_loss`` corresponds to \\mathcal{L}_i^{\\text{alg}} in the paper.\n        gce_softmax_loss = self.model.gce(features[\"global_features\"], target)\n        # ``magnitude_level_loss`` corresponds to \\mathcal{L}_i^{\\text{mlg}} in the paper.\n        magnitude_level_loss = self.compute_magnitude_level_loss(features[\"global_features\"], target)\n        # Note that L2 regularization terms are included in the optimizers.\n        loss = prediction_loss + gce_softmax_loss + magnitude_level_loss * self.lam\n        additional_losses = {\n            \"prediction_loss\": prediction_loss.clone(),\n            \"gce_softmax_loss\": gce_softmax_loss.clone(),\n            \"magnitude_level_loss\": magnitude_level_loss.clone(),\n        }\n        return TrainingLosses(backward=loss, additional_losses=additional_losses)\n\n    def val_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[EvaluationLosses, TorchPredType]:\n        \"\"\"\n        Before performing validation, we need to transform the input and attach the global and personalized\n        conditional tensors to the input.\n\n        Args:\n            input (TorchInputType): Input based on the training data.\n            target (TorchTargetType): The target corresponding to the input..\n\n        Returns:\n            (tuple[EvaluationLosses, TorchPredType]: tuple[EvaluationLosses, TorchPredType]):\n                The losses object from the val step along with a dictionary of the predictions produced\n                by the model.\n        \"\"\"\n        input = self.transform_input(input)\n        return super().val_step(input, target)\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None, lam=0.01, mu=0.01)</code>","text":"<p>This client is used to perform client-side training associated with the GPFL method described in https://arxiv.org/abs/2308.10279.</p> <p>In this approach, the client's model is sequentially split into a feature extractor and a head module. The client also has two extra modules that are trained alongside the main model: a CoV (Conditional Value), and a GCE (Global Category Embedding) module. These sub-modules are trained in the client and shared with the server alongside the feature extractor. In simple words, CoV takes in the output of the feature extractor (feature_tensor) and maps it into two feature tensors (personal f_p and general f_g) computed through affine mapping. <code>f_p</code>is fed into the head module for classification, while <code>f_g</code> is used to train the GCE module. GCE is a lookup table that stores a global representative embedding for each class. The GCE module is used to generate two conditional tensors: <code>global_conditional_input</code> and <code>personalized_conditional_input</code> referred to in the paper as g and p_i, respectively. These conditional inputs are then used in the CoV module. All the components are trained simultaneously via a combined loss.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\"</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>lam</code> <code>float</code> <p>A hyperparameter that controls the weight of the GCE magnitude-level global loss. Defaults to 0.01.</p> <code>0.01</code> <code>mu</code> <code>float</code> <p>A hyperparameter that acts as the weight of the L2 regularization on the GCE and CoV modules. This value is used as the optimizers' weight decay parameter. This can be set in <code>get_optimizer</code> function defined by the client user, or if it is not set by the user, it will be set in <code>set_optimizer</code> method. Defaults to 0.01.</p> <code>0.01</code> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n    lam: float = 0.01,\n    mu: float = 0.01,\n) -&gt; None:\n    \"\"\"\n    This client is used to perform client-side training associated with the GPFL method described in\n    https://arxiv.org/abs/2308.10279.\n\n    In this approach, the client's model is sequentially split into a feature extractor and a head module.\n    The client also has two extra modules that are trained alongside the main model: a CoV (Conditional Value),\n    and a GCE (Global Category Embedding) module. These sub-modules are trained in the client and shared\n    with the server alongside the feature extractor. In simple words, CoV takes in the output of the\n    feature extractor (feature_tensor) and maps it into two feature tensors (personal f_p and general f_g)\n    computed through affine mapping. `f_p`is fed into the head module for classification, while `f_g` is used\n    to train the GCE module. GCE is a lookup table that stores a global representative embedding for each class.\n    The GCE module is used to generate two conditional tensors: ``global_conditional_input`` and\n    ``personalized_conditional_input`` referred to in the paper as g and p_i, respectively.\n    These conditional inputs are then used in the CoV module. All the components are trained simultaneously via\n    a combined loss.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\"\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        lam (float, optional): A hyperparameter that controls the weight of the GCE magnitude-level\n            global loss. Defaults to 0.01.\n        mu (float, optional): A hyperparameter that acts as the weight of the L2 regularization on the GCE and CoV\n            modules. This value is used as the optimizers' weight decay parameter. This can be set in\n            ``get_optimizer`` function defined by the client user, or if it is not set by the user, it will be\n            set in ``set_optimizer`` method. Defaults to 0.01.\n    \"\"\"\n    self.model: GpflModel\n    self.gce_frozen: Gce\n\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.lam = lam\n    self.mu = mu\n    if self.lam == 0.0:\n        log(\n            WARNING,\n            \"Lambda parameter is set to 0.0, which means that the magnitude-level global loss will not be used.\",\n        )\n    # If self.mu is set to 0.0, it means user does not want to use L2 regularization.\n    if self.mu == 0.0:\n        log(\n            WARNING,\n            \"Mu parameter is set to 0.0, which means that the GCE and CoV modules will not be regularized.\",\n        )\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.get_optimizer","title":"<code>get_optimizer(config)</code>","text":"<p>Returns a dictionary with model, gce, and cov optimizers with string keys \"model\", \"gce\", and \"cov\" respectively.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>dict[str, Optimizer]</code> <p>A dictionary of optimizers defined by the user</p> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def get_optimizer(self, config: Config) -&gt; dict[str, Optimizer]:\n    \"\"\"\n    Returns a dictionary with model, gce, and cov optimizers with string keys \"model\", \"gce\",\n    and \"cov\" respectively.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (dict[str, Optimizer]): A dictionary of optimizers defined by the user\n    \"\"\"\n    raise NotImplementedError(\n        \"User Clients must define a function that returns a dict[str, Optimizer] with keys 'model',\"\n        \" 'gce', and 'cov',\"\n        \"defining separate optimizers for different modules of the client.\"\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.set_optimizer","title":"<code>set_optimizer(config)</code>","text":"<p>This function simply ensures that the optimizers setup by the user have the proper keys and that there are three optimizers.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def set_optimizer(self, config: Config) -&gt; None:\n    \"\"\"\n    This function simply ensures that the optimizers setup by the user have the proper keys\n    and that there are three optimizers.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    optimizers = self.get_optimizer(config)\n    assert isinstance(optimizers, dict) and {\"model\", \"gce\", \"cov\"} == set(optimizers.keys()), (\n        \"Three optimizers must be defined with keys 'model', 'gce', and 'cov'. Now, only \"\n        f\"{optimizers.keys()} optimizers are defined.\"\n    )\n    # If user has specified weight decay for the GCE or CoV optimizers,\n    # we will log a warning before overwriting these values with mu.\n    user_gce_weight_decay: float = optimizers[\"gce\"].param_groups[0].get(\"weight_decay\", 0.0)\n    user_cov_weight_decay: float = optimizers[\"cov\"].param_groups[0].get(\"weight_decay\", 0.0)\n    if user_gce_weight_decay != 0.0 or user_cov_weight_decay != 0.0:\n        log(\n            WARNING,\n            \"Your gce or cov optimizer weight decay will be overwritten by the mu parameter.\",\n        )\n    # Set the weight decay for the GCE and CoV optimizers to self.mu to enable\n    # L2 regularization in the loss.\n    log(INFO, f\"Setting the GCE optimizer's weight decay to mu = {self.mu}\")\n    for param_group in optimizers[\"gce\"].param_groups:\n        param_group[\"weight_decay\"] = self.mu\n\n    log(INFO, f\"Setting the CoV optimizer's weight decay to my = {self.mu}\")\n    for param_group in optimizers[\"cov\"].param_groups:\n        param_group[\"weight_decay\"] = self.mu\n    self.optimizers = optimizers\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.get_parameter_exchanger","title":"<code>get_parameter_exchanger(config)</code>","text":"<p>GPFL client uses a fixed layer exchanger to exchange layers in three sub-modules. Sub-modules to be exchanged are defined in the <code>GpflModel</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Config from the server..</p> required <p>Returns:</p> Type Description <code>ParameterExchanger</code> <p>FixedLayerExchanger used to exchange a set of fixed and specific layers.</p> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n    \"\"\"\n    GPFL client uses a fixed layer exchanger to exchange layers in three sub-modules.\n    Sub-modules to be exchanged are defined in the ``GpflModel`` class.\n\n    Args:\n        config (Config): Config from the server..\n\n    Returns:\n        (ParameterExchanger): FixedLayerExchanger used to exchange a set of fixed and specific layers.\n    \"\"\"\n    assert isinstance(self.model, GpflModel)\n    return FixedLayerExchanger(self.model.layers_to_exchange())\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.calculate_class_sample_proportions","title":"<code>calculate_class_sample_proportions()</code>","text":"<p>This method is used to compute the class sample proportions based on the training data. It computes the proportion of samples for each class in the training dataset.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor containing the proportion of samples for each class.</p> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def calculate_class_sample_proportions(self) -&gt; torch.Tensor:\n    \"\"\"\n    This method is used to compute the class sample proportions based on the training data.\n    It computes the proportion of samples for each class in the training dataset.\n\n    Returns:\n        (torch.Tensor): A tensor containing the proportion of samples for each class.\n    \"\"\"\n    class_sample_proportion = torch.zeros(self.num_classes, device=self.device)\n    one_hot_n_dim = 2  # To avoid having magic numbers\n    for _, target in self.train_loader:\n        if target.dim() == one_hot_n_dim:  # Target is one-hot encoded\n            assert target.shape[1] == self.num_classes, (\n                \"Shape of the one-hot encoded labels should be (batch_size, num_classes).\"\n            )\n        else:  # Target is not one-hot encoded\n            target = one_hot(target, num_classes=self.num_classes).to(self.device)\n\n        # Compute the proportion of samples for each class by summing the one-hot encoded targets along each column\n        # which gives the count of samples per class.\n        class_sample_proportion += target.sum(0)\n\n    # Divide the number of samples per class by the total number of samples (sum of all ones).\n    class_sample_proportion /= class_sample_proportion.sum()\n    return class_sample_proportion\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>In addition to dataloaders, optimizers, parameter exchangers, a few GPFL specific parameters are set up in this method. This includes the number of classes, feature dimension, and the sample per class tensor. The global and personalized conditional inputs are also initialized.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    In addition to dataloaders, optimizers, parameter exchangers, a few GPFL specific parameters\n    are set up in this method. This includes the number of classes, feature dimension,\n    and the sample per class tensor. The global and personalized conditional inputs are also initialized.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    super().setup_client(config)\n\n    # Initiate some parameters related to GPFL.\n    # ``num_class`` and ``feature_dim`` are essential parts of the GPFL model construction.\n    self.num_classes = self.model.num_classes\n    self.feature_dim = self.model.feature_dim\n    # class_sample_proportion tensor is used to compute personalized conditional input.\n    self.class_sample_proportion = self.calculate_class_sample_proportions()\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.compute_conditional_inputs","title":"<code>compute_conditional_inputs()</code>","text":"<p>Calculates the conditional inputs (p_i and g) for the CoV module based on the new GCE from the server. The <code>self.global_conditional_input</code> and <code>self.personalized_conditional_input</code> tensors are computed based on a frozen GCE model and the sample per class tensor. These tensors are fixed in each client round, and are recomputed when a new GCE module is shared by the server in every client round.</p> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def compute_conditional_inputs(self) -&gt; None:\n    \"\"\"\n    Calculates the conditional inputs (p_i and g) for the CoV module based on the new GCE from the server.\n    The ``self.global_conditional_input`` and ``self.personalized_conditional_input`` tensors are computed\n    based on a frozen GCE model and the sample per class tensor. These tensors are fixed in each client round,\n    and are recomputed when a new GCE module is shared by the server in every client round.\n    \"\"\"\n    # Initiate g(global_conditional_input) and p_i(personalized_conditional_input) tensors to zeros.\n    self.global_conditional_input = torch.zeros(self.feature_dim).to(self.device)\n    self.personalized_conditional_input = torch.zeros(self.feature_dim).to(self.device)\n\n    embeddings = self.gce_frozen.embedding.weight\n    for i, embedding in enumerate(embeddings):\n        self.global_conditional_input += embedding\n        self.personalized_conditional_input += embedding * self.class_sample_proportion[i]\n\n    self.global_conditional_input = embeddings.sum(0) / self.num_classes\n    self.personalized_conditional_input = (\n        torch.matmul(embeddings.T, self.class_sample_proportion) / self.num_classes\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.update_before_train","title":"<code>update_before_train(current_server_round)</code>","text":"<p>Updates the frozen GCE model and computes the conditional inputs before training starts.</p> <p>Parameters:</p> Name Type Description Default <code>current_server_round</code> <code>int</code> <p>The number of current server round.</p> required Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def update_before_train(self, current_server_round: int) -&gt; None:\n    \"\"\"\n    Updates the frozen GCE model and computes the conditional inputs before training starts.\n\n    Args:\n        current_server_round (int): The number of current server round.\n    \"\"\"\n    # Update the frozen GCE\n    cloned_model = clone_and_freeze_model(self.model.gce)\n    assert isinstance(cloned_model, Gce)\n    self.gce_frozen = cloned_model\n    # Update conditional inputs before training\n    self.compute_conditional_inputs()\n\n    return super().update_before_train(current_server_round)\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.transform_input","title":"<code>transform_input(input)</code>","text":"<p>Extend the input dictionary with <code>global_conditional_input</code> and <code>personalized_conditional_input</code> tensors. This let's use provide these additional tensor to the GPFL model .</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Input tensor.</p> required <p>Returns:</p> Type Description <code>TorchInputType</code> <p>Transformed input tensor.</p> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def transform_input(self, input: TorchInputType) -&gt; TorchInputType:\n    \"\"\"\n    Extend the input dictionary with ``global_conditional_input`` and ``personalized_conditional_input``\n    tensors. This let's use provide these additional tensor to the GPFL model .\n\n    Args:\n        input (TorchInputType): Input tensor.\n\n    Returns:\n        (TorchInputType): Transformed input tensor.\n    \"\"\"\n    # Attach the global and personalized conditional inputs to the input\n    if isinstance(input, torch.Tensor):\n        return {\n            \"input\": input,\n            \"global_conditional_input\": self.global_conditional_input.detach(),\n            \"personalized_conditional_input\": self.personalized_conditional_input.detach(),\n        }\n    assert isinstance(input, dict)\n    input.update(\n        {\n            \"global_conditional_input\": self.global_conditional_input.detach(),\n            \"personalized_conditional_input\": self.personalized_conditional_input.detach(),\n        }\n    )\n    return input\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.train_step","title":"<code>train_step(input, target)</code>","text":"<p>Given a single batch of input and target data, generate predictions, compute loss, update parameters and optionally update metrics if they exist. (i.e. backprop on a single batch of data). Assumes <code>self.model</code> is in train mode already.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>The input to be fed into the model.</p> required <code>target</code> <code>TorchTargetType</code> <p>The target corresponding to the input.</p> required <p>Returns:</p> Type Description <code>tuple[TrainingLosses, TorchPredType]</code> <p>The losses object from the train step along with</p> <p>`            a dictionary of any predictions produced by the model.</p> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n    \"\"\"\n    Given a single batch of input and target data, generate predictions, compute loss, update parameters and\n    optionally update metrics if they exist. (i.e. backprop on a single batch of data).\n    Assumes ``self.model`` is in train mode already.\n\n    Args:\n        input (TorchInputType): The input to be fed into the model.\n        target (TorchTargetType): The target corresponding to the input.\n\n    Returns:\n        (tuple[TrainingLosses, TorchPredType]): The losses object from the train step along with\n    `            a dictionary of any predictions produced by the model.\n    \"\"\"\n    # Clear gradients from the optimizers if they exist\n    self.optimizers[\"model\"].zero_grad()\n    self.optimizers[\"gce\"].zero_grad()\n    self.optimizers[\"cov\"].zero_grad()\n\n    # Call user defined methods to get predictions and compute loss\n    input = self.transform_input(input)\n    preds, features = self.predict(input)\n    target = self.transform_target(target)\n    losses = self.compute_training_loss(preds, features, target)\n\n    # Compute backward pass and update parameters with optimizer\n    losses.backward[\"backward\"].backward()\n    self.transform_gradients(losses)\n    self.optimizers[\"model\"].step()\n    self.optimizers[\"gce\"].step()\n    self.optimizers[\"cov\"].step()\n\n    return losses, preds\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.compute_magnitude_level_loss","title":"<code>compute_magnitude_level_loss(global_features, target)</code>","text":"<p>Computes magnitude level loss corresponds to \\(\\mathcal{L}_i^{  ext{mlg}}\\) in the paper.</p> <p>Parameters:</p> Name Type Description Default <code>global_features</code> <code>Tensor</code> <p>global features computed in this client.</p> required <code>target</code> <code>TorchTargetType</code> <p>Either a tensor of class indices or one-hot encoded tensors.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>L2 norm loss between the global features and the frozen GCE's global features.</p> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def compute_magnitude_level_loss(\n    self,\n    global_features: torch.Tensor,\n    target: TorchTargetType,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes magnitude level loss corresponds to \\\\(\\\\mathcal{L}_i^{\\text{mlg}}\\\\) in the paper.\n\n    Args:\n        global_features (torch.Tensor): global features computed in this client.\n        target (TorchTargetType): Either a tensor of class indices or one-hot encoded tensors.\n\n    Returns:\n        (torch.Tensor): L2 norm loss between the global features and the frozen GCE's global features.\n    \"\"\"\n    # In magnitude level loss, GCE's embedding table is frozen, and the goal is to train\n    # the model to generate good global features by making the generated embeddings closer to\n    # frozen GCE's global embeddings.\n    assert isinstance(target, torch.Tensor), \"GPFL clients take only tensor targets.\"\n    return torch.norm(global_features - self.gce_frozen.lookup(target).detach(), 2)\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.compute_training_loss","title":"<code>compute_training_loss(preds, features, target)</code>","text":"<p>Computes the combined training loss given predictions, global features of the model, and ground truth data. GPFL loss is a combined loss and is defined as <code>prediction_loss + gce_softmax_loss + magnitude_level_loss</code>.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. Anything stored in preds will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes the combined training loss given predictions, global features of the model, and ground truth data.\n    GPFL loss is a combined loss and is defined as ``prediction_loss + gce_softmax_loss + magnitude_level_loss``.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n            in preds will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses\n            indexed by name.\n    \"\"\"\n    # The loss used during training is a combination of the prediction loss (CrossEntropy used in the paper),\n    # angel-level (GCE loss) and magnitude-level global losses.\n    prediction_loss, _ = self.compute_loss_and_additional_losses(preds, features, target)\n    # ``gce_softmax_loss`` corresponds to \\mathcal{L}_i^{\\text{alg}} in the paper.\n    gce_softmax_loss = self.model.gce(features[\"global_features\"], target)\n    # ``magnitude_level_loss`` corresponds to \\mathcal{L}_i^{\\text{mlg}} in the paper.\n    magnitude_level_loss = self.compute_magnitude_level_loss(features[\"global_features\"], target)\n    # Note that L2 regularization terms are included in the optimizers.\n    loss = prediction_loss + gce_softmax_loss + magnitude_level_loss * self.lam\n    additional_losses = {\n        \"prediction_loss\": prediction_loss.clone(),\n        \"gce_softmax_loss\": gce_softmax_loss.clone(),\n        \"magnitude_level_loss\": magnitude_level_loss.clone(),\n    }\n    return TrainingLosses(backward=loss, additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.gpfl_client.GpflClient.val_step","title":"<code>val_step(input, target)</code>","text":"<p>Before performing validation, we need to transform the input and attach the global and personalized conditional tensors to the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Input based on the training data.</p> required <code>target</code> <code>TorchTargetType</code> <p>The target corresponding to the input..</p> required <p>Returns:</p> Type Description <code>tuple[EvaluationLosses, TorchPredType]: tuple[EvaluationLosses, TorchPredType]</code> <p>The losses object from the val step along with a dictionary of the predictions produced by the model.</p> Source code in <code>fl4health/clients/gpfl_client.py</code> <pre><code>def val_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[EvaluationLosses, TorchPredType]:\n    \"\"\"\n    Before performing validation, we need to transform the input and attach the global and personalized\n    conditional tensors to the input.\n\n    Args:\n        input (TorchInputType): Input based on the training data.\n        target (TorchTargetType): The target corresponding to the input..\n\n    Returns:\n        (tuple[EvaluationLosses, TorchPredType]: tuple[EvaluationLosses, TorchPredType]):\n            The losses object from the val step along with a dictionary of the predictions produced\n            by the model.\n    \"\"\"\n    input = self.transform_input(input)\n    return super().val_step(input, target)\n</code></pre>"},{"location":"api/#fl4health.clients.instance_level_dp_client","title":"<code>instance_level_dp_client</code>","text":""},{"location":"api/#fl4health.clients.instance_level_dp_client.InstanceLevelDpClient","title":"<code>InstanceLevelDpClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/instance_level_dp_client.py</code> <pre><code>class InstanceLevelDpClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Client for Instance/Record level Differentially Private Federated Averaging.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.clipping_bound: float\n        self.noise_multiplier: float\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Performs the same flow as ``BasicClient`` to setup a client. This functionality straps on a processing of two\n        configuration variables ``self.clipping_bound`` and ``self.noise_multiplier``. The last step is to do some\n        processing of the model and optimizers with Opacus to make them DP compatible and to setup the privacy engine\n        used for privacy accounting. This is done with the ``setup_opacus_objects`` function.\n\n        Args:\n            config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n        \"\"\"\n        # Ensure that clipping bound and noise multiplier is present in config\n        # Set attributes to be used when setting DP training\n        self.clipping_bound = narrow_dict_type(config, \"clipping_bound\", float)\n        self.noise_multiplier = narrow_dict_type(config, \"noise_multiplier\", float)\n\n        # Do basic client setup\n        super().setup_client(config)\n\n        # Configure DP training\n        self.setup_opacus_objects(config)\n\n    def setup_opacus_objects(self, config: Config) -&gt; None:\n        \"\"\"\n        Validates and potentially fixes the PyTorch model of the client to be compatible with Opacus and privacy\n        mechanisms, sets up the privacy engine of Opacus using the model, optimizer, dataloaders etc. for DP training.\n\n        Args:\n            config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n        \"\"\"\n        # Validate that the model layers are compatible with privacy mechanisms in Opacus and try to replace the layers\n        # with compatible ones if necessary.\n        self.model, reinitialize_optimizer = privacy_validate_and_fix_modules(self.model)\n\n        # If we have fixed the model by changing out layers (and therefore parameters), we need to update the optimizer\n        # parameters to coincide with this fixed model. **NOTE**: It is not done in make_private!\n        if reinitialize_optimizer:\n            self.set_optimizer(config)\n\n        # Create DP training objects\n        privacy_engine = PrivacyEngine()\n        # NOTE: that Opacus make private is NOT idempotent\n        self.model, optimizer, self.train_loader = privacy_engine.make_private(\n            module=self.model,\n            optimizer=self.optimizers[\"global\"],\n            data_loader=self.train_loader,\n            noise_multiplier=self.noise_multiplier,\n            max_grad_norm=self.clipping_bound,\n            clipping=\"flat\",\n        )\n\n        self.optimizers = {\"global\": optimizer}\n</code></pre>"},{"location":"api/#fl4health.clients.instance_level_dp_client.InstanceLevelDpClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>Client for Instance/Record level Differentially Private Federated Averaging.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/instance_level_dp_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Client for Instance/Record level Differentially Private Federated Averaging.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.clipping_bound: float\n    self.noise_multiplier: float\n</code></pre>"},{"location":"api/#fl4health.clients.instance_level_dp_client.InstanceLevelDpClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Performs the same flow as <code>BasicClient</code> to setup a client. This functionality straps on a processing of two configuration variables <code>self.clipping_bound</code> and <code>self.noise_multiplier</code>. The last step is to do some processing of the model and optimizers with Opacus to make them DP compatible and to setup the privacy engine used for privacy accounting. This is done with the <code>setup_opacus_objects</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configurations sent by the server to allow for customization of this functions behavior.</p> required Source code in <code>fl4health/clients/instance_level_dp_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Performs the same flow as ``BasicClient`` to setup a client. This functionality straps on a processing of two\n    configuration variables ``self.clipping_bound`` and ``self.noise_multiplier``. The last step is to do some\n    processing of the model and optimizers with Opacus to make them DP compatible and to setup the privacy engine\n    used for privacy accounting. This is done with the ``setup_opacus_objects`` function.\n\n    Args:\n        config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n    \"\"\"\n    # Ensure that clipping bound and noise multiplier is present in config\n    # Set attributes to be used when setting DP training\n    self.clipping_bound = narrow_dict_type(config, \"clipping_bound\", float)\n    self.noise_multiplier = narrow_dict_type(config, \"noise_multiplier\", float)\n\n    # Do basic client setup\n    super().setup_client(config)\n\n    # Configure DP training\n    self.setup_opacus_objects(config)\n</code></pre>"},{"location":"api/#fl4health.clients.instance_level_dp_client.InstanceLevelDpClient.setup_opacus_objects","title":"<code>setup_opacus_objects(config)</code>","text":"<p>Validates and potentially fixes the PyTorch model of the client to be compatible with Opacus and privacy mechanisms, sets up the privacy engine of Opacus using the model, optimizer, dataloaders etc. for DP training.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configurations sent by the server to allow for customization of this functions behavior.</p> required Source code in <code>fl4health/clients/instance_level_dp_client.py</code> <pre><code>def setup_opacus_objects(self, config: Config) -&gt; None:\n    \"\"\"\n    Validates and potentially fixes the PyTorch model of the client to be compatible with Opacus and privacy\n    mechanisms, sets up the privacy engine of Opacus using the model, optimizer, dataloaders etc. for DP training.\n\n    Args:\n        config (Config): Configurations sent by the server to allow for customization of this functions behavior.\n    \"\"\"\n    # Validate that the model layers are compatible with privacy mechanisms in Opacus and try to replace the layers\n    # with compatible ones if necessary.\n    self.model, reinitialize_optimizer = privacy_validate_and_fix_modules(self.model)\n\n    # If we have fixed the model by changing out layers (and therefore parameters), we need to update the optimizer\n    # parameters to coincide with this fixed model. **NOTE**: It is not done in make_private!\n    if reinitialize_optimizer:\n        self.set_optimizer(config)\n\n    # Create DP training objects\n    privacy_engine = PrivacyEngine()\n    # NOTE: that Opacus make private is NOT idempotent\n    self.model, optimizer, self.train_loader = privacy_engine.make_private(\n        module=self.model,\n        optimizer=self.optimizers[\"global\"],\n        data_loader=self.train_loader,\n        noise_multiplier=self.noise_multiplier,\n        max_grad_norm=self.clipping_bound,\n        clipping=\"flat\",\n    )\n\n    self.optimizers = {\"global\": optimizer}\n</code></pre>"},{"location":"api/#fl4health.clients.mkmmd_clients","title":"<code>mkmmd_clients</code>","text":""},{"location":"api/#fl4health.clients.mkmmd_clients.ditto_mkmmd_client","title":"<code>ditto_mkmmd_client</code>","text":""},{"location":"api/#fl4health.clients.mkmmd_clients.ditto_mkmmd_client.DittoMkMmdClient","title":"<code>DittoMkMmdClient</code>","text":"<p>               Bases: <code>DittoClient</code></p> Source code in <code>fl4health/clients/mkmmd_clients/ditto_mkmmd_client.py</code> <pre><code>class DittoMkMmdClient(DittoClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n        mkmmd_loss_weight: float = 10.0,\n        feature_extraction_layers: Sequence[str] | None = None,\n        feature_l2_norm_weight: float = 0.0,\n        beta_global_update_interval: int = 20,\n        num_accumulating_batches: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client implements the MK-MMD loss function in the Ditto framework. The MK-MMD loss is a measure of the\n        distance between the distributions of the features of the local model and initial global model of each round.\n        The MK-MMD loss is added to the local loss to penalize the local model for drifting away from the global model.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            mkmmd_loss_weight (float, optional): weight applied to the MK-MMD loss. Defaults to 10.0.\n            feature_extraction_layers (Sequence[str] | None, optional): List of layers from which to extract\n                and flatten features. Defaults to None.\n            feature_l2_norm_weight (float, optional): weight applied to the L2 norm of the features.\n                Defaults to 0.0.\n            beta_global_update_interval (int, optional): interval at which to update the betas for the MK-MMD loss. If\n                set to above 0, the betas will be updated based on whole distribution of latent features of data with\n                the given update interval. If set to 0, the betas will not be updated. If set to -1, the betas will be\n                updated after each individual batch based on only that individual batch. Defaults to 20.\n            num_accumulating_batches (int, optional): Number of batches to accumulate features to approximate the whole\n                distribution of the latent features for updating beta of the MK-MMD loss. This parameter is only used\n                if ``beta_global_update_interval`` is set to larger than 0. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.mkmmd_loss_weight = mkmmd_loss_weight\n        if self.mkmmd_loss_weight == 0:\n            log(\n                ERROR,\n                \"MK-MMD loss weight is set to 0. As MK-MMD loss will not be computed, \",\n                \"please use vanilla DittoClient instead.\",\n            )\n\n        self.feature_l2_norm_weight = feature_l2_norm_weight\n        self.beta_global_update_interval = beta_global_update_interval\n        if self.beta_global_update_interval == -1:\n            log(INFO, \"Betas for the MK-MMD loss will be updated for each individual batch.\")\n        elif self.beta_global_update_interval == 0:\n            log(INFO, \"Betas for the MK-MMD loss will not be updated.\")\n        elif self.beta_global_update_interval &gt; 0:\n            log(INFO, f\"Betas for the MK-MMD loss will be updated every {self.beta_global_update_interval} steps.\")\n        else:\n            raise ValueError(\"Invalid beta_global_update_interval. It should be either -1, 0 or a positive integer.\")\n        if feature_extraction_layers:\n            # By default, all of the features should be flattened for the MK-MMD loss\n            self.flatten_feature_extraction_layers = dict.fromkeys(feature_extraction_layers, True)\n        else:\n            self.flatten_feature_extraction_layers = {}\n        self.mkmmd_losses: dict[str, MkMmdLoss] = {}\n        for layer in self.flatten_feature_extraction_layers:\n            self.mkmmd_losses[layer] = MkMmdLoss(\n                device=self.device, minimize_type_two_error=True, normalize_features=True, layer_name=layer\n            ).to(self.device)\n\n        self.initial_global_model: nn.Module\n        self.local_feature_extractor: FeatureExtractorBuffer\n        self.initial_global_feature_extractor: FeatureExtractorBuffer\n        self.num_accumulating_batches = num_accumulating_batches\n\n    def setup_client(self, config: Config) -&gt; None:\n        super().setup_client(config)\n        self.local_feature_extractor = FeatureExtractorBuffer(\n            model=self.model,\n            flatten_feature_extraction_layers=self.flatten_feature_extraction_layers,\n        )\n        # Register hooks to extract features from the local model if not already registered\n        self.local_feature_extractor._maybe_register_hooks()\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        super().update_before_train(current_server_round)\n        assert isinstance(self.global_model, nn.Module)\n        # Clone and freeze the initial weights GLOBAL MODEL. These are used to form the Ditto local\n        # update penalty term.\n        self.initial_global_model = clone_and_freeze_model(self.global_model)\n        self.initial_global_feature_extractor = FeatureExtractorBuffer(\n            model=self.initial_global_model,\n            flatten_feature_extraction_layers=self.flatten_feature_extraction_layers,\n        )\n        # Register hooks to extract features from the initial global model if not already registered\n        self.initial_global_feature_extractor._maybe_register_hooks()\n\n    def _should_optimize_betas(self, step: int) -&gt; bool:\n        step_at_interval = (step - 1) % self.beta_global_update_interval == 0\n        valid_components_present = self.initial_global_model is not None\n        # If the mkmmd loss doesn't matter, we don't bother optimizing betas\n        weighted_mkmmd_loss = self.mkmmd_loss_weight != 0\n        return step_at_interval and valid_components_present and weighted_mkmmd_loss\n\n    def update_after_step(self, step: int, current_round: int | None = None) -&gt; None:\n        if self.beta_global_update_interval &gt; 0 and self._should_optimize_betas(step):\n            # Get the feature distribution of the local and initial global features with evaluation\n            # mode\n            local_distributions, initial_global_distributions = self.update_buffers(\n                self.model, self.initial_global_model\n            )\n            # Update betas for the MK-MMD loss based on gathered features during training\n            for layer, layer_mkmmd_loss in self.mkmmd_losses.items():\n                layer_mkmmd_loss.betas = layer_mkmmd_loss.optimize_betas(\n                    x=local_distributions[layer], y=initial_global_distributions[layer], lambda_m=1e-5\n                )\n        super().update_after_step(step)\n\n    def update_buffers(\n        self, local_model: torch.nn.Module, initial_global_model: torch.nn.Module\n    ) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n        \"\"\"\n        Update the feature buffer of the local and global features.\n\n        Args:\n            local_model (torch.nn.Module): Local model to extract features from.\n            initial_global_model (torch.nn.Module): Initial global model to extract features from.\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple containing the extracted\n                features using the local and initial global models.\n        \"\"\"\n        self.local_feature_extractor.clear_buffers()\n        self.initial_global_feature_extractor.clear_buffers()\n\n        self.local_feature_extractor.enable_accumulating_features()\n        self.initial_global_feature_extractor.enable_accumulating_features()\n\n        # Save the initial state of the local model to restore it after the buffer is populated,\n        # however as initial global model is already cloned and frozen, we don't need to save its state.\n        initial_state_local_model = local_model.training\n\n        # Set local model to evaluation mode, as we don't want to create a computational graph\n        # for the local model when populating the local buffer with features to compute optimal\n        # betas for the MK-MMD loss\n        local_model.eval()\n\n        # Make sure the local model is in evaluation mode before populating the local buffer\n        assert not local_model.training\n\n        # Make sure the initial global model is in evaluation mode before populating the global buffer\n        # as it is already cloned and frozen from the global model\n        assert not initial_global_model.training\n\n        with torch.no_grad():\n            for i, (input, _) in enumerate(self.train_loader):\n                input = input.to(self.device)\n                # Pass the input through the local model to populate the local_feature_extractor buffer\n                local_model(input)\n                # Pass the input through the initial global model to populate the initial_global_feature_extractor\n                # buffer\n                initial_global_model(input)\n                # Break if the number of accumulating batches is reached to avoid memory issues\n                if i == self.num_accumulating_batches:\n                    break\n        local_distributions = self.local_feature_extractor.get_extracted_features()\n        initial_global_distributions = self.initial_global_feature_extractor.get_extracted_features()\n        # Restore the initial state of the local model\n        if initial_state_local_model:\n            local_model.train()\n\n        self.local_feature_extractor.disable_accumulating_features()\n        self.initial_global_feature_extractor.disable_accumulating_features()\n\n        self.local_feature_extractor.clear_buffers()\n        self.initial_global_feature_extractor.clear_buffers()\n\n        return local_distributions, initial_global_distributions\n\n    def predict(\n        self,\n        input: TorchInputType,\n    ) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Computes the predictions for both the **GLOBAL** and **LOCAL** models and pack them into the prediction\n        dictionary.\n\n        Args:\n           input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n               it is assumed that the keys of input match the names of the keyword arguments of\n               ``self.model.forward()``.\n\n        Returns:\n            (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n                predictions indexed by name and the second element contains intermediate activations indexed by name.\n                By passing features, we can compute all the losses. All predictions included in dictionary will by\n                default be used to compute metrics separately.\n\n        Raises:\n            TypeError: Occurs when something other than a tensor or dict of tensors is passed in to the model's\n                forward method.\n            ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n                forward.\n        \"\"\"\n        # We use features from initial_global_model to compute the MK-MMD loss not the global_model\n        global_preds = self.global_model(input)\n        local_preds = self.model(input)\n        features = self.local_feature_extractor.get_extracted_features()\n        if self.mkmmd_loss_weight != 0:\n            # Compute the features of the initial_global_model\n            self.initial_global_model(input)\n            initial_global_features = self.initial_global_feature_extractor.get_extracted_features()\n            for key, initial_global_feature in initial_global_features.items():\n                features[\" \".join([\"init_global\", key])] = initial_global_feature\n\n        return {\"global\": global_preds, \"local\": local_preds}, features\n\n    def _maybe_checkpoint(self, loss: float, metrics: dict[str, Scalar], checkpoint_mode: CheckpointMode) -&gt; None:\n        # Hooks need to be removed before checkpointing the model\n        self.local_feature_extractor.remove_hooks()\n        super()._maybe_checkpoint(loss=loss, metrics=metrics, checkpoint_mode=checkpoint_mode)\n        # As hooks have to be removed to checkpoint the model, so we check if they need to be re-registered\n        # each time.\n        self.local_feature_extractor._maybe_register_hooks()\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training losses given predictions of the global and local models and ground truth data.\n        For the local model we add to the vanilla loss function by including Ditto penalty loss which is the\n        \\\\(\\\\ell^2\\\\) inner product between the initial global model weights and weights of the local model. This is\n        stored in backward The loss to optimize the global model is stored in the additional losses dictionary under\n        \u201cglobal_loss\u201d.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n                dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n                by name. Additional losses includes each loss component and the global model loss tensor.\n        \"\"\"\n        for layer_loss_module in self.mkmmd_losses.values():\n            assert layer_loss_module.training\n        # Check that both models are in training mode\n        assert self.global_model.training and self.model.training\n\n        # local loss is stored in loss, global model loss is stored in additional losses.\n        loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n        # Setting the adaptation loss to that of the local model, as its performance should dictate whether more or\n        # less weight is used to constrain it to the global model (as in FedProx)\n        additional_losses[\"loss_for_adaptation\"] = additional_losses[\"local_loss\"].clone()\n\n        # This is the Ditto penalty loss of the local model compared with the original Global model weights, scaled\n        # by drift_penalty_weight (or lambda in the original paper)\n        penalty_loss = self.compute_penalty_loss()\n        additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n        total_loss = loss + penalty_loss\n\n        # Add MK-MMD loss based on computed features during training\n        if self.mkmmd_loss_weight != 0:\n            total_loss += additional_losses[\"mkmmd_loss_total\"]\n\n        if self.feature_l2_norm_weight != 0:\n            total_loss += additional_losses[\"feature_l2_norm_loss\"]\n\n        additional_losses[\"total_loss\"] = total_loss.clone()\n\n        return TrainingLosses(backward=total_loss, additional_losses=additional_losses)\n\n    def compute_loss_and_additional_losses(\n        self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Computes the loss and any additional losses given predictions of the model and ground truth data.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n                - The tensor for the loss.\n                - A dictionary of additional losses with their names and values, or None if there are no additional\n                  losses.\n        \"\"\"\n        loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n\n        if self.mkmmd_loss_weight != 0:\n            if self.beta_global_update_interval == -1:\n                # Update betas for the MK-MMD loss based on computed features during training\n                for layer, layer_mkmmd_loss in self.mkmmd_losses.items():\n                    layer_mkmmd_loss.betas = layer_mkmmd_loss.optimize_betas(\n                        x=features[layer], y=features[\" \".join([\"init_global\", layer])], lambda_m=1e-5\n                    )\n            # Compute MK-MMD loss\n            total_mkmmd_loss = torch.tensor(0.0, device=self.device)\n            for layer, layer_mkmmd_loss in self.mkmmd_losses.items():\n                mkmmd_loss = layer_mkmmd_loss(features[layer], features[\" \".join([\"init_global\", layer])])\n                additional_losses[\"_\".join([\"mkmmd_loss\", layer])] = mkmmd_loss.clone()\n                total_mkmmd_loss += mkmmd_loss\n            additional_losses[\"mkmmd_loss_total\"] = self.mkmmd_loss_weight * total_mkmmd_loss\n        if self.feature_l2_norm_weight != 0:\n            # Compute the average L2 norm of the features over the batch\n            feature_l2_norm_loss = torch.linalg.norm(features[\"features\"]) / len(features[\"features\"])\n            additional_losses[\"feature_l2_norm_loss\"] = self.feature_l2_norm_weight * feature_l2_norm_loss\n\n        return loss, additional_losses\n</code></pre> <code></code> <code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None, mkmmd_loss_weight=10.0, feature_extraction_layers=None, feature_l2_norm_weight=0.0, beta_global_update_interval=20, num_accumulating_batches=None)</code> \u00b6 <p>This client implements the MK-MMD loss function in the Ditto framework. The MK-MMD loss is a measure of the distance between the distributions of the features of the local model and initial global model of each round. The MK-MMD loss is added to the local loss to penalize the local model for drifting away from the global model.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>mkmmd_loss_weight</code> <code>float</code> <p>weight applied to the MK-MMD loss. Defaults to 10.0.</p> <code>10.0</code> <code>feature_extraction_layers</code> <code>Sequence[str] | None</code> <p>List of layers from which to extract and flatten features. Defaults to None.</p> <code>None</code> <code>feature_l2_norm_weight</code> <code>float</code> <p>weight applied to the L2 norm of the features. Defaults to 0.0.</p> <code>0.0</code> <code>beta_global_update_interval</code> <code>int</code> <p>interval at which to update the betas for the MK-MMD loss. If set to above 0, the betas will be updated based on whole distribution of latent features of data with the given update interval. If set to 0, the betas will not be updated. If set to -1, the betas will be updated after each individual batch based on only that individual batch. Defaults to 20.</p> <code>20</code> <code>num_accumulating_batches</code> <code>int</code> <p>Number of batches to accumulate features to approximate the whole distribution of the latent features for updating beta of the MK-MMD loss. This parameter is only used if <code>beta_global_update_interval</code> is set to larger than 0. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/mkmmd_clients/ditto_mkmmd_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n    mkmmd_loss_weight: float = 10.0,\n    feature_extraction_layers: Sequence[str] | None = None,\n    feature_l2_norm_weight: float = 0.0,\n    beta_global_update_interval: int = 20,\n    num_accumulating_batches: int | None = None,\n) -&gt; None:\n    \"\"\"\n    This client implements the MK-MMD loss function in the Ditto framework. The MK-MMD loss is a measure of the\n    distance between the distributions of the features of the local model and initial global model of each round.\n    The MK-MMD loss is added to the local loss to penalize the local model for drifting away from the global model.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        mkmmd_loss_weight (float, optional): weight applied to the MK-MMD loss. Defaults to 10.0.\n        feature_extraction_layers (Sequence[str] | None, optional): List of layers from which to extract\n            and flatten features. Defaults to None.\n        feature_l2_norm_weight (float, optional): weight applied to the L2 norm of the features.\n            Defaults to 0.0.\n        beta_global_update_interval (int, optional): interval at which to update the betas for the MK-MMD loss. If\n            set to above 0, the betas will be updated based on whole distribution of latent features of data with\n            the given update interval. If set to 0, the betas will not be updated. If set to -1, the betas will be\n            updated after each individual batch based on only that individual batch. Defaults to 20.\n        num_accumulating_batches (int, optional): Number of batches to accumulate features to approximate the whole\n            distribution of the latent features for updating beta of the MK-MMD loss. This parameter is only used\n            if ``beta_global_update_interval`` is set to larger than 0. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.mkmmd_loss_weight = mkmmd_loss_weight\n    if self.mkmmd_loss_weight == 0:\n        log(\n            ERROR,\n            \"MK-MMD loss weight is set to 0. As MK-MMD loss will not be computed, \",\n            \"please use vanilla DittoClient instead.\",\n        )\n\n    self.feature_l2_norm_weight = feature_l2_norm_weight\n    self.beta_global_update_interval = beta_global_update_interval\n    if self.beta_global_update_interval == -1:\n        log(INFO, \"Betas for the MK-MMD loss will be updated for each individual batch.\")\n    elif self.beta_global_update_interval == 0:\n        log(INFO, \"Betas for the MK-MMD loss will not be updated.\")\n    elif self.beta_global_update_interval &gt; 0:\n        log(INFO, f\"Betas for the MK-MMD loss will be updated every {self.beta_global_update_interval} steps.\")\n    else:\n        raise ValueError(\"Invalid beta_global_update_interval. It should be either -1, 0 or a positive integer.\")\n    if feature_extraction_layers:\n        # By default, all of the features should be flattened for the MK-MMD loss\n        self.flatten_feature_extraction_layers = dict.fromkeys(feature_extraction_layers, True)\n    else:\n        self.flatten_feature_extraction_layers = {}\n    self.mkmmd_losses: dict[str, MkMmdLoss] = {}\n    for layer in self.flatten_feature_extraction_layers:\n        self.mkmmd_losses[layer] = MkMmdLoss(\n            device=self.device, minimize_type_two_error=True, normalize_features=True, layer_name=layer\n        ).to(self.device)\n\n    self.initial_global_model: nn.Module\n    self.local_feature_extractor: FeatureExtractorBuffer\n    self.initial_global_feature_extractor: FeatureExtractorBuffer\n    self.num_accumulating_batches = num_accumulating_batches\n</code></pre> <code></code> <code>update_buffers(local_model, initial_global_model)</code> \u00b6 <p>Update the feature buffer of the local and global features.</p> <p>Parameters:</p> Name Type Description Default <code>local_model</code> <code>Module</code> <p>Local model to extract features from.</p> required <code>initial_global_model</code> <code>Module</code> <p>Initial global model to extract features from.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>A tuple containing the extracted features using the local and initial global models.</p> Source code in <code>fl4health/clients/mkmmd_clients/ditto_mkmmd_client.py</code> <pre><code>def update_buffers(\n    self, local_model: torch.nn.Module, initial_global_model: torch.nn.Module\n) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n    \"\"\"\n    Update the feature buffer of the local and global features.\n\n    Args:\n        local_model (torch.nn.Module): Local model to extract features from.\n        initial_global_model (torch.nn.Module): Initial global model to extract features from.\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple containing the extracted\n            features using the local and initial global models.\n    \"\"\"\n    self.local_feature_extractor.clear_buffers()\n    self.initial_global_feature_extractor.clear_buffers()\n\n    self.local_feature_extractor.enable_accumulating_features()\n    self.initial_global_feature_extractor.enable_accumulating_features()\n\n    # Save the initial state of the local model to restore it after the buffer is populated,\n    # however as initial global model is already cloned and frozen, we don't need to save its state.\n    initial_state_local_model = local_model.training\n\n    # Set local model to evaluation mode, as we don't want to create a computational graph\n    # for the local model when populating the local buffer with features to compute optimal\n    # betas for the MK-MMD loss\n    local_model.eval()\n\n    # Make sure the local model is in evaluation mode before populating the local buffer\n    assert not local_model.training\n\n    # Make sure the initial global model is in evaluation mode before populating the global buffer\n    # as it is already cloned and frozen from the global model\n    assert not initial_global_model.training\n\n    with torch.no_grad():\n        for i, (input, _) in enumerate(self.train_loader):\n            input = input.to(self.device)\n            # Pass the input through the local model to populate the local_feature_extractor buffer\n            local_model(input)\n            # Pass the input through the initial global model to populate the initial_global_feature_extractor\n            # buffer\n            initial_global_model(input)\n            # Break if the number of accumulating batches is reached to avoid memory issues\n            if i == self.num_accumulating_batches:\n                break\n    local_distributions = self.local_feature_extractor.get_extracted_features()\n    initial_global_distributions = self.initial_global_feature_extractor.get_extracted_features()\n    # Restore the initial state of the local model\n    if initial_state_local_model:\n        local_model.train()\n\n    self.local_feature_extractor.disable_accumulating_features()\n    self.initial_global_feature_extractor.disable_accumulating_features()\n\n    self.local_feature_extractor.clear_buffers()\n    self.initial_global_feature_extractor.clear_buffers()\n\n    return local_distributions, initial_global_distributions\n</code></pre> <code></code> <code>predict(input)</code> \u00b6 <p>Computes the predictions for both the GLOBAL and LOCAL models and pack them into the prediction dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into the model. If input is of type <code>dict[str, torch.Tensor]</code>,  it is assumed that the keys of input match the names of the keyword arguments of  <code>self.model.forward()</code>.</p> required <p>Returns:</p> Type Description <code>tuple[TorchPredType, TorchFeatureType]</code> <p>A tuple in which the first element contains a dictionary of predictions indexed by name and the second element contains intermediate activations indexed by name. By passing features, we can compute all the losses. All predictions included in dictionary will by default be used to compute metrics separately.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Occurs when something other than a tensor or dict of tensors is passed in to the model's forward method.</p> <code>ValueError</code> <p>Occurs when something other than a tensor or dict of tensors is returned by the model forward.</p> Source code in <code>fl4health/clients/mkmmd_clients/ditto_mkmmd_client.py</code> <pre><code>def predict(\n    self,\n    input: TorchInputType,\n) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Computes the predictions for both the **GLOBAL** and **LOCAL** models and pack them into the prediction\n    dictionary.\n\n    Args:\n       input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n           it is assumed that the keys of input match the names of the keyword arguments of\n           ``self.model.forward()``.\n\n    Returns:\n        (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n            predictions indexed by name and the second element contains intermediate activations indexed by name.\n            By passing features, we can compute all the losses. All predictions included in dictionary will by\n            default be used to compute metrics separately.\n\n    Raises:\n        TypeError: Occurs when something other than a tensor or dict of tensors is passed in to the model's\n            forward method.\n        ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n            forward.\n    \"\"\"\n    # We use features from initial_global_model to compute the MK-MMD loss not the global_model\n    global_preds = self.global_model(input)\n    local_preds = self.model(input)\n    features = self.local_feature_extractor.get_extracted_features()\n    if self.mkmmd_loss_weight != 0:\n        # Compute the features of the initial_global_model\n        self.initial_global_model(input)\n        initial_global_features = self.initial_global_feature_extractor.get_extracted_features()\n        for key, initial_global_feature in initial_global_features.items():\n            features[\" \".join([\"init_global\", key])] = initial_global_feature\n\n    return {\"global\": global_preds, \"local\": local_preds}, features\n</code></pre> <code></code> <code>compute_training_loss(preds, features, target)</code> \u00b6 <p>Computes training losses given predictions of the global and local models and ground truth data. For the local model we add to the vanilla loss function by including Ditto penalty loss which is the \\(\\ell^2\\) inner product between the initial global model weights and weights of the local model. This is stored in backward The loss to optimize the global model is stored in the additional losses dictionary under \u201cglobal_loss\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name. Additional losses includes each loss component and the global model loss tensor.</p> Source code in <code>fl4health/clients/mkmmd_clients/ditto_mkmmd_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training losses given predictions of the global and local models and ground truth data.\n    For the local model we add to the vanilla loss function by including Ditto penalty loss which is the\n    \\\\(\\\\ell^2\\\\) inner product between the initial global model weights and weights of the local model. This is\n    stored in backward The loss to optimize the global model is stored in the additional losses dictionary under\n    \u201cglobal_loss\u201d.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n            dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n            by name. Additional losses includes each loss component and the global model loss tensor.\n    \"\"\"\n    for layer_loss_module in self.mkmmd_losses.values():\n        assert layer_loss_module.training\n    # Check that both models are in training mode\n    assert self.global_model.training and self.model.training\n\n    # local loss is stored in loss, global model loss is stored in additional losses.\n    loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n    # Setting the adaptation loss to that of the local model, as its performance should dictate whether more or\n    # less weight is used to constrain it to the global model (as in FedProx)\n    additional_losses[\"loss_for_adaptation\"] = additional_losses[\"local_loss\"].clone()\n\n    # This is the Ditto penalty loss of the local model compared with the original Global model weights, scaled\n    # by drift_penalty_weight (or lambda in the original paper)\n    penalty_loss = self.compute_penalty_loss()\n    additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n    total_loss = loss + penalty_loss\n\n    # Add MK-MMD loss based on computed features during training\n    if self.mkmmd_loss_weight != 0:\n        total_loss += additional_losses[\"mkmmd_loss_total\"]\n\n    if self.feature_l2_norm_weight != 0:\n        total_loss += additional_losses[\"feature_l2_norm_loss\"]\n\n    additional_losses[\"total_loss\"] = total_loss.clone()\n\n    return TrainingLosses(backward=total_loss, additional_losses=additional_losses)\n</code></pre> <code></code> <code>compute_loss_and_additional_losses(preds, features, target)</code> \u00b6 <p>Computes the loss and any additional losses given predictions of the model and ground truth data.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor]]</code> <p>A tuple with:</p> <ul> <li>The tensor for the loss.</li> <li>A dictionary of additional losses with their names and values, or None if there are no additional   losses.</li> </ul> Source code in <code>fl4health/clients/mkmmd_clients/ditto_mkmmd_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Computes the loss and any additional losses given predictions of the model and ground truth data.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n            - The tensor for the loss.\n            - A dictionary of additional losses with their names and values, or None if there are no additional\n              losses.\n    \"\"\"\n    loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n\n    if self.mkmmd_loss_weight != 0:\n        if self.beta_global_update_interval == -1:\n            # Update betas for the MK-MMD loss based on computed features during training\n            for layer, layer_mkmmd_loss in self.mkmmd_losses.items():\n                layer_mkmmd_loss.betas = layer_mkmmd_loss.optimize_betas(\n                    x=features[layer], y=features[\" \".join([\"init_global\", layer])], lambda_m=1e-5\n                )\n        # Compute MK-MMD loss\n        total_mkmmd_loss = torch.tensor(0.0, device=self.device)\n        for layer, layer_mkmmd_loss in self.mkmmd_losses.items():\n            mkmmd_loss = layer_mkmmd_loss(features[layer], features[\" \".join([\"init_global\", layer])])\n            additional_losses[\"_\".join([\"mkmmd_loss\", layer])] = mkmmd_loss.clone()\n            total_mkmmd_loss += mkmmd_loss\n        additional_losses[\"mkmmd_loss_total\"] = self.mkmmd_loss_weight * total_mkmmd_loss\n    if self.feature_l2_norm_weight != 0:\n        # Compute the average L2 norm of the features over the batch\n        feature_l2_norm_loss = torch.linalg.norm(features[\"features\"]) / len(features[\"features\"])\n        additional_losses[\"feature_l2_norm_loss\"] = self.feature_l2_norm_weight * feature_l2_norm_loss\n\n    return loss, additional_losses\n</code></pre>"},{"location":"api/#fl4health.clients.mkmmd_clients.mr_mtl_mkmmd_client","title":"<code>mr_mtl_mkmmd_client</code>","text":""},{"location":"api/#fl4health.clients.mkmmd_clients.mr_mtl_mkmmd_client.MrMtlMkMmdClient","title":"<code>MrMtlMkMmdClient</code>","text":"<p>               Bases: <code>MrMtlClient</code></p> Source code in <code>fl4health/clients/mkmmd_clients/mr_mtl_mkmmd_client.py</code> <pre><code>class MrMtlMkMmdClient(MrMtlClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n        mkmmd_loss_weight: float = 10.0,\n        feature_extraction_layers: Sequence[str] | None = None,\n        feature_l2_norm_weight: float = 0.0,\n        beta_global_update_interval: int = 20,\n        num_accumulating_batches: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client implements the MK-MMD loss function in the MR-MTL framework. The MK-MMD loss is a measure of the\n        distance between the distributions of the features of the local model and averaged local models of each round.\n        The MK-MMD loss is added to the local loss to penalize the local model for drifting away from the averaged\n        local models.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            mkmmd_loss_weight (float, optional): weight applied to the MK-MMD loss. Defaults to 10.0.\n            feature_extraction_layers (Sequence[str] | None, optional): List of layers from which to extract\n                and flatten features. Defaults to None.\n            feature_l2_norm_weight (float, optional): weight applied to the L2 norm of the features.\n                Defaults to 0.0.\n            beta_global_update_interval (int, optional): interval at which to update the betas for the MK-MMD loss. If\n                set to above 0, the betas will be updated based on whole distribution of latent features of data with\n                the given update interval. If set to 0, the betas will not be updated. If set to -1, the betas will be\n                updated after each individual batch based on only that individual batch. Defaults to 20.\n            num_accumulating_batches (int, optional): Number of batches to accumulate features to approximate the whole\n                distribution of the latent features for updating beta of the MK-MMD loss. This parameter is only used\n                if ``beta_global_update_interval`` is set to larger than 0. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.mkmmd_loss_weight = mkmmd_loss_weight\n        if self.mkmmd_loss_weight == 0:\n            log(\n                ERROR,\n                \"MK-MMD loss weight is set to 0. As MK-MMD loss will not be computed, \",\n                \"please use vanilla MrMtlClient instead.\",\n            )\n\n        self.feature_l2_norm_weight = feature_l2_norm_weight\n        self.beta_global_update_interval = beta_global_update_interval\n        if self.beta_global_update_interval == -1:\n            log(INFO, \"Betas for the MK-MMD loss will be updated for each individual batch.\")\n        elif self.beta_global_update_interval == 0:\n            log(INFO, \"Betas for the MK-MMD loss will not be updated.\")\n        elif self.beta_global_update_interval &gt; 0:\n            log(INFO, f\"Betas for the MK-MMD loss will be updated every {self.beta_global_update_interval} steps.\")\n        else:\n            raise ValueError(\"Invalid beta_global_update_interval. It should be either -1, 0 or a positive integer.\")\n        if feature_extraction_layers:\n            # By default, all of the features should be flattened for the MK-MMD loss\n            self.flatten_feature_extraction_layers = dict.fromkeys(feature_extraction_layers, True)\n        else:\n            self.flatten_feature_extraction_layers = {}\n        self.mkmmd_losses: dict[str, MkMmdLoss] = {}\n        for layer in self.flatten_feature_extraction_layers:\n            self.mkmmd_losses[layer] = MkMmdLoss(\n                device=self.device, minimize_type_two_error=True, normalize_features=True, layer_name=layer\n            ).to(self.device)\n\n        self.local_feature_extractor: FeatureExtractorBuffer\n        self.initial_global_feature_extractor: FeatureExtractorBuffer\n        self.num_accumulating_batches = num_accumulating_batches\n\n    def setup_client(self, config: Config) -&gt; None:\n        super().setup_client(config)\n        self.local_feature_extractor = FeatureExtractorBuffer(\n            model=self.model,\n            flatten_feature_extraction_layers=self.flatten_feature_extraction_layers,\n        )\n        # Register hooks to extract features from the local model if not already registered\n        self.local_feature_extractor._maybe_register_hooks()\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        super().update_before_train(current_server_round)\n        self.initial_global_feature_extractor = FeatureExtractorBuffer(\n            model=self.initial_global_model,\n            flatten_feature_extraction_layers=self.flatten_feature_extraction_layers,\n        )\n        # Register hooks to extract features from the initial global model if not already registered\n        self.initial_global_feature_extractor._maybe_register_hooks()\n\n    def _should_optimize_betas(self, step: int) -&gt; bool:\n        step_at_interval = (step - 1) % self.beta_global_update_interval == 0\n        valid_components_present = self.initial_global_model is not None\n        # If the mkmmd loss doesn't matter, we don't bother optimizing betas\n        weighted_mkmmd_loss = self.mkmmd_loss_weight != 0\n        return step_at_interval and valid_components_present and weighted_mkmmd_loss\n\n    def update_after_step(self, step: int, current_round: int | None = None) -&gt; None:\n        if self.beta_global_update_interval &gt; 0 and self._should_optimize_betas(step):\n            # Get the feature distribution of the local and initial global features with evaluation\n            # mode\n            local_distributions, initial_global_distributions = self.update_buffers(\n                self.model, self.initial_global_model\n            )\n            # Update betas for the MK-MMD loss based on gathered features during training\n            for layer, layer_mkmmd_loss in self.mkmmd_losses.items():\n                layer_mkmmd_loss.betas = layer_mkmmd_loss.optimize_betas(\n                    x=local_distributions[layer], y=initial_global_distributions[layer], lambda_m=1e-5\n                )\n        super().update_after_step(step)\n\n    def update_buffers(\n        self, local_model: torch.nn.Module, initial_global_model: torch.nn.Module\n    ) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n        \"\"\"\n        Update the feature buffer of the local and global features.\n\n        Args:\n            local_model (torch.nn.Module): Local model to extract features from.\n            initial_global_model (torch.nn.Module): Initial global model to extract features from.\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple containing the extracted\n                features using the local and initial global models.\n        \"\"\"\n        self.local_feature_extractor.clear_buffers()\n        self.initial_global_feature_extractor.clear_buffers()\n\n        self.local_feature_extractor.enable_accumulating_features()\n        self.initial_global_feature_extractor.enable_accumulating_features()\n\n        # Save the initial state of the local model to restore it after the buffer is populated,\n        # however as initial global model is already cloned and frozen, we don't need to save its state.\n        initial_state_local_model = local_model.training\n\n        # Set local model to evaluation mode, as we don't want to create a computational graph\n        # for the local model when populating the local buffer with features to compute optimal\n        # betas for the MK-MMD loss\n        local_model.eval()\n\n        # Make sure the local model is in evaluation mode before populating the local buffer\n        assert not local_model.training\n\n        # Make sure the initial global model is in evaluation mode before populating the global buffer\n        # as it is already cloned and frozen from the global model\n        assert not initial_global_model.training\n\n        with torch.no_grad():\n            for i, (input, _) in enumerate(self.train_loader):\n                input = input.to(self.device)\n                # Pass the input through the local model to populate the local_feature_extractor buffer\n                local_model(input)\n                # Pass the input through the initial global model to populate the initial_global_feature_extractor\n                # buffer\n                initial_global_model(input)\n                # Break if the number of accumulating batches is reached to avoid memory issues\n                if i == self.num_accumulating_batches:\n                    break\n        local_distributions = self.local_feature_extractor.get_extracted_features()\n        initial_global_distributions = self.initial_global_feature_extractor.get_extracted_features()\n        # Restore the initial state of the local model\n        if initial_state_local_model:\n            local_model.train()\n\n        self.local_feature_extractor.disable_accumulating_features()\n        self.initial_global_feature_extractor.disable_accumulating_features()\n\n        self.local_feature_extractor.clear_buffers()\n        self.initial_global_feature_extractor.clear_buffers()\n\n        return local_distributions, initial_global_distributions\n\n    def predict(\n        self,\n        input: TorchInputType,\n    ) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Computes the predictions for both models and pack them into the prediction dictionary.\n\n        Args:\n            input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n                it is assumed that the keys of input match the names of the keyword arguments of\n                ``self.model.forward()``.\n\n        Returns:\n            (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n                predictions indexed by name and the second element contains intermediate activations indexed by name.\n                By passing features, we can compute all the losses. All predictions included in dictionary will by\n                default be used to compute metrics separately.\n\n        Raises:\n            TypeError: Occurs when something other than a tensor or dict of tensors is passed in to the model's\n                forward method.\n            ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n                forward.\n        \"\"\"\n        preds = self.model(input)\n        features = self.local_feature_extractor.get_extracted_features()\n        if self.mkmmd_loss_weight != 0:\n            # Compute the features of the initial_global_model using register hooks in the update_before_train method\n            self.initial_global_model(input)\n            initial_global_features = self.initial_global_feature_extractor.get_extracted_features()\n            for key, initial_global_feature in initial_global_features.items():\n                features[\" \".join([\"init_global\", key])] = initial_global_feature\n\n        return {\"prediction\": preds}, features\n\n    def _maybe_checkpoint(self, loss: float, metrics: dict[str, Scalar], checkpoint_mode: CheckpointMode) -&gt; None:\n        # Hooks need to be removed before checkpointing the model\n        self.local_feature_extractor.remove_hooks()\n        super()._maybe_checkpoint(loss=loss, metrics=metrics, checkpoint_mode=checkpoint_mode)\n        # As hooks have to be removed to checkpoint the model, so we check if they need to be re-registered\n        # each time.\n        self.local_feature_extractor._maybe_register_hooks()\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training losses given predictions of the global and local models and ground truth data.\n        For the local model we add to the vanilla loss function by including Ditto penalty loss which is the\n        \\\\(\\\\ell^2\\\\) inner product between the initial global model weights and weights of the local model. This is\n        stored in backward The loss to optimize the global model is stored in the additional losses dictionary\n        under \u201cglobal_loss\u201d.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n                dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n                by name. Additional losses includes each loss component and the global model loss tensor.\n        \"\"\"\n        for layer_loss_module in self.mkmmd_losses.values():\n            assert layer_loss_module.training\n        # Check that both models are in training mode\n        assert self.model.training\n\n        # local loss is stored in loss, global model loss is stored in additional losses.\n        loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n        # Setting the adaptation loss to that of the local model, as its performance should dictate whether more or\n        # less weight is used to constrain it to the global model (as in FedProx)\n        additional_losses[\"loss_for_adaptation\"] = additional_losses[\"loss\"].clone()\n\n        # This is the MR-MTL penalty loss of the local model compared with the original Global model weights, scaled\n        # by drift_penalty_weight (or lambda in the original paper)\n        penalty_loss = self.compute_penalty_loss()\n        additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n        total_loss = loss + penalty_loss\n\n        # Add MK-MMD loss based on computed features during training\n        if self.mkmmd_loss_weight != 0:\n            total_loss += additional_losses[\"mkmmd_loss_total\"]\n\n        if self.feature_l2_norm_weight != 0:\n            total_loss += additional_losses[\"feature_l2_norm_loss\"]\n\n        additional_losses[\"total_loss\"] = total_loss.clone()\n\n        return TrainingLosses(backward=total_loss, additional_losses=additional_losses)\n\n    def compute_loss_and_additional_losses(\n        self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Computes the loss and any additional losses given predictions of the model and ground truth data.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n                - The tensor for the loss.\n                - A dictionary of additional losses with their names and values, or None if there are no additional\n                  losses.\n        \"\"\"\n        assert \"prediction\" in preds\n        # Compute model loss + MR-MTL constraint term\n        loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n\n        if additional_losses is None:\n            additional_losses = {\"loss\": loss}\n\n        if self.mkmmd_loss_weight != 0:\n            if self.beta_global_update_interval == -1:\n                # Update betas for the MK-MMD loss based on computed features during training\n                for layer, layer_mkmmd_loss in self.mkmmd_losses.items():\n                    layer_mkmmd_loss.betas = layer_mkmmd_loss.optimize_betas(\n                        x=features[layer], y=features[\" \".join([\"init_global\", layer])], lambda_m=1e-5\n                    )\n            # Compute MK-MMD loss\n            total_mkmmd_loss = torch.tensor(0.0, device=self.device)\n            for layer, layer_mkmmd_loss in self.mkmmd_losses.items():\n                mkmmd_loss = layer_mkmmd_loss(features[layer], features[\" \".join([\"init_global\", layer])])\n                additional_losses[\"_\".join([\"mkmmd_loss\", layer])] = mkmmd_loss.clone()\n                total_mkmmd_loss += mkmmd_loss\n            additional_losses[\"mkmmd_loss_total\"] = self.mkmmd_loss_weight * total_mkmmd_loss\n        if self.feature_l2_norm_weight != 0:\n            # Compute the average L2 norm of the features over the batch\n            feature_l2_norm_loss = torch.linalg.norm(features[\"features\"]) / len(features[\"features\"])\n            additional_losses[\"feature_l2_norm_loss\"] = self.feature_l2_norm_weight * feature_l2_norm_loss\n\n        return loss, additional_losses\n</code></pre> <code></code> <code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None, mkmmd_loss_weight=10.0, feature_extraction_layers=None, feature_l2_norm_weight=0.0, beta_global_update_interval=20, num_accumulating_batches=None)</code> \u00b6 <p>This client implements the MK-MMD loss function in the MR-MTL framework. The MK-MMD loss is a measure of the distance between the distributions of the features of the local model and averaged local models of each round. The MK-MMD loss is added to the local loss to penalize the local model for drifting away from the averaged local models.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>mkmmd_loss_weight</code> <code>float</code> <p>weight applied to the MK-MMD loss. Defaults to 10.0.</p> <code>10.0</code> <code>feature_extraction_layers</code> <code>Sequence[str] | None</code> <p>List of layers from which to extract and flatten features. Defaults to None.</p> <code>None</code> <code>feature_l2_norm_weight</code> <code>float</code> <p>weight applied to the L2 norm of the features. Defaults to 0.0.</p> <code>0.0</code> <code>beta_global_update_interval</code> <code>int</code> <p>interval at which to update the betas for the MK-MMD loss. If set to above 0, the betas will be updated based on whole distribution of latent features of data with the given update interval. If set to 0, the betas will not be updated. If set to -1, the betas will be updated after each individual batch based on only that individual batch. Defaults to 20.</p> <code>20</code> <code>num_accumulating_batches</code> <code>int</code> <p>Number of batches to accumulate features to approximate the whole distribution of the latent features for updating beta of the MK-MMD loss. This parameter is only used if <code>beta_global_update_interval</code> is set to larger than 0. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/mkmmd_clients/mr_mtl_mkmmd_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n    mkmmd_loss_weight: float = 10.0,\n    feature_extraction_layers: Sequence[str] | None = None,\n    feature_l2_norm_weight: float = 0.0,\n    beta_global_update_interval: int = 20,\n    num_accumulating_batches: int | None = None,\n) -&gt; None:\n    \"\"\"\n    This client implements the MK-MMD loss function in the MR-MTL framework. The MK-MMD loss is a measure of the\n    distance between the distributions of the features of the local model and averaged local models of each round.\n    The MK-MMD loss is added to the local loss to penalize the local model for drifting away from the averaged\n    local models.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        mkmmd_loss_weight (float, optional): weight applied to the MK-MMD loss. Defaults to 10.0.\n        feature_extraction_layers (Sequence[str] | None, optional): List of layers from which to extract\n            and flatten features. Defaults to None.\n        feature_l2_norm_weight (float, optional): weight applied to the L2 norm of the features.\n            Defaults to 0.0.\n        beta_global_update_interval (int, optional): interval at which to update the betas for the MK-MMD loss. If\n            set to above 0, the betas will be updated based on whole distribution of latent features of data with\n            the given update interval. If set to 0, the betas will not be updated. If set to -1, the betas will be\n            updated after each individual batch based on only that individual batch. Defaults to 20.\n        num_accumulating_batches (int, optional): Number of batches to accumulate features to approximate the whole\n            distribution of the latent features for updating beta of the MK-MMD loss. This parameter is only used\n            if ``beta_global_update_interval`` is set to larger than 0. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.mkmmd_loss_weight = mkmmd_loss_weight\n    if self.mkmmd_loss_weight == 0:\n        log(\n            ERROR,\n            \"MK-MMD loss weight is set to 0. As MK-MMD loss will not be computed, \",\n            \"please use vanilla MrMtlClient instead.\",\n        )\n\n    self.feature_l2_norm_weight = feature_l2_norm_weight\n    self.beta_global_update_interval = beta_global_update_interval\n    if self.beta_global_update_interval == -1:\n        log(INFO, \"Betas for the MK-MMD loss will be updated for each individual batch.\")\n    elif self.beta_global_update_interval == 0:\n        log(INFO, \"Betas for the MK-MMD loss will not be updated.\")\n    elif self.beta_global_update_interval &gt; 0:\n        log(INFO, f\"Betas for the MK-MMD loss will be updated every {self.beta_global_update_interval} steps.\")\n    else:\n        raise ValueError(\"Invalid beta_global_update_interval. It should be either -1, 0 or a positive integer.\")\n    if feature_extraction_layers:\n        # By default, all of the features should be flattened for the MK-MMD loss\n        self.flatten_feature_extraction_layers = dict.fromkeys(feature_extraction_layers, True)\n    else:\n        self.flatten_feature_extraction_layers = {}\n    self.mkmmd_losses: dict[str, MkMmdLoss] = {}\n    for layer in self.flatten_feature_extraction_layers:\n        self.mkmmd_losses[layer] = MkMmdLoss(\n            device=self.device, minimize_type_two_error=True, normalize_features=True, layer_name=layer\n        ).to(self.device)\n\n    self.local_feature_extractor: FeatureExtractorBuffer\n    self.initial_global_feature_extractor: FeatureExtractorBuffer\n    self.num_accumulating_batches = num_accumulating_batches\n</code></pre> <code></code> <code>update_buffers(local_model, initial_global_model)</code> \u00b6 <p>Update the feature buffer of the local and global features.</p> <p>Parameters:</p> Name Type Description Default <code>local_model</code> <code>Module</code> <p>Local model to extract features from.</p> required <code>initial_global_model</code> <code>Module</code> <p>Initial global model to extract features from.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>A tuple containing the extracted features using the local and initial global models.</p> Source code in <code>fl4health/clients/mkmmd_clients/mr_mtl_mkmmd_client.py</code> <pre><code>def update_buffers(\n    self, local_model: torch.nn.Module, initial_global_model: torch.nn.Module\n) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n    \"\"\"\n    Update the feature buffer of the local and global features.\n\n    Args:\n        local_model (torch.nn.Module): Local model to extract features from.\n        initial_global_model (torch.nn.Module): Initial global model to extract features from.\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple containing the extracted\n            features using the local and initial global models.\n    \"\"\"\n    self.local_feature_extractor.clear_buffers()\n    self.initial_global_feature_extractor.clear_buffers()\n\n    self.local_feature_extractor.enable_accumulating_features()\n    self.initial_global_feature_extractor.enable_accumulating_features()\n\n    # Save the initial state of the local model to restore it after the buffer is populated,\n    # however as initial global model is already cloned and frozen, we don't need to save its state.\n    initial_state_local_model = local_model.training\n\n    # Set local model to evaluation mode, as we don't want to create a computational graph\n    # for the local model when populating the local buffer with features to compute optimal\n    # betas for the MK-MMD loss\n    local_model.eval()\n\n    # Make sure the local model is in evaluation mode before populating the local buffer\n    assert not local_model.training\n\n    # Make sure the initial global model is in evaluation mode before populating the global buffer\n    # as it is already cloned and frozen from the global model\n    assert not initial_global_model.training\n\n    with torch.no_grad():\n        for i, (input, _) in enumerate(self.train_loader):\n            input = input.to(self.device)\n            # Pass the input through the local model to populate the local_feature_extractor buffer\n            local_model(input)\n            # Pass the input through the initial global model to populate the initial_global_feature_extractor\n            # buffer\n            initial_global_model(input)\n            # Break if the number of accumulating batches is reached to avoid memory issues\n            if i == self.num_accumulating_batches:\n                break\n    local_distributions = self.local_feature_extractor.get_extracted_features()\n    initial_global_distributions = self.initial_global_feature_extractor.get_extracted_features()\n    # Restore the initial state of the local model\n    if initial_state_local_model:\n        local_model.train()\n\n    self.local_feature_extractor.disable_accumulating_features()\n    self.initial_global_feature_extractor.disable_accumulating_features()\n\n    self.local_feature_extractor.clear_buffers()\n    self.initial_global_feature_extractor.clear_buffers()\n\n    return local_distributions, initial_global_distributions\n</code></pre> <code></code> <code>predict(input)</code> \u00b6 <p>Computes the predictions for both models and pack them into the prediction dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into the model. If input is of type <code>dict[str, torch.Tensor]</code>, it is assumed that the keys of input match the names of the keyword arguments of <code>self.model.forward()</code>.</p> required <p>Returns:</p> Type Description <code>tuple[TorchPredType, TorchFeatureType]</code> <p>A tuple in which the first element contains a dictionary of predictions indexed by name and the second element contains intermediate activations indexed by name. By passing features, we can compute all the losses. All predictions included in dictionary will by default be used to compute metrics separately.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Occurs when something other than a tensor or dict of tensors is passed in to the model's forward method.</p> <code>ValueError</code> <p>Occurs when something other than a tensor or dict of tensors is returned by the model forward.</p> Source code in <code>fl4health/clients/mkmmd_clients/mr_mtl_mkmmd_client.py</code> <pre><code>def predict(\n    self,\n    input: TorchInputType,\n) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Computes the predictions for both models and pack them into the prediction dictionary.\n\n    Args:\n        input (TorchInputType): Inputs to be fed into the model. If input is of type ``dict[str, torch.Tensor]``,\n            it is assumed that the keys of input match the names of the keyword arguments of\n            ``self.model.forward()``.\n\n    Returns:\n        (tuple[TorchPredType, TorchFeatureType]): A tuple in which the first element contains a dictionary of\n            predictions indexed by name and the second element contains intermediate activations indexed by name.\n            By passing features, we can compute all the losses. All predictions included in dictionary will by\n            default be used to compute metrics separately.\n\n    Raises:\n        TypeError: Occurs when something other than a tensor or dict of tensors is passed in to the model's\n            forward method.\n        ValueError: Occurs when something other than a tensor or dict of tensors is returned by the model\n            forward.\n    \"\"\"\n    preds = self.model(input)\n    features = self.local_feature_extractor.get_extracted_features()\n    if self.mkmmd_loss_weight != 0:\n        # Compute the features of the initial_global_model using register hooks in the update_before_train method\n        self.initial_global_model(input)\n        initial_global_features = self.initial_global_feature_extractor.get_extracted_features()\n        for key, initial_global_feature in initial_global_features.items():\n            features[\" \".join([\"init_global\", key])] = initial_global_feature\n\n    return {\"prediction\": preds}, features\n</code></pre> <code></code> <code>compute_training_loss(preds, features, target)</code> \u00b6 <p>Computes training losses given predictions of the global and local models and ground truth data. For the local model we add to the vanilla loss function by including Ditto penalty loss which is the \\(\\ell^2\\) inner product between the initial global model weights and weights of the local model. This is stored in backward The loss to optimize the global model is stored in the additional losses dictionary under \u201cglobal_loss\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name. Additional losses includes each loss component and the global model loss tensor.</p> Source code in <code>fl4health/clients/mkmmd_clients/mr_mtl_mkmmd_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training losses given predictions of the global and local models and ground truth data.\n    For the local model we add to the vanilla loss function by including Ditto penalty loss which is the\n    \\\\(\\\\ell^2\\\\) inner product between the initial global model weights and weights of the local model. This is\n    stored in backward The loss to optimize the global model is stored in the additional losses dictionary\n    under \u201cglobal_loss\u201d.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included in\n            dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n            by name. Additional losses includes each loss component and the global model loss tensor.\n    \"\"\"\n    for layer_loss_module in self.mkmmd_losses.values():\n        assert layer_loss_module.training\n    # Check that both models are in training mode\n    assert self.model.training\n\n    # local loss is stored in loss, global model loss is stored in additional losses.\n    loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n    # Setting the adaptation loss to that of the local model, as its performance should dictate whether more or\n    # less weight is used to constrain it to the global model (as in FedProx)\n    additional_losses[\"loss_for_adaptation\"] = additional_losses[\"loss\"].clone()\n\n    # This is the MR-MTL penalty loss of the local model compared with the original Global model weights, scaled\n    # by drift_penalty_weight (or lambda in the original paper)\n    penalty_loss = self.compute_penalty_loss()\n    additional_losses[\"penalty_loss\"] = penalty_loss.clone()\n    total_loss = loss + penalty_loss\n\n    # Add MK-MMD loss based on computed features during training\n    if self.mkmmd_loss_weight != 0:\n        total_loss += additional_losses[\"mkmmd_loss_total\"]\n\n    if self.feature_l2_norm_weight != 0:\n        total_loss += additional_losses[\"feature_l2_norm_loss\"]\n\n    additional_losses[\"total_loss\"] = total_loss.clone()\n\n    return TrainingLosses(backward=total_loss, additional_losses=additional_losses)\n</code></pre> <code></code> <code>compute_loss_and_additional_losses(preds, features, target)</code> \u00b6 <p>Computes the loss and any additional losses given predictions of the model and ground truth data.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor]]</code> <p>A tuple with:</p> <ul> <li>The tensor for the loss.</li> <li>A dictionary of additional losses with their names and values, or None if there are no additional   losses.</li> </ul> Source code in <code>fl4health/clients/mkmmd_clients/mr_mtl_mkmmd_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Computes the loss and any additional losses given predictions of the model and ground truth data.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n            - The tensor for the loss.\n            - A dictionary of additional losses with their names and values, or None if there are no additional\n              losses.\n    \"\"\"\n    assert \"prediction\" in preds\n    # Compute model loss + MR-MTL constraint term\n    loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n\n    if additional_losses is None:\n        additional_losses = {\"loss\": loss}\n\n    if self.mkmmd_loss_weight != 0:\n        if self.beta_global_update_interval == -1:\n            # Update betas for the MK-MMD loss based on computed features during training\n            for layer, layer_mkmmd_loss in self.mkmmd_losses.items():\n                layer_mkmmd_loss.betas = layer_mkmmd_loss.optimize_betas(\n                    x=features[layer], y=features[\" \".join([\"init_global\", layer])], lambda_m=1e-5\n                )\n        # Compute MK-MMD loss\n        total_mkmmd_loss = torch.tensor(0.0, device=self.device)\n        for layer, layer_mkmmd_loss in self.mkmmd_losses.items():\n            mkmmd_loss = layer_mkmmd_loss(features[layer], features[\" \".join([\"init_global\", layer])])\n            additional_losses[\"_\".join([\"mkmmd_loss\", layer])] = mkmmd_loss.clone()\n            total_mkmmd_loss += mkmmd_loss\n        additional_losses[\"mkmmd_loss_total\"] = self.mkmmd_loss_weight * total_mkmmd_loss\n    if self.feature_l2_norm_weight != 0:\n        # Compute the average L2 norm of the features over the batch\n        feature_l2_norm_loss = torch.linalg.norm(features[\"features\"]) / len(features[\"features\"])\n        additional_losses[\"feature_l2_norm_loss\"] = self.feature_l2_norm_weight * feature_l2_norm_loss\n\n    return loss, additional_losses\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client","title":"<code>model_merge_client</code>","text":""},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient","title":"<code>ModelMergeClient</code>","text":"<p>               Bases: <code>NumPyClient</code></p> Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>class ModelMergeClient(NumPyClient):\n    def __init__(\n        self,\n        data_path: Path,\n        model_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        reporters: Sequence[BaseReporter] | None = None,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        ``ModelMergeClient`` to support functionality to simply perform model merging across client models and\n        subsequently evaluate.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            model_path (Path): path to the checkpoint of the client model to be used in model merging.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the client should\n                send data to.\n            client_name (str): An optional client name that uniquely identifies a client. If not passed, a hash is\n                randomly generated.\n        \"\"\"\n        self.data_path = data_path\n        self.model_path = model_path\n        self.metrics = metrics\n        self.device = device\n        self.client_name = client_name if client_name is not None else generate_hash()\n\n        self.initialized = False\n        self.test_metric_manager = MetricManager(metrics=self.metrics, metric_manager_name=\"test\")\n\n        # Initialize reporters with client information.\n        self.reports_manager = ReportsManager(reporters)\n        self.reports_manager.initialize(id=self.client_name)\n\n        self.model: nn.Module\n        self.test_loader: DataLoader\n        self.num_test_samples: int\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Sets up Merge Client by initializing model, dataloader and parameter exchanger with user defined methods.\n        Subsequently, sets initialized attribute to True.\n\n        Args:\n            config (Config): The configuration from the server.\n        \"\"\"\n        self.model = self.get_model(config)\n        self.test_loader = self.get_test_data_loader(config)\n        self.num_test_samples = len(self.test_loader.dataset)  # type: ignore\n        self.parameter_exchanger = self.get_parameter_exchanger(config)\n\n        self.initialized = True\n\n    def get_parameters(self, config: Config) -&gt; NDArrays:\n        \"\"\"\n        Determines which parameters are sent back to the server for aggregation. This uses a parameter exchanger to\n        determine parameters sent.\n\n        For the ``ModelMergeClient``, we assume that ``self.setup_client`` has already been called as it does not\n        support client polling so ``get_parameters`` is called from fit and thus should be initialized by this point.\n\n        Args:\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n        Returns:\n            (NDArrays): These are the parameters to be sent to the server. At minimum they represent the relevant model\n                parameters to be aggregated, but can contain more information.\n        \"\"\"\n        assert self.model is not None\n        return self.parameter_exchanger.push_parameters(self.model, config=config)\n\n    def set_parameters(self, parameters: NDArrays, config: Config) -&gt; None:\n        \"\"\"\n        Sets the local model parameters transferred from the server using a parameter exchanger to coordinate how\n        parameters are set.\n\n        For the ModelMergeClient, we assume that initially parameters are being set to the parameters in the\n        ``nn.Module`` returned by the user defined ``get_model`` method. Thus, ``set_parameters`` is only called once\n        after model merging has occurred and before federated evaluation.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model but may contain more information than that.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        \"\"\"\n        assert self.initialized\n        self.parameter_exchanger.pull_parameters(parameters, self.model)\n\n    def fit(self, parameters: NDArrays, config: Config) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n        \"\"\"\n        Initializes client, validates local client model on local test data and returns parameters, test dataset\n        length and test metrics. Importantly, parameters from Server, which is empty, is not used to initialized the\n        client model.\n\n        **NOTE**: Since we only assume the client provides a ``test_loader``, client evaluation and sample counts are\n        always based off the client ``test_loader``.\n\n        Args:\n            parameters (NDArrays): Not used.\n            config (NDArrays): The config from the server.\n\n        Returns:\n            (tuple[NDArrays, int, dict[str, Scalar]]): The local model parameters along with the number of samples in\n                the local test dataset and the computed metrics of the local model on the local test dataset.\n\n        Raises:\n            AssertionError: If model is initialized prior to fit method being called which should not happen in the\n                case of the ``ModelMergeClient``.\n        \"\"\"\n        assert not self.initialized\n        self.setup_client(config)\n\n        self.reports_manager.report(\n            data={\"host_type\": \"client\", \"fit_start\": datetime.datetime.now()},\n        )\n\n        val_metrics = self.validate()\n\n        self.reports_manager.report(\n            data={\"fit_metrics\": val_metrics, \"host_type\": \"client\", \"fit_end\": datetime.datetime.now()},\n        )\n\n        return self.get_parameters(config), self.num_test_samples, val_metrics\n\n    def _move_data_to_device(self, data: TorchInputType | TorchTargetType) -&gt; TorchTargetType | TorchInputType:\n        \"\"\"\n        Moving data to ``self.device`` where data is intended to be either input to the model or the targets that the\n        model is trying to achieve.\n\n        Args:\n            data (TorchInputType | TorchTargetType): The data to move to ``self.device``. Can be a ``TorchInputType``\n                or a  ``TorchTargetType``\n\n        Raises:\n            TypeError: Raised if data is not one of the types specified by ``TorchInputType`` or ``TorchTargetType``\n\n        Returns:\n            (TorchTargetType | TorchInputType): The data argument except now it's been moved to ``self.device``\n        \"\"\"\n        # Currently we expect both inputs and targets to be either tensors\n        # or dictionaries of tensors\n        if isinstance(data, torch.Tensor):\n            return data.to(self.device)\n        if isinstance(data, dict):\n            return {key: value.to(self.device) for key, value in data.items()}\n        raise TypeError(\n            \"data must be of type torch.Tensor or dict[str, torch.Tensor]. If definition of TorchInputType or \"\n            \" TorchTargetType has  changed this method might need to be updated or split into two\"\n        )\n\n    def validate(self) -&gt; dict[str, Scalar]:\n        \"\"\"\n        Validate the model on the test dataset.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The loss and a dictionary of metrics from test set.\n        \"\"\"\n        self.model.eval()\n        self.test_metric_manager.clear()\n        with torch.no_grad():\n            for input, target in self.test_loader:\n                input = move_data_to_device(input, self.device)\n                target = move_data_to_device(target, self.device)\n                preds = {\"predictions\": self.model(input)}\n                self.test_metric_manager.update(preds, target)\n\n        return self.test_metric_manager.compute()\n\n    def evaluate(self, parameters: NDArrays, config: Config) -&gt; tuple[float, int, dict[str, Scalar]]:\n        \"\"\"\n        Evaluate the provided parameters using the locally held dataset.\n\n        Args:\n            parameters (NDArrays): The current model parameters.\n            config (Config): Configuration object from the server.\n\n        Returns:\n            (tuple[float, int, dict[str, Scalar]): The float represents the loss which is assumed to be 0 for the\n                ``ModelMergeClient``. The int represents the number of examples in the local test dataset and the\n                dictionary is the computed metrics on the test set.\n        \"\"\"\n        self.set_parameters(parameters, config)\n        metrics = self.validate()\n        return 0.0, len(self.test_loader), metrics\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        \"\"\"\n        Parameter exchange is assumed to always be full for model merging clients. However, this functionality\n        may be overridden if a different exchanger is needed.\n\n        Used in non-standard way for ``ModelMergeClient`` as ``set_parameters`` is only called for evaluate as\n        parameters should initially be set to the parameters in the ``nn.Module`` returned by ``get_model``.\n\n        Args:\n            config (Config): Configuration object from the server.\n\n        Returns:\n            (FullParameterExchanger): The parameter exchanger used to set and get parameters.\n        \"\"\"\n        return FullParameterExchanger()\n\n    @abstractmethod\n    def get_model(self, config: Config) -&gt; nn.Module:\n        \"\"\"\n        User defined method that returns PyTorch model. This is the local model that will be communicated to the\n        server for merging.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (nn.Module): The client model.\n\n        Raises:\n            NotImplementedError: To be defined in child class.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_test_data_loader(self, config: Config) -&gt; DataLoader:\n        \"\"\"\n        User defined method that returns a PyTorch Test DataLoader.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (DataLoader): Client test data loader.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient.__init__","title":"<code>__init__(data_path, model_path, metrics, device, reporters=None, client_name=None)</code>","text":"<p><code>ModelMergeClient</code> to support functionality to simply perform model merging across client models and subsequently evaluate.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>model_path</code> <code>Path</code> <p>path to the checkpoint of the client model to be used in model merging.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>reporters</code> <code>Sequence[BaseReporter]</code> <p>A sequence of FL4Health reporters which the client should send data to.</p> <code>None</code> <code>client_name</code> <code>str</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated.</p> <code>None</code> Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    model_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    reporters: Sequence[BaseReporter] | None = None,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    ``ModelMergeClient`` to support functionality to simply perform model merging across client models and\n    subsequently evaluate.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        model_path (Path): path to the checkpoint of the client model to be used in model merging.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the client should\n            send data to.\n        client_name (str): An optional client name that uniquely identifies a client. If not passed, a hash is\n            randomly generated.\n    \"\"\"\n    self.data_path = data_path\n    self.model_path = model_path\n    self.metrics = metrics\n    self.device = device\n    self.client_name = client_name if client_name is not None else generate_hash()\n\n    self.initialized = False\n    self.test_metric_manager = MetricManager(metrics=self.metrics, metric_manager_name=\"test\")\n\n    # Initialize reporters with client information.\n    self.reports_manager = ReportsManager(reporters)\n    self.reports_manager.initialize(id=self.client_name)\n\n    self.model: nn.Module\n    self.test_loader: DataLoader\n    self.num_test_samples: int\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Sets up Merge Client by initializing model, dataloader and parameter exchanger with user defined methods. Subsequently, sets initialized attribute to True.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The configuration from the server.</p> required Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Sets up Merge Client by initializing model, dataloader and parameter exchanger with user defined methods.\n    Subsequently, sets initialized attribute to True.\n\n    Args:\n        config (Config): The configuration from the server.\n    \"\"\"\n    self.model = self.get_model(config)\n    self.test_loader = self.get_test_data_loader(config)\n    self.num_test_samples = len(self.test_loader.dataset)  # type: ignore\n    self.parameter_exchanger = self.get_parameter_exchanger(config)\n\n    self.initialized = True\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>Determines which parameters are sent back to the server for aggregation. This uses a parameter exchanger to determine parameters sent.</p> <p>For the <code>ModelMergeClient</code>, we assume that <code>self.setup_client</code> has already been called as it does not support client polling so <code>get_parameters</code> is called from fit and thus should be initialized by this point.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>These are the parameters to be sent to the server. At minimum they represent the relevant model parameters to be aggregated, but can contain more information.</p> Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>def get_parameters(self, config: Config) -&gt; NDArrays:\n    \"\"\"\n    Determines which parameters are sent back to the server for aggregation. This uses a parameter exchanger to\n    determine parameters sent.\n\n    For the ``ModelMergeClient``, we assume that ``self.setup_client`` has already been called as it does not\n    support client polling so ``get_parameters`` is called from fit and thus should be initialized by this point.\n\n    Args:\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n    Returns:\n        (NDArrays): These are the parameters to be sent to the server. At minimum they represent the relevant model\n            parameters to be aggregated, but can contain more information.\n    \"\"\"\n    assert self.model is not None\n    return self.parameter_exchanger.push_parameters(self.model, config=config)\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient.set_parameters","title":"<code>set_parameters(parameters, config)</code>","text":"<p>Sets the local model parameters transferred from the server using a parameter exchanger to coordinate how parameters are set.</p> <p>For the ModelMergeClient, we assume that initially parameters are being set to the parameters in the <code>nn.Module</code> returned by the user defined <code>get_model</code> method. Thus, <code>set_parameters</code> is only called once after model merging has occurred and before federated evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model but may contain more information than that.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>def set_parameters(self, parameters: NDArrays, config: Config) -&gt; None:\n    \"\"\"\n    Sets the local model parameters transferred from the server using a parameter exchanger to coordinate how\n    parameters are set.\n\n    For the ModelMergeClient, we assume that initially parameters are being set to the parameters in the\n    ``nn.Module`` returned by the user defined ``get_model`` method. Thus, ``set_parameters`` is only called once\n    after model merging has occurred and before federated evaluation.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model but may contain more information than that.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n    \"\"\"\n    assert self.initialized\n    self.parameter_exchanger.pull_parameters(parameters, self.model)\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient.fit","title":"<code>fit(parameters, config)</code>","text":"<p>Initializes client, validates local client model on local test data and returns parameters, test dataset length and test metrics. Importantly, parameters from Server, which is empty, is not used to initialized the client model.</p> <p>NOTE: Since we only assume the client provides a <code>test_loader</code>, client evaluation and sample counts are always based off the client <code>test_loader</code>.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Not used.</p> required <code>config</code> <code>NDArrays</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>tuple[NDArrays, int, dict[str, Scalar]]</code> <p>The local model parameters along with the number of samples in the local test dataset and the computed metrics of the local model on the local test dataset.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If model is initialized prior to fit method being called which should not happen in the case of the <code>ModelMergeClient</code>.</p> Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>def fit(self, parameters: NDArrays, config: Config) -&gt; tuple[NDArrays, int, dict[str, Scalar]]:\n    \"\"\"\n    Initializes client, validates local client model on local test data and returns parameters, test dataset\n    length and test metrics. Importantly, parameters from Server, which is empty, is not used to initialized the\n    client model.\n\n    **NOTE**: Since we only assume the client provides a ``test_loader``, client evaluation and sample counts are\n    always based off the client ``test_loader``.\n\n    Args:\n        parameters (NDArrays): Not used.\n        config (NDArrays): The config from the server.\n\n    Returns:\n        (tuple[NDArrays, int, dict[str, Scalar]]): The local model parameters along with the number of samples in\n            the local test dataset and the computed metrics of the local model on the local test dataset.\n\n    Raises:\n        AssertionError: If model is initialized prior to fit method being called which should not happen in the\n            case of the ``ModelMergeClient``.\n    \"\"\"\n    assert not self.initialized\n    self.setup_client(config)\n\n    self.reports_manager.report(\n        data={\"host_type\": \"client\", \"fit_start\": datetime.datetime.now()},\n    )\n\n    val_metrics = self.validate()\n\n    self.reports_manager.report(\n        data={\"fit_metrics\": val_metrics, \"host_type\": \"client\", \"fit_end\": datetime.datetime.now()},\n    )\n\n    return self.get_parameters(config), self.num_test_samples, val_metrics\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient.validate","title":"<code>validate()</code>","text":"<p>Validate the model on the test dataset.</p> <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]]</code> <p>The loss and a dictionary of metrics from test set.</p> Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>def validate(self) -&gt; dict[str, Scalar]:\n    \"\"\"\n    Validate the model on the test dataset.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]]): The loss and a dictionary of metrics from test set.\n    \"\"\"\n    self.model.eval()\n    self.test_metric_manager.clear()\n    with torch.no_grad():\n        for input, target in self.test_loader:\n            input = move_data_to_device(input, self.device)\n            target = move_data_to_device(target, self.device)\n            preds = {\"predictions\": self.model(input)}\n            self.test_metric_manager.update(preds, target)\n\n    return self.test_metric_manager.compute()\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient.evaluate","title":"<code>evaluate(parameters, config)</code>","text":"<p>Evaluate the provided parameters using the locally held dataset.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>The current model parameters.</p> required <code>config</code> <code>Config</code> <p>Configuration object from the server.</p> required <p>Returns:</p> Type Description <code>tuple[float, int, dict[str, Scalar]</code> <p>The float represents the loss which is assumed to be 0 for the <code>ModelMergeClient</code>. The int represents the number of examples in the local test dataset and the dictionary is the computed metrics on the test set.</p> Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>def evaluate(self, parameters: NDArrays, config: Config) -&gt; tuple[float, int, dict[str, Scalar]]:\n    \"\"\"\n    Evaluate the provided parameters using the locally held dataset.\n\n    Args:\n        parameters (NDArrays): The current model parameters.\n        config (Config): Configuration object from the server.\n\n    Returns:\n        (tuple[float, int, dict[str, Scalar]): The float represents the loss which is assumed to be 0 for the\n            ``ModelMergeClient``. The int represents the number of examples in the local test dataset and the\n            dictionary is the computed metrics on the test set.\n    \"\"\"\n    self.set_parameters(parameters, config)\n    metrics = self.validate()\n    return 0.0, len(self.test_loader), metrics\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient.get_parameter_exchanger","title":"<code>get_parameter_exchanger(config)</code>","text":"<p>Parameter exchange is assumed to always be full for model merging clients. However, this functionality may be overridden if a different exchanger is needed.</p> <p>Used in non-standard way for <code>ModelMergeClient</code> as <code>set_parameters</code> is only called for evaluate as parameters should initially be set to the parameters in the <code>nn.Module</code> returned by <code>get_model</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration object from the server.</p> required <p>Returns:</p> Type Description <code>FullParameterExchanger</code> <p>The parameter exchanger used to set and get parameters.</p> Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n    \"\"\"\n    Parameter exchange is assumed to always be full for model merging clients. However, this functionality\n    may be overridden if a different exchanger is needed.\n\n    Used in non-standard way for ``ModelMergeClient`` as ``set_parameters`` is only called for evaluate as\n    parameters should initially be set to the parameters in the ``nn.Module`` returned by ``get_model``.\n\n    Args:\n        config (Config): Configuration object from the server.\n\n    Returns:\n        (FullParameterExchanger): The parameter exchanger used to set and get parameters.\n    \"\"\"\n    return FullParameterExchanger()\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient.get_model","title":"<code>get_model(config)</code>  <code>abstractmethod</code>","text":"<p>User defined method that returns PyTorch model. This is the local model that will be communicated to the server for merging.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The client model.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in child class.</p> Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>@abstractmethod\ndef get_model(self, config: Config) -&gt; nn.Module:\n    \"\"\"\n    User defined method that returns PyTorch model. This is the local model that will be communicated to the\n    server for merging.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (nn.Module): The client model.\n\n    Raises:\n        NotImplementedError: To be defined in child class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.model_merge_client.ModelMergeClient.get_test_data_loader","title":"<code>get_test_data_loader(config)</code>  <code>abstractmethod</code>","text":"<p>User defined method that returns a PyTorch Test DataLoader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>DataLoader</code> <p>Client test data loader.</p> Source code in <code>fl4health/clients/model_merge_client.py</code> <pre><code>@abstractmethod\ndef get_test_data_loader(self, config: Config) -&gt; DataLoader:\n    \"\"\"\n    User defined method that returns a PyTorch Test DataLoader.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (DataLoader): Client test data loader.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.moon_client","title":"<code>moon_client</code>","text":""},{"location":"api/#fl4health.clients.moon_client.MoonClient","title":"<code>MoonClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/moon_client.py</code> <pre><code>class MoonClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n        temperature: float = 0.5,\n        contrastive_weight: float = 1.0,\n        len_old_models_buffer: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        This client implements the MOON algorithm from Model-Contrastive Federated Learning. The key idea of MOON is\n        to enforce similarity between representations from the global and current local model through a contrastive\n        loss to constrain the local training of individual parties in the non-IID setting.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            temperature (float, optional): Temperature used in the calculation of the contrastive loss.\n                Defaults to 0.5.\n            contrastive_weight (float, optional): Weight placed on the contrastive loss function. Referred to as mu\n                in the original paper. Defaults to 1.0.\n            len_old_models_buffer (int, optional): Number of old models to be stored for computation in the\n                contrastive learning loss function. Defaults to 1.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.temperature = temperature\n        self.contrastive_weight = contrastive_weight\n        if self.contrastive_weight == 0:\n            log(WARNING, \"Contrastive loss weight is set to 0, thus Contrastive loss will not be computed.\")\n        self.contrastive_loss_function = MoonContrastiveLoss(self.device, temperature=temperature)\n\n        # Saving previous local models and a global model at each communication round to compute contrastive loss\n        self.len_old_models_buffer = len_old_models_buffer\n        self.old_models_list: list[torch.nn.Module] = []\n        self.global_model: torch.nn.Module | None = None\n\n    def predict(self, input: TorchInputType) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Computes the prediction(s) and features of the model(s) given the input. This function also produces the\n        necessary features from the ``global_model`` (aggregated model from server) and ``old_models`` (previous\n        client-side optimized models) in order to be able to compute the appropriate contrastive loss.\n\n        Args:\n            input (TorchInputType): Inputs to be fed into the model. ``TorchInputType`` is simply an alias\n                for the union of ``torch.Tensor`` and ``dict[str, torch.Tensor]``. Here, the MOON models require\n                input to simply be of type ``torch.Tensor``\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple in which the first element\n                contains predictions indexed by name and the second element contains intermediate activations\n                index by name. Specifically the features of the model, features of the global model and features of\n                the old model are returned. All predictions included in dictionary will be used to compute metrics.\n        \"\"\"\n        assert isinstance(input, torch.Tensor)\n        preds, features = self.model(input)\n        assert \"features\" in features, \"Model must produce a features dictionary with a 'features' key\"\n\n        # If there are no models in the old_models_list, we don't compute the features for the contrastive loss\n        if len(self.old_models_list) &gt; 0:\n            # Features from each of the old models are packed into a single tensor\n            old_features = torch.zeros(len(self.old_models_list), *features[\"features\"].size()).to(self.device)\n            for i, old_model in enumerate(self.old_models_list):\n                _, old_model_features = old_model(input)\n                old_features[i] = old_model_features[\"features\"]\n            features.update({\"old_features\": old_features})\n\n        if self.global_model is not None:\n            _, global_model_features = self.global_model(input)\n            features.update({\"global_features\": global_model_features[\"features\"]})\n        return preds, features\n\n    def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n        \"\"\"\n        This function is called immediately after client-side training has completed. This function saves the final\n        trained model to the list of old models to be used in subsequent server rounds.\n\n        Args:\n            local_steps (int): Number of local steps performed during training\n            loss_dict (dict[str, float]): Loss dictionary associated with training.\n            config (Config): The config from the server\n        \"\"\"\n        assert isinstance(self.model, SequentiallySplitModel)\n        # Save the parameters of the old LOCAL model\n        old_model = clone_and_freeze_model(self.model)\n        # Current model is appended to the back of the list\n        self.old_models_list.append(old_model)\n        # If the list is longer than desired, the element at the front of the list is removed.\n        if len(self.old_models_list) &gt; self.len_old_models_buffer:\n            self.old_models_list.pop(0)\n\n        super().update_after_train(local_steps, loss_dict, config)\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        \"\"\"\n        This function is called before training, immediately after injecting the aggregated server weights into the\n        client model. We clone and free the current model to preserve the aggregated server weights state (i.e. the\n        initial model before training starts).\n\n        Args:\n            current_server_round (int): Current federated training round being executed.\n        \"\"\"\n        # Save the parameters of the global model\n        self.global_model = clone_and_freeze_model(self.model)\n\n        super().update_before_train(current_server_round)\n\n    def compute_loss_and_additional_losses(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Computes the loss and any additional losses given predictions of the model and ground truth data.\n        For MOON, the loss is the total loss (criterion and weighted contrastive loss) and the additional losses are\n        the loss, (unweighted) contrastive loss, and total loss.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n                - The tensor for the total loss\n                - A dictionary with ``loss``, ``contrastive_loss`` and ``total_loss`` keys and their calculated values.\n        \"\"\"\n        loss = self.criterion(preds[\"prediction\"], target)\n        total_loss = loss.clone()\n        additional_losses = {\n            \"loss\": loss,\n        }\n\n        if \"old_features\" in features and \"global_features\" in features:\n            contrastive_loss = self.contrastive_loss_function(\n                features[\"features\"], features[\"global_features\"].unsqueeze(0), features[\"old_features\"]\n            )\n            total_loss += self.contrastive_weight * contrastive_loss\n            additional_losses[\"contrastive_loss\"] = contrastive_loss\n\n        additional_losses[\"total_loss\"] = total_loss\n\n        return total_loss, additional_losses\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training loss given predictions and features of the model and ground truth data. Loss includes\n        base loss plus a model contrastive loss.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n                All predictions included in dictionary will be used to compute metrics.\n            features (dict[str, torch.Tensor]): Feature(s) of the model(s) indexed by name.\n            target (torch.Tensor): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses\n                indexed by name.\n        \"\"\"\n        # Check that the model is in training mode\n        assert self.model.training\n\n        # If there are no old local models in the list (first pass of MOON training), we just do basic loss\n        #  calculations\n        if len(self.old_models_list) == 0:\n            total_loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n        else:\n            total_loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n        return TrainingLosses(backward=total_loss, additional_losses=additional_losses)\n\n    def compute_evaluation_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses:\n        \"\"\"\n        Computes evaluation loss given predictions and features of the model and ground truth data. Loss includes\n        base loss plus a model contrastive loss.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n                All predictions included in dictionary will be used to compute metrics.\n            features (dict[str, torch.Tensor]): Feature(s) of the model(s) indexed by name.\n            target (torch.Tensor): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n                indexed by name.\n        \"\"\"\n        # Check that the model is in evaluation mode\n        assert not self.model.training\n\n        # If there are no old local models in the list (first pass of MOON training), we just do basic loss\n        # calculations\n        if len(self.old_models_list) == 0:\n            checkpoint_loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n        else:\n            _, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n            # Note that the first parameter returned is the \"total loss\", which includes the contrastive loss\n            # So we use the vanilla loss stored in additional losses for checkpointing.\n            checkpoint_loss = additional_losses[\"loss\"]\n        return EvaluationLosses(checkpoint=checkpoint_loss, additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.moon_client.MoonClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None, temperature=0.5, contrastive_weight=1.0, len_old_models_buffer=1)</code>","text":"<p>This client implements the MOON algorithm from Model-Contrastive Federated Learning. The key idea of MOON is to enforce similarity between representations from the global and current local model through a contrastive loss to constrain the local training of individual parties in the non-IID setting.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>Temperature used in the calculation of the contrastive loss. Defaults to 0.5.</p> <code>0.5</code> <code>contrastive_weight</code> <code>float</code> <p>Weight placed on the contrastive loss function. Referred to as mu in the original paper. Defaults to 1.0.</p> <code>1.0</code> <code>len_old_models_buffer</code> <code>int</code> <p>Number of old models to be stored for computation in the contrastive learning loss function. Defaults to 1.</p> <code>1</code> Source code in <code>fl4health/clients/moon_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n    temperature: float = 0.5,\n    contrastive_weight: float = 1.0,\n    len_old_models_buffer: int = 1,\n) -&gt; None:\n    \"\"\"\n    This client implements the MOON algorithm from Model-Contrastive Federated Learning. The key idea of MOON is\n    to enforce similarity between representations from the global and current local model through a contrastive\n    loss to constrain the local training of individual parties in the non-IID setting.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        temperature (float, optional): Temperature used in the calculation of the contrastive loss.\n            Defaults to 0.5.\n        contrastive_weight (float, optional): Weight placed on the contrastive loss function. Referred to as mu\n            in the original paper. Defaults to 1.0.\n        len_old_models_buffer (int, optional): Number of old models to be stored for computation in the\n            contrastive learning loss function. Defaults to 1.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.temperature = temperature\n    self.contrastive_weight = contrastive_weight\n    if self.contrastive_weight == 0:\n        log(WARNING, \"Contrastive loss weight is set to 0, thus Contrastive loss will not be computed.\")\n    self.contrastive_loss_function = MoonContrastiveLoss(self.device, temperature=temperature)\n\n    # Saving previous local models and a global model at each communication round to compute contrastive loss\n    self.len_old_models_buffer = len_old_models_buffer\n    self.old_models_list: list[torch.nn.Module] = []\n    self.global_model: torch.nn.Module | None = None\n</code></pre>"},{"location":"api/#fl4health.clients.moon_client.MoonClient.predict","title":"<code>predict(input)</code>","text":"<p>Computes the prediction(s) and features of the model(s) given the input. This function also produces the necessary features from the <code>global_model</code> (aggregated model from server) and <code>old_models</code> (previous client-side optimized models) in order to be able to compute the appropriate contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into the model. <code>TorchInputType</code> is simply an alias for the union of <code>torch.Tensor</code> and <code>dict[str, torch.Tensor]</code>. Here, the MOON models require input to simply be of type <code>torch.Tensor</code></p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>A tuple in which the first element contains predictions indexed by name and the second element contains intermediate activations index by name. Specifically the features of the model, features of the global model and features of the old model are returned. All predictions included in dictionary will be used to compute metrics.</p> Source code in <code>fl4health/clients/moon_client.py</code> <pre><code>def predict(self, input: TorchInputType) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Computes the prediction(s) and features of the model(s) given the input. This function also produces the\n    necessary features from the ``global_model`` (aggregated model from server) and ``old_models`` (previous\n    client-side optimized models) in order to be able to compute the appropriate contrastive loss.\n\n    Args:\n        input (TorchInputType): Inputs to be fed into the model. ``TorchInputType`` is simply an alias\n            for the union of ``torch.Tensor`` and ``dict[str, torch.Tensor]``. Here, the MOON models require\n            input to simply be of type ``torch.Tensor``\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple in which the first element\n            contains predictions indexed by name and the second element contains intermediate activations\n            index by name. Specifically the features of the model, features of the global model and features of\n            the old model are returned. All predictions included in dictionary will be used to compute metrics.\n    \"\"\"\n    assert isinstance(input, torch.Tensor)\n    preds, features = self.model(input)\n    assert \"features\" in features, \"Model must produce a features dictionary with a 'features' key\"\n\n    # If there are no models in the old_models_list, we don't compute the features for the contrastive loss\n    if len(self.old_models_list) &gt; 0:\n        # Features from each of the old models are packed into a single tensor\n        old_features = torch.zeros(len(self.old_models_list), *features[\"features\"].size()).to(self.device)\n        for i, old_model in enumerate(self.old_models_list):\n            _, old_model_features = old_model(input)\n            old_features[i] = old_model_features[\"features\"]\n        features.update({\"old_features\": old_features})\n\n    if self.global_model is not None:\n        _, global_model_features = self.global_model(input)\n        features.update({\"global_features\": global_model_features[\"features\"]})\n    return preds, features\n</code></pre>"},{"location":"api/#fl4health.clients.moon_client.MoonClient.update_after_train","title":"<code>update_after_train(local_steps, loss_dict, config)</code>","text":"<p>This function is called immediately after client-side training has completed. This function saves the final trained model to the list of old models to be used in subsequent server rounds.</p> <p>Parameters:</p> Name Type Description Default <code>local_steps</code> <code>int</code> <p>Number of local steps performed during training</p> required <code>loss_dict</code> <code>dict[str, float]</code> <p>Loss dictionary associated with training.</p> required <code>config</code> <code>Config</code> <p>The config from the server</p> required Source code in <code>fl4health/clients/moon_client.py</code> <pre><code>def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n    \"\"\"\n    This function is called immediately after client-side training has completed. This function saves the final\n    trained model to the list of old models to be used in subsequent server rounds.\n\n    Args:\n        local_steps (int): Number of local steps performed during training\n        loss_dict (dict[str, float]): Loss dictionary associated with training.\n        config (Config): The config from the server\n    \"\"\"\n    assert isinstance(self.model, SequentiallySplitModel)\n    # Save the parameters of the old LOCAL model\n    old_model = clone_and_freeze_model(self.model)\n    # Current model is appended to the back of the list\n    self.old_models_list.append(old_model)\n    # If the list is longer than desired, the element at the front of the list is removed.\n    if len(self.old_models_list) &gt; self.len_old_models_buffer:\n        self.old_models_list.pop(0)\n\n    super().update_after_train(local_steps, loss_dict, config)\n</code></pre>"},{"location":"api/#fl4health.clients.moon_client.MoonClient.update_before_train","title":"<code>update_before_train(current_server_round)</code>","text":"<p>This function is called before training, immediately after injecting the aggregated server weights into the client model. We clone and free the current model to preserve the aggregated server weights state (i.e. the initial model before training starts).</p> <p>Parameters:</p> Name Type Description Default <code>current_server_round</code> <code>int</code> <p>Current federated training round being executed.</p> required Source code in <code>fl4health/clients/moon_client.py</code> <pre><code>def update_before_train(self, current_server_round: int) -&gt; None:\n    \"\"\"\n    This function is called before training, immediately after injecting the aggregated server weights into the\n    client model. We clone and free the current model to preserve the aggregated server weights state (i.e. the\n    initial model before training starts).\n\n    Args:\n        current_server_round (int): Current federated training round being executed.\n    \"\"\"\n    # Save the parameters of the global model\n    self.global_model = clone_and_freeze_model(self.model)\n\n    super().update_before_train(current_server_round)\n</code></pre>"},{"location":"api/#fl4health.clients.moon_client.MoonClient.compute_loss_and_additional_losses","title":"<code>compute_loss_and_additional_losses(preds, features, target)</code>","text":"<p>Computes the loss and any additional losses given predictions of the model and ground truth data. For MOON, the loss is the total loss (criterion and weighted contrastive loss) and the additional losses are the loss, (unweighted) contrastive loss, and total loss.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor]]</code> <p>A tuple with:</p> <ul> <li>The tensor for the total loss</li> <li>A dictionary with <code>loss</code>, <code>contrastive_loss</code> and <code>total_loss</code> keys and their calculated values.</li> </ul> Source code in <code>fl4health/clients/moon_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Computes the loss and any additional losses given predictions of the model and ground truth data.\n    For MOON, the loss is the total loss (criterion and weighted contrastive loss) and the additional losses are\n    the loss, (unweighted) contrastive loss, and total loss.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n            - The tensor for the total loss\n            - A dictionary with ``loss``, ``contrastive_loss`` and ``total_loss`` keys and their calculated values.\n    \"\"\"\n    loss = self.criterion(preds[\"prediction\"], target)\n    total_loss = loss.clone()\n    additional_losses = {\n        \"loss\": loss,\n    }\n\n    if \"old_features\" in features and \"global_features\" in features:\n        contrastive_loss = self.contrastive_loss_function(\n            features[\"features\"], features[\"global_features\"].unsqueeze(0), features[\"old_features\"]\n        )\n        total_loss += self.contrastive_weight * contrastive_loss\n        additional_losses[\"contrastive_loss\"] = contrastive_loss\n\n    additional_losses[\"total_loss\"] = total_loss\n\n    return total_loss, additional_losses\n</code></pre>"},{"location":"api/#fl4health.clients.moon_client.MoonClient.compute_training_loss","title":"<code>compute_training_loss(preds, features, target)</code>","text":"<p>Computes training loss given predictions and features of the model and ground truth data. Loss includes base loss plus a model contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>dict[str, Tensor]</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>Tensor</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/moon_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training loss given predictions and features of the model and ground truth data. Loss includes\n    base loss plus a model contrastive loss.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            All predictions included in dictionary will be used to compute metrics.\n        features (dict[str, torch.Tensor]): Feature(s) of the model(s) indexed by name.\n        target (torch.Tensor): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses\n            indexed by name.\n    \"\"\"\n    # Check that the model is in training mode\n    assert self.model.training\n\n    # If there are no old local models in the list (first pass of MOON training), we just do basic loss\n    #  calculations\n    if len(self.old_models_list) == 0:\n        total_loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n    else:\n        total_loss, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n\n    return TrainingLosses(backward=total_loss, additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.moon_client.MoonClient.compute_evaluation_loss","title":"<code>compute_evaluation_loss(preds, features, target)</code>","text":"<p>Computes evaluation loss given predictions and features of the model and ground truth data. Loss includes base loss plus a model contrastive loss.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>dict[str, Tensor]</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>Tensor</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> containing checkpoint loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/moon_client.py</code> <pre><code>def compute_evaluation_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; EvaluationLosses:\n    \"\"\"\n    Computes evaluation loss given predictions and features of the model and ground truth data. Loss includes\n    base loss plus a model contrastive loss.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            All predictions included in dictionary will be used to compute metrics.\n        features (dict[str, torch.Tensor]): Feature(s) of the model(s) indexed by name.\n        target (torch.Tensor): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n            indexed by name.\n    \"\"\"\n    # Check that the model is in evaluation mode\n    assert not self.model.training\n\n    # If there are no old local models in the list (first pass of MOON training), we just do basic loss\n    # calculations\n    if len(self.old_models_list) == 0:\n        checkpoint_loss, additional_losses = super().compute_loss_and_additional_losses(preds, features, target)\n    else:\n        _, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n        # Note that the first parameter returned is the \"total loss\", which includes the contrastive loss\n        # So we use the vanilla loss stored in additional losses for checkpointing.\n        checkpoint_loss = additional_losses[\"loss\"]\n    return EvaluationLosses(checkpoint=checkpoint_loss, additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.mr_mtl_client","title":"<code>mr_mtl_client</code>","text":""},{"location":"api/#fl4health.clients.mr_mtl_client.MrMtlClient","title":"<code>MrMtlClient</code>","text":"<p>               Bases: <code>AdaptiveDriftConstraintClient</code></p> Source code in <code>fl4health/clients/mr_mtl_client.py</code> <pre><code>class MrMtlClient(AdaptiveDriftConstraintClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This client implements the MR-MTL algorithm from MR-MTL: On Privacy and Personalization in Cross-Silo\n        Federated Learning. The idea is that we want to train personalized versions of the global model for each\n        client. However, instead of using a separate solver for the global model, as in Ditto, we update the initial\n        global model with aggregated local models on the server-side and use those weights to also constrain the\n        training of a local model. The constraint for this local model is identical to the FedProx loss. The key\n        difference is that the local model is never replaced with aggregated weights. It is always local.\n\n        **NOTE**: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the\n        heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the\n        corresponding strategy used by the server\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        # NOTE: The initial global model is used to house the aggregate weight updates at the beginning of a round,\n        # because in MR-MTL, the local models are not updated with these aggregates.\n        self.initial_global_model: nn.Module\n        self.initial_global_tensors: list[torch.Tensor]\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n        Then set initialized attribute to True.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        # Need to setup the init global model here as well. It should be the same architecture as the model so\n        # we reuse the get_model call. We explicitly send the model to the desired device. This is idempotent.\n        self.initial_global_model = self.get_model(config).to(self.device)\n        # The rest of the setup is the same\n        super().setup_client(config)\n\n    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n        \"\"\"\n        The parameters being passed are to be routed to the initial global model to be used in a penalty term in\n        training the local model. Despite the usual FL setup, we actually never pass the aggregated model to the\n        **LOCAL** model. Instead, we use the aggregated model to form the MR-MTL penalty term.\n\n        NOTE: In MR-MTL, unlike Ditto, the local model weights are not synced across clients to the initial global\n        model, even in the **FIRST ROUND**.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model. It will also contain a penalty weight from the server at each round (possibly adapted)\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n                round or an evaluation round. Not used here.\n        \"\"\"\n        # Make sure that the proper components exist.\n        assert self.initial_global_model is not None and self.parameter_exchanger is not None\n\n        # Route the parameters to the GLOBAL model only in MR-MTL\n        log(INFO, \"Setting the global model weights\")\n        server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n        log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n        self.parameter_exchanger.pull_parameters(server_model_state, self.initial_global_model, config)\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        assert self.initial_global_model is not None\n        # Freeze the initial weights of the INITIAL GLOBAL MODEL. These are used to form the MR-MTL\n        # update penalty term.\n        for param in self.initial_global_model.parameters():\n            param.requires_grad = False\n        self.initial_global_model.eval()\n\n        # Saving the initial GLOBAL MODEL weights and detaching them so that we don't compute gradients with\n        # respect to the tensors. These are used to form the MR-MTL local update penalty term.\n        self.drift_penalty_tensors = [\n            initial_layer_weights.detach().clone() for initial_layer_weights in self.initial_global_model.parameters()\n        ]\n\n        return super().update_before_train(current_server_round)\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training losses given predictions of the modes and ground truth data. We add to vanilla loss\n        function by including Mean Regularized (MR) penalty loss which is the \\\\(\\\\ell^2\\\\) inner product between the\n        initial global model weights and weights of the current model.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n                All predictions included in dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n                by name. Additional losses includes each loss component of the total loss.\n        \"\"\"\n        # Check that the initial global model isn't in training mode and that the local model is in training mode\n        assert not self.initial_global_model.training and self.model.training\n        # Use the rest of the training loss computation from the AdaptiveDriftConstraintClient parent\n        return super().compute_training_loss(preds, features, target)\n\n    def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Validate the current model on the entire validation dataset.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n        \"\"\"\n        # ensure that the initial global model is in eval mode\n        assert not self.initial_global_model.training\n        return super().validate(include_losses_in_metrics=include_losses_in_metrics)\n</code></pre>"},{"location":"api/#fl4health.clients.mr_mtl_client.MrMtlClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>This client implements the MR-MTL algorithm from MR-MTL: On Privacy and Personalization in Cross-Silo Federated Learning. The idea is that we want to train personalized versions of the global model for each client. However, instead of using a separate solver for the global model, as in Ditto, we update the initial global model with aggregated local models on the server-side and use those weights to also constrain the training of a local model. The constraint for this local model is identical to the FedProx loss. The key difference is that the local model is never replaced with aggregated weights. It is always local.</p> <p>NOTE: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the corresponding strategy used by the server</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/mr_mtl_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    This client implements the MR-MTL algorithm from MR-MTL: On Privacy and Personalization in Cross-Silo\n    Federated Learning. The idea is that we want to train personalized versions of the global model for each\n    client. However, instead of using a separate solver for the global model, as in Ditto, we update the initial\n    global model with aggregated local models on the server-side and use those weights to also constrain the\n    training of a local model. The constraint for this local model is identical to the FedProx loss. The key\n    difference is that the local model is never replaced with aggregated weights. It is always local.\n\n    **NOTE**: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the\n    heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the\n    corresponding strategy used by the server\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    # NOTE: The initial global model is used to house the aggregate weight updates at the beginning of a round,\n    # because in MR-MTL, the local models are not updated with these aggregates.\n    self.initial_global_model: nn.Module\n    self.initial_global_tensors: list[torch.Tensor]\n</code></pre>"},{"location":"api/#fl4health.clients.mr_mtl_client.MrMtlClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set initialized attribute to True.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/mr_mtl_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n    Then set initialized attribute to True.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    # Need to setup the init global model here as well. It should be the same architecture as the model so\n    # we reuse the get_model call. We explicitly send the model to the desired device. This is idempotent.\n    self.initial_global_model = self.get_model(config).to(self.device)\n    # The rest of the setup is the same\n    super().setup_client(config)\n</code></pre>"},{"location":"api/#fl4health.clients.mr_mtl_client.MrMtlClient.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>The parameters being passed are to be routed to the initial global model to be used in a penalty term in training the local model. Despite the usual FL setup, we actually never pass the aggregated model to the LOCAL model. Instead, we use the aggregated model to form the MR-MTL penalty term.</p> <p>NOTE: In MR-MTL, unlike Ditto, the local model weights are not synced across clients to the initial global model, even in the FIRST ROUND.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model. It will also contain a penalty weight from the server at each round (possibly adapted)</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. Not used here.</p> required Source code in <code>fl4health/clients/mr_mtl_client.py</code> <pre><code>def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n    \"\"\"\n    The parameters being passed are to be routed to the initial global model to be used in a penalty term in\n    training the local model. Despite the usual FL setup, we actually never pass the aggregated model to the\n    **LOCAL** model. Instead, we use the aggregated model to form the MR-MTL penalty term.\n\n    NOTE: In MR-MTL, unlike Ditto, the local model weights are not synced across clients to the initial global\n    model, even in the **FIRST ROUND**.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model. It will also contain a penalty weight from the server at each round (possibly adapted)\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n            round or an evaluation round. Not used here.\n    \"\"\"\n    # Make sure that the proper components exist.\n    assert self.initial_global_model is not None and self.parameter_exchanger is not None\n\n    # Route the parameters to the GLOBAL model only in MR-MTL\n    log(INFO, \"Setting the global model weights\")\n    server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n    log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n    self.parameter_exchanger.pull_parameters(server_model_state, self.initial_global_model, config)\n</code></pre>"},{"location":"api/#fl4health.clients.mr_mtl_client.MrMtlClient.compute_training_loss","title":"<code>compute_training_loss(preds, features, target)</code>","text":"<p>Computes training losses given predictions of the modes and ground truth data. We add to vanilla loss function by including Mean Regularized (MR) penalty loss which is the \\(\\ell^2\\) inner product between the initial global model weights and weights of the current model.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name. Additional losses includes each loss component of the total loss.</p> Source code in <code>fl4health/clients/mr_mtl_client.py</code> <pre><code>def compute_training_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training losses given predictions of the modes and ground truth data. We add to vanilla loss\n    function by including Mean Regularized (MR) penalty loss which is the \\\\(\\\\ell^2\\\\) inner product between the\n    initial global model weights and weights of the current model.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            All predictions included in dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n            by name. Additional losses includes each loss component of the total loss.\n    \"\"\"\n    # Check that the initial global model isn't in training mode and that the local model is in training mode\n    assert not self.initial_global_model.training and self.model.training\n    # Use the rest of the training loss computation from the AdaptiveDriftConstraintClient parent\n    return super().compute_training_loss(preds, features, target)\n</code></pre>"},{"location":"api/#fl4health.clients.mr_mtl_client.MrMtlClient.validate","title":"<code>validate(include_losses_in_metrics=False)</code>","text":"<p>Validate the current model on the entire validation dataset.</p> <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]]</code> <p>The validation loss and a dictionary of metrics from validation.</p> Source code in <code>fl4health/clients/mr_mtl_client.py</code> <pre><code>def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]:\n    \"\"\"\n    Validate the current model on the entire validation dataset.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n    \"\"\"\n    # ensure that the initial global model is in eval mode\n    assert not self.initial_global_model.training\n    return super().validate(include_losses_in_metrics=include_losses_in_metrics)\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client","title":"<code>nnunet_client</code>","text":""},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient","title":"<code>NnunetClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>class NnunetClient(BasicClient):\n    def __init__(\n        self,\n        device: torch.device,\n        dataset_id: int,\n        fold: int | str,\n        data_identifier: str | None = None,\n        plans_identifier: str | None = None,\n        compile: bool = True,\n        always_preprocess: bool = False,\n        max_grad_norm: float = 12,\n        n_dataload_processes: int | None = None,\n        verbose: bool = True,\n        metrics: Sequence[Metric] | None = None,\n        progress_bar: bool = False,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        client_name: str | None = None,\n        nnunet_trainer_class: type[nnUNetTrainer] = nnUNetTrainer,\n        nnunet_trainer_class_kwargs: dict[str, Any] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        A client for training nnunet models. Requires the nnunet environment variables to be set. Also requires the\n        following additional keys in the config sent from the server as follows.\n\n        - \"nnunet_plans\": (serialized dict)\n        - \"nnunet_config\": (str)\n\n        Args:\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\" or \"mps\"\n            dataset_id (int): The nnunet dataset id for the local client dataset to use for training and validation.\n            fold (int | str): Which fold of the local client dataset to use for validation. nnunet defaults to\n                5 folds (0 to 4). Can also be set to \"all\" to use all the data for both training and validation.\n            data_identifier (str | None, optional): The nnunet data identifier prefix to use. The final data\n                identifier will be ``{data_identifier}_config`` where \"config\" is the nnunet config (e.g. 2d,\n                ``3d_fullres``, etc.). If preprocessed data already exists can be used to specify which preprocessed\n                data to use. By default, the ``plans_identifier`` is used as the ``data_identifier``.\n            plans_identifier (str | None, optional): Specify what to save the client's local copy of the plans file\n                as. The client makes a local modified copy of the global source plans file sent by the server. If left\n                as default None, the plans identifier will be set as \"FL-plansname-000local\" where 000 is the\n                ``dataset_id`` and plansname is the \"plans_name\" value of the source plans file. The original plans\n                will be saved under the ``source_plans_name`` key in the modified plans file.\n            compile (bool, optional): If set to True, the client will Just-In-Time (JIT) compile the nnUNet model and\n                perform optimizations at the start of training. This process significantly reduces the runtime for\n                nnUNet models, especially for larger models or long-running jobs. However, it introduces some overhead\n                time and computation during the initial step. It is recommended to keep this option enabled. The\n                default value is True.\n            always_preprocess (bool, optional): If True, will preprocess the local client dataset even if the\n                preprocessed data already seems to exist. The existence of the preprocessed data is determined by\n                matching the provided ``data_identifier`` with that of the preprocessed data already  on the client.\n                Defaults to False.\n            max_grad_norm (float, optional): The maximum gradient norm to use for gradient clipping. Defaults to 12\n                which is the nnunetv2 2.5.1 default.\n            n_dataload_processes (int | None, optional): The number of processes to spawn for each nnunet\n                dataloader. If left as None we use the nnunetv2 version 2.5.1 defaults for each config\n            verbose (bool, optional): If True the client will log some extra INFO logs. Defaults to False unless\n                the log level is DEBUG or lower.\n            metrics (Sequence[Metric], optional): Metrics to be computed based on the labels and predictions of the\n                client model. Defaults to None.\n            progress_bar (bool, optional): Whether or not to print a progress bar to stdout for training. Defaults\n                to False\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over each\n                batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the client should\n                send data to.\n            client_name (str | None, optional): Name of the client. If None the class will set a default and random\n                name. Defaults to None.\n            nnunet_trainer_class (type[nnUNetTrainer]): A ``nnUNetTrainer`` constructor. Useful for passing custom\n                ``nnUNetTrainer``. Defaults to the standard ``nnUNetTrainer`` class. Must match the\n                ``nnunet_trainer_class`` passed to the ``NnunetServer``.\n            nnunet_trainer_class_kwargs (dict[str, Any]): Additional kwargs to pass to ``nnunet_trainer_class``.\n                Defaults to empty dictionary.\n        \"\"\"\n        metrics = metrics if metrics else []\n\n        # Parent method sets up several class attributes\n        super().__init__(\n            data_path=Path(\"dummy/path\"),  # data_path not used by NnunetClient\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n\n        # Some nnunet client specific attributes\n        self.dataset_id: int = dataset_id\n        self.dataset_name = convert_id_to_dataset_name(self.dataset_id)\n        self.fold = fold\n        self.data_identifier = data_identifier\n        self.always_preprocess = always_preprocess\n        self.plans_name = plans_identifier\n        self.fingerprint_extracted = False\n        self.grad_scaler = GradScaler()\n        self.max_grad_norm = max_grad_norm\n        self.n_dataload_proc = n_dataload_processes\n        try:\n            self.dataset_json = load_json(join(nnUNet_raw, self.dataset_name, \"dataset.json\"))\n        except Exception:\n            try:\n                self.dataset_json = load_json(join(nnUNet_preprocessed, self.dataset_name, \"dataset.json\"))\n            except Exception as e:\n                log(ERROR, \"Could not load the nnunet dataset json from nnUNet_raw or nnUNet_preprocessed.\")\n                raise e  # Raising e will raise both exceptions since it is nested.\n\n        # Auto set verbose to True if console handler is on DEBUG mode\n        self.verbose = verbose if console_handler.level &gt;= INFO else True\n\n        # Used to redirect stdout to logger\n        self.stream2debug = StreamToLogger(FLOWER_LOGGER, DEBUG)\n\n        # nnunet specific attributes to be initialized in setup_client\n        self.nnunet_trainer_class = nnunet_trainer_class\n        self.nnunet_trainer_class_kwargs = (\n            nnunet_trainer_class_kwargs if nnunet_trainer_class_kwargs is not None else {}\n        )\n        self.nnunet_trainer: nnUNetTrainer\n        self.nnunet_config: NnunetConfig\n        self.plans: dict[str, Any] | None = None\n        self.steps_per_round: int  # N steps per server round\n        self.max_steps: int  # N_rounds x steps_per_round\n\n        # Turn on/off model optimizations for decreasing runtime efficiency.\n        if compile:\n            # Turning on cudnn.benchmark reduces nnUNet runtimes by 2-3x in our experiments.\n            # Limit of 0 tells cudnn to benchmark all available conv algorithms (default is 10)\n            torch.backends.cudnn.benchmark = True\n            torch.backends.cudnn.benchmark_limit = 0\n            os.environ[\"nnUNet_compile\"] = str(\"true\")  # noqa: SIM112\n        else:\n            torch.backends.cudnn.benchmark = False\n            os.environ[\"nnUNet_compile\"] = str(\"false\")  # noqa: SIM112\n            if self.verbose:\n                log(INFO, \"Disabling model optimizations and JIT compilation. This may impact runtime performance.\")\n\n    def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Given a single batch of input and target data, generate predictions, compute loss, update parameters and\n        optionally update metrics if they exist. (i.e. backprop on a single batch of data). Assumes ``self.model`` is\n        in train mode already.\n\n        Overrides parent to include mixed precision training (autocasting and corresponding gradient scaling) as per\n        the original ``nnUNetTrainer``.\n\n        Args:\n            input (TorchInputType): The input to be fed into the model.\n            target (TorchTargetType): The target corresponding to the input.\n\n        Returns:\n            (tuple[TrainingLosses, TorchPredType]): The losses object from the train step along with a dictionary of\n                any predictions produced by the model.\n        \"\"\"\n        # If the device type is not cuda, we don't use mixed precision training and therefore can use parent method.\n        if self.device.type != \"cuda\":\n            return super().train_step(input, target)\n\n        # As in the nnUNetTrainer, we implement mixed precision using torch.autocast and torch.GradScaler\n        # Clear gradients from optimizer if they exist\n        self.optimizers[\"global\"].zero_grad()\n\n        # Call user defined methods to get predictions and compute loss\n        preds, features = self.predict(input)\n        target = self.transform_target(target)\n        losses = self.compute_training_loss(preds, features, target)\n\n        # Compute scaled loss and perform backward pass\n        scaled_backward_loss = self.grad_scaler.scale(losses.backward[\"backward\"])\n        scaled_backward_loss.backward()\n\n        # Rescale gradients then clip based on specified norm\n        self.grad_scaler.unscale_(self.optimizers[\"global\"])\n        self.transform_gradients(losses)\n\n        # Update parameters and scaler\n        self.grad_scaler.step(self.optimizers[\"global\"])\n        self.grad_scaler.update()\n\n        return losses, preds\n\n    @use_default_signal_handlers  # Dataloaders use multiprocessing\n    def get_data_loaders(self, config: Config) -&gt; tuple[DataLoader, DataLoader]:\n        \"\"\"\n        Gets the nnunet dataloaders and wraps them in another class to make them pytorch iterators.\n\n        Args:\n            config (Config): The config file from the server\n\n        Returns:\n            (tuple[DataLoader, DataLoader]): A tuple of length two. The client train and validation dataloaders as\n                pytorch dataloaders\n        \"\"\"\n        start_time = time.time()\n        # Set the number of processes for each dataloader.\n        if self.n_dataload_proc is None:\n            # Nnunet default is 12 or max cpu's. We decrease max by 1 just in case\n            # NOTE: The type: ignore here is to skip issues where a local operating system is not compatible\n            # with sched_getaffinity (older versions of MacOS, for example). The code still won't run but mypy won't\n            # complain. Workarounds like using os.cpu_count(), while not exactly the same, are possible.\n            try:\n                self.n_dataload_proc = min(12, len(os.sched_getaffinity(0)) - 1)  # type: ignore\n            except AttributeError:\n                # TODO: this is pretty brittle\n                if cpu_count := os.cpu_count():\n                    self.n_dataload_proc = min(12, cpu_count - 2)\n                else:\n                    self.n_dataload_proc = 1\n        os.environ[\"nnUNet_n_proc_DA\"] = str(self.n_dataload_proc)  # noqa: SIM112\n\n        # The batchgenerators package used under the hood by the dataloaders creates an additional stream handler for\n        # the root logger Therefore all logs get printed twice. First stop flwr logger from propagating logs to root.\n        # Issue: https://github.com/MIC-DKFZ/batchgenerators/issues/123 PR:\n        # https://github.com/MIC-DKFZ/batchgenerators/pull/124\n        FLOWER_LOGGER.propagate = False\n\n        # Redirect nnunet output to flwr logger at DEBUG level.\n        with redirect_stdout(self.stream2debug):\n            # Get the nnunet dataloader iterators. (Technically augmenter classes)\n            train_loader, val_loader = self.nnunet_trainer.get_dataloaders()\n\n        # Now clear root handler that was created when get_dataloaders was called and reset flwr logger propagate\n        root_logger = logging.getLogger()\n        root_logger.handlers.clear()\n        FLOWER_LOGGER.propagate = True\n\n        # Get accurate estimate of image shape so that we can get accurate dataloader length\n        if self.plans is None:\n            self.plans = self.create_plans(config)  # Local plans will have metadata we need\n        fullres_cfg = \"3d_fullres\" if \"3d_fullres\" in self.plans[\"configurations\"] else \"2d\"\n        shape = self.plans[\"configurations\"][fullres_cfg][\"median_image_size_in_voxels\"]\n\n        # Wrap nnunet dataloaders to make them compatible with fl4health\n        train_loader = NnUNetDataLoaderWrapper(\n            nnunet_augmenter=train_loader, nnunet_config=self.nnunet_config, ref_image_shape=shape\n        )\n        val_loader = NnUNetDataLoaderWrapper(\n            nnunet_augmenter=val_loader, nnunet_config=self.nnunet_config, ref_image_shape=shape\n        )\n        log(INFO, f\"{len(val_loader)}, {len(val_loader.dataset)}, {val_loader.nnunet_dataloader.batch_size}\")\n\n        if self.verbose:\n            log(INFO, f\"\\tDataloaders initialized in {time.time() - start_time:.1f}s\")\n\n        return train_loader, val_loader\n\n    def get_model(self, config: Config) -&gt; nn.Module:\n        return self.nnunet_trainer.network\n\n    def get_criterion(self, config: Config) -&gt; _Loss:\n        if isinstance(self.nnunet_trainer.loss, DeepSupervisionWrapper):\n            self.reports_manager.report({\"Criterion\": self.nnunet_trainer.loss.loss.__class__.__name__})\n        else:\n            self.reports_manager.report({\"Criterion\": self.nnunet_trainer.loss.__class__.__name__})\n\n        return Module2LossWrapper(self.nnunet_trainer.loss)\n\n    def get_optimizer(self, config: Config) -&gt; Optimizer:\n        self.reports_manager.report({\"Optimizer\": self.nnunet_trainer.optimizer.__class__.__name__})\n        return self.nnunet_trainer.optimizer\n\n    def get_lr_scheduler(self, optimizer_key: str, config: Config) -&gt; _LRScheduler:\n        \"\"\"\n        Creates an LR Scheduler similar to the nnunet default except we set max steps to the total number of steps\n        and update every step. Initial and final LR are the same as nnunet, difference is nnunet sets max steps to\n        num \"epochs\", but they define an \"epoch\" as exactly 250 steps. Therefore they update every 250 steps. Override\n        this method to set your own LR scheduler.\n\n        Args:\n            optimizer_key (str): Key of the optimizer to which the scheduler will be applied.\n            config (Config): The server config. This method will determine the total number of steps that will be\n                applied across all server rounds on FL training.\n\n        Raises:\n            ValueError: Thrown if the configuration does not contain either a steps or epochs specification.\n\n        Returns:\n            (_LRScheduler): The default nnunet LR Scheduler for nnunetv2 2.5.1\n        \"\"\"\n        if not isinstance(self.nnunet_trainer.lr_scheduler, PolyLRScheduler):\n            log(\n                WARNING,\n                (\n                    \"Nnunet seems to have changed their default LR scheduler to \"\n                    f\"type: {type(self.nnunet_trainer.lr_scheduler)}. \"\n                    \"Using PolyLRScheduler instead. Override or update the \"\n                    \"get_lr_scheduler method of nnUNetClient to change this\"\n                ),\n            )\n\n        # Determine total number of steps throughout all FL rounds\n        local_epochs, local_steps, _, _, _ = self.process_config(config)\n        if local_steps is not None:\n            steps_per_round = local_steps\n        elif local_epochs is not None:\n            steps_per_round = local_epochs * len(self.train_loader)\n        else:\n            raise ValueError(\"One of local steps or local epochs must be specified\")\n\n        total_steps = int(config[\"n_server_rounds\"]) * steps_per_round\n\n        # Create and return LR Scheduler Wrapper for the PolyLRScheduler so that it is\n        # compatible with Torch LRScheduler\n        # Create and return LR Scheduler. This is nnunet default for version 2.5.1\n        self.reports_manager.report({\"LR Scheduler\": \"PolyLRScheduler\"})\n        return PolyLRSchedulerWrapper(\n            self.optimizers[optimizer_key],\n            initial_lr=self.nnunet_trainer.initial_lr,\n            max_steps=total_steps,\n        )\n\n    def create_plans(self, config: Config) -&gt; dict[str, Any]:\n        \"\"\"\n        Modifies the provided plans file to work with the local client dataset and then saves it to disk. Requires the\n        local ``dataset_fingerprint.json`` to exist, the local ``dataset_name``, ``plans_name``, ``data_identifier``\n        and ``dataset_json``.\n\n        The following fields are modified:\n\n        - plans_name\n        - dataset_name\n        - original_median_shape_after_transp\n        - original_median_spacing_after_transp\n        - configurations.{config}.data_identifier\n        - configurations.{config}.batch_size\n        - configurations.{config}.median_image_size_in_voxels\n        - foreground_intensity_properties_per_channel\n\n        Args:\n            config (Config): The config provided by the server. Expects the \"nnunet_plans\" key with a pickled\n                dictionary as the value\n\n        Returns:\n            (dict[str, Any]): The modified nnunet plans for the client\n        \"\"\"\n        # TODO: Make this an external function or part of another class and explicitly accept the required arguments\n        # rather than using class attributes.\n\n        # Get the source nnunet plans specified by the server\n        plans = pickle.loads(narrow_dict_type(config, \"nnunet_plans\", bytes))\n\n        # Change plans name.\n        if self.plans_name is None:\n            self.plans_name = f\"FL-{plans['plans_name']}-{self.dataset_id}local\"\n\n        plans[\"source_plans_name\"] = plans[\"plans_name\"]\n        plans[\"plans_name\"] = self.plans_name\n\n        # Change dataset name\n        plans[\"dataset_name\"] = self.dataset_name\n\n        # Load the dataset fingerprint for the local client dataset\n        fp_path = Path(nnUNet_preprocessed) / self.dataset_name / \"dataset_fingerprint.json\"\n        assert fp_path.exists(), \"Could not find the dataset fingerprint file\"\n        fp = load_json(fp_path)\n\n        # Change the foreground intensity properties per channel\n        plans[\"foreground_intensity_properties_per_channel\"] = fp[\"foreground_intensity_properties_per_channel\"]\n\n        # Compute the median image size and spacing of the local client dataset\n        median_shape = np.median(fp[\"shapes_after_crop\"], axis=0)[plans[\"transpose_forward\"]]\n        median_spacing = np.median(fp[\"spacings\"], axis=0)[plans[\"transpose_forward\"]]\n        plans[\"original_median_shape_after_transp\"] = [int(round(i)) for i in median_shape]\n        plans[\"original_median_spacing_after_transp\"] = [float(i) for i in median_spacing]\n\n        # Get the spacing that the network will resample to. Need to check if samples are 2d or 3d\n        fullres_cfg = \"3d_fullres\" if \"3d_fullres\" in plans[\"configurations\"] else \"2d\"\n        target_spacing = plans[\"configurations\"][fullres_cfg][\"spacing\"]\n\n        # Get the median shape after resampling to the desired/input voxel spacing\n        resampled_shapes = [\n            compute_new_shape(i, j, target_spacing) for i, j in zip(fp[\"shapes_after_crop\"], fp[\"spacings\"])\n        ]\n        resampled_median_shape = np.median(resampled_shapes, axis=0)[plans[\"transpose_forward\"]].tolist()\n\n        # Change data identifier\n        if self.data_identifier is None:\n            self.data_identifier = self.plans_name\n\n        # To be consistent with nnunet, a batch cannot contain more than 5% of the voxels in the dataset.\n        max_voxels = np.prod(resampled_median_shape, dtype=np.float64) * self.dataset_json[\"numTraining\"] * 0.05\n\n        # Iterate through nnunet configs in plans file\n        for c in plans[\"configurations\"]:\n            # Change the data identifier\n            plans[\"configurations\"][c][\"data_identifier\"] = self.data_identifier + \"_\" + c\n\n            # Ensure batch size is at least 2 and at most 5 percent of dataset\n            # Otherwise we keep it the same as it affects the target VRAM consumption\n            if \"batch_size\" in plans[\"configurations\"][c]:\n                old_bs = plans[\"configurations\"][c][\"batch_size\"]\n                bs_5percent = round(max_voxels / np.prod(plans[\"configurations\"][c][\"patch_size\"], dtype=np.float64))\n                new_bs = max(min(old_bs, bs_5percent), 2)\n                plans[\"configurations\"][c][\"batch_size\"] = new_bs\n            else:\n                log(WARNING, f\"Did not find a 'batch_size' key in the nnunet plans dict for nnunet config: {c}\")\n\n            # Update median shape of resampled input images\n            if str(c).startswith(\"2d\"):\n                plans[\"configurations\"][c][\"median_image_size_in_voxels\"] = resampled_median_shape[1:]\n            else:\n                plans[\"configurations\"][c][\"median_image_size_in_voxels\"] = resampled_median_shape\n\n        # Save local plans file\n        os.makedirs(join(nnUNet_preprocessed, self.dataset_name), exist_ok=True)\n        plans_save_path = join(nnUNet_preprocessed, self.dataset_name, self.plans_name + \".json\")\n        save_json(plans, plans_save_path, sort_keys=False)\n        return plans\n\n    @use_default_signal_handlers  # Preprocessing spawns subprocesses\n    def maybe_preprocess(self, nnunet_config: NnunetConfig) -&gt; None:\n        \"\"\"\n        Checks if preprocessed data for current plans exists and if not preprocesses the nnunet_raw dataset. The\n        preprocessed data is saved in '{nnUNet_preprocessed}/{dataset_name}/{data_identifier} as follows.\n\n        - ``nnUNet_preprocessed`` is the directory specified by the ``nnUNet_preprocessed`` environment variable.\n        - ``dataset_name`` is the nnunet dataset name (e.g. Dataset123_MyDataset).\n        - ``data_identifier`` is ``{self.data_identifier}_{self.nnunet_config}``.\n\n        Args:\n            nnunet_config (NnunetConfig): The nnunet config as a ``NnunetConfig`` Enum. Enum type ensures nnunet\n                config is valid.\n        \"\"\"\n        assert self.data_identifier is not None, \"Was expecting data identifier to be initialized in self.create_plans\"\n\n        # Preprocess data if it's not already there\n        if self.always_preprocess or not exists(self.nnunet_trainer.preprocessed_dataset_folder):\n            if self.verbose:\n                log(INFO, f\"\\tPreprocessing local client dataset: {self.dataset_name}\")\n            # Unless log level is debugging or lower, hide nnunet output\n            with redirect_stdout(self.stream2debug):\n                preprocess_dataset(\n                    dataset_id=self.dataset_id,\n                    plans_identifier=self.plans_name,\n                    num_processes=[NNUNET_DEFAULT_NP[nnunet_config]],\n                    configurations=[nnunet_config.value],\n                )\n        elif self.verbose:\n            log(\n                INFO,\n                \"\\tnnunet preprocessed data seems to already exist. Skipping preprocessing\",\n            )\n\n    @use_default_signal_handlers  # Fingerprint extraction spawns subprocess\n    def maybe_extract_fingerprint(self) -&gt; None:\n        \"\"\"Checks if nnunet dataset fingerprint already exists and if not extracts one from the dataset.\"\"\"\n        # Check first whether this client instance has already extracted a dataset fp\n        # Possible if the client was asked to generate the nnunet plans for the server\n        if not self.fingerprint_extracted:\n            fp_path = join(nnUNet_preprocessed, self.dataset_name, \"dataset_fingerprint.json\")\n            # Check if fp already exists or if we want to redo fp extraction\n            if self.always_preprocess or not exists(fp_path):\n                start = time.time()\n                # Unless log level is DEBUG or lower hide nnunet output\n                with redirect_stdout(self.stream2debug):\n                    extract_fingerprints(dataset_ids=[self.dataset_id])\n                if self.verbose:\n                    log(\n                        INFO,\n                        f\"\\tExtracted dataset fingerprint in {time.time() - start:.1f}s\",\n                    )\n            elif self.verbose:\n                log(\n                    INFO,\n                    \"\\tnnunet dataset fingerprint already exists. Skipping fingerprint extraction\",\n                )\n        elif self.verbose:\n            log(\n                INFO,\n                \"\\tThis client has already extracted the dataset fingerprint during this session. Skipping.\",\n            )\n        # Avoid extracting fingerprint multiple times when always_preprocess is true\n        self.fingerprint_extracted = True\n\n    # Several subprocesses spawned in setup during torch.compile and dataset unpacking\n    @use_default_signal_handlers\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Ensures the necessary files for training are on disk and initializes several class attributes that depend on\n        values in the config from the server. This is called once when the client is sampled by the server for the\n        first time.\n\n        Args:\n            config (Config): The config file from the server. The ``nnUNetClient`` expects the keys 'nnunet_config'\n                and 'nnunet_plans' in addition to those required by ``BasicClient``\n        \"\"\"\n        log(INFO, \"Setting up the nnUNetClient\")\n\n        # Empty gpu cache because nnunet does it\n        self.empty_cache()\n\n        # Get nnunet config\n        self.nnunet_config = NnunetConfig(config[\"nnunet_config\"])\n\n        # Check if dataset fingerprint has already been extracted\n        self.maybe_extract_fingerprint()\n\n        # Create the nnunet plans for the local client\n        if self.plans is None:\n            self.plans = self.create_plans(config=config)\n\n        # Unless log level is DEBUG or lower hide nnunet output\n        with redirect_stdout(self.stream2debug):\n            # Create the nnunet trainer\n            self.nnunet_trainer = self.nnunet_trainer_class(\n                plans=self.plans,\n                configuration=self.nnunet_config.value,\n                fold=self.fold,\n                dataset_json=self.dataset_json,\n                device=self.device,\n                **self.nnunet_trainer_class_kwargs,\n            )\n            # nnunet_trainer initialization\n            self.nnunet_trainer.initialize()\n            # This is done by nnunet_trainer in self.on_train_start, we\n            # do it manually since nnunet_trainer not being used for training\n            self.nnunet_trainer.set_deep_supervision_enabled(self.nnunet_trainer.enable_deep_supervision)\n\n        # Prevent nnunet from generating log files and delete empty output directories\n        os.remove(self.nnunet_trainer.log_file)\n        self.nnunet_trainer.log_file = os.devnull\n        output_folder = Path(self.nnunet_trainer.output_folder)\n        while len(os.listdir(output_folder)) == 0:\n            os.rmdir(output_folder)\n            output_folder = output_folder.parent\n\n        # Preprocess nnunet_raw data if needed\n        self.maybe_preprocess(self.nnunet_config)\n        start = time.time()\n        unpack_dataset(  # Reduces load on CPU and RAM during training\n            folder=self.nnunet_trainer.preprocessed_dataset_folder,\n            unpack_segmentation=self.nnunet_trainer.unpack_dataset,\n            overwrite_existing=self.always_preprocess,\n            verify_npy=True,\n        )\n        if self.verbose:\n            log(INFO, f\"\\tUnpacked dataset in {time.time() - start:.1f}s\")\n\n        # We have to call parent method after setting up nnunet trainer\n        super().setup_client(config)\n\n    def predict(self, input: TorchInputType) -&gt; tuple[TorchPredType, dict[str, torch.Tensor]]:\n        \"\"\"\n        Generate model outputs. Overridden because nnunets outputs lists when deep supervision is on so we have to\n        reformat the output into dicts.\n\n        Additionally if device type is cuda, loss computed in mixed precision.\n\n        Args:\n            input (TorchInputType): The model inputs\n\n        Returns:\n            (tuple[TorchPredType, dict[str, torch.Tensor]]): A tuple in which the first element model outputs indexed\n                by name. The second element is unused by this subclass and therefore is always an empty dict\n        \"\"\"\n        if isinstance(input, torch.Tensor):\n            # If device type is cuda, nnUNet defaults to mixed precision forward pass\n            if self.device.type == \"cuda\":\n                with torch.autocast(self.device.type, enabled=True):\n                    output = self.model(input)\n            else:\n                output = self.model(input)\n        else:\n            raise TypeError('\"input\" must be of type torch.Tensor for nnUNetClient')\n\n        if isinstance(output, torch.Tensor):\n            return {\"prediction\": output}, {}\n        # If output is a list or tuple then deep supervision is on and we need to convert preds into a dict\n        if isinstance(output, (list, tuple)):\n            num_spatial_dims = NNUNET_N_SPATIAL_DIMS[self.nnunet_config]\n            preds = convert_deep_supervision_list_to_dict(output, num_spatial_dims)\n            return preds, {}\n        raise TypeError(\n            \"Was expecting nnunet model output to be either a torch.Tensor or a list/tuple of torch.Tensors\"\n        )\n\n    def compute_loss_and_additional_losses(\n        self,\n        preds: TorchPredType,\n        features: dict[str, torch.Tensor],\n        target: TorchTargetType,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor] | None]:\n        \"\"\"\n        Checks the pred and target types and computes the loss. If device type is cuda, loss computed in mixed\n        precision.\n\n        Args:\n            preds (TorchPredType): Dictionary of model output tensors indexed by name\n            features (dict[str, torch.Tensor]): Not used by this subclass\n            target (TorchTargetType): The targets to evaluate the predictions with. If multiple prediction tensors\n                are given, target must be a dictionary with the same number of tensors\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor] | None]): A tuple where the first element is the loss and the\n                second element is an optional additional loss\n        \"\"\"\n        # If deep supervision is turned on we must convert loss and target dicts into lists\n        loss_preds = prepare_loss_arg(preds)\n        loss_targets = prepare_loss_arg(target)\n\n        # Ensure we have the same number of predictions and targets\n        assert isinstance(loss_preds, type(loss_targets)), (\n            f\"Got unexpected types for preds and targets: {type(loss_preds)} and {type(loss_targets)}\"\n        )\n\n        if isinstance(loss_preds, list):\n            assert len(loss_preds) == len(loss_targets), (\n                \"Was expecting the number of predictions and targets to be the same. \"\n                f\"Got {len(loss_preds)} predictions and {len(loss_targets)} targets.\"\n            )\n\n        # If device type is cuda, nnUNet defaults to compute loss in mixed precision\n        if self.device.type == \"cuda\":\n            with torch.autocast(self.device.type, enabled=True):\n                loss = self.criterion(loss_preds, loss_targets), None\n        else:\n            loss = self.criterion(loss_preds, loss_targets), None\n\n        return loss\n\n    def mask_data(self, pred: torch.Tensor, target: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Masks the pred and target tensors according to nnunet ``ignore_label``. The number of classes in the input\n        tensors should be at least 3 corresponding to 2 classes for binary segmentation and 1 class which is\n        the ignore class specified by ignore label.\n\n        See nnunet documentation for more info:\n\n        https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/ignore_label.md\n\n        Args:\n            pred (torch.Tensor): The one hot encoded predicted segmentation maps with shape\n                ``(batch, classes, x, y(, z))``\n            target (torch.Tensor): The ground truth segmentation map with shape ``(batch, classes, x, y(, z))``\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Tuple of:\n\n                - torch.Tensor: The masked one hot encoded predicted segmentation maps.\n                - torch.Tensor: The masked target segmentation maps.\n        \"\"\"\n        # create mask where 1 is where pixels in target are not ignore label\n        # Modify target to remove the last class which is the ignore_label class\n        new_target = target\n        if self.nnunet_trainer.label_manager.has_regions:  # nnunet returns a ohe target if has_regions is true\n            # omit the last class, ie the ignore_label\n            mask = ~target[:, -1:] if target.dtype == torch.bool else 1 - target[:, -1:]\n            new_target = new_target[:, :-1]  # Remove final ignore_label class from target\n        else:  # target is not one hot encoded\n            mask = (target != self.nnunet_trainer.label_manager.ignore_label).float()\n            # Set ignore label to background essentially removing it as a class\n            new_target[new_target == self.nnunet_trainer.label_manager.ignore_label] = 0\n\n        # Tile the mask to be one hot encoded\n        mask_here = torch.tile(mask, (1, pred.shape[1], *[1 for _ in range(2, pred.ndim)]))\n\n        return (\n            pred * mask_here,\n            new_target,\n        )  # Mask the input tensor and return the modified target\n\n    def update_metric_manager(\n        self,\n        preds: TorchPredType,\n        target: TorchTargetType,\n        metric_manager: MetricManager,\n    ) -&gt; None:\n        \"\"\"\n        Update the metrics with preds and target. Overridden because we might need to manipulate inputs due to deep\n        supervision.\n\n        Args:\n            preds (TorchPredType): dictionary of model outputs.\n            target (TorchTargetType): the targets generated by the dataloader to evaluate the preds with.\n            metric_manager (MetricManager): the metric manager to update.\n        \"\"\"\n        if len(preds) &gt; 1:\n            # for nnunet the first pred in the output list is the main one\n            m_pred = convert_deep_supervision_dict_to_list(preds)[0]\n\n        if isinstance(target, torch.Tensor):\n            m_target = target\n        elif isinstance(target, dict):\n            if len(target) &gt; 1:\n                # If deep supervision is in use, we drop the additional targets\n                # when calculating the metrics as we only care about the\n                # original target which by default in nnunet is at index 0\n                m_target = convert_deep_supervision_dict_to_list(target)[0]\n            else:\n                m_target = list(target.values())[0]\n        else:\n            raise TypeError(\"Was expecting target to be type dict[str, torch.Tensor] or torch.Tensor\")\n\n        # Check if target is one hot encoded. Prediction always is for nnunet\n        # Add channel dimension if there isn't one\n        if m_pred.ndim != m_target.ndim:\n            m_target = m_target.view(m_target.shape[0], 1, *m_target.shape[1:])\n\n        # One hot encode targets if needed\n        if m_pred.shape != m_target.shape:\n            m_target_one_hot = torch.zeros(m_pred.shape, device=self.device, dtype=torch.bool)\n            # This is how nnunet does ohe in their functions\n            # Its a weird function that is not intuitive\n            # CAREFUL: Notice the underscore at the end of the scatter function.\n            # It makes a difference, was a hard bug to find!\n            m_target_one_hot.scatter_(1, m_target.long(), 1)\n        else:\n            m_target_one_hot = m_target\n\n        # Check if ignore label is in use. The nnunet loss figures this out on\n        # it's own, but we do it manually here for the metrics\n        if self.nnunet_trainer.label_manager.ignore_label is not None:\n            m_pred, m_target_one_hot = self.mask_data(m_pred, m_target_one_hot)\n\n        # m_pred is one hot encoded (OHE) output logits. Maybe masked by ignore label\n        # m_target_one_hot is OHE boolean label. Maybe masked by ignore label\n        metric_manager.update({\"prediction\": m_pred}, m_target_one_hot)\n\n    def empty_cache(self) -&gt; None:\n        \"\"\"Checks torch device and empties cache before training to optimize VRAM usage.\"\"\"\n        if self.device.type == \"cuda\":\n            torch.cuda.empty_cache()\n        elif self.device.type == \"mps\":\n            torch.mps.empty_cache()\n\n    def get_client_specific_logs(\n        self,\n        current_round: int | None,\n        current_epoch: int | None,\n        logging_mode: LoggingMode,\n    ) -&gt; tuple[str, list[tuple[LogLevel, str]]]:\n        if logging_mode == LoggingMode.TRAIN:\n            lr = float(self.optimizers[\"global\"].param_groups[0][\"lr\"])\n            if current_epoch is None:\n                # Assume training by steps\n                return f\"Initial LR {lr}\", []\n            return f\" Current LR: {lr}\", []\n        return \"\", []\n\n    def get_client_specific_reports(self) -&gt; dict[str, Any]:\n        return {\"learning_rate\": float(self.optimizers[\"global\"].param_groups[0][\"lr\"])}\n\n    @use_default_signal_handlers  # Experiment planner spawns a process I think\n    def get_properties(self, config: Config) -&gt; dict[str, Scalar]:\n        \"\"\"\n        Return properties (sample counts and nnunet plans) of client.\n\n        If nnunet plans are not provided by the server, creates a new set of nnunet plans from the local client\n        dataset. These plans are intended to be used for initializing global nnunet plans when they are not\n        provided.\n\n        Args:\n            config (Config): The config from the server\n\n        Returns:\n            (dict[str, Scalar]): A dictionary containing the train and validation sample counts as well as the\n                serialized nnunet plans\n        \"\"\"\n        # Check if nnunet plans have already been initialized\n        if \"nnunet_plans\" not in config:\n            log(INFO, \"Initializing the global plans using local dataset\")\n            # Local client will initialize global nnunet plans\n            # Check if local nnunet dataset fingerprint needs to be extracted\n            self.maybe_extract_fingerprint()\n\n            # Create experiment planner and plans.\n            # Plans name must be temp_plans so that we can safely delete the generated plans file\n            planner = ExperimentPlanner(dataset_name_or_id=self.dataset_id, plans_name=\"temp_plans\")\n\n            # Unless log level is DEBUG or lower, hide nnunet output\n            with redirect_stdout(self.stream2debug):\n                plans = planner.plan_experiment()\n\n            # Set plans name to local dataset so we know the source\n            plans[\"plans_name\"] = self.dataset_name + \"_plans\"\n            plans_bytes = pickle.dumps(plans)\n\n            # Remove plans file . A new one will be generated in self.setup_client\n            plans_path = join(nnUNet_preprocessed, self.dataset_name, planner.plans_identifier + \".json\")\n            if exists(plans_path):\n                os.remove(plans_path)\n\n            # Update local config with plans\n            config[\"nnunet_plans\"] = plans_bytes\n\n        # Get client properties. We are now sure that config contains plans\n        properties = super().get_properties(config)\n        properties[\"nnunet_plans\"] = config[\"nnunet_plans\"]\n\n        # super.get_properties should setup the client anyways, but we can add a check here as a precaution.\n        if not self.initialized:\n            self.setup_client(config)  # Client must be setup in order to initialize nnunet_trainer\n\n        # Add additional properties from nnunet trainer to properties dict. We may want to add more keys later\n        properties[\"num_input_channels\"] = self.nnunet_trainer.num_input_channels\n        properties[\"num_segmentation_heads\"] = self.nnunet_trainer.label_manager.num_segmentation_heads\n        properties[\"enable_deep_supervision\"] = self.nnunet_trainer.enable_deep_supervision\n\n        return properties\n\n    def shutdown_dataloader(self, dataloader: DataLoader | None, dl_name: str | None = None) -&gt; None:\n        \"\"\"\n        The nnunet dataloader/augmenter uses multiprocessing under the hood, so the shutdown method terminates the\n        child processes gracefully.\n\n        Args:\n            dataloader (DataLoader): The dataloader to shutdown\n            dl_name (str | None): A string that identifies the dataloader to shutdown. Used for logging purposes.\n                Defaults to None\n        \"\"\"\n        if dataloader is not None and isinstance(dataloader, NnUNetDataLoaderWrapper):\n            if self.verbose:\n                log(INFO, f\"\\tShutting down nnunet dataloader: {dl_name}\")\n            dataloader.shutdown()\n\n        del dataloader\n\n    def shutdown(self) -&gt; None:\n        # Unfreeze and collect memory that was frozen during training\n        # See self.update_before_train()\n        gc.unfreeze()\n        gc.collect()\n\n        # Shutdown dataloader subprocesses gracefully\n        self.shutdown_dataloader(self.train_loader, \"train_loader\")\n        self.shutdown_dataloader(self.val_loader, \"val_loader\")\n        self.shutdown_dataloader(self.test_loader, \"test_loader\")\n\n        # Parent shutdown\n        super().shutdown()\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        # Was getting OOM errors that could only be fixed by manually cleaning up RAM\n        # https://github.com/pytorch/pytorch/issues/95462\n        # The above issue seems to be the situation I was in.\n        gc.collect()  # Cleans up unused variables\n        # As the linked issue above points out, calling gc.freeze() greatly reduces the\n        # overhead of garbage collection. (from 1.5s to 0.005s)\n        if current_server_round == 2:  # noqa: PLR2004\n            # Collect runs even faster if we freeze after the end of the first iteration\n            # Likely because a lot of variables are created in the first pass. If we\n            # freeze before the first pass, gc.collect has to check all those variables\n            gc.freeze()\n\n    def transform_gradients(self, losses: TrainingLosses) -&gt; None:\n        \"\"\"\n        Apply the gradient clipping performed by the default nnunet trainer. This is the default behavior for\n        nnunet 2.5.1.\n\n        Args:\n            losses (TrainingLosses): Not used for this transformation.\n        \"\"\"\n        nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.__init__","title":"<code>__init__(device, dataset_id, fold, data_identifier=None, plans_identifier=None, compile=True, always_preprocess=False, max_grad_norm=12, n_dataload_processes=None, verbose=True, metrics=None, progress_bar=False, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, client_name=None, nnunet_trainer_class=nnUNetTrainer, nnunet_trainer_class_kwargs=None)</code>","text":"<p>A client for training nnunet models. Requires the nnunet environment variables to be set. Also requires the following additional keys in the config sent from the server as follows.</p> <ul> <li>\"nnunet_plans\": (serialized dict)</li> <li>\"nnunet_config\": (str)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\" or \"mps\"</p> required <code>dataset_id</code> <code>int</code> <p>The nnunet dataset id for the local client dataset to use for training and validation.</p> required <code>fold</code> <code>int | str</code> <p>Which fold of the local client dataset to use for validation. nnunet defaults to 5 folds (0 to 4). Can also be set to \"all\" to use all the data for both training and validation.</p> required <code>data_identifier</code> <code>str | None</code> <p>The nnunet data identifier prefix to use. The final data identifier will be <code>{data_identifier}_config</code> where \"config\" is the nnunet config (e.g. 2d, <code>3d_fullres</code>, etc.). If preprocessed data already exists can be used to specify which preprocessed data to use. By default, the <code>plans_identifier</code> is used as the <code>data_identifier</code>.</p> <code>None</code> <code>plans_identifier</code> <code>str | None</code> <p>Specify what to save the client's local copy of the plans file as. The client makes a local modified copy of the global source plans file sent by the server. If left as default None, the plans identifier will be set as \"FL-plansname-000local\" where 000 is the <code>dataset_id</code> and plansname is the \"plans_name\" value of the source plans file. The original plans will be saved under the <code>source_plans_name</code> key in the modified plans file.</p> <code>None</code> <code>compile</code> <code>bool</code> <p>If set to True, the client will Just-In-Time (JIT) compile the nnUNet model and perform optimizations at the start of training. This process significantly reduces the runtime for nnUNet models, especially for larger models or long-running jobs. However, it introduces some overhead time and computation during the initial step. It is recommended to keep this option enabled. The default value is True.</p> <code>True</code> <code>always_preprocess</code> <code>bool</code> <p>If True, will preprocess the local client dataset even if the preprocessed data already seems to exist. The existence of the preprocessed data is determined by matching the provided <code>data_identifier</code> with that of the preprocessed data already  on the client. Defaults to False.</p> <code>False</code> <code>max_grad_norm</code> <code>float</code> <p>The maximum gradient norm to use for gradient clipping. Defaults to 12 which is the nnunetv2 2.5.1 default.</p> <code>12</code> <code>n_dataload_processes</code> <code>int | None</code> <p>The number of processes to spawn for each nnunet dataloader. If left as None we use the nnunetv2 version 2.5.1 defaults for each config</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True the client will log some extra INFO logs. Defaults to False unless the log level is DEBUG or lower.</p> <code>True</code> <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to print a progress bar to stdout for training. Defaults to False</p> <code>False</code> <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter]</code> <p>A sequence of FL4Health reporters which the client should send data to.</p> <code>None</code> <code>client_name</code> <code>str | None</code> <p>Name of the client. If None the class will set a default and random name. Defaults to None.</p> <code>None</code> <code>nnunet_trainer_class</code> <code>type[nnUNetTrainer]</code> <p>A <code>nnUNetTrainer</code> constructor. Useful for passing custom <code>nnUNetTrainer</code>. Defaults to the standard <code>nnUNetTrainer</code> class. Must match the <code>nnunet_trainer_class</code> passed to the <code>NnunetServer</code>.</p> <code>nnUNetTrainer</code> <code>nnunet_trainer_class_kwargs</code> <code>dict[str, Any]</code> <p>Additional kwargs to pass to <code>nnunet_trainer_class</code>. Defaults to empty dictionary.</p> <code>None</code> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def __init__(\n    self,\n    device: torch.device,\n    dataset_id: int,\n    fold: int | str,\n    data_identifier: str | None = None,\n    plans_identifier: str | None = None,\n    compile: bool = True,\n    always_preprocess: bool = False,\n    max_grad_norm: float = 12,\n    n_dataload_processes: int | None = None,\n    verbose: bool = True,\n    metrics: Sequence[Metric] | None = None,\n    progress_bar: bool = False,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    client_name: str | None = None,\n    nnunet_trainer_class: type[nnUNetTrainer] = nnUNetTrainer,\n    nnunet_trainer_class_kwargs: dict[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    A client for training nnunet models. Requires the nnunet environment variables to be set. Also requires the\n    following additional keys in the config sent from the server as follows.\n\n    - \"nnunet_plans\": (serialized dict)\n    - \"nnunet_config\": (str)\n\n    Args:\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\" or \"mps\"\n        dataset_id (int): The nnunet dataset id for the local client dataset to use for training and validation.\n        fold (int | str): Which fold of the local client dataset to use for validation. nnunet defaults to\n            5 folds (0 to 4). Can also be set to \"all\" to use all the data for both training and validation.\n        data_identifier (str | None, optional): The nnunet data identifier prefix to use. The final data\n            identifier will be ``{data_identifier}_config`` where \"config\" is the nnunet config (e.g. 2d,\n            ``3d_fullres``, etc.). If preprocessed data already exists can be used to specify which preprocessed\n            data to use. By default, the ``plans_identifier`` is used as the ``data_identifier``.\n        plans_identifier (str | None, optional): Specify what to save the client's local copy of the plans file\n            as. The client makes a local modified copy of the global source plans file sent by the server. If left\n            as default None, the plans identifier will be set as \"FL-plansname-000local\" where 000 is the\n            ``dataset_id`` and plansname is the \"plans_name\" value of the source plans file. The original plans\n            will be saved under the ``source_plans_name`` key in the modified plans file.\n        compile (bool, optional): If set to True, the client will Just-In-Time (JIT) compile the nnUNet model and\n            perform optimizations at the start of training. This process significantly reduces the runtime for\n            nnUNet models, especially for larger models or long-running jobs. However, it introduces some overhead\n            time and computation during the initial step. It is recommended to keep this option enabled. The\n            default value is True.\n        always_preprocess (bool, optional): If True, will preprocess the local client dataset even if the\n            preprocessed data already seems to exist. The existence of the preprocessed data is determined by\n            matching the provided ``data_identifier`` with that of the preprocessed data already  on the client.\n            Defaults to False.\n        max_grad_norm (float, optional): The maximum gradient norm to use for gradient clipping. Defaults to 12\n            which is the nnunetv2 2.5.1 default.\n        n_dataload_processes (int | None, optional): The number of processes to spawn for each nnunet\n            dataloader. If left as None we use the nnunetv2 version 2.5.1 defaults for each config\n        verbose (bool, optional): If True the client will log some extra INFO logs. Defaults to False unless\n            the log level is DEBUG or lower.\n        metrics (Sequence[Metric], optional): Metrics to be computed based on the labels and predictions of the\n            client model. Defaults to None.\n        progress_bar (bool, optional): Whether or not to print a progress bar to stdout for training. Defaults\n            to False\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over each\n            batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the client should\n            send data to.\n        client_name (str | None, optional): Name of the client. If None the class will set a default and random\n            name. Defaults to None.\n        nnunet_trainer_class (type[nnUNetTrainer]): A ``nnUNetTrainer`` constructor. Useful for passing custom\n            ``nnUNetTrainer``. Defaults to the standard ``nnUNetTrainer`` class. Must match the\n            ``nnunet_trainer_class`` passed to the ``NnunetServer``.\n        nnunet_trainer_class_kwargs (dict[str, Any]): Additional kwargs to pass to ``nnunet_trainer_class``.\n            Defaults to empty dictionary.\n    \"\"\"\n    metrics = metrics if metrics else []\n\n    # Parent method sets up several class attributes\n    super().__init__(\n        data_path=Path(\"dummy/path\"),  # data_path not used by NnunetClient\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n\n    # Some nnunet client specific attributes\n    self.dataset_id: int = dataset_id\n    self.dataset_name = convert_id_to_dataset_name(self.dataset_id)\n    self.fold = fold\n    self.data_identifier = data_identifier\n    self.always_preprocess = always_preprocess\n    self.plans_name = plans_identifier\n    self.fingerprint_extracted = False\n    self.grad_scaler = GradScaler()\n    self.max_grad_norm = max_grad_norm\n    self.n_dataload_proc = n_dataload_processes\n    try:\n        self.dataset_json = load_json(join(nnUNet_raw, self.dataset_name, \"dataset.json\"))\n    except Exception:\n        try:\n            self.dataset_json = load_json(join(nnUNet_preprocessed, self.dataset_name, \"dataset.json\"))\n        except Exception as e:\n            log(ERROR, \"Could not load the nnunet dataset json from nnUNet_raw or nnUNet_preprocessed.\")\n            raise e  # Raising e will raise both exceptions since it is nested.\n\n    # Auto set verbose to True if console handler is on DEBUG mode\n    self.verbose = verbose if console_handler.level &gt;= INFO else True\n\n    # Used to redirect stdout to logger\n    self.stream2debug = StreamToLogger(FLOWER_LOGGER, DEBUG)\n\n    # nnunet specific attributes to be initialized in setup_client\n    self.nnunet_trainer_class = nnunet_trainer_class\n    self.nnunet_trainer_class_kwargs = (\n        nnunet_trainer_class_kwargs if nnunet_trainer_class_kwargs is not None else {}\n    )\n    self.nnunet_trainer: nnUNetTrainer\n    self.nnunet_config: NnunetConfig\n    self.plans: dict[str, Any] | None = None\n    self.steps_per_round: int  # N steps per server round\n    self.max_steps: int  # N_rounds x steps_per_round\n\n    # Turn on/off model optimizations for decreasing runtime efficiency.\n    if compile:\n        # Turning on cudnn.benchmark reduces nnUNet runtimes by 2-3x in our experiments.\n        # Limit of 0 tells cudnn to benchmark all available conv algorithms (default is 10)\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.benchmark_limit = 0\n        os.environ[\"nnUNet_compile\"] = str(\"true\")  # noqa: SIM112\n    else:\n        torch.backends.cudnn.benchmark = False\n        os.environ[\"nnUNet_compile\"] = str(\"false\")  # noqa: SIM112\n        if self.verbose:\n            log(INFO, \"Disabling model optimizations and JIT compilation. This may impact runtime performance.\")\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.train_step","title":"<code>train_step(input, target)</code>","text":"<p>Given a single batch of input and target data, generate predictions, compute loss, update parameters and optionally update metrics if they exist. (i.e. backprop on a single batch of data). Assumes <code>self.model</code> is in train mode already.</p> <p>Overrides parent to include mixed precision training (autocasting and corresponding gradient scaling) as per the original <code>nnUNetTrainer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>The input to be fed into the model.</p> required <code>target</code> <code>TorchTargetType</code> <p>The target corresponding to the input.</p> required <p>Returns:</p> Type Description <code>tuple[TrainingLosses, TorchPredType]</code> <p>The losses object from the train step along with a dictionary of any predictions produced by the model.</p> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def train_step(self, input: TorchInputType, target: TorchTargetType) -&gt; tuple[TrainingLosses, TorchPredType]:\n    \"\"\"\n    Given a single batch of input and target data, generate predictions, compute loss, update parameters and\n    optionally update metrics if they exist. (i.e. backprop on a single batch of data). Assumes ``self.model`` is\n    in train mode already.\n\n    Overrides parent to include mixed precision training (autocasting and corresponding gradient scaling) as per\n    the original ``nnUNetTrainer``.\n\n    Args:\n        input (TorchInputType): The input to be fed into the model.\n        target (TorchTargetType): The target corresponding to the input.\n\n    Returns:\n        (tuple[TrainingLosses, TorchPredType]): The losses object from the train step along with a dictionary of\n            any predictions produced by the model.\n    \"\"\"\n    # If the device type is not cuda, we don't use mixed precision training and therefore can use parent method.\n    if self.device.type != \"cuda\":\n        return super().train_step(input, target)\n\n    # As in the nnUNetTrainer, we implement mixed precision using torch.autocast and torch.GradScaler\n    # Clear gradients from optimizer if they exist\n    self.optimizers[\"global\"].zero_grad()\n\n    # Call user defined methods to get predictions and compute loss\n    preds, features = self.predict(input)\n    target = self.transform_target(target)\n    losses = self.compute_training_loss(preds, features, target)\n\n    # Compute scaled loss and perform backward pass\n    scaled_backward_loss = self.grad_scaler.scale(losses.backward[\"backward\"])\n    scaled_backward_loss.backward()\n\n    # Rescale gradients then clip based on specified norm\n    self.grad_scaler.unscale_(self.optimizers[\"global\"])\n    self.transform_gradients(losses)\n\n    # Update parameters and scaler\n    self.grad_scaler.step(self.optimizers[\"global\"])\n    self.grad_scaler.update()\n\n    return losses, preds\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.get_data_loaders","title":"<code>get_data_loaders(config)</code>","text":"<p>Gets the nnunet dataloaders and wraps them in another class to make them pytorch iterators.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config file from the server</p> required <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader]</code> <p>A tuple of length two. The client train and validation dataloaders as pytorch dataloaders</p> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>@use_default_signal_handlers  # Dataloaders use multiprocessing\ndef get_data_loaders(self, config: Config) -&gt; tuple[DataLoader, DataLoader]:\n    \"\"\"\n    Gets the nnunet dataloaders and wraps them in another class to make them pytorch iterators.\n\n    Args:\n        config (Config): The config file from the server\n\n    Returns:\n        (tuple[DataLoader, DataLoader]): A tuple of length two. The client train and validation dataloaders as\n            pytorch dataloaders\n    \"\"\"\n    start_time = time.time()\n    # Set the number of processes for each dataloader.\n    if self.n_dataload_proc is None:\n        # Nnunet default is 12 or max cpu's. We decrease max by 1 just in case\n        # NOTE: The type: ignore here is to skip issues where a local operating system is not compatible\n        # with sched_getaffinity (older versions of MacOS, for example). The code still won't run but mypy won't\n        # complain. Workarounds like using os.cpu_count(), while not exactly the same, are possible.\n        try:\n            self.n_dataload_proc = min(12, len(os.sched_getaffinity(0)) - 1)  # type: ignore\n        except AttributeError:\n            # TODO: this is pretty brittle\n            if cpu_count := os.cpu_count():\n                self.n_dataload_proc = min(12, cpu_count - 2)\n            else:\n                self.n_dataload_proc = 1\n    os.environ[\"nnUNet_n_proc_DA\"] = str(self.n_dataload_proc)  # noqa: SIM112\n\n    # The batchgenerators package used under the hood by the dataloaders creates an additional stream handler for\n    # the root logger Therefore all logs get printed twice. First stop flwr logger from propagating logs to root.\n    # Issue: https://github.com/MIC-DKFZ/batchgenerators/issues/123 PR:\n    # https://github.com/MIC-DKFZ/batchgenerators/pull/124\n    FLOWER_LOGGER.propagate = False\n\n    # Redirect nnunet output to flwr logger at DEBUG level.\n    with redirect_stdout(self.stream2debug):\n        # Get the nnunet dataloader iterators. (Technically augmenter classes)\n        train_loader, val_loader = self.nnunet_trainer.get_dataloaders()\n\n    # Now clear root handler that was created when get_dataloaders was called and reset flwr logger propagate\n    root_logger = logging.getLogger()\n    root_logger.handlers.clear()\n    FLOWER_LOGGER.propagate = True\n\n    # Get accurate estimate of image shape so that we can get accurate dataloader length\n    if self.plans is None:\n        self.plans = self.create_plans(config)  # Local plans will have metadata we need\n    fullres_cfg = \"3d_fullres\" if \"3d_fullres\" in self.plans[\"configurations\"] else \"2d\"\n    shape = self.plans[\"configurations\"][fullres_cfg][\"median_image_size_in_voxels\"]\n\n    # Wrap nnunet dataloaders to make them compatible with fl4health\n    train_loader = NnUNetDataLoaderWrapper(\n        nnunet_augmenter=train_loader, nnunet_config=self.nnunet_config, ref_image_shape=shape\n    )\n    val_loader = NnUNetDataLoaderWrapper(\n        nnunet_augmenter=val_loader, nnunet_config=self.nnunet_config, ref_image_shape=shape\n    )\n    log(INFO, f\"{len(val_loader)}, {len(val_loader.dataset)}, {val_loader.nnunet_dataloader.batch_size}\")\n\n    if self.verbose:\n        log(INFO, f\"\\tDataloaders initialized in {time.time() - start_time:.1f}s\")\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.get_lr_scheduler","title":"<code>get_lr_scheduler(optimizer_key, config)</code>","text":"<p>Creates an LR Scheduler similar to the nnunet default except we set max steps to the total number of steps and update every step. Initial and final LR are the same as nnunet, difference is nnunet sets max steps to num \"epochs\", but they define an \"epoch\" as exactly 250 steps. Therefore they update every 250 steps. Override this method to set your own LR scheduler.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer_key</code> <code>str</code> <p>Key of the optimizer to which the scheduler will be applied.</p> required <code>config</code> <code>Config</code> <p>The server config. This method will determine the total number of steps that will be applied across all server rounds on FL training.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Thrown if the configuration does not contain either a steps or epochs specification.</p> <p>Returns:</p> Type Description <code>_LRScheduler</code> <p>The default nnunet LR Scheduler for nnunetv2 2.5.1</p> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def get_lr_scheduler(self, optimizer_key: str, config: Config) -&gt; _LRScheduler:\n    \"\"\"\n    Creates an LR Scheduler similar to the nnunet default except we set max steps to the total number of steps\n    and update every step. Initial and final LR are the same as nnunet, difference is nnunet sets max steps to\n    num \"epochs\", but they define an \"epoch\" as exactly 250 steps. Therefore they update every 250 steps. Override\n    this method to set your own LR scheduler.\n\n    Args:\n        optimizer_key (str): Key of the optimizer to which the scheduler will be applied.\n        config (Config): The server config. This method will determine the total number of steps that will be\n            applied across all server rounds on FL training.\n\n    Raises:\n        ValueError: Thrown if the configuration does not contain either a steps or epochs specification.\n\n    Returns:\n        (_LRScheduler): The default nnunet LR Scheduler for nnunetv2 2.5.1\n    \"\"\"\n    if not isinstance(self.nnunet_trainer.lr_scheduler, PolyLRScheduler):\n        log(\n            WARNING,\n            (\n                \"Nnunet seems to have changed their default LR scheduler to \"\n                f\"type: {type(self.nnunet_trainer.lr_scheduler)}. \"\n                \"Using PolyLRScheduler instead. Override or update the \"\n                \"get_lr_scheduler method of nnUNetClient to change this\"\n            ),\n        )\n\n    # Determine total number of steps throughout all FL rounds\n    local_epochs, local_steps, _, _, _ = self.process_config(config)\n    if local_steps is not None:\n        steps_per_round = local_steps\n    elif local_epochs is not None:\n        steps_per_round = local_epochs * len(self.train_loader)\n    else:\n        raise ValueError(\"One of local steps or local epochs must be specified\")\n\n    total_steps = int(config[\"n_server_rounds\"]) * steps_per_round\n\n    # Create and return LR Scheduler Wrapper for the PolyLRScheduler so that it is\n    # compatible with Torch LRScheduler\n    # Create and return LR Scheduler. This is nnunet default for version 2.5.1\n    self.reports_manager.report({\"LR Scheduler\": \"PolyLRScheduler\"})\n    return PolyLRSchedulerWrapper(\n        self.optimizers[optimizer_key],\n        initial_lr=self.nnunet_trainer.initial_lr,\n        max_steps=total_steps,\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.create_plans","title":"<code>create_plans(config)</code>","text":"<p>Modifies the provided plans file to work with the local client dataset and then saves it to disk. Requires the local <code>dataset_fingerprint.json</code> to exist, the local <code>dataset_name</code>, <code>plans_name</code>, <code>data_identifier</code> and <code>dataset_json</code>.</p> <p>The following fields are modified:</p> <ul> <li>plans_name</li> <li>dataset_name</li> <li>original_median_shape_after_transp</li> <li>original_median_spacing_after_transp</li> <li>configurations.{config}.data_identifier</li> <li>configurations.{config}.batch_size</li> <li>configurations.{config}.median_image_size_in_voxels</li> <li>foreground_intensity_properties_per_channel</li> </ul> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config provided by the server. Expects the \"nnunet_plans\" key with a pickled dictionary as the value</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>The modified nnunet plans for the client</p> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def create_plans(self, config: Config) -&gt; dict[str, Any]:\n    \"\"\"\n    Modifies the provided plans file to work with the local client dataset and then saves it to disk. Requires the\n    local ``dataset_fingerprint.json`` to exist, the local ``dataset_name``, ``plans_name``, ``data_identifier``\n    and ``dataset_json``.\n\n    The following fields are modified:\n\n    - plans_name\n    - dataset_name\n    - original_median_shape_after_transp\n    - original_median_spacing_after_transp\n    - configurations.{config}.data_identifier\n    - configurations.{config}.batch_size\n    - configurations.{config}.median_image_size_in_voxels\n    - foreground_intensity_properties_per_channel\n\n    Args:\n        config (Config): The config provided by the server. Expects the \"nnunet_plans\" key with a pickled\n            dictionary as the value\n\n    Returns:\n        (dict[str, Any]): The modified nnunet plans for the client\n    \"\"\"\n    # TODO: Make this an external function or part of another class and explicitly accept the required arguments\n    # rather than using class attributes.\n\n    # Get the source nnunet plans specified by the server\n    plans = pickle.loads(narrow_dict_type(config, \"nnunet_plans\", bytes))\n\n    # Change plans name.\n    if self.plans_name is None:\n        self.plans_name = f\"FL-{plans['plans_name']}-{self.dataset_id}local\"\n\n    plans[\"source_plans_name\"] = plans[\"plans_name\"]\n    plans[\"plans_name\"] = self.plans_name\n\n    # Change dataset name\n    plans[\"dataset_name\"] = self.dataset_name\n\n    # Load the dataset fingerprint for the local client dataset\n    fp_path = Path(nnUNet_preprocessed) / self.dataset_name / \"dataset_fingerprint.json\"\n    assert fp_path.exists(), \"Could not find the dataset fingerprint file\"\n    fp = load_json(fp_path)\n\n    # Change the foreground intensity properties per channel\n    plans[\"foreground_intensity_properties_per_channel\"] = fp[\"foreground_intensity_properties_per_channel\"]\n\n    # Compute the median image size and spacing of the local client dataset\n    median_shape = np.median(fp[\"shapes_after_crop\"], axis=0)[plans[\"transpose_forward\"]]\n    median_spacing = np.median(fp[\"spacings\"], axis=0)[plans[\"transpose_forward\"]]\n    plans[\"original_median_shape_after_transp\"] = [int(round(i)) for i in median_shape]\n    plans[\"original_median_spacing_after_transp\"] = [float(i) for i in median_spacing]\n\n    # Get the spacing that the network will resample to. Need to check if samples are 2d or 3d\n    fullres_cfg = \"3d_fullres\" if \"3d_fullres\" in plans[\"configurations\"] else \"2d\"\n    target_spacing = plans[\"configurations\"][fullres_cfg][\"spacing\"]\n\n    # Get the median shape after resampling to the desired/input voxel spacing\n    resampled_shapes = [\n        compute_new_shape(i, j, target_spacing) for i, j in zip(fp[\"shapes_after_crop\"], fp[\"spacings\"])\n    ]\n    resampled_median_shape = np.median(resampled_shapes, axis=0)[plans[\"transpose_forward\"]].tolist()\n\n    # Change data identifier\n    if self.data_identifier is None:\n        self.data_identifier = self.plans_name\n\n    # To be consistent with nnunet, a batch cannot contain more than 5% of the voxels in the dataset.\n    max_voxels = np.prod(resampled_median_shape, dtype=np.float64) * self.dataset_json[\"numTraining\"] * 0.05\n\n    # Iterate through nnunet configs in plans file\n    for c in plans[\"configurations\"]:\n        # Change the data identifier\n        plans[\"configurations\"][c][\"data_identifier\"] = self.data_identifier + \"_\" + c\n\n        # Ensure batch size is at least 2 and at most 5 percent of dataset\n        # Otherwise we keep it the same as it affects the target VRAM consumption\n        if \"batch_size\" in plans[\"configurations\"][c]:\n            old_bs = plans[\"configurations\"][c][\"batch_size\"]\n            bs_5percent = round(max_voxels / np.prod(plans[\"configurations\"][c][\"patch_size\"], dtype=np.float64))\n            new_bs = max(min(old_bs, bs_5percent), 2)\n            plans[\"configurations\"][c][\"batch_size\"] = new_bs\n        else:\n            log(WARNING, f\"Did not find a 'batch_size' key in the nnunet plans dict for nnunet config: {c}\")\n\n        # Update median shape of resampled input images\n        if str(c).startswith(\"2d\"):\n            plans[\"configurations\"][c][\"median_image_size_in_voxels\"] = resampled_median_shape[1:]\n        else:\n            plans[\"configurations\"][c][\"median_image_size_in_voxels\"] = resampled_median_shape\n\n    # Save local plans file\n    os.makedirs(join(nnUNet_preprocessed, self.dataset_name), exist_ok=True)\n    plans_save_path = join(nnUNet_preprocessed, self.dataset_name, self.plans_name + \".json\")\n    save_json(plans, plans_save_path, sort_keys=False)\n    return plans\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.maybe_preprocess","title":"<code>maybe_preprocess(nnunet_config)</code>","text":"<p>Checks if preprocessed data for current plans exists and if not preprocesses the nnunet_raw dataset. The preprocessed data is saved in '{nnUNet_preprocessed}/{dataset_name}/{data_identifier} as follows.</p> <ul> <li><code>nnUNet_preprocessed</code> is the directory specified by the <code>nnUNet_preprocessed</code> environment variable.</li> <li><code>dataset_name</code> is the nnunet dataset name (e.g. Dataset123_MyDataset).</li> <li><code>data_identifier</code> is <code>{self.data_identifier}_{self.nnunet_config}</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>nnunet_config</code> <code>NnunetConfig</code> <p>The nnunet config as a <code>NnunetConfig</code> Enum. Enum type ensures nnunet config is valid.</p> required Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>@use_default_signal_handlers  # Preprocessing spawns subprocesses\ndef maybe_preprocess(self, nnunet_config: NnunetConfig) -&gt; None:\n    \"\"\"\n    Checks if preprocessed data for current plans exists and if not preprocesses the nnunet_raw dataset. The\n    preprocessed data is saved in '{nnUNet_preprocessed}/{dataset_name}/{data_identifier} as follows.\n\n    - ``nnUNet_preprocessed`` is the directory specified by the ``nnUNet_preprocessed`` environment variable.\n    - ``dataset_name`` is the nnunet dataset name (e.g. Dataset123_MyDataset).\n    - ``data_identifier`` is ``{self.data_identifier}_{self.nnunet_config}``.\n\n    Args:\n        nnunet_config (NnunetConfig): The nnunet config as a ``NnunetConfig`` Enum. Enum type ensures nnunet\n            config is valid.\n    \"\"\"\n    assert self.data_identifier is not None, \"Was expecting data identifier to be initialized in self.create_plans\"\n\n    # Preprocess data if it's not already there\n    if self.always_preprocess or not exists(self.nnunet_trainer.preprocessed_dataset_folder):\n        if self.verbose:\n            log(INFO, f\"\\tPreprocessing local client dataset: {self.dataset_name}\")\n        # Unless log level is debugging or lower, hide nnunet output\n        with redirect_stdout(self.stream2debug):\n            preprocess_dataset(\n                dataset_id=self.dataset_id,\n                plans_identifier=self.plans_name,\n                num_processes=[NNUNET_DEFAULT_NP[nnunet_config]],\n                configurations=[nnunet_config.value],\n            )\n    elif self.verbose:\n        log(\n            INFO,\n            \"\\tnnunet preprocessed data seems to already exist. Skipping preprocessing\",\n        )\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.maybe_extract_fingerprint","title":"<code>maybe_extract_fingerprint()</code>","text":"<p>Checks if nnunet dataset fingerprint already exists and if not extracts one from the dataset.</p> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>@use_default_signal_handlers  # Fingerprint extraction spawns subprocess\ndef maybe_extract_fingerprint(self) -&gt; None:\n    \"\"\"Checks if nnunet dataset fingerprint already exists and if not extracts one from the dataset.\"\"\"\n    # Check first whether this client instance has already extracted a dataset fp\n    # Possible if the client was asked to generate the nnunet plans for the server\n    if not self.fingerprint_extracted:\n        fp_path = join(nnUNet_preprocessed, self.dataset_name, \"dataset_fingerprint.json\")\n        # Check if fp already exists or if we want to redo fp extraction\n        if self.always_preprocess or not exists(fp_path):\n            start = time.time()\n            # Unless log level is DEBUG or lower hide nnunet output\n            with redirect_stdout(self.stream2debug):\n                extract_fingerprints(dataset_ids=[self.dataset_id])\n            if self.verbose:\n                log(\n                    INFO,\n                    f\"\\tExtracted dataset fingerprint in {time.time() - start:.1f}s\",\n                )\n        elif self.verbose:\n            log(\n                INFO,\n                \"\\tnnunet dataset fingerprint already exists. Skipping fingerprint extraction\",\n            )\n    elif self.verbose:\n        log(\n            INFO,\n            \"\\tThis client has already extracted the dataset fingerprint during this session. Skipping.\",\n        )\n    # Avoid extracting fingerprint multiple times when always_preprocess is true\n    self.fingerprint_extracted = True\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Ensures the necessary files for training are on disk and initializes several class attributes that depend on values in the config from the server. This is called once when the client is sampled by the server for the first time.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config file from the server. The <code>nnUNetClient</code> expects the keys 'nnunet_config' and 'nnunet_plans' in addition to those required by <code>BasicClient</code></p> required Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>@use_default_signal_handlers\ndef setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Ensures the necessary files for training are on disk and initializes several class attributes that depend on\n    values in the config from the server. This is called once when the client is sampled by the server for the\n    first time.\n\n    Args:\n        config (Config): The config file from the server. The ``nnUNetClient`` expects the keys 'nnunet_config'\n            and 'nnunet_plans' in addition to those required by ``BasicClient``\n    \"\"\"\n    log(INFO, \"Setting up the nnUNetClient\")\n\n    # Empty gpu cache because nnunet does it\n    self.empty_cache()\n\n    # Get nnunet config\n    self.nnunet_config = NnunetConfig(config[\"nnunet_config\"])\n\n    # Check if dataset fingerprint has already been extracted\n    self.maybe_extract_fingerprint()\n\n    # Create the nnunet plans for the local client\n    if self.plans is None:\n        self.plans = self.create_plans(config=config)\n\n    # Unless log level is DEBUG or lower hide nnunet output\n    with redirect_stdout(self.stream2debug):\n        # Create the nnunet trainer\n        self.nnunet_trainer = self.nnunet_trainer_class(\n            plans=self.plans,\n            configuration=self.nnunet_config.value,\n            fold=self.fold,\n            dataset_json=self.dataset_json,\n            device=self.device,\n            **self.nnunet_trainer_class_kwargs,\n        )\n        # nnunet_trainer initialization\n        self.nnunet_trainer.initialize()\n        # This is done by nnunet_trainer in self.on_train_start, we\n        # do it manually since nnunet_trainer not being used for training\n        self.nnunet_trainer.set_deep_supervision_enabled(self.nnunet_trainer.enable_deep_supervision)\n\n    # Prevent nnunet from generating log files and delete empty output directories\n    os.remove(self.nnunet_trainer.log_file)\n    self.nnunet_trainer.log_file = os.devnull\n    output_folder = Path(self.nnunet_trainer.output_folder)\n    while len(os.listdir(output_folder)) == 0:\n        os.rmdir(output_folder)\n        output_folder = output_folder.parent\n\n    # Preprocess nnunet_raw data if needed\n    self.maybe_preprocess(self.nnunet_config)\n    start = time.time()\n    unpack_dataset(  # Reduces load on CPU and RAM during training\n        folder=self.nnunet_trainer.preprocessed_dataset_folder,\n        unpack_segmentation=self.nnunet_trainer.unpack_dataset,\n        overwrite_existing=self.always_preprocess,\n        verify_npy=True,\n    )\n    if self.verbose:\n        log(INFO, f\"\\tUnpacked dataset in {time.time() - start:.1f}s\")\n\n    # We have to call parent method after setting up nnunet trainer\n    super().setup_client(config)\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.predict","title":"<code>predict(input)</code>","text":"<p>Generate model outputs. Overridden because nnunets outputs lists when deep supervision is on so we have to reformat the output into dicts.</p> <p>Additionally if device type is cuda, loss computed in mixed precision.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>The model inputs</p> required <p>Returns:</p> Type Description <code>tuple[TorchPredType, dict[str, Tensor]]</code> <p>A tuple in which the first element model outputs indexed by name. The second element is unused by this subclass and therefore is always an empty dict</p> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def predict(self, input: TorchInputType) -&gt; tuple[TorchPredType, dict[str, torch.Tensor]]:\n    \"\"\"\n    Generate model outputs. Overridden because nnunets outputs lists when deep supervision is on so we have to\n    reformat the output into dicts.\n\n    Additionally if device type is cuda, loss computed in mixed precision.\n\n    Args:\n        input (TorchInputType): The model inputs\n\n    Returns:\n        (tuple[TorchPredType, dict[str, torch.Tensor]]): A tuple in which the first element model outputs indexed\n            by name. The second element is unused by this subclass and therefore is always an empty dict\n    \"\"\"\n    if isinstance(input, torch.Tensor):\n        # If device type is cuda, nnUNet defaults to mixed precision forward pass\n        if self.device.type == \"cuda\":\n            with torch.autocast(self.device.type, enabled=True):\n                output = self.model(input)\n        else:\n            output = self.model(input)\n    else:\n        raise TypeError('\"input\" must be of type torch.Tensor for nnUNetClient')\n\n    if isinstance(output, torch.Tensor):\n        return {\"prediction\": output}, {}\n    # If output is a list or tuple then deep supervision is on and we need to convert preds into a dict\n    if isinstance(output, (list, tuple)):\n        num_spatial_dims = NNUNET_N_SPATIAL_DIMS[self.nnunet_config]\n        preds = convert_deep_supervision_list_to_dict(output, num_spatial_dims)\n        return preds, {}\n    raise TypeError(\n        \"Was expecting nnunet model output to be either a torch.Tensor or a list/tuple of torch.Tensors\"\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.compute_loss_and_additional_losses","title":"<code>compute_loss_and_additional_losses(preds, features, target)</code>","text":"<p>Checks the pred and target types and computes the loss. If device type is cuda, loss computed in mixed precision.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Dictionary of model output tensors indexed by name</p> required <code>features</code> <code>dict[str, Tensor]</code> <p>Not used by this subclass</p> required <code>target</code> <code>TorchTargetType</code> <p>The targets to evaluate the predictions with. If multiple prediction tensors are given, target must be a dictionary with the same number of tensors</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor] | None]</code> <p>A tuple where the first element is the loss and the second element is an optional additional loss</p> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self,\n    preds: TorchPredType,\n    features: dict[str, torch.Tensor],\n    target: TorchTargetType,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor] | None]:\n    \"\"\"\n    Checks the pred and target types and computes the loss. If device type is cuda, loss computed in mixed\n    precision.\n\n    Args:\n        preds (TorchPredType): Dictionary of model output tensors indexed by name\n        features (dict[str, torch.Tensor]): Not used by this subclass\n        target (TorchTargetType): The targets to evaluate the predictions with. If multiple prediction tensors\n            are given, target must be a dictionary with the same number of tensors\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor] | None]): A tuple where the first element is the loss and the\n            second element is an optional additional loss\n    \"\"\"\n    # If deep supervision is turned on we must convert loss and target dicts into lists\n    loss_preds = prepare_loss_arg(preds)\n    loss_targets = prepare_loss_arg(target)\n\n    # Ensure we have the same number of predictions and targets\n    assert isinstance(loss_preds, type(loss_targets)), (\n        f\"Got unexpected types for preds and targets: {type(loss_preds)} and {type(loss_targets)}\"\n    )\n\n    if isinstance(loss_preds, list):\n        assert len(loss_preds) == len(loss_targets), (\n            \"Was expecting the number of predictions and targets to be the same. \"\n            f\"Got {len(loss_preds)} predictions and {len(loss_targets)} targets.\"\n        )\n\n    # If device type is cuda, nnUNet defaults to compute loss in mixed precision\n    if self.device.type == \"cuda\":\n        with torch.autocast(self.device.type, enabled=True):\n            loss = self.criterion(loss_preds, loss_targets), None\n    else:\n        loss = self.criterion(loss_preds, loss_targets), None\n\n    return loss\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.mask_data","title":"<code>mask_data(pred, target)</code>","text":"<p>Masks the pred and target tensors according to nnunet <code>ignore_label</code>. The number of classes in the input tensors should be at least 3 corresponding to 2 classes for binary segmentation and 1 class which is the ignore class specified by ignore label.</p> <p>See nnunet documentation for more info:</p> <p>https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/ignore_label.md</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>The one hot encoded predicted segmentation maps with shape <code>(batch, classes, x, y(, z))</code></p> required <code>target</code> <code>Tensor</code> <p>The ground truth segmentation map with shape <code>(batch, classes, x, y(, z))</code></p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple of:</p> <ul> <li>torch.Tensor: The masked one hot encoded predicted segmentation maps.</li> <li>torch.Tensor: The masked target segmentation maps.</li> </ul> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def mask_data(self, pred: torch.Tensor, target: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Masks the pred and target tensors according to nnunet ``ignore_label``. The number of classes in the input\n    tensors should be at least 3 corresponding to 2 classes for binary segmentation and 1 class which is\n    the ignore class specified by ignore label.\n\n    See nnunet documentation for more info:\n\n    https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/ignore_label.md\n\n    Args:\n        pred (torch.Tensor): The one hot encoded predicted segmentation maps with shape\n            ``(batch, classes, x, y(, z))``\n        target (torch.Tensor): The ground truth segmentation map with shape ``(batch, classes, x, y(, z))``\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Tuple of:\n\n            - torch.Tensor: The masked one hot encoded predicted segmentation maps.\n            - torch.Tensor: The masked target segmentation maps.\n    \"\"\"\n    # create mask where 1 is where pixels in target are not ignore label\n    # Modify target to remove the last class which is the ignore_label class\n    new_target = target\n    if self.nnunet_trainer.label_manager.has_regions:  # nnunet returns a ohe target if has_regions is true\n        # omit the last class, ie the ignore_label\n        mask = ~target[:, -1:] if target.dtype == torch.bool else 1 - target[:, -1:]\n        new_target = new_target[:, :-1]  # Remove final ignore_label class from target\n    else:  # target is not one hot encoded\n        mask = (target != self.nnunet_trainer.label_manager.ignore_label).float()\n        # Set ignore label to background essentially removing it as a class\n        new_target[new_target == self.nnunet_trainer.label_manager.ignore_label] = 0\n\n    # Tile the mask to be one hot encoded\n    mask_here = torch.tile(mask, (1, pred.shape[1], *[1 for _ in range(2, pred.ndim)]))\n\n    return (\n        pred * mask_here,\n        new_target,\n    )  # Mask the input tensor and return the modified target\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.update_metric_manager","title":"<code>update_metric_manager(preds, target, metric_manager)</code>","text":"<p>Update the metrics with preds and target. Overridden because we might need to manipulate inputs due to deep supervision.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>dictionary of model outputs.</p> required <code>target</code> <code>TorchTargetType</code> <p>the targets generated by the dataloader to evaluate the preds with.</p> required <code>metric_manager</code> <code>MetricManager</code> <p>the metric manager to update.</p> required Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def update_metric_manager(\n    self,\n    preds: TorchPredType,\n    target: TorchTargetType,\n    metric_manager: MetricManager,\n) -&gt; None:\n    \"\"\"\n    Update the metrics with preds and target. Overridden because we might need to manipulate inputs due to deep\n    supervision.\n\n    Args:\n        preds (TorchPredType): dictionary of model outputs.\n        target (TorchTargetType): the targets generated by the dataloader to evaluate the preds with.\n        metric_manager (MetricManager): the metric manager to update.\n    \"\"\"\n    if len(preds) &gt; 1:\n        # for nnunet the first pred in the output list is the main one\n        m_pred = convert_deep_supervision_dict_to_list(preds)[0]\n\n    if isinstance(target, torch.Tensor):\n        m_target = target\n    elif isinstance(target, dict):\n        if len(target) &gt; 1:\n            # If deep supervision is in use, we drop the additional targets\n            # when calculating the metrics as we only care about the\n            # original target which by default in nnunet is at index 0\n            m_target = convert_deep_supervision_dict_to_list(target)[0]\n        else:\n            m_target = list(target.values())[0]\n    else:\n        raise TypeError(\"Was expecting target to be type dict[str, torch.Tensor] or torch.Tensor\")\n\n    # Check if target is one hot encoded. Prediction always is for nnunet\n    # Add channel dimension if there isn't one\n    if m_pred.ndim != m_target.ndim:\n        m_target = m_target.view(m_target.shape[0], 1, *m_target.shape[1:])\n\n    # One hot encode targets if needed\n    if m_pred.shape != m_target.shape:\n        m_target_one_hot = torch.zeros(m_pred.shape, device=self.device, dtype=torch.bool)\n        # This is how nnunet does ohe in their functions\n        # Its a weird function that is not intuitive\n        # CAREFUL: Notice the underscore at the end of the scatter function.\n        # It makes a difference, was a hard bug to find!\n        m_target_one_hot.scatter_(1, m_target.long(), 1)\n    else:\n        m_target_one_hot = m_target\n\n    # Check if ignore label is in use. The nnunet loss figures this out on\n    # it's own, but we do it manually here for the metrics\n    if self.nnunet_trainer.label_manager.ignore_label is not None:\n        m_pred, m_target_one_hot = self.mask_data(m_pred, m_target_one_hot)\n\n    # m_pred is one hot encoded (OHE) output logits. Maybe masked by ignore label\n    # m_target_one_hot is OHE boolean label. Maybe masked by ignore label\n    metric_manager.update({\"prediction\": m_pred}, m_target_one_hot)\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.empty_cache","title":"<code>empty_cache()</code>","text":"<p>Checks torch device and empties cache before training to optimize VRAM usage.</p> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def empty_cache(self) -&gt; None:\n    \"\"\"Checks torch device and empties cache before training to optimize VRAM usage.\"\"\"\n    if self.device.type == \"cuda\":\n        torch.cuda.empty_cache()\n    elif self.device.type == \"mps\":\n        torch.mps.empty_cache()\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.get_properties","title":"<code>get_properties(config)</code>","text":"<p>Return properties (sample counts and nnunet plans) of client.</p> <p>If nnunet plans are not provided by the server, creates a new set of nnunet plans from the local client dataset. These plans are intended to be used for initializing global nnunet plans when they are not provided.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server</p> required <p>Returns:</p> Type Description <code>dict[str, Scalar]</code> <p>A dictionary containing the train and validation sample counts as well as the serialized nnunet plans</p> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>@use_default_signal_handlers  # Experiment planner spawns a process I think\ndef get_properties(self, config: Config) -&gt; dict[str, Scalar]:\n    \"\"\"\n    Return properties (sample counts and nnunet plans) of client.\n\n    If nnunet plans are not provided by the server, creates a new set of nnunet plans from the local client\n    dataset. These plans are intended to be used for initializing global nnunet plans when they are not\n    provided.\n\n    Args:\n        config (Config): The config from the server\n\n    Returns:\n        (dict[str, Scalar]): A dictionary containing the train and validation sample counts as well as the\n            serialized nnunet plans\n    \"\"\"\n    # Check if nnunet plans have already been initialized\n    if \"nnunet_plans\" not in config:\n        log(INFO, \"Initializing the global plans using local dataset\")\n        # Local client will initialize global nnunet plans\n        # Check if local nnunet dataset fingerprint needs to be extracted\n        self.maybe_extract_fingerprint()\n\n        # Create experiment planner and plans.\n        # Plans name must be temp_plans so that we can safely delete the generated plans file\n        planner = ExperimentPlanner(dataset_name_or_id=self.dataset_id, plans_name=\"temp_plans\")\n\n        # Unless log level is DEBUG or lower, hide nnunet output\n        with redirect_stdout(self.stream2debug):\n            plans = planner.plan_experiment()\n\n        # Set plans name to local dataset so we know the source\n        plans[\"plans_name\"] = self.dataset_name + \"_plans\"\n        plans_bytes = pickle.dumps(plans)\n\n        # Remove plans file . A new one will be generated in self.setup_client\n        plans_path = join(nnUNet_preprocessed, self.dataset_name, planner.plans_identifier + \".json\")\n        if exists(plans_path):\n            os.remove(plans_path)\n\n        # Update local config with plans\n        config[\"nnunet_plans\"] = plans_bytes\n\n    # Get client properties. We are now sure that config contains plans\n    properties = super().get_properties(config)\n    properties[\"nnunet_plans\"] = config[\"nnunet_plans\"]\n\n    # super.get_properties should setup the client anyways, but we can add a check here as a precaution.\n    if not self.initialized:\n        self.setup_client(config)  # Client must be setup in order to initialize nnunet_trainer\n\n    # Add additional properties from nnunet trainer to properties dict. We may want to add more keys later\n    properties[\"num_input_channels\"] = self.nnunet_trainer.num_input_channels\n    properties[\"num_segmentation_heads\"] = self.nnunet_trainer.label_manager.num_segmentation_heads\n    properties[\"enable_deep_supervision\"] = self.nnunet_trainer.enable_deep_supervision\n\n    return properties\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.shutdown_dataloader","title":"<code>shutdown_dataloader(dataloader, dl_name=None)</code>","text":"<p>The nnunet dataloader/augmenter uses multiprocessing under the hood, so the shutdown method terminates the child processes gracefully.</p> <p>Parameters:</p> Name Type Description Default <code>dataloader</code> <code>DataLoader</code> <p>The dataloader to shutdown</p> required <code>dl_name</code> <code>str | None</code> <p>A string that identifies the dataloader to shutdown. Used for logging purposes. Defaults to None</p> <code>None</code> Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def shutdown_dataloader(self, dataloader: DataLoader | None, dl_name: str | None = None) -&gt; None:\n    \"\"\"\n    The nnunet dataloader/augmenter uses multiprocessing under the hood, so the shutdown method terminates the\n    child processes gracefully.\n\n    Args:\n        dataloader (DataLoader): The dataloader to shutdown\n        dl_name (str | None): A string that identifies the dataloader to shutdown. Used for logging purposes.\n            Defaults to None\n    \"\"\"\n    if dataloader is not None and isinstance(dataloader, NnUNetDataLoaderWrapper):\n        if self.verbose:\n            log(INFO, f\"\\tShutting down nnunet dataloader: {dl_name}\")\n        dataloader.shutdown()\n\n    del dataloader\n</code></pre>"},{"location":"api/#fl4health.clients.nnunet_client.NnunetClient.transform_gradients","title":"<code>transform_gradients(losses)</code>","text":"<p>Apply the gradient clipping performed by the default nnunet trainer. This is the default behavior for nnunet 2.5.1.</p> <p>Parameters:</p> Name Type Description Default <code>losses</code> <code>TrainingLosses</code> <p>Not used for this transformation.</p> required Source code in <code>fl4health/clients/nnunet_client.py</code> <pre><code>def transform_gradients(self, losses: TrainingLosses) -&gt; None:\n    \"\"\"\n    Apply the gradient clipping performed by the default nnunet trainer. This is the default behavior for\n    nnunet 2.5.1.\n\n    Args:\n        losses (TrainingLosses): Not used for this transformation.\n    \"\"\"\n    nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n</code></pre>"},{"location":"api/#fl4health.clients.partial_weight_exchange_client","title":"<code>partial_weight_exchange_client</code>","text":""},{"location":"api/#fl4health.clients.partial_weight_exchange_client.PartialWeightExchangeClient","title":"<code>PartialWeightExchangeClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/partial_weight_exchange_client.py</code> <pre><code>class PartialWeightExchangeClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n        store_initial_model: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Client that only exchanges a subset of its parameters with the server in each communication round.\n\n        The strategy for selecting which parameters to exchange is determined by ``self.parameter_exchanger``,\n        which must be a subclass of ``PartialParameterExchanger``.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            store_initial_model (bool): Indicates whether the client should store a copy of the model weights\n                at the beginning of each training round. The model copy might be required to select the subset\n                of model parameters to be exchanged with the server, depending on the selection criterion used.\n                Defaults to False.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        # Initial model parameters to be used in selecting parameters to be exchanged during training.\n        self.initial_model: nn.Module | None\n        # Parameter exchanger to be used in server-client exchange of dynamic layers.\n        self.parameter_exchanger: PartialParameterExchanger\n        self.store_initial_model = store_initial_model\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Setup the components of the client necessary for client-side training and parameter exchange. Mostly handled\n        by a call to the basic client flow, but also sets up the initial model to facilitate storage of initial\n        parameters during training.\n\n        Args:\n            config (Config): Configuration used to setup the client properly.\n        \"\"\"\n        super().setup_client(config)\n        if self.store_initial_model:\n            self.initial_model = copy.deepcopy(self.model).to(self.device)\n        else:\n            self.initial_model = None\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        \"\"\"\n        This method configures and instantiates a ``PartialParameterExchanger`` and should be implemented by the user\n        since there are various strategies to select parameters to be exchanged.\n\n        Args:\n            config (Config): Configuration used to setup the weight exchanger properties for dynamic exchange\n\n        Returns:\n            (ParameterExchanger): This exchanger handles the exchange orchestration between clients and server during\n                federated training\n        \"\"\"\n        raise NotImplementedError\n\n    def get_parameters(self, config: Config) -&gt; NDArrays:\n        \"\"\"\n        Determines which weights are sent back to the server for aggregation. This uses a parameter exchanger to\n        determine parameters sent. Note that this overrides the basic client ``get_parameters`` function to send the\n        initial model so that starting weights may be extracted and compared to current weights after local\n        training.\n\n        Args:\n            config (Config): configuration used to setup the exchange\n\n        Returns:\n            (NDArrays): The list of weights to be sent to the server from the client\n        \"\"\"\n        if not self.initialized:\n            return self.setup_client_and_return_all_model_parameters(config)\n\n        assert self.model is not None and self.parameter_exchanger is not None\n        return self.parameter_exchanger.push_parameters(self.model, self.initial_model, config=config)\n\n    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n        \"\"\"\n        Sets the local model parameters transferred from the server using a parameter exchanger to coordinate how\n        parameters are set.\n\n        In the first fitting round, we assume the full model is being initialized and use the\n        ``FullParameterExchanger()`` to set all model weights.\n\n        In other times, this approach uses a partial weight exchanger to set model weights.\n\n        Args:\n            parameters (NDArrays): parameters is the set of weights and their corresponding model component names,\n                corresponding to the state dict names. These are woven together in the ``NDArrays`` object. These are\n                unwound properly by the parameter exchanger.\n            config (Config): configuration if required to control parameter exchange.\n            fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n                round or an evaluation round. This is used to help determine which parameter exchange should be used\n                for pulling parameters. A full parameter exchanger is only used if the current federated learning\n                round is the very first fitting round. Otherwise, use a ``PartialParameterExchanger``.\n        \"\"\"\n        super().set_parameters(parameters, config, fitting_round)\n        if self.store_initial_model:\n            assert self.initial_model is not None\n            # Stores the values of the new model parameters at the beginning of each client training round.\n            self.initial_model.load_state_dict(self.model.state_dict(), strict=True)\n</code></pre>"},{"location":"api/#fl4health.clients.partial_weight_exchange_client.PartialWeightExchangeClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None, store_initial_model=False)</code>","text":"<p>Client that only exchanges a subset of its parameters with the server in each communication round.</p> <p>The strategy for selecting which parameters to exchange is determined by <code>self.parameter_exchanger</code>, which must be a subclass of <code>PartialParameterExchanger</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>store_initial_model</code> <code>bool</code> <p>Indicates whether the client should store a copy of the model weights at the beginning of each training round. The model copy might be required to select the subset of model parameters to be exchanged with the server, depending on the selection criterion used. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/clients/partial_weight_exchange_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n    store_initial_model: bool = False,\n) -&gt; None:\n    \"\"\"\n    Client that only exchanges a subset of its parameters with the server in each communication round.\n\n    The strategy for selecting which parameters to exchange is determined by ``self.parameter_exchanger``,\n    which must be a subclass of ``PartialParameterExchanger``.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        store_initial_model (bool): Indicates whether the client should store a copy of the model weights\n            at the beginning of each training round. The model copy might be required to select the subset\n            of model parameters to be exchanged with the server, depending on the selection criterion used.\n            Defaults to False.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    # Initial model parameters to be used in selecting parameters to be exchanged during training.\n    self.initial_model: nn.Module | None\n    # Parameter exchanger to be used in server-client exchange of dynamic layers.\n    self.parameter_exchanger: PartialParameterExchanger\n    self.store_initial_model = store_initial_model\n</code></pre>"},{"location":"api/#fl4health.clients.partial_weight_exchange_client.PartialWeightExchangeClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Setup the components of the client necessary for client-side training and parameter exchange. Mostly handled by a call to the basic client flow, but also sets up the initial model to facilitate storage of initial parameters during training.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration used to setup the client properly.</p> required Source code in <code>fl4health/clients/partial_weight_exchange_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Setup the components of the client necessary for client-side training and parameter exchange. Mostly handled\n    by a call to the basic client flow, but also sets up the initial model to facilitate storage of initial\n    parameters during training.\n\n    Args:\n        config (Config): Configuration used to setup the client properly.\n    \"\"\"\n    super().setup_client(config)\n    if self.store_initial_model:\n        self.initial_model = copy.deepcopy(self.model).to(self.device)\n    else:\n        self.initial_model = None\n</code></pre>"},{"location":"api/#fl4health.clients.partial_weight_exchange_client.PartialWeightExchangeClient.get_parameter_exchanger","title":"<code>get_parameter_exchanger(config)</code>","text":"<p>This method configures and instantiates a <code>PartialParameterExchanger</code> and should be implemented by the user since there are various strategies to select parameters to be exchanged.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration used to setup the weight exchanger properties for dynamic exchange</p> required <p>Returns:</p> Type Description <code>ParameterExchanger</code> <p>This exchanger handles the exchange orchestration between clients and server during federated training</p> Source code in <code>fl4health/clients/partial_weight_exchange_client.py</code> <pre><code>def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n    \"\"\"\n    This method configures and instantiates a ``PartialParameterExchanger`` and should be implemented by the user\n    since there are various strategies to select parameters to be exchanged.\n\n    Args:\n        config (Config): Configuration used to setup the weight exchanger properties for dynamic exchange\n\n    Returns:\n        (ParameterExchanger): This exchanger handles the exchange orchestration between clients and server during\n            federated training\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.partial_weight_exchange_client.PartialWeightExchangeClient.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>Determines which weights are sent back to the server for aggregation. This uses a parameter exchanger to determine parameters sent. Note that this overrides the basic client <code>get_parameters</code> function to send the initial model so that starting weights may be extracted and compared to current weights after local training.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>configuration used to setup the exchange</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>The list of weights to be sent to the server from the client</p> Source code in <code>fl4health/clients/partial_weight_exchange_client.py</code> <pre><code>def get_parameters(self, config: Config) -&gt; NDArrays:\n    \"\"\"\n    Determines which weights are sent back to the server for aggregation. This uses a parameter exchanger to\n    determine parameters sent. Note that this overrides the basic client ``get_parameters`` function to send the\n    initial model so that starting weights may be extracted and compared to current weights after local\n    training.\n\n    Args:\n        config (Config): configuration used to setup the exchange\n\n    Returns:\n        (NDArrays): The list of weights to be sent to the server from the client\n    \"\"\"\n    if not self.initialized:\n        return self.setup_client_and_return_all_model_parameters(config)\n\n    assert self.model is not None and self.parameter_exchanger is not None\n    return self.parameter_exchanger.push_parameters(self.model, self.initial_model, config=config)\n</code></pre>"},{"location":"api/#fl4health.clients.partial_weight_exchange_client.PartialWeightExchangeClient.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>Sets the local model parameters transferred from the server using a parameter exchanger to coordinate how parameters are set.</p> <p>In the first fitting round, we assume the full model is being initialized and use the <code>FullParameterExchanger()</code> to set all model weights.</p> <p>In other times, this approach uses a partial weight exchanger to set model weights.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>parameters is the set of weights and their corresponding model component names, corresponding to the state dict names. These are woven together in the <code>NDArrays</code> object. These are unwound properly by the parameter exchanger.</p> required <code>config</code> <code>Config</code> <p>configuration if required to control parameter exchange.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. This is used to help determine which parameter exchange should be used for pulling parameters. A full parameter exchanger is only used if the current federated learning round is the very first fitting round. Otherwise, use a <code>PartialParameterExchanger</code>.</p> required Source code in <code>fl4health/clients/partial_weight_exchange_client.py</code> <pre><code>def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n    \"\"\"\n    Sets the local model parameters transferred from the server using a parameter exchanger to coordinate how\n    parameters are set.\n\n    In the first fitting round, we assume the full model is being initialized and use the\n    ``FullParameterExchanger()`` to set all model weights.\n\n    In other times, this approach uses a partial weight exchanger to set model weights.\n\n    Args:\n        parameters (NDArrays): parameters is the set of weights and their corresponding model component names,\n            corresponding to the state dict names. These are woven together in the ``NDArrays`` object. These are\n            unwound properly by the parameter exchanger.\n        config (Config): configuration if required to control parameter exchange.\n        fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n            round or an evaluation round. This is used to help determine which parameter exchange should be used\n            for pulling parameters. A full parameter exchanger is only used if the current federated learning\n            round is the very first fitting round. Otherwise, use a ``PartialParameterExchanger``.\n    \"\"\"\n    super().set_parameters(parameters, config, fitting_round)\n    if self.store_initial_model:\n        assert self.initial_model is not None\n        # Stores the values of the new model parameters at the beginning of each client training round.\n        self.initial_model.load_state_dict(self.model.state_dict(), strict=True)\n</code></pre>"},{"location":"api/#fl4health.clients.perfcl_client","title":"<code>perfcl_client</code>","text":""},{"location":"api/#fl4health.clients.perfcl_client.PerFclClient","title":"<code>PerFclClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/perfcl_client.py</code> <pre><code>class PerFclClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n        global_feature_loss_temperature: float = 0.5,\n        local_feature_loss_temperature: float = 0.5,\n        global_feature_contrastive_loss_weight: float = 1.0,\n        local_feature_contrastive_loss_weight: float = 1.0,\n    ) -&gt; None:\n        \"\"\"\n        This client is used to perform client-side training associated with the PerFCL method derived in\n        https://www.sciencedirect.com/science/article/pii/S0031320323002078.\n\n        The approach attempts to manipulate the training dynamics of a parallel weight split model with a global\n        feature extractor, that is aggregated on the server-side with FedAvg and a local feature extractor that is\n        only locally trained. This method is related to FENDA, but with additional losses on the latent spaces of the\n        local and global feature extractors.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n            global_feature_loss_temperature (float, optional): Temperature to be used in the contrastive loss\n                associated with constraining the global feature extractor in the PerFCL loss. Defaults to 0.5.\n            local_feature_loss_temperature (float, optional): Temperature to be used in the contrastive loss\n                associated with constraining the local feature extractor in the PerFCL loss. Defaults to 0.5.\n            global_feature_contrastive_loss_weight (float, optional): Weight on the contrastive loss value associated\n                with the global feature extractor. **REFERRED TO AS MU** in the original paper. Defaults to 1.0.\n            local_feature_contrastive_loss_weight (float, optional): Weight on the contrastive loss value associated\n                with the local feature extractor. **REFERRED TO AS GAMMA** in the original paper.  Defaults to 1.0.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.global_feature_contrastive_loss_weight = global_feature_contrastive_loss_weight\n        self.local_feature_contrastive_loss_weight = local_feature_contrastive_loss_weight\n        self.perfcl_loss_function = PerFclLoss(\n            self.device, global_feature_loss_temperature, local_feature_loss_temperature\n        )\n\n        # In order to compute the PerFCL losses, we need to save final local module and global modules from the\n        # previous iteration of client-side training and initial global module passed to the client after server-side\n        # aggregation at each communication round\n        self.old_local_module: torch.nn.Module | None = None\n        self.old_global_module: torch.nn.Module | None = None\n        self.initial_global_module: torch.nn.Module | None = None\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        \"\"\"\n        Sets the parameter exchanger to be used by the clients to send parameters to and receive them from the server\n        For PerFCL clients, a ``FixedLayerExchanger`` is used by default. We also required that the model being\n        exchanged is of the ``PerFclModel`` type to ensure that the appropriate layers are exchanged.\n\n        Args:\n            config (Config): Configuration provided by the server.\n\n        Returns:\n            (ParameterExchanger): ``FixedLayerExchanger`` meant to only exchange a subset of model layers with the\n                server for aggregation.\n        \"\"\"\n        assert isinstance(self.model, PerFclModel)\n        return FixedLayerExchanger(self.model.layers_to_exchange())\n\n    def _flatten(self, features: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Flatten the provided features ASSUMING they are provided in batch-first format.\n\n        Args:\n            features (torch.Tensor): features to be flattened\n\n        Returns:\n            (torch.Tensor): flattened feature vectors of shape (batch, -1)\n        \"\"\"\n        return features.reshape(len(features), -1)\n\n    def _all_contrastive_loss_modules_defined(self) -&gt; bool:\n        \"\"\"\n        Checks whether all of the components required to compute the PerFCL features and loss function are defined.\n        There are instances where some are defined but not others. For example, in the very first round of training\n        The initial_global_module will have been defined before training starts, but the old_local_module and\n        old_global_module components will not have been.\n\n        Returns:\n            (bool): Indicates True if all of the modules are not None\n        \"\"\"\n        return (\n            self.old_local_module is not None\n            and self.old_global_module is not None\n            and self.initial_global_module is not None\n        )\n\n    def predict(self, input: TorchInputType) -&gt; tuple[TorchPredType, TorchFeatureType]:\n        \"\"\"\n        Computes the prediction(s) and features of the model(s) given the input.\n\n        Args:\n            input (TorchInputType): Inputs to be fed into the model. ``TorchInputType`` is simply an alias\n                for the union of ``torch.Tensor`` and ``dict[str, torch.Tensor]``.\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple in which the first element\n                contains predictions indexed by name and the second element contains intermediate activations\n                index by name. Specifically the features of the model, features of the global model and features of\n                the old model are returned. All predictions included in dictionary will be used to compute metrics.\n        \"\"\"\n        # For PerFCL models, we required the input to simply be a torch.Tensor\n        assert isinstance(input, torch.Tensor)\n        preds, features = self.model(input)\n        # In the first server round, these module will not have been set.\n        if (\n            self.old_local_module is not None\n            and self.old_global_module is not None\n            and self.initial_global_module is not None\n        ):\n            # Pass the input through the old feature extractors and the initial global model after aggregation and\n            # flatten them\n            features[\"old_local_features\"] = self._flatten(self.old_local_module.forward(input))\n            features[\"old_global_features\"] = self._flatten(self.old_global_module.forward(input))\n            features[\"initial_global_features\"] = self._flatten(self.initial_global_module.forward(input))\n\n        return preds, features\n\n    def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n        \"\"\"\n        This function is called after client-side training concludes. In this case, it is used to save the local\n        and global feature extraction weights/modules to be used in the next round of client-side training.\n\n        Args:\n            local_steps (int): Number of steps performed during training.\n            loss_dict (dict[str, float]): Losses computed during training.\n            config (Config): The config from the server\n        \"\"\"\n        assert isinstance(self.model, PerFclModel)\n        # First module is the local feature extractor for PerFcl Models\n        self.old_local_module = clone_and_freeze_model(self.model.first_feature_extractor)\n        # Second module is the global feature extractor for PerFcl Models\n        self.old_global_module = clone_and_freeze_model(self.model.second_feature_extractor)\n\n        super().update_after_train(local_steps, loss_dict, config)\n\n    def update_before_train(self, current_server_round: int) -&gt; None:\n        \"\"\"\n        This function is called prior to the start of client-side training, but after the server parameters have be\n        received and injected into the model. In this case, it is used to save the aggregated global feature extractor\n        weights/module representing the initial state of this module **BEFORE** this iteration of client-side training\n        but **AFTER** server-side aggregation.\n\n        Args:\n            current_server_round (int): Current server round being performed.\n        \"\"\"\n        # Save the parameters of the aggregated global model\n        assert isinstance(self.model, PerFclModel)\n        self.initial_global_module = clone_and_freeze_model(self.model.second_feature_extractor)\n\n        super().update_before_train(current_server_round)\n\n    def compute_loss_and_additional_losses(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n        \"\"\"\n        Computes the loss and any additional losses given predictions of the model and ground truth data.\n        For PerFCL, the total loss is the standard criterion loss provided by the user and the PerFCL contrastive\n        losses aimed at manipulating the local and global feature extractor latent spaces.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n                - The tensor for the total loss\n                - A dictionary with ``loss``, ``total_loss``, ``global_feature_contrastive_loss``, and\n                  ``local_feature_contrastive_loss`` representing the various and relevant pieces of the loss\n                  calculations\n        \"\"\"\n        loss = self.criterion(preds[\"prediction\"], target)\n        # If any of these are None then we don't compute the PerFCL loss. This will happen on the first client-side\n        # training run.\n        if self.old_local_module is None or self.old_global_module is None or self.initial_global_module is None:\n            return loss, {\"loss\": loss}\n\n        total_loss = loss.clone()\n        global_feature_contrastive_loss, local_feature_contrastive_loss = self.perfcl_loss_function(\n            features[\"local_features\"],\n            features[\"old_local_features\"],\n            features[\"global_features\"],\n            features[\"old_global_features\"],\n            features[\"initial_global_features\"],\n        )\n\n        total_loss += (\n            self.global_feature_contrastive_loss_weight * global_feature_contrastive_loss\n            + self.local_feature_contrastive_loss_weight * local_feature_contrastive_loss\n        )\n\n        additional_losses = {\n            \"loss\": loss,\n            \"global_feature_contrastive_loss\": global_feature_contrastive_loss,\n            \"local_feature_contrastive_loss\": local_feature_contrastive_loss,\n            \"total_loss\": total_loss,\n        }\n\n        return total_loss, additional_losses\n\n    def compute_evaluation_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses:\n        \"\"\"\n        Computes evaluation loss given predictions of the model and ground truth data. Also computes\n        additional loss components associated with the PerFCL loss function.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included\n                in dictionary will be used to compute metrics.\n            features (dict[str, torch.Tensor]): Feature(s) of the model(s) indexed by name.\n            target (torch.Tensor): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n                indexed by name.\n        \"\"\"\n        _, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n        return EvaluationLosses(checkpoint=additional_losses[\"loss\"], additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.perfcl_client.PerFclClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None, global_feature_loss_temperature=0.5, local_feature_loss_temperature=0.5, global_feature_contrastive_loss_weight=1.0, local_feature_contrastive_loss_weight=1.0)</code>","text":"<p>This client is used to perform client-side training associated with the PerFCL method derived in https://www.sciencedirect.com/science/article/pii/S0031320323002078.</p> <p>The approach attempts to manipulate the training dynamics of a parallel weight split model with a global feature extractor, that is aggregated on the server-side with FedAvg and a local feature extractor that is only locally trained. This method is related to FENDA, but with additional losses on the latent spaces of the local and global feature extractors.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> <code>global_feature_loss_temperature</code> <code>float</code> <p>Temperature to be used in the contrastive loss associated with constraining the global feature extractor in the PerFCL loss. Defaults to 0.5.</p> <code>0.5</code> <code>local_feature_loss_temperature</code> <code>float</code> <p>Temperature to be used in the contrastive loss associated with constraining the local feature extractor in the PerFCL loss. Defaults to 0.5.</p> <code>0.5</code> <code>global_feature_contrastive_loss_weight</code> <code>float</code> <p>Weight on the contrastive loss value associated with the global feature extractor. REFERRED TO AS MU in the original paper. Defaults to 1.0.</p> <code>1.0</code> <code>local_feature_contrastive_loss_weight</code> <code>float</code> <p>Weight on the contrastive loss value associated with the local feature extractor. REFERRED TO AS GAMMA in the original paper.  Defaults to 1.0.</p> <code>1.0</code> Source code in <code>fl4health/clients/perfcl_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n    global_feature_loss_temperature: float = 0.5,\n    local_feature_loss_temperature: float = 0.5,\n    global_feature_contrastive_loss_weight: float = 1.0,\n    local_feature_contrastive_loss_weight: float = 1.0,\n) -&gt; None:\n    \"\"\"\n    This client is used to perform client-side training associated with the PerFCL method derived in\n    https://www.sciencedirect.com/science/article/pii/S0031320323002078.\n\n    The approach attempts to manipulate the training dynamics of a parallel weight split model with a global\n    feature extractor, that is aggregated on the server-side with FedAvg and a local feature extractor that is\n    only locally trained. This method is related to FENDA, but with additional losses on the latent spaces of the\n    local and global feature extractors.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n        global_feature_loss_temperature (float, optional): Temperature to be used in the contrastive loss\n            associated with constraining the global feature extractor in the PerFCL loss. Defaults to 0.5.\n        local_feature_loss_temperature (float, optional): Temperature to be used in the contrastive loss\n            associated with constraining the local feature extractor in the PerFCL loss. Defaults to 0.5.\n        global_feature_contrastive_loss_weight (float, optional): Weight on the contrastive loss value associated\n            with the global feature extractor. **REFERRED TO AS MU** in the original paper. Defaults to 1.0.\n        local_feature_contrastive_loss_weight (float, optional): Weight on the contrastive loss value associated\n            with the local feature extractor. **REFERRED TO AS GAMMA** in the original paper.  Defaults to 1.0.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.global_feature_contrastive_loss_weight = global_feature_contrastive_loss_weight\n    self.local_feature_contrastive_loss_weight = local_feature_contrastive_loss_weight\n    self.perfcl_loss_function = PerFclLoss(\n        self.device, global_feature_loss_temperature, local_feature_loss_temperature\n    )\n\n    # In order to compute the PerFCL losses, we need to save final local module and global modules from the\n    # previous iteration of client-side training and initial global module passed to the client after server-side\n    # aggregation at each communication round\n    self.old_local_module: torch.nn.Module | None = None\n    self.old_global_module: torch.nn.Module | None = None\n    self.initial_global_module: torch.nn.Module | None = None\n</code></pre>"},{"location":"api/#fl4health.clients.perfcl_client.PerFclClient.get_parameter_exchanger","title":"<code>get_parameter_exchanger(config)</code>","text":"<p>Sets the parameter exchanger to be used by the clients to send parameters to and receive them from the server For PerFCL clients, a <code>FixedLayerExchanger</code> is used by default. We also required that the model being exchanged is of the <code>PerFclModel</code> type to ensure that the appropriate layers are exchanged.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration provided by the server.</p> required <p>Returns:</p> Type Description <code>ParameterExchanger</code> <p><code>FixedLayerExchanger</code> meant to only exchange a subset of model layers with the server for aggregation.</p> Source code in <code>fl4health/clients/perfcl_client.py</code> <pre><code>def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n    \"\"\"\n    Sets the parameter exchanger to be used by the clients to send parameters to and receive them from the server\n    For PerFCL clients, a ``FixedLayerExchanger`` is used by default. We also required that the model being\n    exchanged is of the ``PerFclModel`` type to ensure that the appropriate layers are exchanged.\n\n    Args:\n        config (Config): Configuration provided by the server.\n\n    Returns:\n        (ParameterExchanger): ``FixedLayerExchanger`` meant to only exchange a subset of model layers with the\n            server for aggregation.\n    \"\"\"\n    assert isinstance(self.model, PerFclModel)\n    return FixedLayerExchanger(self.model.layers_to_exchange())\n</code></pre>"},{"location":"api/#fl4health.clients.perfcl_client.PerFclClient.predict","title":"<code>predict(input)</code>","text":"<p>Computes the prediction(s) and features of the model(s) given the input.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Inputs to be fed into the model. <code>TorchInputType</code> is simply an alias for the union of <code>torch.Tensor</code> and <code>dict[str, torch.Tensor]</code>.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>A tuple in which the first element contains predictions indexed by name and the second element contains intermediate activations index by name. Specifically the features of the model, features of the global model and features of the old model are returned. All predictions included in dictionary will be used to compute metrics.</p> Source code in <code>fl4health/clients/perfcl_client.py</code> <pre><code>def predict(self, input: TorchInputType) -&gt; tuple[TorchPredType, TorchFeatureType]:\n    \"\"\"\n    Computes the prediction(s) and features of the model(s) given the input.\n\n    Args:\n        input (TorchInputType): Inputs to be fed into the model. ``TorchInputType`` is simply an alias\n            for the union of ``torch.Tensor`` and ``dict[str, torch.Tensor]``.\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple in which the first element\n            contains predictions indexed by name and the second element contains intermediate activations\n            index by name. Specifically the features of the model, features of the global model and features of\n            the old model are returned. All predictions included in dictionary will be used to compute metrics.\n    \"\"\"\n    # For PerFCL models, we required the input to simply be a torch.Tensor\n    assert isinstance(input, torch.Tensor)\n    preds, features = self.model(input)\n    # In the first server round, these module will not have been set.\n    if (\n        self.old_local_module is not None\n        and self.old_global_module is not None\n        and self.initial_global_module is not None\n    ):\n        # Pass the input through the old feature extractors and the initial global model after aggregation and\n        # flatten them\n        features[\"old_local_features\"] = self._flatten(self.old_local_module.forward(input))\n        features[\"old_global_features\"] = self._flatten(self.old_global_module.forward(input))\n        features[\"initial_global_features\"] = self._flatten(self.initial_global_module.forward(input))\n\n    return preds, features\n</code></pre>"},{"location":"api/#fl4health.clients.perfcl_client.PerFclClient.update_after_train","title":"<code>update_after_train(local_steps, loss_dict, config)</code>","text":"<p>This function is called after client-side training concludes. In this case, it is used to save the local and global feature extraction weights/modules to be used in the next round of client-side training.</p> <p>Parameters:</p> Name Type Description Default <code>local_steps</code> <code>int</code> <p>Number of steps performed during training.</p> required <code>loss_dict</code> <code>dict[str, float]</code> <p>Losses computed during training.</p> required <code>config</code> <code>Config</code> <p>The config from the server</p> required Source code in <code>fl4health/clients/perfcl_client.py</code> <pre><code>def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n    \"\"\"\n    This function is called after client-side training concludes. In this case, it is used to save the local\n    and global feature extraction weights/modules to be used in the next round of client-side training.\n\n    Args:\n        local_steps (int): Number of steps performed during training.\n        loss_dict (dict[str, float]): Losses computed during training.\n        config (Config): The config from the server\n    \"\"\"\n    assert isinstance(self.model, PerFclModel)\n    # First module is the local feature extractor for PerFcl Models\n    self.old_local_module = clone_and_freeze_model(self.model.first_feature_extractor)\n    # Second module is the global feature extractor for PerFcl Models\n    self.old_global_module = clone_and_freeze_model(self.model.second_feature_extractor)\n\n    super().update_after_train(local_steps, loss_dict, config)\n</code></pre>"},{"location":"api/#fl4health.clients.perfcl_client.PerFclClient.update_before_train","title":"<code>update_before_train(current_server_round)</code>","text":"<p>This function is called prior to the start of client-side training, but after the server parameters have be received and injected into the model. In this case, it is used to save the aggregated global feature extractor weights/module representing the initial state of this module BEFORE this iteration of client-side training but AFTER server-side aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>current_server_round</code> <code>int</code> <p>Current server round being performed.</p> required Source code in <code>fl4health/clients/perfcl_client.py</code> <pre><code>def update_before_train(self, current_server_round: int) -&gt; None:\n    \"\"\"\n    This function is called prior to the start of client-side training, but after the server parameters have be\n    received and injected into the model. In this case, it is used to save the aggregated global feature extractor\n    weights/module representing the initial state of this module **BEFORE** this iteration of client-side training\n    but **AFTER** server-side aggregation.\n\n    Args:\n        current_server_round (int): Current server round being performed.\n    \"\"\"\n    # Save the parameters of the aggregated global model\n    assert isinstance(self.model, PerFclModel)\n    self.initial_global_module = clone_and_freeze_model(self.model.second_feature_extractor)\n\n    super().update_before_train(current_server_round)\n</code></pre>"},{"location":"api/#fl4health.clients.perfcl_client.PerFclClient.compute_loss_and_additional_losses","title":"<code>compute_loss_and_additional_losses(preds, features, target)</code>","text":"<p>Computes the loss and any additional losses given predictions of the model and ground truth data. For PerFCL, the total loss is the standard criterion loss provided by the user and the PerFCL contrastive losses aimed at manipulating the local and global feature extractor latent spaces.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, dict[str, Tensor]]</code> <p>A tuple with:</p> <ul> <li>The tensor for the total loss</li> <li>A dictionary with <code>loss</code>, <code>total_loss</code>, <code>global_feature_contrastive_loss</code>, and   <code>local_feature_contrastive_loss</code> representing the various and relevant pieces of the loss   calculations</li> </ul> Source code in <code>fl4health/clients/perfcl_client.py</code> <pre><code>def compute_loss_and_additional_losses(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor]]:\n    \"\"\"\n    Computes the loss and any additional losses given predictions of the model and ground truth data.\n    For PerFCL, the total loss is the standard criterion loss provided by the user and the PerFCL contrastive\n    losses aimed at manipulating the local and global feature extractor latent spaces.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (tuple[torch.Tensor, dict[str, torch.Tensor]]): A tuple with:\n\n            - The tensor for the total loss\n            - A dictionary with ``loss``, ``total_loss``, ``global_feature_contrastive_loss``, and\n              ``local_feature_contrastive_loss`` representing the various and relevant pieces of the loss\n              calculations\n    \"\"\"\n    loss = self.criterion(preds[\"prediction\"], target)\n    # If any of these are None then we don't compute the PerFCL loss. This will happen on the first client-side\n    # training run.\n    if self.old_local_module is None or self.old_global_module is None or self.initial_global_module is None:\n        return loss, {\"loss\": loss}\n\n    total_loss = loss.clone()\n    global_feature_contrastive_loss, local_feature_contrastive_loss = self.perfcl_loss_function(\n        features[\"local_features\"],\n        features[\"old_local_features\"],\n        features[\"global_features\"],\n        features[\"old_global_features\"],\n        features[\"initial_global_features\"],\n    )\n\n    total_loss += (\n        self.global_feature_contrastive_loss_weight * global_feature_contrastive_loss\n        + self.local_feature_contrastive_loss_weight * local_feature_contrastive_loss\n    )\n\n    additional_losses = {\n        \"loss\": loss,\n        \"global_feature_contrastive_loss\": global_feature_contrastive_loss,\n        \"local_feature_contrastive_loss\": local_feature_contrastive_loss,\n        \"total_loss\": total_loss,\n    }\n\n    return total_loss, additional_losses\n</code></pre>"},{"location":"api/#fl4health.clients.perfcl_client.PerFclClient.compute_evaluation_loss","title":"<code>compute_evaluation_loss(preds, features, target)</code>","text":"<p>Computes evaluation loss given predictions of the model and ground truth data. Also computes additional loss components associated with the PerFCL loss function.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>dict[str, Tensor]</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>Tensor</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> containing checkpoint loss and additional losses indexed by name.</p> Source code in <code>fl4health/clients/perfcl_client.py</code> <pre><code>def compute_evaluation_loss(\n    self,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; EvaluationLosses:\n    \"\"\"\n    Computes evaluation loss given predictions of the model and ground truth data. Also computes\n    additional loss components associated with the PerFCL loss function.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. All predictions included\n            in dictionary will be used to compute metrics.\n        features (dict[str, torch.Tensor]): Feature(s) of the model(s) indexed by name.\n        target (torch.Tensor): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n            indexed by name.\n    \"\"\"\n    _, additional_losses = self.compute_loss_and_additional_losses(preds, features, target)\n    return EvaluationLosses(checkpoint=additional_losses[\"loss\"], additional_losses=additional_losses)\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client","title":"<code>scaffold_client</code>","text":""},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient","title":"<code>ScaffoldClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>class ScaffoldClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Federated Learning Client for Scaffold strategy.\n\n        Implementation based on https://arxiv.org/pdf/1910.06378.pdf.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.learning_rate: float  # eta_l in paper\n        self.client_control_variates: NDArrays | None = None  # c_i in paper\n        self.client_control_variates_updates: NDArrays | None = None  # delta_c_i in paper\n        self.server_control_variates: NDArrays | None = None  # c in paper\n        # Scaffold require vanilla SGD as optimizer, will assert during setup_client\n        self.optimizers: dict[str, torch.optim.Optimizer]\n\n        self.server_model_weights: NDArrays | None = None  # x in paper\n        self.parameter_exchanger: FullParameterExchangerWithPacking[NDArrays]\n\n    def get_parameters(self, config: Config) -&gt; NDArrays:\n        \"\"\"\n        Packs the parameters and control variates into a single ``NDArrays`` to be sent to the server for aggregation.\n\n        Args:\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n        Returns:\n            (NDArrays): Model parameters and control variates packed together.\n        \"\"\"\n        if not self.initialized:\n            return self.setup_client_and_return_all_model_parameters(config)\n\n        assert self.model is not None and self.parameter_exchanger is not None\n\n        model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n\n        # Weights and control variates updates sent to server for aggregation\n        # Control variates updates sent because only client has access to previous client control variate\n        # Therefore it can only be computed locally\n        assert self.client_control_variates_updates is not None\n        return self.parameter_exchanger.pack_parameters(model_weights, self.client_control_variates_updates)\n\n    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n        \"\"\"\n        Assumes that the parameters being passed contain model parameters concatenated with server control variates.\n        They are unpacked for the clients to use in training. If it's the first time the model is being initialized,\n        we assume the full model is being initialized and use the ``FullParameterExchanger()`` to set all model\n        weights.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model and also the server control variates (initial or after aggregation)\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Which fitting round (i.e. server round of fitting) that we're on.\n        \"\"\"\n        assert self.model is not None and self.parameter_exchanger is not None\n\n        server_model_state, server_control_variates = self.parameter_exchanger.unpack_parameters(parameters)\n        self.server_control_variates = server_control_variates\n\n        super().set_parameters(server_model_state, config, fitting_round)\n\n        # Note that we are restricting to weights that require a gradient here because they are used to compute\n        # control variates\n        self.server_model_weights = [\n            model_params.cpu().detach().clone().numpy()\n            for model_params in self.model.parameters()\n            if model_params.requires_grad\n        ]\n\n        # If client control variates do not exist, initialize them to be the same as the server control variates.\n        # Server variates default to be 0, but as stated in the paper the control variates should be the uniform\n        # average of the client variates. So if server_control_variates are non-zero, this ensures that average\n        # still holds.\n        if self.client_control_variates is None:\n            self.client_control_variates = copy.deepcopy(self.server_control_variates)\n\n    def update_control_variates(self, local_steps: int) -&gt; None:\n        \"\"\"\n        Updates local control variates along with the corresponding updates according to the option 2 in Equation 4 in\n        https://arxiv.org/pdf/1910.06378.pdf.\n\n        To be called after weights of local model have been updated.\n\n        Args:\n            local_steps (int): Number of local steps performed during training.\n        \"\"\"\n        assert self.client_control_variates is not None\n        assert self.server_control_variates is not None\n        assert self.server_model_weights is not None\n        assert self.learning_rate is not None\n\n        # y_i\n        client_model_weights = [\n            val.cpu().detach().clone().numpy() for val in self.model.parameters() if val.requires_grad\n        ]\n\n        # (x - y_i)\n        delta_model_weights = self.compute_parameters_delta(self.server_model_weights, client_model_weights)\n\n        # (c_i - c)\n        delta_control_variates = self.compute_parameters_delta(\n            self.client_control_variates, self.server_control_variates\n        )\n\n        updated_client_control_variates = self.compute_updated_control_variates(\n            local_steps, delta_model_weights, delta_control_variates\n        )\n        self.client_control_variates_updates = self.compute_parameters_delta(\n            updated_client_control_variates, self.client_control_variates\n        )\n\n        # c_i = c_i^plus\n        self.client_control_variates = updated_client_control_variates\n\n    def modify_grad(self) -&gt; None:\n        \"\"\"\n        Modifies the gradient of the local model to correct for client drift. To be called after the gradients have\n        been computed on a batch of data. Updates not applied to params until step is called on optimizer.\n        \"\"\"\n        assert self.client_control_variates is not None\n        assert self.server_control_variates is not None\n\n        model_params_with_grad = [\n            model_params for model_params in self.model.parameters() if model_params.requires_grad\n        ]\n\n        for param, client_cv, server_cv in zip(\n            model_params_with_grad,\n            self.client_control_variates,\n            self.server_control_variates,\n        ):\n            assert param.grad is not None\n            tensor_type = param.grad.dtype\n            server_cv_tensor = torch.from_numpy(server_cv).type(tensor_type)\n            client_cv_tensor = torch.from_numpy(client_cv).type(tensor_type)\n            update = server_cv_tensor.to(self.device) - client_cv_tensor.to(self.device)\n            param.grad += update\n\n    def compute_parameters_delta(self, params_1: NDArrays, params_2: NDArrays) -&gt; NDArrays:\n        \"\"\"\n        Computes element-wise difference of two lists of ``NDarray`` where elements in ``params_2`` are subtracted from\n        elements in ``params_1``.\n\n        Each ``NDArray`` in the list of ``NDArrays`` are subtracted as\n\n        \\\\[\\\\text{params}_{1, i} - \\\\text{params}_{2, i}\\\\]\n\n        Args:\n            params_1 (NDArrays): First set of parameters\n            params_2 (NDArrays): Second set of parameters\n\n        Returns:\n            (NDArrays): \\\\(\\\\text{params}_1 - \\\\text{params}_2\\\\)\n        \"\"\"\n        parameter_delta: NDArrays = [param_1 - param_2 for param_1, param_2 in zip(params_1, params_2)]\n\n        return parameter_delta\n\n    def transform_gradients(self, losses: TrainingLosses) -&gt; None:\n        \"\"\"\n        Hook function for model training only called after backwards pass but before optimizer step. Used to modify\n        gradient to correct for client drift in Scaffold.\n\n        Args:\n            losses (TrainingLosses): losses is not used in this transformation.\n        \"\"\"\n        self.modify_grad()\n\n    def compute_updated_control_variates(\n        self,\n        local_steps: int,\n        delta_model_weights: NDArrays,\n        delta_control_variates: NDArrays,\n    ) -&gt; NDArrays:\n        \"\"\"\n        Computes the updated local control variates according to option 2 in Equation 4 of paper.\n\n        The calculation is\n\n        \\\\[c_i^+ = c_i - c + \\\\frac{1}{(K \\\\cdot lr)} \\\\cdot (x - y_i)\\\\]\n\n        where lr is the local learning rate.\n\n        Args:\n            local_steps (int): Number of local steps that were taken during local training (\\\\(K\\\\))\n            delta_model_weights (NDArrays): difference between the locally trained weights and the initial weights\n                prior to local training\n            delta_control_variates (NDArrays): difference between local (\\\\(c_i\\\\)) and server (\\\\(c\\\\)) control\n                variates \\\\(c_i - c\\\\).\n\n        Returns:\n            (NDArrays): Updated client control variates\n        \"\"\"\n        # coef = 1 / (K * eta_l)\n        scaling_coefficient = 1 / (local_steps * self.learning_rate)\n\n        # c_i^plus = c_i - c + 1/(K*lr) * (x - y_i)\n        return [\n            delta_control_variate + scaling_coefficient * delta_model_weight\n            for delta_control_variate, delta_model_weight in zip(delta_control_variates, delta_model_weights)\n        ]\n\n    def get_parameter_exchanger(self, config: Config) -&gt; ParameterExchanger:\n        assert self.model is not None\n        model_size = len(self.model.state_dict())\n        return FullParameterExchangerWithPacking(ParameterPackerWithControlVariates(model_size))\n\n    def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n        r\"\"\"\n        Called after training with the number of ``local_steps`` performed over the FL round and the corresponding\n        loss dictionary.\n\n        Args:\n            local_steps (int): Number of local steps that were taken during local training (\\(K\\))\n            loss_dict (dict[str, float]): dictionary of losses computed during training\n            config (Config): The config from the server.\n        \"\"\"\n        self.update_control_variates(local_steps)\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set\n        initialized attribute to True. Extends the basic client to extract the learning rate from the optimizer and\n        set the ``learning_rate`` attribute (used to compute updated control variates).\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        super().setup_client(config)\n        if isinstance(self, DPScaffoldClient):\n            assert isinstance(self.optimizers[\"global\"], DPOptimizer)\n        else:\n            assert isinstance(self.optimizers[\"global\"], torch.optim.SGD)\n        self.learning_rate = self.optimizers[\"global\"].defaults[\"lr\"]\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>Federated Learning Client for Scaffold strategy.</p> <p>Implementation based on https://arxiv.org/pdf/1910.06378.pdf.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Federated Learning Client for Scaffold strategy.\n\n    Implementation based on https://arxiv.org/pdf/1910.06378.pdf.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.learning_rate: float  # eta_l in paper\n    self.client_control_variates: NDArrays | None = None  # c_i in paper\n    self.client_control_variates_updates: NDArrays | None = None  # delta_c_i in paper\n    self.server_control_variates: NDArrays | None = None  # c in paper\n    # Scaffold require vanilla SGD as optimizer, will assert during setup_client\n    self.optimizers: dict[str, torch.optim.Optimizer]\n\n    self.server_model_weights: NDArrays | None = None  # x in paper\n    self.parameter_exchanger: FullParameterExchangerWithPacking[NDArrays]\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>Packs the parameters and control variates into a single <code>NDArrays</code> to be sent to the server for aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Model parameters and control variates packed together.</p> Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def get_parameters(self, config: Config) -&gt; NDArrays:\n    \"\"\"\n    Packs the parameters and control variates into a single ``NDArrays`` to be sent to the server for aggregation.\n\n    Args:\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n    Returns:\n        (NDArrays): Model parameters and control variates packed together.\n    \"\"\"\n    if not self.initialized:\n        return self.setup_client_and_return_all_model_parameters(config)\n\n    assert self.model is not None and self.parameter_exchanger is not None\n\n    model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n\n    # Weights and control variates updates sent to server for aggregation\n    # Control variates updates sent because only client has access to previous client control variate\n    # Therefore it can only be computed locally\n    assert self.client_control_variates_updates is not None\n    return self.parameter_exchanger.pack_parameters(model_weights, self.client_control_variates_updates)\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>Assumes that the parameters being passed contain model parameters concatenated with server control variates. They are unpacked for the clients to use in training. If it's the first time the model is being initialized, we assume the full model is being initialized and use the <code>FullParameterExchanger()</code> to set all model weights.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model and also the server control variates (initial or after aggregation)</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Which fitting round (i.e. server round of fitting) that we're on.</p> required Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None:\n    \"\"\"\n    Assumes that the parameters being passed contain model parameters concatenated with server control variates.\n    They are unpacked for the clients to use in training. If it's the first time the model is being initialized,\n    we assume the full model is being initialized and use the ``FullParameterExchanger()`` to set all model\n    weights.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model and also the server control variates (initial or after aggregation)\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Which fitting round (i.e. server round of fitting) that we're on.\n    \"\"\"\n    assert self.model is not None and self.parameter_exchanger is not None\n\n    server_model_state, server_control_variates = self.parameter_exchanger.unpack_parameters(parameters)\n    self.server_control_variates = server_control_variates\n\n    super().set_parameters(server_model_state, config, fitting_round)\n\n    # Note that we are restricting to weights that require a gradient here because they are used to compute\n    # control variates\n    self.server_model_weights = [\n        model_params.cpu().detach().clone().numpy()\n        for model_params in self.model.parameters()\n        if model_params.requires_grad\n    ]\n\n    # If client control variates do not exist, initialize them to be the same as the server control variates.\n    # Server variates default to be 0, but as stated in the paper the control variates should be the uniform\n    # average of the client variates. So if server_control_variates are non-zero, this ensures that average\n    # still holds.\n    if self.client_control_variates is None:\n        self.client_control_variates = copy.deepcopy(self.server_control_variates)\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient.update_control_variates","title":"<code>update_control_variates(local_steps)</code>","text":"<p>Updates local control variates along with the corresponding updates according to the option 2 in Equation 4 in https://arxiv.org/pdf/1910.06378.pdf.</p> <p>To be called after weights of local model have been updated.</p> <p>Parameters:</p> Name Type Description Default <code>local_steps</code> <code>int</code> <p>Number of local steps performed during training.</p> required Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def update_control_variates(self, local_steps: int) -&gt; None:\n    \"\"\"\n    Updates local control variates along with the corresponding updates according to the option 2 in Equation 4 in\n    https://arxiv.org/pdf/1910.06378.pdf.\n\n    To be called after weights of local model have been updated.\n\n    Args:\n        local_steps (int): Number of local steps performed during training.\n    \"\"\"\n    assert self.client_control_variates is not None\n    assert self.server_control_variates is not None\n    assert self.server_model_weights is not None\n    assert self.learning_rate is not None\n\n    # y_i\n    client_model_weights = [\n        val.cpu().detach().clone().numpy() for val in self.model.parameters() if val.requires_grad\n    ]\n\n    # (x - y_i)\n    delta_model_weights = self.compute_parameters_delta(self.server_model_weights, client_model_weights)\n\n    # (c_i - c)\n    delta_control_variates = self.compute_parameters_delta(\n        self.client_control_variates, self.server_control_variates\n    )\n\n    updated_client_control_variates = self.compute_updated_control_variates(\n        local_steps, delta_model_weights, delta_control_variates\n    )\n    self.client_control_variates_updates = self.compute_parameters_delta(\n        updated_client_control_variates, self.client_control_variates\n    )\n\n    # c_i = c_i^plus\n    self.client_control_variates = updated_client_control_variates\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient.modify_grad","title":"<code>modify_grad()</code>","text":"<p>Modifies the gradient of the local model to correct for client drift. To be called after the gradients have been computed on a batch of data. Updates not applied to params until step is called on optimizer.</p> Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def modify_grad(self) -&gt; None:\n    \"\"\"\n    Modifies the gradient of the local model to correct for client drift. To be called after the gradients have\n    been computed on a batch of data. Updates not applied to params until step is called on optimizer.\n    \"\"\"\n    assert self.client_control_variates is not None\n    assert self.server_control_variates is not None\n\n    model_params_with_grad = [\n        model_params for model_params in self.model.parameters() if model_params.requires_grad\n    ]\n\n    for param, client_cv, server_cv in zip(\n        model_params_with_grad,\n        self.client_control_variates,\n        self.server_control_variates,\n    ):\n        assert param.grad is not None\n        tensor_type = param.grad.dtype\n        server_cv_tensor = torch.from_numpy(server_cv).type(tensor_type)\n        client_cv_tensor = torch.from_numpy(client_cv).type(tensor_type)\n        update = server_cv_tensor.to(self.device) - client_cv_tensor.to(self.device)\n        param.grad += update\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient.compute_parameters_delta","title":"<code>compute_parameters_delta(params_1, params_2)</code>","text":"<p>Computes element-wise difference of two lists of <code>NDarray</code> where elements in <code>params_2</code> are subtracted from elements in <code>params_1</code>.</p> <p>Each <code>NDArray</code> in the list of <code>NDArrays</code> are subtracted as</p> \\[\\text{params}_{1, i} - \\text{params}_{2, i}\\] <p>Parameters:</p> Name Type Description Default <code>params_1</code> <code>NDArrays</code> <p>First set of parameters</p> required <code>params_2</code> <code>NDArrays</code> <p>Second set of parameters</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>\\(\\text{params}_1 - \\text{params}_2\\)</p> Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def compute_parameters_delta(self, params_1: NDArrays, params_2: NDArrays) -&gt; NDArrays:\n    \"\"\"\n    Computes element-wise difference of two lists of ``NDarray`` where elements in ``params_2`` are subtracted from\n    elements in ``params_1``.\n\n    Each ``NDArray`` in the list of ``NDArrays`` are subtracted as\n\n    \\\\[\\\\text{params}_{1, i} - \\\\text{params}_{2, i}\\\\]\n\n    Args:\n        params_1 (NDArrays): First set of parameters\n        params_2 (NDArrays): Second set of parameters\n\n    Returns:\n        (NDArrays): \\\\(\\\\text{params}_1 - \\\\text{params}_2\\\\)\n    \"\"\"\n    parameter_delta: NDArrays = [param_1 - param_2 for param_1, param_2 in zip(params_1, params_2)]\n\n    return parameter_delta\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient.transform_gradients","title":"<code>transform_gradients(losses)</code>","text":"<p>Hook function for model training only called after backwards pass but before optimizer step. Used to modify gradient to correct for client drift in Scaffold.</p> <p>Parameters:</p> Name Type Description Default <code>losses</code> <code>TrainingLosses</code> <p>losses is not used in this transformation.</p> required Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def transform_gradients(self, losses: TrainingLosses) -&gt; None:\n    \"\"\"\n    Hook function for model training only called after backwards pass but before optimizer step. Used to modify\n    gradient to correct for client drift in Scaffold.\n\n    Args:\n        losses (TrainingLosses): losses is not used in this transformation.\n    \"\"\"\n    self.modify_grad()\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient.compute_updated_control_variates","title":"<code>compute_updated_control_variates(local_steps, delta_model_weights, delta_control_variates)</code>","text":"<p>Computes the updated local control variates according to option 2 in Equation 4 of paper.</p> <p>The calculation is</p> \\[c_i^+ = c_i - c + \\frac{1}{(K \\cdot lr)} \\cdot (x - y_i)\\] <p>where lr is the local learning rate.</p> <p>Parameters:</p> Name Type Description Default <code>local_steps</code> <code>int</code> <p>Number of local steps that were taken during local training (\\(K\\))</p> required <code>delta_model_weights</code> <code>NDArrays</code> <p>difference between the locally trained weights and the initial weights prior to local training</p> required <code>delta_control_variates</code> <code>NDArrays</code> <p>difference between local (\\(c_i\\)) and server (\\(c\\)) control variates \\(c_i - c\\).</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Updated client control variates</p> Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def compute_updated_control_variates(\n    self,\n    local_steps: int,\n    delta_model_weights: NDArrays,\n    delta_control_variates: NDArrays,\n) -&gt; NDArrays:\n    \"\"\"\n    Computes the updated local control variates according to option 2 in Equation 4 of paper.\n\n    The calculation is\n\n    \\\\[c_i^+ = c_i - c + \\\\frac{1}{(K \\\\cdot lr)} \\\\cdot (x - y_i)\\\\]\n\n    where lr is the local learning rate.\n\n    Args:\n        local_steps (int): Number of local steps that were taken during local training (\\\\(K\\\\))\n        delta_model_weights (NDArrays): difference between the locally trained weights and the initial weights\n            prior to local training\n        delta_control_variates (NDArrays): difference between local (\\\\(c_i\\\\)) and server (\\\\(c\\\\)) control\n            variates \\\\(c_i - c\\\\).\n\n    Returns:\n        (NDArrays): Updated client control variates\n    \"\"\"\n    # coef = 1 / (K * eta_l)\n    scaling_coefficient = 1 / (local_steps * self.learning_rate)\n\n    # c_i^plus = c_i - c + 1/(K*lr) * (x - y_i)\n    return [\n        delta_control_variate + scaling_coefficient * delta_model_weight\n        for delta_control_variate, delta_model_weight in zip(delta_control_variates, delta_model_weights)\n    ]\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient.update_after_train","title":"<code>update_after_train(local_steps, loss_dict, config)</code>","text":"<p>Called after training with the number of <code>local_steps</code> performed over the FL round and the corresponding loss dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>local_steps</code> <code>int</code> <p>Number of local steps that were taken during local training (\\(K\\))</p> required <code>loss_dict</code> <code>dict[str, float]</code> <p>dictionary of losses computed during training</p> required <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None:\n    r\"\"\"\n    Called after training with the number of ``local_steps`` performed over the FL round and the corresponding\n    loss dictionary.\n\n    Args:\n        local_steps (int): Number of local steps that were taken during local training (\\(K\\))\n        loss_dict (dict[str, float]): dictionary of losses computed during training\n        config (Config): The config from the server.\n    \"\"\"\n    self.update_control_variates(local_steps)\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.ScaffoldClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set initialized attribute to True. Extends the basic client to extract the learning rate from the optimizer and set the <code>learning_rate</code> attribute (used to compute updated control variates).</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set\n    initialized attribute to True. Extends the basic client to extract the learning rate from the optimizer and\n    set the ``learning_rate`` attribute (used to compute updated control variates).\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    super().setup_client(config)\n    if isinstance(self, DPScaffoldClient):\n        assert isinstance(self.optimizers[\"global\"], DPOptimizer)\n    else:\n        assert isinstance(self.optimizers[\"global\"], torch.optim.SGD)\n    self.learning_rate = self.optimizers[\"global\"].defaults[\"lr\"]\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.DPScaffoldClient","title":"<code>DPScaffoldClient</code>","text":"<p>               Bases: <code>ScaffoldClient</code>, <code>InstanceLevelDpClient</code></p> Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>class DPScaffoldClient(ScaffoldClient, InstanceLevelDpClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Federated Learning client for Instance Level Differentially Private Scaffold strategy.\n\n        Implemented as specified in https://arxiv.org/abs/2111.09278\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False.\n            client_name (str | None, optional): n optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        ScaffoldClient.__init__(\n            self,\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n\n        InstanceLevelDpClient.__init__(\n            self,\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n</code></pre>"},{"location":"api/#fl4health.clients.scaffold_client.DPScaffoldClient.__init__","title":"<code>__init__(data_path, metrics, device, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>Federated Learning client for Instance Level Differentially Private Scaffold strategy.</p> <p>Implemented as specified in https://arxiv.org/abs/2111.09278</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False.</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>n optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/scaffold_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Federated Learning client for Instance Level Differentially Private Scaffold strategy.\n\n    Implemented as specified in https://arxiv.org/abs/2111.09278\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False.\n        client_name (str | None, optional): n optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    ScaffoldClient.__init__(\n        self,\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n\n    InstanceLevelDpClient.__init__(\n        self,\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n</code></pre>"},{"location":"api/#fl4health.clients.tabular_data_client","title":"<code>tabular_data_client</code>","text":""},{"location":"api/#fl4health.clients.tabular_data_client.TabularDataClient","title":"<code>TabularDataClient</code>","text":"<p>               Bases: <code>BasicClient</code></p> Source code in <code>fl4health/clients/tabular_data_client.py</code> <pre><code>class TabularDataClient(BasicClient):\n    def __init__(\n        self,\n        data_path: Path,\n        metrics: Sequence[Metric],\n        device: torch.device,\n        id_column: str,\n        targets: str | list[str],\n        loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n        checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        progress_bar: bool = False,\n        client_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Client to facilitate federated feature space alignment, specifically for tabular data, and then perform\n        federated training.\n\n        Args:\n            data_path (Path): path to the data to be used to load the data for client-side training.\n            metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n            device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n                \"cuda\".\n            id_column (str): ID column. This is required for tabular encoding. It should be unique per row, but need\n                not necessarily be a meaningful identifier (i.e. could be row number)\n            targets (str | list[str]): The target column or columns name. This allows for multiple targets to\n                be specified if desired.\n            loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n                each batch. Defaults to ``LossMeterType.AVERAGE``.\n            checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n                both checkpointing and state saving. The module, and its underlying model and state checkpointing\n                components will determine when and how to do checkpointing during client-side training.\n                No checkpointing (state or model) is done if not provided. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n                validation. Uses ``tqdm``. Defaults to False\n            client_name (str | None, optional): An optional client name that uniquely identifies a client.\n                If not passed, a hash is randomly generated. Client state will use this as part of its state file\n                name. Defaults to None.\n        \"\"\"\n        super().__init__(\n            data_path=data_path,\n            metrics=metrics,\n            device=device,\n            loss_meter_type=loss_meter_type,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            progress_bar=progress_bar,\n            client_name=client_name,\n        )\n        self.tabular_features_info_encoder: TabularFeaturesInfoEncoder\n        self.tabular_features_preprocessor: TabularFeaturesPreprocessor\n        self.df: pd.DataFrame\n        self.input_dimension: int\n        self.output_dimension: int\n        self.id_column = id_column\n        self.targets = targets\n        # The aligned data and targets, which are used to construct dataloaders.\n        self.aligned_features: NDArray\n        self.aligned_targets: NDArray\n        self.feature_specific_pipelines: dict[str, Pipeline] = {}\n\n    def setup_client(self, config: Config) -&gt; None:\n        \"\"\"\n        Initialize the client by encoding the information of its tabular data and initializing the corresponding\n        ``TabularFeaturesPreprocessor``.\n\n        ``config[SOURCE_SPECIFIED]`` indicates whether the server has obtained the source of information to perform\n        feature alignment. If it is True, it means the server has obtained such information (either a priori or by\n        polling a client). So the client will encode that information and use it instead to perform feature\n        preprocessing.\n\n        Args:\n            config (Config): Configuration sent by the server for customization of the function\n        \"\"\"\n        source_specified = narrow_dict_type(config, SOURCE_SPECIFIED, bool)\n        self.df = self.get_data_frame(config)\n\n        if source_specified:\n            # Since the server has obtained its source of information,\n            # the client will encode that instead.\n            self.tabular_features_info_encoder = TabularFeaturesInfoEncoder.from_json(\n                narrow_dict_type(config, FEATURE_INFO, str)\n            )\n            self.tabular_features_preprocessor = TabularFeaturesPreprocessor(self.tabular_features_info_encoder)\n\n            # Set feature specific pipelines if the user has defined them.\n            self.set_feature_specific_pipelines()\n\n            # preprocess features.\n            self.aligned_features, self.aligned_targets = self.tabular_features_preprocessor.preprocess_features(\n                self.df\n            )\n\n            # Obtain the input and output dimensions to be sent to the server for global model initialization. Assuming\n            # that the first dimension is the number of rows.\n            self.input_dimension = self.aligned_features.shape[1]\n            self.output_dimension = self.tabular_features_info_encoder.get_target_dimension()\n            log(INFO, f\"input dimension: {self.input_dimension}, target dimension: {self.output_dimension}\")\n\n            super().setup_client(config)\n\n            # freeing the memory of aligned features/targets and data.\n            del self.aligned_features\n            del self.aligned_targets\n            del self.df\n        else:\n            # Encode the information of the client's local tabular data. This is expected to happen only once\n            # if the client is requested to provide alignment information.\n            self.tabular_features_info_encoder = TabularFeaturesInfoEncoder.encoder_from_dataframe(\n                self.df, self.id_column, self.targets\n            )\n\n    def get_data_frame(self, config: Config) -&gt; pd.DataFrame:\n        \"\"\"\n        User defined method that returns a pandas dataframe.\n\n        Args:\n            config (Config): Configuration sent by the server for customization of the function\n\n        \"\"\"\n        raise NotImplementedError\n\n    def get_properties(self, config: Config) -&gt; dict[str, Scalar]:\n        \"\"\"\n        Return properties of client to be sent to the server. Depending on whether the server has communicated the\n        information to be used for feature alignment, the client will send the input/output dimensions so the server\n        can use them to initialize the global model.\n\n        First initializes the client because this is called prior to the first federated learning round.\n\n        Args:\n            config (Config): Configuration sent by the server for customization of the function\n\n        Returns:\n            (dict[str, Scalar]): Properties to be returned to the server, providing information about the client.\n        \"\"\"\n        if not self.initialized:\n            self.setup_client(config)\n        source_specified = narrow_dict_type(config, SOURCE_SPECIFIED, bool)\n        if not source_specified:\n            return {\n                FEATURE_INFO: self.tabular_features_info_encoder.to_json(),\n            }\n        return {\n            INPUT_DIMENSION: self.input_dimension,\n            OUTPUT_DIMENSION: self.output_dimension,\n        }\n\n    def preset_specific_pipeline(self, feature_name: str, pipeline: Pipeline) -&gt; None:\n        \"\"\"\n        The user may use this method to specify a specific pipeline to be applied to a particular feature. This\n        function stores the provided pipeline associated with the provided ``feature_name``.\n\n        Args:\n            feature_name (str): Name of the feature as a column in the dataframe\n            pipeline (Pipeline): Pipeline of transformations to be applied to the target feature\n        \"\"\"\n        self.feature_specific_pipelines[feature_name] = pipeline\n\n    def set_feature_specific_pipelines(self) -&gt; None:\n        \"\"\"Given the feature specific pipelines, at them to the tabular feature preprocessor.\"\"\"\n        assert self.tabular_features_preprocessor is not None\n        for feature_name, pipeline in self.feature_specific_pipelines.items():\n            self.tabular_features_preprocessor.set_feature_pipeline(feature_name, pipeline)\n</code></pre>"},{"location":"api/#fl4health.clients.tabular_data_client.TabularDataClient.__init__","title":"<code>__init__(data_path, metrics, device, id_column, targets, loss_meter_type=LossMeterType.AVERAGE, checkpoint_and_state_module=None, reporters=None, progress_bar=False, client_name=None)</code>","text":"<p>Client to facilitate federated feature space alignment, specifically for tabular data, and then perform federated training.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>path to the data to be used to load the data for client-side training.</p> required <code>metrics</code> <code>Sequence[Metric]</code> <p>Metrics to be computed based on the labels and predictions of the client model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or \"cuda\".</p> required <code>id_column</code> <code>str</code> <p>ID column. This is required for tabular encoding. It should be unique per row, but need not necessarily be a meaningful identifier (i.e. could be row number)</p> required <code>targets</code> <code>str | list[str]</code> <p>The target column or columns name. This allows for multiple targets to be specified if desired.</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>Type of meter used to track and compute the losses over each batch. Defaults to <code>LossMeterType.AVERAGE</code>.</p> <code>AVERAGE</code> <code>checkpoint_and_state_module</code> <code>ClientCheckpointAndStateModule | None</code> <p>A module meant to handle both checkpointing and state saving. The module, and its underlying model and state checkpointing components will determine when and how to do checkpointing during client-side training. No checkpointing (state or model) is done if not provided. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>progress_bar</code> <code>bool</code> <p>Whether or not to display a progress bar during client training and validation. Uses <code>tqdm</code>. Defaults to False</p> <code>False</code> <code>client_name</code> <code>str | None</code> <p>An optional client name that uniquely identifies a client. If not passed, a hash is randomly generated. Client state will use this as part of its state file name. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/clients/tabular_data_client.py</code> <pre><code>def __init__(\n    self,\n    data_path: Path,\n    metrics: Sequence[Metric],\n    device: torch.device,\n    id_column: str,\n    targets: str | list[str],\n    loss_meter_type: LossMeterType = LossMeterType.AVERAGE,\n    checkpoint_and_state_module: ClientCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    progress_bar: bool = False,\n    client_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Client to facilitate federated feature space alignment, specifically for tabular data, and then perform\n    federated training.\n\n    Args:\n        data_path (Path): path to the data to be used to load the data for client-side training.\n        metrics (Sequence[Metric]): Metrics to be computed based on the labels and predictions of the client model.\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often \"cpu\" or\n            \"cuda\".\n        id_column (str): ID column. This is required for tabular encoding. It should be unique per row, but need\n            not necessarily be a meaningful identifier (i.e. could be row number)\n        targets (str | list[str]): The target column or columns name. This allows for multiple targets to\n            be specified if desired.\n        loss_meter_type (LossMeterType, optional): Type of meter used to track and compute the losses over\n            each batch. Defaults to ``LossMeterType.AVERAGE``.\n        checkpoint_and_state_module (ClientCheckpointAndStateModule | None, optional): A module meant to handle\n            both checkpointing and state saving. The module, and its underlying model and state checkpointing\n            components will determine when and how to do checkpointing during client-side training.\n            No checkpointing (state or model) is done if not provided. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        progress_bar (bool, optional): Whether or not to display a progress bar during client training and\n            validation. Uses ``tqdm``. Defaults to False\n        client_name (str | None, optional): An optional client name that uniquely identifies a client.\n            If not passed, a hash is randomly generated. Client state will use this as part of its state file\n            name. Defaults to None.\n    \"\"\"\n    super().__init__(\n        data_path=data_path,\n        metrics=metrics,\n        device=device,\n        loss_meter_type=loss_meter_type,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        progress_bar=progress_bar,\n        client_name=client_name,\n    )\n    self.tabular_features_info_encoder: TabularFeaturesInfoEncoder\n    self.tabular_features_preprocessor: TabularFeaturesPreprocessor\n    self.df: pd.DataFrame\n    self.input_dimension: int\n    self.output_dimension: int\n    self.id_column = id_column\n    self.targets = targets\n    # The aligned data and targets, which are used to construct dataloaders.\n    self.aligned_features: NDArray\n    self.aligned_targets: NDArray\n    self.feature_specific_pipelines: dict[str, Pipeline] = {}\n</code></pre>"},{"location":"api/#fl4health.clients.tabular_data_client.TabularDataClient.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Initialize the client by encoding the information of its tabular data and initializing the corresponding <code>TabularFeaturesPreprocessor</code>.</p> <p><code>config[SOURCE_SPECIFIED]</code> indicates whether the server has obtained the source of information to perform feature alignment. If it is True, it means the server has obtained such information (either a priori or by polling a client). So the client will encode that information and use it instead to perform feature preprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration sent by the server for customization of the function</p> required Source code in <code>fl4health/clients/tabular_data_client.py</code> <pre><code>def setup_client(self, config: Config) -&gt; None:\n    \"\"\"\n    Initialize the client by encoding the information of its tabular data and initializing the corresponding\n    ``TabularFeaturesPreprocessor``.\n\n    ``config[SOURCE_SPECIFIED]`` indicates whether the server has obtained the source of information to perform\n    feature alignment. If it is True, it means the server has obtained such information (either a priori or by\n    polling a client). So the client will encode that information and use it instead to perform feature\n    preprocessing.\n\n    Args:\n        config (Config): Configuration sent by the server for customization of the function\n    \"\"\"\n    source_specified = narrow_dict_type(config, SOURCE_SPECIFIED, bool)\n    self.df = self.get_data_frame(config)\n\n    if source_specified:\n        # Since the server has obtained its source of information,\n        # the client will encode that instead.\n        self.tabular_features_info_encoder = TabularFeaturesInfoEncoder.from_json(\n            narrow_dict_type(config, FEATURE_INFO, str)\n        )\n        self.tabular_features_preprocessor = TabularFeaturesPreprocessor(self.tabular_features_info_encoder)\n\n        # Set feature specific pipelines if the user has defined them.\n        self.set_feature_specific_pipelines()\n\n        # preprocess features.\n        self.aligned_features, self.aligned_targets = self.tabular_features_preprocessor.preprocess_features(\n            self.df\n        )\n\n        # Obtain the input and output dimensions to be sent to the server for global model initialization. Assuming\n        # that the first dimension is the number of rows.\n        self.input_dimension = self.aligned_features.shape[1]\n        self.output_dimension = self.tabular_features_info_encoder.get_target_dimension()\n        log(INFO, f\"input dimension: {self.input_dimension}, target dimension: {self.output_dimension}\")\n\n        super().setup_client(config)\n\n        # freeing the memory of aligned features/targets and data.\n        del self.aligned_features\n        del self.aligned_targets\n        del self.df\n    else:\n        # Encode the information of the client's local tabular data. This is expected to happen only once\n        # if the client is requested to provide alignment information.\n        self.tabular_features_info_encoder = TabularFeaturesInfoEncoder.encoder_from_dataframe(\n            self.df, self.id_column, self.targets\n        )\n</code></pre>"},{"location":"api/#fl4health.clients.tabular_data_client.TabularDataClient.get_data_frame","title":"<code>get_data_frame(config)</code>","text":"<p>User defined method that returns a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration sent by the server for customization of the function</p> required Source code in <code>fl4health/clients/tabular_data_client.py</code> <pre><code>def get_data_frame(self, config: Config) -&gt; pd.DataFrame:\n    \"\"\"\n    User defined method that returns a pandas dataframe.\n\n    Args:\n        config (Config): Configuration sent by the server for customization of the function\n\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.clients.tabular_data_client.TabularDataClient.get_properties","title":"<code>get_properties(config)</code>","text":"<p>Return properties of client to be sent to the server. Depending on whether the server has communicated the information to be used for feature alignment, the client will send the input/output dimensions so the server can use them to initialize the global model.</p> <p>First initializes the client because this is called prior to the first federated learning round.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration sent by the server for customization of the function</p> required <p>Returns:</p> Type Description <code>dict[str, Scalar]</code> <p>Properties to be returned to the server, providing information about the client.</p> Source code in <code>fl4health/clients/tabular_data_client.py</code> <pre><code>def get_properties(self, config: Config) -&gt; dict[str, Scalar]:\n    \"\"\"\n    Return properties of client to be sent to the server. Depending on whether the server has communicated the\n    information to be used for feature alignment, the client will send the input/output dimensions so the server\n    can use them to initialize the global model.\n\n    First initializes the client because this is called prior to the first federated learning round.\n\n    Args:\n        config (Config): Configuration sent by the server for customization of the function\n\n    Returns:\n        (dict[str, Scalar]): Properties to be returned to the server, providing information about the client.\n    \"\"\"\n    if not self.initialized:\n        self.setup_client(config)\n    source_specified = narrow_dict_type(config, SOURCE_SPECIFIED, bool)\n    if not source_specified:\n        return {\n            FEATURE_INFO: self.tabular_features_info_encoder.to_json(),\n        }\n    return {\n        INPUT_DIMENSION: self.input_dimension,\n        OUTPUT_DIMENSION: self.output_dimension,\n    }\n</code></pre>"},{"location":"api/#fl4health.clients.tabular_data_client.TabularDataClient.preset_specific_pipeline","title":"<code>preset_specific_pipeline(feature_name, pipeline)</code>","text":"<p>The user may use this method to specify a specific pipeline to be applied to a particular feature. This function stores the provided pipeline associated with the provided <code>feature_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Name of the feature as a column in the dataframe</p> required <code>pipeline</code> <code>Pipeline</code> <p>Pipeline of transformations to be applied to the target feature</p> required Source code in <code>fl4health/clients/tabular_data_client.py</code> <pre><code>def preset_specific_pipeline(self, feature_name: str, pipeline: Pipeline) -&gt; None:\n    \"\"\"\n    The user may use this method to specify a specific pipeline to be applied to a particular feature. This\n    function stores the provided pipeline associated with the provided ``feature_name``.\n\n    Args:\n        feature_name (str): Name of the feature as a column in the dataframe\n        pipeline (Pipeline): Pipeline of transformations to be applied to the target feature\n    \"\"\"\n    self.feature_specific_pipelines[feature_name] = pipeline\n</code></pre>"},{"location":"api/#fl4health.clients.tabular_data_client.TabularDataClient.set_feature_specific_pipelines","title":"<code>set_feature_specific_pipelines()</code>","text":"<p>Given the feature specific pipelines, at them to the tabular feature preprocessor.</p> Source code in <code>fl4health/clients/tabular_data_client.py</code> <pre><code>def set_feature_specific_pipelines(self) -&gt; None:\n    \"\"\"Given the feature specific pipelines, at them to the tabular feature preprocessor.\"\"\"\n    assert self.tabular_features_preprocessor is not None\n    for feature_name, pipeline in self.feature_specific_pipelines.items():\n        self.tabular_features_preprocessor.set_feature_pipeline(feature_name, pipeline)\n</code></pre>"},{"location":"api/#fl4health.datasets","title":"<code>datasets</code>","text":""},{"location":"api/#fl4health.datasets.rxrx1","title":"<code>rxrx1</code>","text":""},{"location":"api/#fl4health.datasets.rxrx1.load_data","title":"<code>load_data</code>","text":""},{"location":"api/#fl4health.datasets.rxrx1.load_data.construct_rxrx1_tensor_dataset","title":"<code>construct_rxrx1_tensor_dataset(metadata, data_path, client_num, dataset_type, transform=None)</code>","text":"<p>Construct a <code>TensorDataset</code> for rxrx1 data (https://www.rxrx.ai/rxrx1).</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>A <code>DataFrame</code> containing image metadata.</p> required <code>data_path</code> <code>Path</code> <p>Root directory which the image data should be loaded.</p> required <code>client_num</code> <code>int</code> <p>Client number to load data for.</p> required <code>dataset_type</code> <code>str</code> <p>\"train\" or \"test\" to specify dataset type.</p> required <code>transform</code> <code>Callable | None</code> <p>Transformation function to apply to the images. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[TensorDataset, dict[int, int]]</code> <p>A <code>TensorDataset</code> containing the processed images and label map.</p> Source code in <code>fl4health/datasets/rxrx1/load_data.py</code> <pre><code>def construct_rxrx1_tensor_dataset(\n    metadata: pd.DataFrame,\n    data_path: Path,\n    client_num: int,\n    dataset_type: str,\n    transform: Callable | None = None,\n) -&gt; tuple[TensorDataset, dict[int, int]]:\n    \"\"\"\n    Construct a ``TensorDataset`` for rxrx1 data (https://www.rxrx.ai/rxrx1).\n\n    Args:\n        metadata (DataFrame): A ``DataFrame`` containing image metadata.\n        data_path (Path): Root directory which the image data should be loaded.\n        client_num (int): Client number to load data for.\n        dataset_type (str): \"train\" or \"test\" to specify dataset type.\n        transform (Callable | None): Transformation function to apply to the images. Defaults to None.\n\n    Returns:\n        (tuple[TensorDataset, dict[int, int]]): A ``TensorDataset`` containing the processed images and label map.\n    \"\"\"\n    label_map = {label: idx for idx, label in enumerate(sorted(metadata[\"sirna_id\"].unique()))}\n    original_label_map = {new_label: original_label for original_label, new_label in label_map.items()}\n    metadata = metadata[metadata[\"dataset\"] == dataset_type]\n    targets_tensor = torch.Tensor(list(metadata[\"sirna_id\"].map(label_map))).type(torch.long)\n    data_list = []\n    for index in range(len(targets_tensor)):\n        with open(\n            os.path.join(data_path, f\"clients/{dataset_type}_data_{client_num + 1}/image_{index}.pkl\"), \"rb\"\n        ) as file:\n            data_list.append(torch.Tensor(pickle.load(file)).unsqueeze(0))\n    data_tensor = torch.cat(data_list)\n    return TensorDataset(data_tensor, targets_tensor, transform), original_label_map\n</code></pre>"},{"location":"api/#fl4health.datasets.rxrx1.load_data.label_frequency","title":"<code>label_frequency(dataset, original_label_map)</code>","text":"<p>Prints the frequency of each label in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>TensorDataset | Subset</code> <p>The dataset to analyze.</p> required <code>original_label_map</code> <code>dict[int, int]</code> <p>A mapping of the original labels to their new labels.</p> required Source code in <code>fl4health/datasets/rxrx1/load_data.py</code> <pre><code>def label_frequency(dataset: TensorDataset | Subset, original_label_map: dict[int, int]) -&gt; None:\n    \"\"\"\n    Prints the frequency of each label in the dataset.\n\n    Args:\n        dataset (TensorDataset | Subset): The dataset to analyze.\n        original_label_map (dict[int, int]): A mapping of the original labels to their new labels.\n    \"\"\"\n    # Extract metadata and label map\n    if isinstance(dataset, TensorDataset):\n        targets = dataset.targets\n    elif isinstance(dataset, Subset):\n        assert isinstance(dataset.dataset, TensorDataset), \"Subset dataset must be an TensorDataset instance.\"\n        targets = dataset.dataset.targets\n    else:\n        raise TypeError(\"Dataset must be of type TensorDataset or Subset containing an TensorDataset.\")\n\n    # Count label frequencies\n    label_to_indices = defaultdict(list)\n    assert isinstance(targets, torch.Tensor)\n    for idx, label in enumerate(targets):  # Assumes dataset[idx] returns (data, label)\n        label_to_indices[label].append(idx)\n\n    # Print frequency of labels their names\n    for label, count in label_to_indices.items():\n        assert isinstance(label, int)\n        original_label = original_label_map.get(label)\n        log(INFO, f\"Label {label} (original: {original_label}): {len(count)} samples\")\n</code></pre>"},{"location":"api/#fl4health.datasets.rxrx1.load_data.create_splits","title":"<code>create_splits(dataset, seed=None, train_fraction=0.8)</code>","text":"<p>Splits the dataset into training and validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>TensorDataset</code> <p>The dataset to split.</p> required <code>seed</code> <code>int | None</code> <p>Seed meant to fix the sampling process associated with splitting. Defaults to None.</p> <code>None</code> <code>train_fraction</code> <code>float</code> <p>Fraction of data to use for training. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Type Description <code>tuple[list[int], list[int]]</code> <p>Indices associated with the selected datapoints for the train and validation sets</p> Source code in <code>fl4health/datasets/rxrx1/load_data.py</code> <pre><code>def create_splits(\n    dataset: TensorDataset, seed: int | None = None, train_fraction: float = 0.8\n) -&gt; tuple[list[int], list[int]]:\n    \"\"\"\n    Splits the dataset into training and validation sets.\n\n    Args:\n        dataset (TensorDataset): The dataset to split.\n        seed (int | None, optional): Seed meant to fix the sampling process associated with splitting.\n            Defaults to None.\n        train_fraction (float, optional): Fraction of data to use for training. Defaults to 0.8.\n\n    Returns:\n        (tuple[list[int], list[int]]): Indices associated with the selected datapoints for the train and validation\n            sets\n    \"\"\"\n    # Group indices by label\n    label_to_indices = defaultdict(list)\n    assert isinstance(dataset.targets, torch.Tensor)\n    for idx, label in enumerate(dataset.targets):  # Assumes dataset[idx] returns (data, label)\n        label_to_indices[label.item()].append(idx)\n\n    # Stratified splitting\n    train_indices, val_indices = [], []\n    for indices in label_to_indices.values():\n        if seed is not None:\n            np_generator = np.random.default_rng(seed)\n            np_generator.shuffle(indices)\n        else:\n            np.random.shuffle(indices)\n        split_point = int(len(indices) * train_fraction)\n        train_indices.extend(indices[:split_point])\n        val_indices.extend(indices[split_point:])\n        if len(val_indices) == 0:\n            log(INFO, \"Warning: Validation set is empty. Consider changing the train_fraction parameter.\")\n\n    return train_indices, val_indices\n</code></pre>"},{"location":"api/#fl4health.datasets.rxrx1.load_data.load_rxrx1_data","title":"<code>load_rxrx1_data(data_path, client_num, batch_size, seed=None, train_val_split=0.8, num_workers=0)</code>","text":"<p>Load and split the data into training and validation dataloaders.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>Path to the full set of data.</p> required <code>client_num</code> <code>int</code> <p>Client number for the data you want to load.</p> required <code>batch_size</code> <code>int</code> <p>batch size for the data loaders.</p> required <code>seed</code> <code>int | None</code> <p>Seed to fix randomness associated with data splitting. Defaults to None.</p> <code>None</code> <code>train_val_split</code> <code>float</code> <p>Percentage of data to put in the training loader. The remainder flow to the validation dataloader. Defaults to 0.8.</p> <code>0.8</code> <code>num_workers</code> <code>int</code> <p>Number of threads to be used by the dataloaders. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader, dict[str, int]]</code> <p>Train and validation dataloaders and a dictionary holding the</p> <code>DataLoader</code> <p>size of each dataset.</p> Source code in <code>fl4health/datasets/rxrx1/load_data.py</code> <pre><code>def load_rxrx1_data(\n    data_path: Path,\n    client_num: int,\n    batch_size: int,\n    seed: int | None = None,\n    train_val_split: float = 0.8,\n    num_workers: int = 0,\n) -&gt; tuple[DataLoader, DataLoader, dict[str, int]]:\n    \"\"\"\n    Load and split the data into training and validation dataloaders.\n\n    Args:\n        data_path (Path): Path to the full set of data.\n        client_num (int): Client number for the data you want to load.\n        batch_size (int): batch size for the data loaders.\n        seed (int | None, optional): Seed to fix randomness associated with data splitting. Defaults to None.\n        train_val_split (float, optional): Percentage of data to put in the training loader. The remainder flow to the\n            validation dataloader. Defaults to 0.8.\n        num_workers (int, optional): Number of threads to be used by the dataloaders. Defaults to 0.\n\n    Returns:\n        (tuple[DataLoader, DataLoader, dict[str, int]]): Train and validation dataloaders and a dictionary holding the\n        size of each dataset.\n    \"\"\"\n    # Read the CSV file\n    data = pd.read_csv(f\"{data_path}/clients/meta_data_{client_num + 1}.csv\")\n\n    dataset, _ = construct_rxrx1_tensor_dataset(data, data_path, client_num, \"train\")\n\n    train_indices, val_indices = create_splits(dataset, seed=seed, train_fraction=train_val_split)\n    train_set = copy.deepcopy(dataset)\n    train_set.data = train_set.data[train_indices]\n    assert train_set.targets is not None\n    train_set.targets = train_set.targets[train_indices]\n\n    validation_set = copy.deepcopy(dataset)\n    validation_set.data = validation_set.data[val_indices]\n    assert validation_set.targets is not None\n    validation_set.targets = validation_set.targets[val_indices]\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    validation_loader = DataLoader(validation_set, batch_size=batch_size)\n    num_examples = {\n        \"train_set\": len(train_set.data),\n        \"validation_set\": len(validation_set.data),\n    }\n\n    return train_loader, validation_loader, num_examples\n</code></pre>"},{"location":"api/#fl4health.datasets.rxrx1.load_data.load_rxrx1_test_data","title":"<code>load_rxrx1_test_data(data_path, client_num, batch_size, num_workers=0)</code>","text":"<p>Create a dataloader for the reserved rxrx1 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>Path</code> <p>Path to the test data.</p> required <code>client_num</code> <code>int</code> <p>Client number to be loaded.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for processing of the test scripts.</p> required <code>num_workers</code> <code>int</code> <p>Number of workers associated with the test dataloader. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, dict[str, int]]</code> <p>Test dataloader, dictionary containing count of the data points in the set.</p> Source code in <code>fl4health/datasets/rxrx1/load_data.py</code> <pre><code>def load_rxrx1_test_data(\n    data_path: Path, client_num: int, batch_size: int, num_workers: int = 0\n) -&gt; tuple[DataLoader, dict[str, int]]:\n    \"\"\"\n    Create a dataloader for the reserved rxrx1 dataset.\n\n    Args:\n        data_path (Path): Path to the test data.\n        client_num (int): Client number to be loaded.\n        batch_size (int): Batch size for processing of the test scripts.\n        num_workers (int, optional): Number of workers associated with the test dataloader. Defaults to 0.\n\n    Returns:\n        (tuple[DataLoader, dict[str, int]]): Test dataloader, dictionary containing count of the data points in the\n            set.\n    \"\"\"\n    # Read the CSV file\n    data = pd.read_csv(f\"{data_path}/clients/meta_data_{client_num + 1}.csv\")\n\n    dataset, _ = construct_rxrx1_tensor_dataset(data, data_path, client_num, \"test\")\n\n    evaluation_loader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n    )\n    num_examples = {\"eval_set\": len(dataset.data)}\n    return evaluation_loader, num_examples\n</code></pre>"},{"location":"api/#fl4health.datasets.rxrx1.preprocess","title":"<code>preprocess</code>","text":""},{"location":"api/#fl4health.datasets.rxrx1.preprocess.filter_and_save_data","title":"<code>filter_and_save_data(metadata, top_sirna_ids, cell_type, output_path)</code>","text":"<p>Filters data for the given cell type and frequency of their <code>sirna_id</code> and saves it to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata containing information about all images.</p> required <code>top_sirna_ids</code> <code>list[int]</code> <p>Top <code>sirna_id</code> values to filter by.</p> required <code>cell_type</code> <code>str</code> <p>Cell type to filter by.</p> required <code>output_path</code> <code>Path</code> <p>Path to save the filtered metadata.</p> required Source code in <code>fl4health/datasets/rxrx1/preprocess.py</code> <pre><code>def filter_and_save_data(metadata: pd.DataFrame, top_sirna_ids: list[int], cell_type: str, output_path: Path) -&gt; None:\n    \"\"\"\n    Filters data for the given cell type and frequency of their ``sirna_id`` and saves it to a CSV file.\n\n    Args:\n        metadata (pd.DataFrame): Metadata containing information about all images.\n        top_sirna_ids (list[int]): Top ``sirna_id`` values to filter by.\n        cell_type (str): Cell type to filter by.\n        output_path (Path): Path to save the filtered metadata.\n    \"\"\"\n    filtered_metadata = metadata[(metadata[\"sirna_id\"].isin(top_sirna_ids)) &amp; (metadata[\"cell_type\"] == cell_type)]\n    filtered_metadata.to_csv(output_path, index=False)\n</code></pre>"},{"location":"api/#fl4health.datasets.rxrx1.preprocess.load_image","title":"<code>load_image(row, root)</code>","text":"<p>Load an image tensor for a given row of metadata.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>dict[str, Any]</code> <p>A row of metadata containing experiment, plate, well, and site information.</p> required <code>root</code> <code>Path</code> <p>Root directory containing the image files.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The loaded image tensor.</p> Source code in <code>fl4health/datasets/rxrx1/preprocess.py</code> <pre><code>def load_image(row: dict[Hashable, Any], root: Path) -&gt; torch.Tensor:\n    \"\"\"\n    Load an image tensor for a given row of metadata.\n\n    Args:\n        row (dict[str, Any]): A row of metadata containing experiment, plate, well, and site information.\n        root (Path): Root directory containing the image files.\n\n    Returns:\n        (torch.Tensor): The loaded image tensor.\n    \"\"\"\n    experiment = row[\"experiment\"]\n    plate = row[\"plate\"]\n    well = row[\"well\"]\n    site = row[\"site\"]\n\n    images = []\n    # Rxrx1 originally consists of 6 channels, but to reduce the computational cost, we only use 3 channels\n    # following previous works such as https://github.com/p-lambda/wildYe.\n    for channel in range(1, 4):\n        image_path = os.path.join(root, f\"images/{experiment}/Plate{plate}/{well}_s{site}_w{channel}.png\")\n        if not Path(image_path).exists():\n            raise FileNotFoundError(f\"Image not found at {image_path}\")\n        image = ToTensor()(Image.open(image_path).convert(\"L\"))\n        images.append(image)\n\n    # Concatenate the three channels into one tensor\n    return torch.cat(images, dim=0)\n</code></pre>"},{"location":"api/#fl4health.datasets.rxrx1.preprocess.process_data","title":"<code>process_data(metadata, input_dir, output_dir, client_num, type_data)</code>","text":"<p>Process the entire dataset, loading image tensors for each row.</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>DataFrame</code> <p>Metadata containing information about all images.</p> required <code>input_dir</code> <code>Path</code> <p>Input directory containing the image files.</p> required <code>output_dir</code> <code>Path</code> <p>Output directory containing the image files.</p> required <code>client_num</code> <code>int</code> <p>Client number to load data for.</p> required <code>type_data</code> <code>str</code> <p>\"train\" or \"test\" to specify dataset type.</p> required Source code in <code>fl4health/datasets/rxrx1/preprocess.py</code> <pre><code>def process_data(metadata: pd.DataFrame, input_dir: Path, output_dir: Path, client_num: int, type_data: str) -&gt; None:\n    \"\"\"\n    Process the entire dataset, loading image tensors for each row.\n\n    Args:\n        metadata (pd.DataFrame): Metadata containing information about all images.\n        input_dir (Path): Input directory containing the image files.\n        output_dir (Path): Output directory containing the image files.\n        client_num (int): Client number to load data for.\n        type_data (str): \"train\" or \"test\" to specify dataset type.\n    \"\"\"\n    for i, row in metadata.iterrows():\n        image_tensor = load_image(row.to_dict(), Path(input_dir))\n        save_to_pkl(image_tensor, os.path.join(output_dir, f\"{type_data}_data_{client_num + 1}\", f\"image_{i}.pkl\"))\n</code></pre>"},{"location":"api/#fl4health.datasets.rxrx1.preprocess.save_to_pkl","title":"<code>save_to_pkl(data, output_path)</code>","text":"<p>Save data to a pickle file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Data to save.</p> required <code>output_path</code> <code>str</code> <p>Path to the output pickle file.</p> required Source code in <code>fl4health/datasets/rxrx1/preprocess.py</code> <pre><code>def save_to_pkl(data: torch.Tensor, output_path: str) -&gt; None:\n    \"\"\"\n    Save data to a pickle file.\n\n    Args:\n        data (torch.Tensor): Data to save.\n        output_path (str): Path to the output pickle file.\n    \"\"\"\n    with open(output_path, \"wb\") as f:\n        pickle.dump(data, f)\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer","title":"<code>skin_cancer</code>","text":""},{"location":"api/#fl4health.datasets.skin_cancer.load_data","title":"<code>load_data</code>","text":""},{"location":"api/#fl4health.datasets.skin_cancer.load_data.load_image","title":"<code>load_image(item, transform)</code>","text":"<p>Load and transform an image from a given item dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>dict[str, Any]</code> <p>A dictionary containing image path and labels.</p> required <code>transform</code> <code>Callable | None</code> <p>Transformation function to apply to the images.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, int]</code> <p>A tuple containing the transformed image tensor and the target label.</p> Source code in <code>fl4health/datasets/skin_cancer/load_data.py</code> <pre><code>def load_image(item: dict[str, Any], transform: Callable | None) -&gt; tuple[torch.Tensor, int]:\n    \"\"\"\n    Load and transform an image from a given item dictionary.\n\n    Args:\n        item (dict[str, Any]): A dictionary containing image path and labels.\n        transform (Callable | None): Transformation function to apply to the images.\n\n    Returns:\n        (tuple[torch.Tensor, int]): A tuple containing the transformed image tensor and the target label.\n    \"\"\"\n    image_path = item[\"img_path\"]\n    image = Image.open(image_path).convert(\"RGB\")\n    image = transform(image) if transform else transforms.ToTensor()(image)\n\n    # Default transformation if none provided\n    assert isinstance(image, torch.Tensor), f\"Image at {image_path} is not a Tensor\"\n    target = int(torch.tensor(item[\"extended_labels\"]).argmax().item())\n    return image, target\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.load_data.construct_skin_cancer_tensor_dataset","title":"<code>construct_skin_cancer_tensor_dataset(data, transform=None, num_workers=8)</code>","text":"<p>Construct a <code>TensorDataset</code> for skin cancer data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list[dict[str, Any]]</code> <p>List of dictionaries containing image paths and labels.</p> required <code>transform</code> <code>Callable | None</code> <p>Transformation function to apply to the images. Defaults to None.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of workers for parallel processing. Defaults to 8.</p> <code>8</code> <p>Returns:</p> Type Description <code>TensorDataset</code> <p>A <code>TensorDataset</code> containing the processed images and labels.</p> Source code in <code>fl4health/datasets/skin_cancer/load_data.py</code> <pre><code>def construct_skin_cancer_tensor_dataset(\n    data: list[dict[str, Any]], transform: Callable | None = None, num_workers: int = 8\n) -&gt; TensorDataset:\n    \"\"\"\n    Construct a ``TensorDataset`` for skin cancer data.\n\n    Args:\n        data (list[dict[str, Any]]): List of dictionaries containing image paths and labels.\n        transform (Callable | None): Transformation function to apply to the images. Defaults to None.\n        num_workers (int): Number of workers for parallel processing. Defaults to 8.\n\n    Returns:\n        (TensorDataset): A ``TensorDataset`` containing the processed images and labels.\n    \"\"\"\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        results = list(executor.map(lambda item: load_image(item, transform), data))\n\n    data_list, targets_list = zip(*results)\n    data_tensor = torch.stack(list(data_list))\n    targets_tensor = torch.tensor(list(targets_list))\n\n    return TensorDataset(data_tensor, targets_tensor)\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.load_data.load_skin_cancer_data","title":"<code>load_skin_cancer_data(data_dir, dataset_name, batch_size, split_percents=(0.7, 0.15, 0.15), sampler=None, train_transform=None, val_transform=None, test_transform=None, dataset_converter=None, seed=None)</code>","text":"<p>Load skin cancer dataset (training, validation, and test set).</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>Directory containing the dataset files.</p> required <code>dataset_name</code> <code>str</code> <p>Name of the dataset to load.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for the DataLoader.</p> required <code>split_percents</code> <code>tuple[float, float, float]</code> <p>Percentages for splitting the data into train, val, and test sets.</p> <code>(0.7, 0.15, 0.15)</code> <code>sampler</code> <code>LabelBasedSampler | None</code> <p>Sampler for the dataset. Defaults to None.</p> <code>None</code> <code>train_transform</code> <code>Callable | None</code> <p>Transformations to apply to the training data. Defaults to None.</p> <code>None</code> <code>val_transform</code> <code>Callable | None</code> <p>Transformations to apply to the validation data. Defaults to None.</p> <code>None</code> <code>test_transform</code> <code>Callable | None</code> <p>Transformations to apply to the test data. Defaults to None.</p> <code>None</code> <code>dataset_converter</code> <code>DatasetConverter | None</code> <p>Converter to apply to the dataset. Defaults to None.</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for shuffling data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader, DataLoader, dict[str, int]]</code> <p>DataLoaders for the training, validation, and test sets, and a dictionary with the number of examples in each set.</p> Source code in <code>fl4health/datasets/skin_cancer/load_data.py</code> <pre><code>def load_skin_cancer_data(\n    data_dir: Path,\n    dataset_name: str,\n    batch_size: int,\n    split_percents: tuple[float, float, float] = (0.7, 0.15, 0.15),\n    sampler: LabelBasedSampler | None = None,\n    train_transform: Callable | None = None,\n    val_transform: Callable | None = None,\n    test_transform: Callable | None = None,\n    dataset_converter: DatasetConverter | None = None,\n    seed: int | None = None,\n) -&gt; tuple[DataLoader, DataLoader, DataLoader, dict[str, int]]:\n    \"\"\"\n    Load skin cancer dataset (training, validation, and test set).\n\n    Args:\n        data_dir (Path): Directory containing the dataset files.\n        dataset_name (str): Name of the dataset to load.\n        batch_size (int): Batch size for the DataLoader.\n        split_percents (tuple[float, float, float]): Percentages for splitting the data into train, val, and test sets.\n        sampler (LabelBasedSampler | None): Sampler for the dataset. Defaults to None.\n        train_transform (Callable | None): Transformations to apply to the training data. Defaults to None.\n        val_transform (Callable | None): Transformations to apply to the validation data. Defaults to None.\n        test_transform (Callable | None): Transformations to apply to the test data. Defaults to None.\n        dataset_converter (DatasetConverter | None): Converter to apply to the dataset. Defaults to None.\n        seed (int | None): Random seed for shuffling data. Defaults to None.\n\n    Returns:\n        (tuple[DataLoader, DataLoader, DataLoader, dict[str, int]]): DataLoaders for the training, validation,\n            and test sets, and a dictionary with the number of examples in each set.\n    \"\"\"\n    if sum(split_percents) != 1.0:\n        raise ValueError(\"The split percentages must sum to 1.0\")\n\n    dataset_paths = {\n        \"Barcelona\": data_dir.joinpath(\"ISIC_2019\", \"ISIC_19_Barcelona.json\"),\n        \"Rosendahl\": data_dir.joinpath(\"HAM10000\", \"HAM_rosendahl.json\"),\n        \"Vienna\": data_dir.joinpath(\"HAM10000\", \"HAM_vienna.json\"),\n        \"UFES\": data_dir.joinpath(\"PAD-UFES-20\", \"PAD_UFES_20.json\"),\n        \"Canada\": data_dir.joinpath(\"Derm7pt\", \"Derm7pt.json\"),\n    }\n\n    if dataset_name not in dataset_paths:\n        raise ValueError(f\"Dataset {dataset_name} not found in available datasets.\")\n\n    dataset_path = dataset_paths[dataset_name]\n\n    if not dataset_path.exists():\n        raise FileNotFoundError(\n            f\"Dataset file {dataset_path} does not exist.\\\n            Please follow the instructions in fl4health/datasets/skin_cancer/README.md.\"\n        )\n\n    log(INFO, f\"Data directory: {str(dataset_path)}\")\n\n    with open(dataset_path, \"r\") as f:\n        data = json.load(f)[\"data\"]\n\n    if seed is not None:\n        random.seed(seed)\n    random.shuffle(data)\n\n    total_size = len(data)\n    train_size = int(split_percents[0] * total_size)\n    valid_size = int(split_percents[1] * total_size)\n\n    train_data = data[:train_size]\n    valid_data = data[train_size : train_size + valid_size]\n    test_data = data[train_size + valid_size :]\n\n    # this is the default transform if more specific ones aren't defined.\n    val_test_transform = transforms.Compose(\n        [\n            transforms.Resize([256, 256]),\n            transforms.ToTensor(),\n            transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0)),\n        ]\n    )\n\n    if train_transform is None:\n        # this is the default transform if more specific ones aren't defined.\n        train_transform = transforms.Compose(\n            [\n                transforms.RandomHorizontalFlip(),\n                transforms.RandomVerticalFlip(),\n                transforms.RandomRotation(20),\n                transforms.ColorJitter(brightness=32.0 / 255.0, saturation=0.5),\n                transforms.Resize([256, 256]),\n                transforms.ToTensor(),\n                transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0)),\n            ]\n        )\n    if val_transform is None:\n        val_transform = val_test_transform\n    if test_transform is None:\n        test_transform = val_test_transform\n\n    train_ds: TensorDataset = construct_skin_cancer_tensor_dataset(train_data, transform=train_transform)\n    valid_ds: TensorDataset = construct_skin_cancer_tensor_dataset(valid_data, transform=val_transform)\n    test_ds: TensorDataset = construct_skin_cancer_tensor_dataset(test_data, transform=test_transform)\n\n    if sampler is not None:\n        train_ds = sampler.subsample(train_ds)\n        valid_ds = sampler.subsample(valid_ds)\n        test_ds = sampler.subsample(test_ds)\n\n    if dataset_converter is not None:\n        train_ds = dataset_converter.convert_dataset(train_ds)\n        valid_ds = dataset_converter.convert_dataset(valid_ds)\n        test_ds = dataset_converter.convert_dataset(test_ds)\n\n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    validation_loader = DataLoader(valid_ds, batch_size=batch_size)\n    test_loader = DataLoader(test_ds, batch_size=batch_size)\n\n    num_examples = {\"train_set\": len(train_ds), \"validation_set\": len(valid_ds), \"test_set\": len(test_ds)}\n\n    return train_loader, validation_loader, test_loader, num_examples\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin","title":"<code>preprocess_skin</code>","text":"<p>The following code is adapted from the <code>preprocess_skin.py</code> script from the medical_federated GitHub repository by Seongjun Yang et al.</p> <p>Paper: https://arxiv.org/abs/2207.03075 Code: https://github.com/wns823/medical_federated.git - medical_federated/skin_cancer_federated/preprocess_skin.py</p>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.save_to_json","title":"<code>save_to_json(data, path)</code>","text":"<p>Saves a dictionary to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>A dictionary to save.</p> required <code>path</code> <code>str</code> <p>The file path to save the JSON data.</p> required Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def save_to_json(data: dict[str, Any], path: str) -&gt; None:\n    \"\"\"\n    Saves a dictionary to a JSON file.\n\n    Args:\n        data: A dictionary to save.\n        path: The file path to save the JSON data.\n    \"\"\"\n    with open(path, \"w\", encoding=\"utf-8\") as file:\n        json.dump(data, file, indent=\"\\t\")\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.process_client_data","title":"<code>process_client_data(dataframe, client_name, data_path, image_path_func, label_map_func, original_columns, official_columns)</code>","text":"<p>Processes and saves the client-specific dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>The dataframe containing the client data.</p> required <code>client_name</code> <code>str</code> <p>The name of the client.</p> required <code>data_path</code> <code>str</code> <p>The base path to the dataset.</p> required <code>image_path_func</code> <code>Callable[[Series], str]</code> <p>A function that constructs the image path from a dataframe row.</p> required <code>label_map_func</code> <code>Callable[[Series], str]</code> <p>A function that maps the original label to the new label.</p> required <code>original_columns</code> <code>list[str]</code> <p>The list of original columns for the dataset.</p> required <code>official_columns</code> <code>list[str]</code> <p>The list of official columns for the dataset.</p> required Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def process_client_data(\n    dataframe: pd.DataFrame,\n    client_name: str,\n    data_path: str,\n    image_path_func: Callable[[pd.Series], str],\n    label_map_func: Callable[[pd.Series], str],\n    original_columns: list[str],\n    official_columns: list[str],\n) -&gt; None:\n    \"\"\"\n    Processes and saves the client-specific dataset.\n\n    Args:\n        dataframe: The dataframe containing the client data.\n        client_name: The name of the client.\n        data_path: The base path to the dataset.\n        image_path_func: A function that constructs the image path from a dataframe row.\n        label_map_func: A function that maps the original label to the new label.\n        original_columns: The list of original columns for the dataset.\n        official_columns: The list of official columns for the dataset.\n    \"\"\"\n    preprocessed_data: dict[str, Any] = {\n        \"columns\": official_columns,\n        \"original_columns\": original_columns,\n        \"data\": [],\n    }\n\n    for i in range(len(dataframe)):\n        row = dataframe.iloc[i]\n        img_path = image_path_func(row)\n        label = label_map_func(row)\n        origin_labels = [0] * len(original_columns)\n        extended_labels = [0] * len(official_columns)\n        origin_labels[original_columns.index(label)] = 1\n        extended_labels[official_columns.index(label)] = 1\n        preprocessed_data[\"data\"].append(\n            {\n                \"img_path\": img_path,\n                \"origin_labels\": origin_labels,\n                \"extended_labels\": extended_labels,\n            }\n        )\n\n    save_to_json(preprocessed_data, os.path.join(data_path, f\"{client_name}.json\"))\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.preprocess_isic_2019","title":"<code>preprocess_isic_2019(data_path, official_columns)</code>","text":"<p>Preprocesses the ISIC 2019 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The base path to the dataset.</p> required <code>official_columns</code> <code>list[str]</code> <p>The list of official columns for the dataset.</p> required Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def preprocess_isic_2019(data_path: str, official_columns: list[str]) -&gt; None:\n    \"\"\"\n    Preprocesses the ISIC 2019 dataset.\n\n    Args:\n        data_path (str): The base path to the dataset.\n        official_columns (list[str]): The list of official columns for the dataset.\n    \"\"\"\n    isic_2019_path = os.path.join(data_path, \"ISIC_2019\")\n    isic_csv_path = os.path.join(isic_2019_path, \"ISIC_2019_Training_GroundTruth.csv\")\n    isic_df = pd.read_csv(isic_csv_path)\n\n    isic_meta = pd.read_csv(os.path.join(isic_2019_path, \"ISIC_2019_Training_Metadata.csv\"))\n    barcelona_list = [i for i in isic_meta[\"lesion_id\"].dropna() if \"BCN\" in i]\n    barcelona_core = isic_meta[isic_meta[\"lesion_id\"].isin(barcelona_list)]\n    core_2019 = isic_df[isic_df[\"image\"].isin(barcelona_core[\"image\"])]\n    core_2019.to_csv(os.path.join(isic_2019_path, \"ISIC_2019_core.csv\"), mode=\"w\")\n\n    isic_2019_data_path = os.path.join(data_path, \"ISIC_2019\", \"ISIC_2019_Training_Input\")\n    barcelona_df = pd.read_csv(os.path.join(isic_2019_path, \"ISIC_2019_core.csv\"))\n    barcelona_new = barcelona_df[[\"image\"] + official_columns + [\"UNK\"]]\n    preprocessed_data: dict[str, Any] = {\n        \"columns\": official_columns,\n        \"original_columns\": official_columns,\n        \"data\": [],\n    }\n\n    for i in range(len(barcelona_new)):\n        # Extract the row values, leaving off the last element (\"UNK\" column)\n        temp = list(barcelona_new.loc[i].values[:-1])\n        img_path = os.path.join(isic_2019_data_path, temp[0] + \".jpg\")\n        origin_labels = temp[1:]\n        extended_labels = temp[1:]\n        preprocessed_data[\"data\"].append(\n            {\n                \"img_path\": img_path,\n                \"origin_labels\": origin_labels,\n                \"extended_labels\": extended_labels,\n            }\n        )\n\n    save_to_json(preprocessed_data, os.path.join(data_path, \"ISIC_2019\", \"ISIC_19_Barcelona.json\"))\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.ham_image_path_func","title":"<code>ham_image_path_func(row)</code>","text":"<p>Constructs the image path for the HAM10000 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row from the dataframe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The constructed image path.</p> Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def ham_image_path_func(row: pd.Series) -&gt; str:\n    \"\"\"\n    Constructs the image path for the HAM10000 dataset.\n\n    Args:\n        row (pd.Series): A row from the dataframe.\n\n    Returns:\n        (str): The constructed image path.\n    \"\"\"\n    return os.path.join(\"fl4health\", \"datasets\", \"skin_cancer\", \"HAM10000\", row[\"image_id\"] + \".jpg\")\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.ham_label_map_func","title":"<code>ham_label_map_func(row)</code>","text":"<p>Maps the original label to the new label for the HAM10000 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row from the dataframe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The mapped label.</p> Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def ham_label_map_func(row: pd.Series) -&gt; str:\n    \"\"\"\n    Maps the original label to the new label for the HAM10000 dataset.\n\n    Args:\n        row (pd.Series): A row from the dataframe.\n\n    Returns:\n        (str): The mapped label.\n    \"\"\"\n    ham_labelmap = {\n        \"akiec\": \"AK\",\n        \"bcc\": \"BCC\",\n        \"bkl\": \"BKL\",\n        \"df\": \"DF\",\n        \"mel\": \"MEL\",\n        \"nv\": \"NV\",\n        \"vasc\": \"VASC\",\n    }\n    return ham_labelmap[row[\"dx\"]]\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.preprocess_ham10000","title":"<code>preprocess_ham10000(data_path, official_columns)</code>","text":"<p>Preprocesses the HAM10000 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The base path to the dataset.</p> required <code>official_columns</code> <code>list[str]</code> <p>The list of official columns for the dataset.</p> required Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def preprocess_ham10000(data_path: str, official_columns: list[str]) -&gt; None:\n    \"\"\"\n    Preprocesses the HAM10000 dataset.\n\n    Args:\n        data_path (str): The base path to the dataset.\n        official_columns (list[str]): The list of official columns for the dataset.\n    \"\"\"\n    ham_10000_path = os.path.join(data_path, \"HAM10000\")\n    ham_csv_path = os.path.join(ham_10000_path, \"HAM10000_metadata\")\n    ham_df = pd.read_csv(ham_csv_path)\n\n    rosendahl_data = ham_df[ham_df[\"dataset\"] == \"rosendahl\"]\n    rosendahl_data.to_csv(os.path.join(ham_10000_path, \"HAM_rosendahl.csv\"), mode=\"w\")\n    vienna_data = ham_df[ham_df[\"dataset\"] != \"rosendahl\"]\n    vienna_data.to_csv(os.path.join(ham_10000_path, \"HAM_vienna.csv\"), mode=\"w\")\n\n    ham_columns = [\"MEL\", \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\"]\n\n    process_client_data(\n        pd.read_csv(os.path.join(ham_10000_path, \"HAM_rosendahl.csv\")),\n        \"HAM_rosendahl\",\n        ham_10000_path,\n        ham_image_path_func,\n        ham_label_map_func,\n        ham_columns,\n        official_columns,\n    )\n    process_client_data(\n        pd.read_csv(os.path.join(ham_10000_path, \"HAM_vienna.csv\")),\n        \"HAM_vienna\",\n        ham_10000_path,\n        ham_image_path_func,\n        ham_label_map_func,\n        ham_columns,\n        official_columns,\n    )\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.pad_image_path_func","title":"<code>pad_image_path_func(row)</code>","text":"<p>Constructs the image path for the PAD-UFES-20 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row from the dataframe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The constructed image path.</p> Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def pad_image_path_func(row: pd.Series) -&gt; str:\n    \"\"\"\n    Constructs the image path for the PAD-UFES-20 dataset.\n\n    Args:\n        row (pd.Series): A row from the dataframe.\n\n    Returns:\n        (str): The constructed image path.\n    \"\"\"\n    return os.path.join(\"fl4health\", \"datasets\", \"skin_cancer\", \"PAD-UFES-20\", row[\"img_id\"])\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.pad_label_map_func","title":"<code>pad_label_map_func(row)</code>","text":"<p>Maps the original label to the new label for the PAD-UFES-20 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row from the dataframe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The mapped label.</p> Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def pad_label_map_func(row: pd.Series) -&gt; str:\n    \"\"\"\n    Maps the original label to the new label for the PAD-UFES-20 dataset.\n\n    Args:\n        row (pd.Series): A row from the dataframe.\n\n    Returns:\n        (str): The mapped label.\n    \"\"\"\n    pad_ufes_20_labelmap = {\n        \"ACK\": \"AK\",\n        \"BCC\": \"BCC\",\n        \"MEL\": \"MEL\",\n        \"NEV\": \"NV\",\n        \"SCC\": \"SCC\",\n        \"SEK\": \"BKL\",\n    }\n    return pad_ufes_20_labelmap[row[\"diagnostic\"]]\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.preprocess_pad_ufes_20","title":"<code>preprocess_pad_ufes_20(data_path, official_columns)</code>","text":"<p>Preprocesses the PAD-UFES-20 dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The base path to the dataset.</p> required <code>official_columns</code> <code>list[str]</code> <p>The list of official columns for the dataset.</p> required Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def preprocess_pad_ufes_20(data_path: str, official_columns: list[str]) -&gt; None:\n    \"\"\"\n    Preprocesses the PAD-UFES-20 dataset.\n\n    Args:\n        data_path (str): The base path to the dataset.\n        official_columns (list[str]): The list of official columns for the dataset.\n    \"\"\"\n    pad_ufes_20_path = os.path.join(data_path, \"PAD-UFES-20\")\n    pad_ufes_20_csv_path = os.path.join(pad_ufes_20_path, \"metadata.csv\")\n    pad_ufes_20_df = pd.read_csv(pad_ufes_20_csv_path)\n\n    pad_columns = [\"MEL\", \"NV\", \"BCC\", \"AK\", \"BKL\", \"SCC\"]\n\n    process_client_data(\n        pad_ufes_20_df,\n        \"PAD_UFES_20\",\n        pad_ufes_20_path,\n        pad_image_path_func,\n        pad_label_map_func,\n        pad_columns,\n        official_columns,\n    )\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.derm7pt_image_path_func","title":"<code>derm7pt_image_path_func(row)</code>","text":"<p>Constructs the image path for the Derm7pt dataset.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row from the dataframe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The constructed image path.</p> Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def derm7pt_image_path_func(row: pd.Series) -&gt; str:\n    \"\"\"\n    Constructs the image path for the Derm7pt dataset.\n\n    Args:\n        row (pd.Series): A row from the dataframe.\n\n    Returns:\n        (str):  The constructed image path.\n    \"\"\"\n    return os.path.join(\"fl4health\", \"datasets\", \"skin_cancer\", \"Derm7pt\", \"images\", row[\"derm\"])\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.derm7pt_label_map_func","title":"<code>derm7pt_label_map_func(row)</code>","text":"<p>Maps the original label to the new label for the Derm7pt dataset.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row from the dataframe.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The mapped label.</p> Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def derm7pt_label_map_func(row: pd.Series) -&gt; str:\n    \"\"\"\n    Maps the original label to the new label for the Derm7pt dataset.\n\n    Args:\n        row (pd.Series): A row from the dataframe.\n\n    Returns:\n        (str):  The mapped label.\n    \"\"\"\n    derm7pt_labelmap = {\n        \"basal cell carcinoma\": \"BCC\",\n        \"blue nevus\": \"NV\",\n        \"clark nevus\": \"NV\",\n        \"combined nevus\": \"NV\",\n        \"congenital nevus\": \"NV\",\n        \"dermal nevus\": \"NV\",\n        \"dermatofibroma\": \"DF\",  # MISC\n        \"lentigo\": \"MISC\",\n        \"melanoma\": \"MEL\",\n        \"melanoma (0.76 to 1.5 mm)\": \"MEL\",\n        \"melanoma (in situ)\": \"MEL\",\n        \"melanoma (less than 0.76 mm)\": \"MEL\",\n        \"melanoma (more than 1.5 mm)\": \"MEL\",\n        \"melanoma metastasis\": \"MEL\",\n        \"melanosis\": \"MISC\",\n        \"miscellaneous\": \"MISC\",\n        \"recurrent nevus\": \"NV\",\n        \"reed or spitz nevus\": \"NV\",\n        \"seborrheic keratosis\": \"BKL\",\n        \"vascular lesion\": \"VASC\",  # MISC\n    }\n    return derm7pt_labelmap[row[\"diagnosis\"]]\n</code></pre>"},{"location":"api/#fl4health.datasets.skin_cancer.preprocess_skin.preprocess_derm7pt","title":"<code>preprocess_derm7pt(data_path, official_columns)</code>","text":"<p>Preprocesses the Derm7pt dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>The base path to the dataset.</p> required <code>official_columns</code> <code>list[str]</code> <p>The list of official columns for the dataset.</p> required Source code in <code>fl4health/datasets/skin_cancer/preprocess_skin.py</code> <pre><code>def preprocess_derm7pt(data_path: str, official_columns: list[str]) -&gt; None:\n    \"\"\"\n    Preprocesses the Derm7pt dataset.\n\n    Args:\n        data_path (str): The base path to the dataset.\n        official_columns (list[str]): The list of official columns for the dataset.\n    \"\"\"\n    derm7pt_path = os.path.join(data_path, \"Derm7pt\")\n    derm7pt_df = pd.read_csv(os.path.join(derm7pt_path, \"meta\", \"meta_core.csv\"))\n\n    derm7pt_columns = [\"MEL\", \"NV\", \"BCC\", \"BKL\", \"DF\", \"VASC\"]\n\n    process_client_data(\n        derm7pt_df,\n        \"Derm7pt\",\n        derm7pt_path,\n        derm7pt_image_path_func,\n        derm7pt_label_map_func,\n        derm7pt_columns,\n        official_columns,\n    )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment","title":"<code>feature_alignment</code>","text":""},{"location":"api/#fl4health.feature_alignment.feature_type_extraction","title":"<code>feature_type_extraction</code>","text":"<p>Largely taken from https://github.com/VectorInstitute/cyclops.</p>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.FeatureMeta","title":"<code>FeatureMeta</code>","text":"Source code in <code>fl4health/feature_alignment/feature_type_extraction.py</code> <pre><code>class FeatureMeta:\n    def __init__(self, **kwargs: Any) -&gt; None:\n        \"\"\"Feature metadata class.\"\"\"\n        # Feature type checking\n        if FEATURE_TYPE_ATTR not in kwargs:\n            raise ValueError(\"Must specify feature type.\")\n\n        if kwargs[FEATURE_TYPE_ATTR] not in FEATURE_TYPES:\n            all_feature_types = \", \".join([types.value for types in FEATURE_TYPES])\n            raise ValueError(\n                f\"Feature type '{kwargs[FEATURE_TYPE_ATTR]}'\\nnot in {all_feature_types}.\",\n            )\n\n        # Set attributes\n        for attr in FEATURE_META_ATTRS:\n            if attr in kwargs:\n                setattr(self, attr, kwargs[attr])\n            else:\n                setattr(self, attr, FEATURE_META_ATTR_DEFAULTS[attr])\n\n        # Check for invalid parameters\n        invalid_params = [kwarg for kwarg in kwargs if kwarg not in FEATURE_META_ATTRS]\n        if len(invalid_params) &gt; 0:\n            raise ValueError(\n                f\"Invalid feature meta parameters {', '.join(invalid_params)}.\",\n            )\n\n    def get_type(self) -&gt; FeatureType:\n        \"\"\"\n        Get the feature type.\n\n        Returns:\n            (str):  Feature type.\n        \"\"\"\n        return FeatureType(getattr(self, FEATURE_TYPE_ATTR))\n\n    def update(self, meta: list[tuple[str, Any]]) -&gt; None:\n        \"\"\"\n        Update meta attributes.\n\n        Args:\n            meta (list[tuple[str, Any]]): List of tuples in the format (attribute name, attribute value).\n        \"\"\"\n        for info in meta:\n            setattr(self, *info)\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.FeatureMeta.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Feature metadata class.</p> Source code in <code>fl4health/feature_alignment/feature_type_extraction.py</code> <pre><code>def __init__(self, **kwargs: Any) -&gt; None:\n    \"\"\"Feature metadata class.\"\"\"\n    # Feature type checking\n    if FEATURE_TYPE_ATTR not in kwargs:\n        raise ValueError(\"Must specify feature type.\")\n\n    if kwargs[FEATURE_TYPE_ATTR] not in FEATURE_TYPES:\n        all_feature_types = \", \".join([types.value for types in FEATURE_TYPES])\n        raise ValueError(\n            f\"Feature type '{kwargs[FEATURE_TYPE_ATTR]}'\\nnot in {all_feature_types}.\",\n        )\n\n    # Set attributes\n    for attr in FEATURE_META_ATTRS:\n        if attr in kwargs:\n            setattr(self, attr, kwargs[attr])\n        else:\n            setattr(self, attr, FEATURE_META_ATTR_DEFAULTS[attr])\n\n    # Check for invalid parameters\n    invalid_params = [kwarg for kwarg in kwargs if kwarg not in FEATURE_META_ATTRS]\n    if len(invalid_params) &gt; 0:\n        raise ValueError(\n            f\"Invalid feature meta parameters {', '.join(invalid_params)}.\",\n        )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.FeatureMeta.get_type","title":"<code>get_type()</code>","text":"<p>Get the feature type.</p> <p>Returns:</p> Type Description <code>str</code> <p>Feature type.</p> Source code in <code>fl4health/feature_alignment/feature_type_extraction.py</code> <pre><code>def get_type(self) -&gt; FeatureType:\n    \"\"\"\n    Get the feature type.\n\n    Returns:\n        (str):  Feature type.\n    \"\"\"\n    return FeatureType(getattr(self, FEATURE_TYPE_ATTR))\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.FeatureMeta.update","title":"<code>update(meta)</code>","text":"<p>Update meta attributes.</p> <p>Parameters:</p> Name Type Description Default <code>meta</code> <code>list[tuple[str, Any]]</code> <p>List of tuples in the format (attribute name, attribute value).</p> required Source code in <code>fl4health/feature_alignment/feature_type_extraction.py</code> <pre><code>def update(self, meta: list[tuple[str, Any]]) -&gt; None:\n    \"\"\"\n    Update meta attributes.\n\n    Args:\n        meta (list[tuple[str, Any]]): List of tuples in the format (attribute name, attribute value).\n    \"\"\"\n    for info in meta:\n        setattr(self, *info)\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.Features","title":"<code>Features</code>","text":"Source code in <code>fl4health/feature_alignment/feature_type_extraction.py</code> <pre><code>class Features:\n    def __init__(\n        self,\n        data: pd.DataFrame,\n        features: str | list[str],\n        by: str | list[str] | None = None,\n        targets: str | list[str] | None = None,\n        force_types: dict[str, FeatureType] | None = None,\n    ):\n        \"\"\"\n        Features.\n\n        Args:\n            data (pd.DataFrame): Features data.\n            features (str | list[str]): List of feature columns. The remaining columns are treated as metadata.\n            by (str | list[str] | None, optional): Columns to groupby during processing, affecting how the features\n                are treated. Defaults to None.\n            targets (str | list[str] | None, optional): Column names to specify as target features. Defaults to None.\n            force_types (dict[str, FeatureType] | None, optional): Mapping of column names to type. These columns are\n                forced to be of the specified type. Defaults to None.\n        \"\"\"\n        # Check data\n        if not isinstance(data, pd.DataFrame):\n            raise ValueError(\"Feature data must be a pandas.DataFrame.\")\n\n        target_list = [] if targets is None else to_list(targets)\n        feature_list = to_list(features)\n        if len(feature_list) == 0:\n            raise ValueError(\"Must specify at least one feature.\")\n\n        has_columns(data, feature_list, raise_error=True)\n        has_columns(data, target_list, raise_error=True)\n\n        self.by_ = [] if by is None else to_list(by)\n        if len(self.by_) &gt; 0:\n            has_columns(data, self.by_, raise_error=True)\n            if len(set(self.by_).intersection(set(feature_list))) != 0:\n                raise ValueError(\"Columns in 'by' cannot be considered features.\")\n\n        # Add targets to the list of features if they were not included\n        self.features = list(set(feature_list + target_list))\n        self.data = data\n        self.meta: dict[str, FeatureMeta] = {}\n        self._infer_feature_types(force_types=force_types)\n\n    @property\n    def types(self) -&gt; dict[str, FeatureType]:\n        \"\"\"\n        Access as attribute, feature type names.\n\n        NOTE: These are framework-specific feature names.\n\n        Returns:\n            (dict[str, str]): Feature type mapped for each feature.\n        \"\"\"\n        return {name: meta.get_type() for name, meta in self.meta.items()}\n\n    def _update_meta(self, meta_update: dict[str, dict[str, Any]]) -&gt; None:\n        \"\"\"\n        Update feature metadata.\n\n        Args:\n            meta_update (dict[str, dict[str, Any]]): A dictionary in which the values will add/update\n                the existing feature metadata dictionary.\n        \"\"\"\n        for col, info in meta_update.items():\n            if col in self.meta:\n                self.meta[col].update(list(info.items()))\n            else:\n                self.meta[col] = FeatureMeta(**info)\n\n    def _to_feature_types(\n        self, data: pd.DataFrame, new_types: dict[str, FeatureType], inplace: bool = True\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert feature types.\n\n        Args:\n            data (pd.DataFrame): Features data.\n            new_types (dict[str, FeatureType]): A map from the feature name to the new feature type.\n            inplace (bool, optional): Whether to perform in-place, or to simply return the DataFrame. Defaults to True.\n\n        Raises:\n            ValueError: Unrecognized features\n            ValueError: When conversion fails\n\n        Returns:\n            (pd.DataFrame): The features data with the relevant conversions.\n        \"\"\"\n        invalid = set(new_types.keys()) - set(self.features)\n        if len(invalid) &gt; 0:\n            raise ValueError(f\"Unrecognized features: {', '.join(invalid)}\")\n        for col, new_type in new_types.items():\n            if col in self.meta and inplace and new_type == FeatureType.CATEGORICAL_INDICATOR:\n                raise ValueError(\n                    f\"Cannot convert {col} to binary categorical indicators.\",\n                )\n        data, meta = to_types(data, new_types)\n        if inplace:\n            # Append any new indicator features\n            for col, fmeta in meta.items():\n                if FEATURE_INDICATOR_ATTR in fmeta:\n                    self.features.append(col)\n\n            self.data = data\n            self._update_meta(meta)\n\n        return data\n\n    def _infer_feature_types(self, force_types: dict[str, FeatureType] | None = None) -&gt; None:\n        \"\"\"\n        Infer feature types. Can optionally force certain types on specified features.\n\n        Args:\n            force_types (dict[str, FeatureType] | None, optional): A map from the feature name to the forced feature\n                type. Defaults to None.\n        \"\"\"\n        if force_types is None:\n            force_types = {}\n        infer_features = to_list(\n            set(self.features) - set(to_list(force_types)),\n        )\n        new_types = infer_types(self.data, infer_features)\n        # Force certain features to be specific types\n        if force_types is not None:\n            for feature, type_ in force_types.items():\n                new_types[feature] = type_\n\n        self._to_feature_types(self.data, new_types)\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.Features.types","title":"<code>types</code>  <code>property</code>","text":"<p>Access as attribute, feature type names.</p> <p>NOTE: These are framework-specific feature names.</p> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Feature type mapped for each feature.</p>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.Features.__init__","title":"<code>__init__(data, features, by=None, targets=None, force_types=None)</code>","text":"<p>Features.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Features data.</p> required <code>features</code> <code>str | list[str]</code> <p>List of feature columns. The remaining columns are treated as metadata.</p> required <code>by</code> <code>str | list[str] | None</code> <p>Columns to groupby during processing, affecting how the features are treated. Defaults to None.</p> <code>None</code> <code>targets</code> <code>str | list[str] | None</code> <p>Column names to specify as target features. Defaults to None.</p> <code>None</code> <code>force_types</code> <code>dict[str, FeatureType] | None</code> <p>Mapping of column names to type. These columns are forced to be of the specified type. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/feature_alignment/feature_type_extraction.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    features: str | list[str],\n    by: str | list[str] | None = None,\n    targets: str | list[str] | None = None,\n    force_types: dict[str, FeatureType] | None = None,\n):\n    \"\"\"\n    Features.\n\n    Args:\n        data (pd.DataFrame): Features data.\n        features (str | list[str]): List of feature columns. The remaining columns are treated as metadata.\n        by (str | list[str] | None, optional): Columns to groupby during processing, affecting how the features\n            are treated. Defaults to None.\n        targets (str | list[str] | None, optional): Column names to specify as target features. Defaults to None.\n        force_types (dict[str, FeatureType] | None, optional): Mapping of column names to type. These columns are\n            forced to be of the specified type. Defaults to None.\n    \"\"\"\n    # Check data\n    if not isinstance(data, pd.DataFrame):\n        raise ValueError(\"Feature data must be a pandas.DataFrame.\")\n\n    target_list = [] if targets is None else to_list(targets)\n    feature_list = to_list(features)\n    if len(feature_list) == 0:\n        raise ValueError(\"Must specify at least one feature.\")\n\n    has_columns(data, feature_list, raise_error=True)\n    has_columns(data, target_list, raise_error=True)\n\n    self.by_ = [] if by is None else to_list(by)\n    if len(self.by_) &gt; 0:\n        has_columns(data, self.by_, raise_error=True)\n        if len(set(self.by_).intersection(set(feature_list))) != 0:\n            raise ValueError(\"Columns in 'by' cannot be considered features.\")\n\n    # Add targets to the list of features if they were not included\n    self.features = list(set(feature_list + target_list))\n    self.data = data\n    self.meta: dict[str, FeatureMeta] = {}\n    self._infer_feature_types(force_types=force_types)\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.TabularFeatures","title":"<code>TabularFeatures</code>","text":"<p>               Bases: <code>Features</code></p> Source code in <code>fl4health/feature_alignment/feature_type_extraction.py</code> <pre><code>class TabularFeatures(Features):\n    def __init__(\n        self,\n        data: pd.DataFrame,\n        features: str | list[str],\n        by: str,\n        targets: str | list[str] | None = None,\n        force_types: dict[str, FeatureType] | None = None,\n    ):\n        \"\"\"\n        Tabular features.\n\n        Args:\n            data (pd.DataFrame): Data for the table\n            features (str | list[str]): List of feature columns. The remaining columns are treated as metadata.\n            by (str): Columns to groupby during processing, affecting how the features are treated.\n            targets (str | list[str] | None, optional): Column names to specify as target features.\n                Defaults to None.\n            force_types (dict[str, FeatureType] | None, optional): Mapping of column names to type. These columns are\n                forced to be of the specified type. Defaults to None.\n\n        Raises:\n            ValueError: Tabular features index input as a string representing a column\n        \"\"\"\n        if not isinstance(by, str):\n            raise ValueError(\n                \"Tabular features index input as a string representing a column.\",\n            )\n\n        super().__init__(\n            data,\n            features,\n            by,\n            targets=targets,\n            force_types=force_types,\n        )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.TabularFeatures.__init__","title":"<code>__init__(data, features, by, targets=None, force_types=None)</code>","text":"<p>Tabular features.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Data for the table</p> required <code>features</code> <code>str | list[str]</code> <p>List of feature columns. The remaining columns are treated as metadata.</p> required <code>by</code> <code>str</code> <p>Columns to groupby during processing, affecting how the features are treated.</p> required <code>targets</code> <code>str | list[str] | None</code> <p>Column names to specify as target features. Defaults to None.</p> <code>None</code> <code>force_types</code> <code>dict[str, FeatureType] | None</code> <p>Mapping of column names to type. These columns are forced to be of the specified type. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Tabular features index input as a string representing a column</p> Source code in <code>fl4health/feature_alignment/feature_type_extraction.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    features: str | list[str],\n    by: str,\n    targets: str | list[str] | None = None,\n    force_types: dict[str, FeatureType] | None = None,\n):\n    \"\"\"\n    Tabular features.\n\n    Args:\n        data (pd.DataFrame): Data for the table\n        features (str | list[str]): List of feature columns. The remaining columns are treated as metadata.\n        by (str): Columns to groupby during processing, affecting how the features are treated.\n        targets (str | list[str] | None, optional): Column names to specify as target features.\n            Defaults to None.\n        force_types (dict[str, FeatureType] | None, optional): Mapping of column names to type. These columns are\n            forced to be of the specified type. Defaults to None.\n\n    Raises:\n        ValueError: Tabular features index input as a string representing a column\n    \"\"\"\n    if not isinstance(by, str):\n        raise ValueError(\n            \"Tabular features index input as a string representing a column.\",\n        )\n\n    super().__init__(\n        data,\n        features,\n        by,\n        targets=targets,\n        force_types=force_types,\n    )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.to_list","title":"<code>to_list(obj)</code>","text":"<p>Convert some object to a list of object(s) unless already one.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to convert to a list.</p> required <p>Returns:</p> Type Description <code>list[Any]</code> <p>The processed object.</p> Source code in <code>fl4health/feature_alignment/feature_type_extraction.py</code> <pre><code>def to_list(obj: Any) -&gt; list[Any]:\n    \"\"\"\n    Convert some object to a list of object(s) unless already one.\n\n    Args:\n        obj (Any): The object to convert to a list.\n\n    Returns:\n        (list[Any]): The processed object.\n    \"\"\"\n    if isinstance(obj, list):\n        return obj\n\n    if isinstance(obj, (np.ndarray, set, dict)):\n        return list(obj)\n\n    return [obj]\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.feature_type_extraction.has_columns","title":"<code>has_columns(data, cols, exactly=False, raise_error=False)</code>","text":"<p>Check if data has required columns for processing.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame to check.</p> required <code>cols</code> <code>str | list[str]</code> <p>List of column names that must be present in data.</p> required <code>exactly</code> <code>bool</code> <p>Whether columns need to be an exact match. Defaults to False.</p> <code>False</code> <code>raise_error</code> <code>bool</code> <p>Whether to raise a ValueError if there are missing columns. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Missing required columns.</p> <code>ValueError</code> <p>Must have exactly the columns, will throw if not and exactly is True.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if all required columns are present, otherwise False.</p> Source code in <code>fl4health/feature_alignment/feature_type_extraction.py</code> <pre><code>def has_columns(data: pd.DataFrame, cols: str | list[str], exactly: bool = False, raise_error: bool = False) -&gt; bool:\n    \"\"\"\n    Check if data has required columns for processing.\n\n    Args:\n        data (pd.DataFrame): DataFrame to check.\n        cols (str | list[str]): List of column names that must be present in data.\n        exactly (bool, optional): Whether columns need to be an exact match. Defaults to False.\n        raise_error (bool, optional): Whether to raise a ValueError if there are missing columns. Defaults to False.\n\n    Raises:\n        ValueError: Missing required columns.\n        ValueError: Must have exactly the columns, will throw if not and exactly is True.\n\n    Returns:\n        (bool): True if all required columns are present, otherwise False.\n    \"\"\"\n    cols = to_list(cols)\n    required_set = set(cols)\n    columns = set(data.columns)\n    present = required_set.issubset(columns)\n\n    if not present and raise_error:\n        missing = required_set - columns\n        raise ValueError(f\"Missing required columns: {', '.join(missing)}.\")\n\n    if exactly:\n        exact = present and len(data.columns) == len(cols)\n        if not exact and raise_error:\n            raise ValueError(f\"Must have exactly the columns: {', '.join(cols)}.\")\n\n    return present\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.handle_types","title":"<code>handle_types</code>","text":"<p>Largely taken from https://github.com/VectorInstitute/cyclops.</p>"},{"location":"api/#fl4health.feature_alignment.handle_types.convertible_to_type","title":"<code>convertible_to_type(series, type, unique=None, raise_error=False)</code>","text":"<p>Check whether a feature can be converted to some type.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>Feature data.</p> required <code>type</code> <code>FeatureType</code> <p>Feature type name to check for conversion.</p> required <code>unique</code> <code>ndarray | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>raise_error</code> <code>bool</code> <p>Unique values which can be optionally specified. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Supported type has no corresponding datatype</p> <code>ValueError</code> <p>Cannot convert series to the provided type and <code>raise_error</code> is true.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the feature can be converted.</p> Source code in <code>fl4health/feature_alignment/handle_types.py</code> <pre><code>def convertible_to_type(\n    series: pd.Series, type: FeatureType, unique: np.ndarray | None = None, raise_error: bool = False\n) -&gt; bool:\n    \"\"\"\n    Check whether a feature can be converted to some type.\n\n    Args:\n        series (pd.Series): Feature data.\n        type (FeatureType): Feature type name to check for conversion.\n        unique (np.ndarray | None, optional): _description_. Defaults to None.\n        raise_error (bool, optional): Unique values which can be optionally specified. Defaults to False.\n\n    Raises:\n        ValueError: Supported type has no corresponding datatype\n        ValueError: Cannot convert series to the provided type and ``raise_error`` is true.\n\n    Returns:\n        (bool): Whether the feature can be converted.\n    \"\"\"\n    if type == FeatureType.NUMERIC:\n        convertible = _convertible_to_numeric(series)\n\n    elif type == FeatureType.STRING:\n        convertible = True\n\n    elif type == FeatureType.BINARY:\n        convertible = _convertible_to_binary(series, unique=unique)\n\n    elif type == FeatureType.ORDINAL:\n        convertible = _convertible_to_ordinal(series, unique=unique)\n\n    elif type == FeatureType.CATEGORICAL_INDICATOR:\n        convertible = _convertible_to_categorical_indicators(series, unique=unique)\n\n    elif valid_feature_type(type, raise_error=True):\n        # Check first if the type is valid, if so, then it isn't supported here.\n        raise ValueError(\"Supported type has no corresponding datatype.\")\n\n    if raise_error and not convertible:\n        raise ValueError(f\"Cannot convert series {series.name} to type {type}.\")\n\n    return convertible\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.handle_types.get_unique","title":"<code>get_unique(values, unique=None)</code>","text":"<p>Get the unique values of pandas series.</p> <p>The utility of this function comes from checking whether the unique values have already been calculated. This function assumes that if the unique values are passed, they are correct.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray | Series</code> <p>Values for which to get the unique values.</p> required <code>unique</code> <code>ndarray | None</code> <p>Unique values which can be optionally specified. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The unique values.</p> Source code in <code>fl4health/feature_alignment/handle_types.py</code> <pre><code>def get_unique(values: np.ndarray | pd.Series, unique: np.ndarray | None = None) -&gt; np.ndarray:\n    \"\"\"\n    Get the unique values of pandas series.\n\n    The utility of this function comes from checking whether the unique values have already been calculated. This\n    function assumes that if the unique values are passed, they are correct.\n\n    Args:\n        values (np.ndarray | pd.Series): Values for which to get the unique values.\n        unique (np.ndarray | None, optional): Unique values which can be optionally specified. Defaults to None.\n\n    Returns:\n        (np.ndarray): The unique values.\n    \"\"\"\n    if unique is None:\n        return np.array(values.unique())  # type: ignore\n\n    return unique\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.handle_types.valid_feature_type","title":"<code>valid_feature_type(type, raise_error=True)</code>","text":"<p>Check whether a feature type name is valid.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>FeatureType</code> <p>Feature type name.</p> required <code>raise_error</code> <code>bool</code> <p>Whether to raise an error is the type is invalid. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raise when the type is invalid and <code>raise_error</code> is True</p> <p>Returns:</p> Type Description <code>bool</code> <p>Whether the type is valid.</p> Source code in <code>fl4health/feature_alignment/handle_types.py</code> <pre><code>def valid_feature_type(type: FeatureType, raise_error: bool = True) -&gt; bool:\n    \"\"\"\n    Check whether a feature type name is valid.\n\n    Args:\n        type (FeatureType): Feature type name.\n        raise_error (bool, optional): Whether to raise an error is the type is invalid. Defaults to True.\n\n    Raises:\n        ValueError: Raise when the type is invalid and ``raise_error`` is True\n\n    Returns:\n        (bool): Whether the type is valid.\n    \"\"\"\n    if type in FEATURE_TYPES:\n        return True\n\n    if raise_error:\n        all_feature_types = \", \".join([types.value for types in FEATURE_TYPES])\n        raise ValueError(f\"Feature type '{type.value}' not in {all_feature_types}.\")\n\n    return False\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.handle_types.to_dtype","title":"<code>to_dtype(series, type)</code>","text":"<p>Set the series datatype according to the feature type.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>Feature data.</p> required <code>type</code> <code>FeatureType</code> <p>Feature type name.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The feature with the corresponding datatype.</p> Source code in <code>fl4health/feature_alignment/handle_types.py</code> <pre><code>def to_dtype(series: pd.Series, type: FeatureType) -&gt; pd.Series:\n    \"\"\"\n    Set the series datatype according to the feature type.\n\n    Args:\n        series (pd.Series): Feature data.\n        type (FeatureType): Feature type name.\n\n    Returns:\n        (pd.Series): The feature with the corresponding datatype.\n    \"\"\"\n    dtype = _type_to_dtype(type)\n\n    if dtype is None:\n        return series\n\n    if series.dtype == dtype:\n        return series\n\n    return series.astype(dtype)  # type: ignore\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.handle_types.infer_types","title":"<code>infer_types(data, features)</code>","text":"<p>Infer intended feature types and perform the relevant conversions.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Feature data.</p> required <code>features</code> <code>list[str]</code> <p>Features to consider.</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Feature name to feature type dictionary.</p> Source code in <code>fl4health/feature_alignment/handle_types.py</code> <pre><code>def infer_types(data: pd.DataFrame, features: list[str]) -&gt; dict[str, FeatureType]:\n    \"\"\"\n    Infer intended feature types and perform the relevant conversions.\n\n    Args:\n        data (pd.DataFrame): Feature data.\n        features (list[str]): Features to consider.\n\n    Returns:\n        (dict[str, str]): Feature name to feature type dictionary.\n    \"\"\"\n    new_types = {}\n    for col in features:\n        new_types[col] = _infer_type(data[col])\n\n    return new_types\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.handle_types.to_types","title":"<code>to_types(data, new_types)</code>","text":"<p>Convert features to given types.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Features data.</p> required <code>new_types</code> <code>dict[str, str]</code> <p>Map from the feature column name to its new type.</p> required <p>Returns:</p> Type Description <code>tuple[DataFrame, dict[str, Any]]</code> <p>Tuple of pandas.DataFrame and dict with the updated features data and metadata respectively.</p> Source code in <code>fl4health/feature_alignment/handle_types.py</code> <pre><code>def to_types(data: pd.DataFrame, new_types: dict[str, FeatureType]) -&gt; tuple[pd.DataFrame, dict[str, Any]]:\n    \"\"\"\n    Convert features to given types.\n\n    Args:\n        data (pd.DataFrame): Features data.\n        new_types (dict[str, str]): Map from the feature column name to its new type.\n\n    Returns:\n        (tuple[pd.DataFrame, dict[str, Any]]): Tuple of pandas.DataFrame and dict with the updated features data and\n            metadata respectively.\n    \"\"\"\n    meta = {}\n    for col, new_type in new_types.items():\n        data, fmeta = _to_type(data, col, new_type)\n        meta.update(fmeta)\n\n    return data, meta\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.string_columns_transformer","title":"<code>string_columns_transformer</code>","text":""},{"location":"api/#fl4health.feature_alignment.string_columns_transformer.TextMulticolumnTransformer","title":"<code>TextMulticolumnTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>fl4health/feature_alignment/string_columns_transformer.py</code> <pre><code>class TextMulticolumnTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, transformer: TextFeatureTransformer):\n        \"\"\"\n        The purpose of this class is to enable the application of text feature transformers from sklearn\n        to multiple string columns, which is not supported in the first place.\n\n        Args:\n            transformer (TextFeatureTransformer): Transformer to be applied\n        \"\"\"\n        self.transformer = transformer\n\n    def fit(self, x: pd.DataFrame, y: pd.DataFrame | None = None) -&gt; TextMulticolumnTransformer:\n        \"\"\"\n        Fit the transformer to the provided dataframe. The dataframe should have multiple string columns\n        The transformer is fit on the appended text from all columns in the ``x`` dataframe.\n\n        Args:\n            x (pd.DataFrame): Columns on which to fit the transformer\n            y (pd.DataFrame | None, optional): Not used. Defaults to None.\n\n        Returns:\n            (TextMulticolumnTransformer): The fit transformer\n        \"\"\"\n        joined_x = x.apply(\" \".join, axis=1)\n        self.transformer.fit(joined_x)\n        return self\n\n    def transform(self, x: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transforms the concatenation of all columns of text in the ``x`` dataframe.\n\n        Args:\n            x (pd.DataFrame): Dataframe of text-based columns to be transformed\n\n        Returns:\n            (pd.DataFrame): Transformed dataframe.\n        \"\"\"\n        joined_x = x.apply(\" \".join, axis=1)\n        return self.transformer.transform(joined_x)\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.string_columns_transformer.TextMulticolumnTransformer.__init__","title":"<code>__init__(transformer)</code>","text":"<p>The purpose of this class is to enable the application of text feature transformers from sklearn to multiple string columns, which is not supported in the first place.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>TextFeatureTransformer</code> <p>Transformer to be applied</p> required Source code in <code>fl4health/feature_alignment/string_columns_transformer.py</code> <pre><code>def __init__(self, transformer: TextFeatureTransformer):\n    \"\"\"\n    The purpose of this class is to enable the application of text feature transformers from sklearn\n    to multiple string columns, which is not supported in the first place.\n\n    Args:\n        transformer (TextFeatureTransformer): Transformer to be applied\n    \"\"\"\n    self.transformer = transformer\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.string_columns_transformer.TextMulticolumnTransformer.fit","title":"<code>fit(x, y=None)</code>","text":"<p>Fit the transformer to the provided dataframe. The dataframe should have multiple string columns The transformer is fit on the appended text from all columns in the <code>x</code> dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame</code> <p>Columns on which to fit the transformer</p> required <code>y</code> <code>DataFrame | None</code> <p>Not used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>TextMulticolumnTransformer</code> <p>The fit transformer</p> Source code in <code>fl4health/feature_alignment/string_columns_transformer.py</code> <pre><code>def fit(self, x: pd.DataFrame, y: pd.DataFrame | None = None) -&gt; TextMulticolumnTransformer:\n    \"\"\"\n    Fit the transformer to the provided dataframe. The dataframe should have multiple string columns\n    The transformer is fit on the appended text from all columns in the ``x`` dataframe.\n\n    Args:\n        x (pd.DataFrame): Columns on which to fit the transformer\n        y (pd.DataFrame | None, optional): Not used. Defaults to None.\n\n    Returns:\n        (TextMulticolumnTransformer): The fit transformer\n    \"\"\"\n    joined_x = x.apply(\" \".join, axis=1)\n    self.transformer.fit(joined_x)\n    return self\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.string_columns_transformer.TextMulticolumnTransformer.transform","title":"<code>transform(x)</code>","text":"<p>Transforms the concatenation of all columns of text in the <code>x</code> dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame</code> <p>Dataframe of text-based columns to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed dataframe.</p> Source code in <code>fl4health/feature_alignment/string_columns_transformer.py</code> <pre><code>def transform(self, x: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms the concatenation of all columns of text in the ``x`` dataframe.\n\n    Args:\n        x (pd.DataFrame): Dataframe of text-based columns to be transformed\n\n    Returns:\n        (pd.DataFrame): Transformed dataframe.\n    \"\"\"\n    joined_x = x.apply(\" \".join, axis=1)\n    return self.transformer.transform(joined_x)\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.string_columns_transformer.TextColumnTransformer","title":"<code>TextColumnTransformer</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> Source code in <code>fl4health/feature_alignment/string_columns_transformer.py</code> <pre><code>class TextColumnTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, transformer: TextFeatureTransformer):\n        \"\"\"\n        The purpose of this class is to enable the application of text feature transformers from sklearn\n        to a single-column pandas dataframe, which is not supported in the first place.\n\n        Args:\n            transformer (TextFeatureTransformer): Transformer to be applied\n        \"\"\"\n        self.transformer = transformer\n\n    def fit(self, x: pd.DataFrame, y: pd.DataFrame | None = None) -&gt; TextColumnTransformer:\n        \"\"\"\n        Fit the transformer to the provided dataframe. The dataframe should have a single string column\n        The transformer is fit on the text from the single columns in the ``x`` dataframe.\n\n        Args:\n            x (pd.DataFrame): Column on which to fit the transformer\n            y (pd.DataFrame | None, optional): Not used. Defaults to None.\n\n        Returns:\n            (TextColumnTransformer): The fit transformer\n        \"\"\"\n        assert isinstance(x, pd.DataFrame) and x.shape[1] == 1\n        self.transformer.fit(x[x.columns[0]])\n        return self\n\n    def transform(self, x: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Transforms the concatenation of a single column of text in the ``x`` dataframe.\n\n        Args:\n            x (pd.DataFrame): Dataframe of text-based column to be transformed\n\n        Returns:\n            (pd.DataFrame): Transformed dataframe.\n        \"\"\"\n        assert isinstance(x, pd.DataFrame) and x.shape[1] == 1\n        return self.transformer.transform(x[x.columns[0]])\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.string_columns_transformer.TextColumnTransformer.__init__","title":"<code>__init__(transformer)</code>","text":"<p>The purpose of this class is to enable the application of text feature transformers from sklearn to a single-column pandas dataframe, which is not supported in the first place.</p> <p>Parameters:</p> Name Type Description Default <code>transformer</code> <code>TextFeatureTransformer</code> <p>Transformer to be applied</p> required Source code in <code>fl4health/feature_alignment/string_columns_transformer.py</code> <pre><code>def __init__(self, transformer: TextFeatureTransformer):\n    \"\"\"\n    The purpose of this class is to enable the application of text feature transformers from sklearn\n    to a single-column pandas dataframe, which is not supported in the first place.\n\n    Args:\n        transformer (TextFeatureTransformer): Transformer to be applied\n    \"\"\"\n    self.transformer = transformer\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.string_columns_transformer.TextColumnTransformer.fit","title":"<code>fit(x, y=None)</code>","text":"<p>Fit the transformer to the provided dataframe. The dataframe should have a single string column The transformer is fit on the text from the single columns in the <code>x</code> dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame</code> <p>Column on which to fit the transformer</p> required <code>y</code> <code>DataFrame | None</code> <p>Not used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>TextColumnTransformer</code> <p>The fit transformer</p> Source code in <code>fl4health/feature_alignment/string_columns_transformer.py</code> <pre><code>def fit(self, x: pd.DataFrame, y: pd.DataFrame | None = None) -&gt; TextColumnTransformer:\n    \"\"\"\n    Fit the transformer to the provided dataframe. The dataframe should have a single string column\n    The transformer is fit on the text from the single columns in the ``x`` dataframe.\n\n    Args:\n        x (pd.DataFrame): Column on which to fit the transformer\n        y (pd.DataFrame | None, optional): Not used. Defaults to None.\n\n    Returns:\n        (TextColumnTransformer): The fit transformer\n    \"\"\"\n    assert isinstance(x, pd.DataFrame) and x.shape[1] == 1\n    self.transformer.fit(x[x.columns[0]])\n    return self\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.string_columns_transformer.TextColumnTransformer.transform","title":"<code>transform(x)</code>","text":"<p>Transforms the concatenation of a single column of text in the <code>x</code> dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame</code> <p>Dataframe of text-based column to be transformed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Transformed dataframe.</p> Source code in <code>fl4health/feature_alignment/string_columns_transformer.py</code> <pre><code>def transform(self, x: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms the concatenation of a single column of text in the ``x`` dataframe.\n\n    Args:\n        x (pd.DataFrame): Dataframe of text-based column to be transformed\n\n    Returns:\n        (pd.DataFrame): Transformed dataframe.\n    \"\"\"\n    assert isinstance(x, pd.DataFrame) and x.shape[1] == 1\n    return self.transformer.transform(x[x.columns[0]])\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_info_encoder","title":"<code>tab_features_info_encoder</code>","text":""},{"location":"api/#fl4health.feature_alignment.tab_features_info_encoder.TabularFeaturesInfoEncoder","title":"<code>TabularFeaturesInfoEncoder</code>","text":"Source code in <code>fl4health/feature_alignment/tab_features_info_encoder.py</code> <pre><code>class TabularFeaturesInfoEncoder:\n    def __init__(self, tabular_features: list[TabularFeature], tabular_targets: list[TabularFeature]) -&gt; None:\n        \"\"\"\n        This class encodes all the information required to perform feature alignment on tabular datasets.\n\n        **NOTE**: targets are not included in tabular_features\n\n        Args:\n            tabular_features (list[TabularFeature]): List of all tabular features.\n            tabular_targets (list[TabularFeature]): List of all targets.\n        \"\"\"\n        self.tabular_features = sorted(tabular_features, key=TabularFeature.get_feature_name)\n        self.tabular_targets = sorted(tabular_targets, key=TabularFeature.get_feature_name)\n\n    def get_tabular_features(self) -&gt; list[TabularFeature]:\n        return self.tabular_features\n\n    def get_tabular_targets(self) -&gt; list[TabularFeature]:\n        return self.tabular_targets\n\n    def get_feature_columns(self) -&gt; list[str]:\n        return sorted([feature.get_feature_name() for feature in self.tabular_features])\n\n    def get_target_columns(self) -&gt; list[str]:\n        return sorted([target.get_feature_name() for target in self.tabular_targets])\n\n    def features_by_type(self, tabular_type: TabularType) -&gt; list[TabularFeature]:\n        return sorted(\n            [feature for feature in self.tabular_features if feature.get_feature_type() == tabular_type],\n            key=TabularFeature.get_feature_name,\n        )\n\n    def type_to_features(self) -&gt; dict[TabularType, list[TabularFeature]]:\n        return {tabular_type: self.features_by_type(tabular_type) for tabular_type in TabularType}\n\n    def get_categories_list(self) -&gt; list[MetaData]:\n        return [cat_feature.get_metadata() for cat_feature in self.features_by_type(TabularType.ORDINAL)]\n\n    def get_target_dimension(self) -&gt; int:\n        # Return the dimension of the target array *after* feature alignment is performed.\n        dimension = 0\n        for target in self.tabular_targets:\n            dimension += target.get_metadata_dimension()\n        return dimension\n\n    @staticmethod\n    def _construct_tab_feature(\n        df: pd.DataFrame,\n        feature_name: str,\n        feature_type: TabularType,\n        fill_values: dict[str, Scalar] | None,\n    ) -&gt; TabularFeature:\n        if fill_values is None or feature_name not in fill_values:\n            fill_value = TabularType.get_default_fill_value(feature_type)\n        else:\n            fill_value = fill_values[feature_name]\n\n        if feature_type in {TabularType.ORDINAL, TabularType.BINARY}:\n            # Extract categories information.\n            feature_categories = sorted(df[feature_name].unique().tolist())\n            return TabularFeature(feature_name, feature_type, fill_value, feature_categories)\n        if feature_type == TabularType.STRING:\n            # Extract vocabulary from a string column of df.\n            count_vectorizer = CountVectorizer()\n            count_vectorizer.fit(df[feature_name])\n            vocabulary = count_vectorizer.vocabulary_\n            return TabularFeature(feature_name, feature_type, fill_value, vocabulary)\n        return TabularFeature(feature_name, feature_type, fill_value)\n\n    @staticmethod\n    def encoder_from_dataframe(\n        df: pd.DataFrame,\n        id_column: str,\n        target_columns: str | list[str],\n        fill_values: dict[str, Scalar] | None = None,\n    ) -&gt; TabularFeaturesInfoEncoder:\n        features_list = sorted(df.columns.values.tolist())\n        features_list.remove(id_column)\n        tab_features = TabularFeatures(\n            data=df.reset_index(), features=features_list, by=id_column, targets=target_columns\n        )\n        features_to_types = tab_features.types\n\n        tabular_targets = []\n        tabular_features = []\n        # Construct TabularFeature objects.\n        for feature_name in features_to_types:\n            feature_type = TabularType(features_to_types[feature_name].value)\n            tabular_feature = TabularFeaturesInfoEncoder._construct_tab_feature(\n                df, feature_name, feature_type, fill_values\n            )\n            if feature_name == target_columns or feature_name in target_columns:\n                tabular_targets.append(tabular_feature)\n            else:\n                tabular_features.append(tabular_feature)\n        return TabularFeaturesInfoEncoder(tabular_features, tabular_targets)\n\n    def to_json(self) -&gt; str:\n        return json.dumps(\n            {\n                \"tabular_features\": json.dumps([tab_feature.to_json() for tab_feature in self.tabular_features]),\n                \"tabular_targets\": json.dumps([tab_target.to_json() for tab_target in self.tabular_targets]),\n            }\n        )\n\n    @staticmethod\n    def from_json(json_str: str) -&gt; TabularFeaturesInfoEncoder:\n        attributes = json.loads(json_str)\n        return TabularFeaturesInfoEncoder(\n            [TabularFeature.from_json(tab_str) for tab_str in json.loads(attributes[\"tabular_features\"])],\n            [TabularFeature.from_json(target_str) for target_str in json.loads(attributes[\"tabular_targets\"])],\n        )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_info_encoder.TabularFeaturesInfoEncoder.__init__","title":"<code>__init__(tabular_features, tabular_targets)</code>","text":"<p>This class encodes all the information required to perform feature alignment on tabular datasets.</p> <p>NOTE: targets are not included in tabular_features</p> <p>Parameters:</p> Name Type Description Default <code>tabular_features</code> <code>list[TabularFeature]</code> <p>List of all tabular features.</p> required <code>tabular_targets</code> <code>list[TabularFeature]</code> <p>List of all targets.</p> required Source code in <code>fl4health/feature_alignment/tab_features_info_encoder.py</code> <pre><code>def __init__(self, tabular_features: list[TabularFeature], tabular_targets: list[TabularFeature]) -&gt; None:\n    \"\"\"\n    This class encodes all the information required to perform feature alignment on tabular datasets.\n\n    **NOTE**: targets are not included in tabular_features\n\n    Args:\n        tabular_features (list[TabularFeature]): List of all tabular features.\n        tabular_targets (list[TabularFeature]): List of all targets.\n    \"\"\"\n    self.tabular_features = sorted(tabular_features, key=TabularFeature.get_feature_name)\n    self.tabular_targets = sorted(tabular_targets, key=TabularFeature.get_feature_name)\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor","title":"<code>tab_features_preprocessor</code>","text":""},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor","title":"<code>TabularFeaturesPreprocessor</code>","text":"Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>class TabularFeaturesPreprocessor:\n    def __init__(self, tab_feature_encoder: TabularFeaturesInfoEncoder) -&gt; None:\n        \"\"\"\n        ``TabularFeaturesPreprocessor`` is responsible for constructing the appropriate column transformers based on\n        the information encoded in ``tab_feature_encoder``. These transformers will then be applied to a pandas\n        dataframe.\n\n        Each tabular feature, which corresponds to a column in the pandas dataframe, has its own column transformer.\n        A default transformer is initialized for each feature based on its data type, but the user may also manually\n        specify a transformer for this feature.\n\n        Args:\n            tab_feature_encoder (TabularFeaturesInfoEncoder): Encodes the information necessary for constructing the\n                column transformers.\n        \"\"\"\n        self.features_to_pipelines: dict[str, Pipeline] = {}\n        self.targets_to_pipelines: dict[str, Pipeline] = {}\n\n        self.tabular_features = tab_feature_encoder.get_tabular_features()\n        self.tabular_targets = tab_feature_encoder.get_tabular_targets()\n\n        self.feature_columns = tab_feature_encoder.get_feature_columns()\n        self.target_columns = tab_feature_encoder.get_target_columns()\n\n        self.features_to_pipelines = self.initialize_default_pipelines(self.tabular_features, one_hot=True)\n        self.targets_to_pipelines = self.initialize_default_pipelines(self.tabular_targets, one_hot=False)\n\n        self.data_column_transformer = self.return_column_transformer(self.features_to_pipelines)\n        self.target_column_transformer = self.return_column_transformer(self.targets_to_pipelines)\n\n    def get_default_numeric_pipeline(self) -&gt; Pipeline:\n        \"\"\"\n        Default numeric pipeline factory. Mean imputation and default min-max scaler.\n\n        Returns:\n            (Pipeline): Default numeric pipeline\n        \"\"\"\n        return Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n\n    def get_default_binary_pipeline(self) -&gt; Pipeline:\n        \"\"\"\n        Default binary pipeline factor. Most frequent imputer and an ordinal encoder.\n\n        Returns:\n            (Pipeline): Default binary pipeline\n        \"\"\"\n        return Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"encoder\", OrdinalEncoder())])\n\n    def get_default_one_hot_pipeline(self, categories: MetaData) -&gt; Pipeline:\n        \"\"\"\n        Default one hot encoding pipeline. Unknowns are ignored, categories are provided as an input.\n\n        Args:\n            categories (MetaData): Categories to be one hot encoded.\n\n        Returns:\n            (Pipeline): Default one-hot encoding pipeline\n        \"\"\"\n        return Pipeline(steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", categories=[categories]))])\n\n    def get_default_ordinal_pipeline(self, categories: MetaData) -&gt; Pipeline:\n        \"\"\"\n        Default ordinal pipeline. Unknowns have a category. Other categories are provided.\n\n        Args:\n            categories (MetaData): Categories to be used in encoding\n\n        Returns:\n            (Pipeline): Default ordinal pipeline\n        \"\"\"\n        return Pipeline(\n            steps=[\n                (\n                    \"encoder\",\n                    OrdinalEncoder(\n                        unknown_value=len(categories) + 1,\n                        handle_unknown=\"use_encoded_value\",\n                        categories=[categories],\n                    ),\n                )\n            ]\n        )\n\n    def get_default_string_pipeline(self, vocabulary: MetaData) -&gt; Pipeline:\n        \"\"\"\n        Default string/text encoding pipeline. The vocabulary is provided and this is used to instantiate a default\n        ``TfidfVectorizer``.\n\n        Args:\n            vocabulary (MetaData): Vocabulary to serve as the ``TfidfVectorizer`` vocab.\n\n        Returns:\n            (Pipeline): Default string/text encoding pipeline.\n        \"\"\"\n        return Pipeline(steps=[(\"vectorizer\", TextColumnTransformer(TfidfVectorizer(vocabulary=vocabulary)))])\n\n    def initialize_default_pipelines(\n        self, tabular_features: list[TabularFeature], one_hot: bool\n    ) -&gt; dict[str, Pipeline]:\n        \"\"\"\n        Initialize a default Pipeline for every data column in ``tabular_features``.\n\n        Args:\n            tabular_features (list[TabularFeature]): list of tabular features in the data columns.\n            one_hot (bool): Whether or not to apply a default one-hot pipeline.\n\n        Returns:\n            (dict[str, Pipeline]): Default feature processing pipeline per feature in the list.\n        \"\"\"\n        columns_to_pipelines = {}\n        for tab_feature in tabular_features:\n            feature_type = tab_feature.get_feature_type()\n            feature_name = tab_feature.get_feature_name()\n            if feature_type == TabularType.NUMERIC:\n                feature_pipeline = self.get_default_numeric_pipeline()\n            elif feature_type == TabularType.BINARY:\n                feature_pipeline = self.get_default_binary_pipeline()\n            elif feature_type == TabularType.ORDINAL:\n                feature_categories = tab_feature.get_metadata()\n                if one_hot:\n                    feature_pipeline = self.get_default_one_hot_pipeline(feature_categories)\n                else:\n                    feature_pipeline = self.get_default_ordinal_pipeline(feature_categories)\n            else:\n                vocabulary = tab_feature.get_metadata()\n                feature_pipeline = self.get_default_string_pipeline(vocabulary)\n            columns_to_pipelines[feature_name] = feature_pipeline\n        return columns_to_pipelines\n\n    def return_column_transformer(self, pipelines: dict[str, Pipeline]) -&gt; ColumnTransformer:\n        \"\"\"\n        Given a set of pipelines create a set of column transformations based on those pipelines.\n\n        Args:\n            pipelines (dict[str, Pipeline]): Dictionary of pipelines for columns with the keys of the dictionary\n                corresponding to the column names\n\n        Returns:\n            (ColumnTransformer): Transformer for the specified columns. The unspecified columns are dropped.\n        \"\"\"\n        transformers = [\n            (f\"{feature_name}_pipeline\", pipelines[feature_name], [feature_name])\n            for feature_name in sorted(pipelines.keys())\n        ]\n        # If a column does not have an associated transformer then it is dropped from the df.\n        return ColumnTransformer(\n            transformers=transformers,\n            remainder=\"drop\",\n        )\n\n    def set_feature_pipeline(self, feature_name: str, pipeline: Pipeline) -&gt; None:\n        \"\"\"\n        This method allows the user to customize a specific pipeline to be applied to a specific feature.\n        For example, the user may want to use different scalers for two distinct numerical features.\n\n        Args:\n            feature_name (str): target column name in the dataframe to apply the pipeline to\n            pipeline (Pipeline): Pipeline to apply to the associated column.\n        \"\"\"\n        if feature_name in self.features_to_pipelines:\n            self.features_to_pipelines[feature_name] = pipeline\n            self.data_column_transformer = self.return_column_transformer(self.features_to_pipelines)\n        elif feature_name in self.targets_to_pipelines:\n            self.targets_to_pipelines[feature_name] = pipeline\n            self.target_column_transformer = self.return_column_transformer(self.targets_to_pipelines)\n        else:\n            log(WARNING, f\"{feature_name} is neither a feature nor target and the provided pipeline will be ignored.\")\n\n    def preprocess_features(self, df: pd.DataFrame) -&gt; tuple[NDArray, NDArray]:\n        \"\"\"\n        Preprocess the provided dataframe with the specified pipelines.\n\n        Args:\n            df (pd.DataFrame): Dataframe to be processed.\n\n        Returns:\n            (tuple[NDArray, NDArray]): Resulting input and target numpy arrays after preprocessing.\n        \"\"\"\n        # If the dataframe has an entire column missing, we need to fill it with some default value first.\n        df_filled = self.fill_in_missing_columns(df)\n        # After filling in missing columns, apply the feature alignment transform.\n        return (\n            self.data_column_transformer.fit_transform(df_filled[self.feature_columns]),\n            self.target_column_transformer.fit_transform(df_filled[self.target_columns]),\n        )\n\n    def fill_in_missing_columns(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Return a new DataFrame where entire missing columns are filled with values specified in each column's\n        default fill value.\n\n        Args:\n            df (pd.DataFrame): Dataframe to be filled\n\n        Returns:\n            (pd.DataFrame): Filled dataframe\n        \"\"\"\n        df_new = df.copy(deep=True)\n        for tab_feature in self.tabular_features:\n            self._fill_in_missing_column(df_new, tab_feature.get_feature_name(), tab_feature.get_fill_value())\n        return df_new\n\n    def _fill_in_missing_column(self, df: pd.DataFrame, column_name: str, value: Scalar) -&gt; None:\n        if column_name not in df.columns:\n            df[column_name] = value\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.__init__","title":"<code>__init__(tab_feature_encoder)</code>","text":"<p><code>TabularFeaturesPreprocessor</code> is responsible for constructing the appropriate column transformers based on the information encoded in <code>tab_feature_encoder</code>. These transformers will then be applied to a pandas dataframe.</p> <p>Each tabular feature, which corresponds to a column in the pandas dataframe, has its own column transformer. A default transformer is initialized for each feature based on its data type, but the user may also manually specify a transformer for this feature.</p> <p>Parameters:</p> Name Type Description Default <code>tab_feature_encoder</code> <code>TabularFeaturesInfoEncoder</code> <p>Encodes the information necessary for constructing the column transformers.</p> required Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def __init__(self, tab_feature_encoder: TabularFeaturesInfoEncoder) -&gt; None:\n    \"\"\"\n    ``TabularFeaturesPreprocessor`` is responsible for constructing the appropriate column transformers based on\n    the information encoded in ``tab_feature_encoder``. These transformers will then be applied to a pandas\n    dataframe.\n\n    Each tabular feature, which corresponds to a column in the pandas dataframe, has its own column transformer.\n    A default transformer is initialized for each feature based on its data type, but the user may also manually\n    specify a transformer for this feature.\n\n    Args:\n        tab_feature_encoder (TabularFeaturesInfoEncoder): Encodes the information necessary for constructing the\n            column transformers.\n    \"\"\"\n    self.features_to_pipelines: dict[str, Pipeline] = {}\n    self.targets_to_pipelines: dict[str, Pipeline] = {}\n\n    self.tabular_features = tab_feature_encoder.get_tabular_features()\n    self.tabular_targets = tab_feature_encoder.get_tabular_targets()\n\n    self.feature_columns = tab_feature_encoder.get_feature_columns()\n    self.target_columns = tab_feature_encoder.get_target_columns()\n\n    self.features_to_pipelines = self.initialize_default_pipelines(self.tabular_features, one_hot=True)\n    self.targets_to_pipelines = self.initialize_default_pipelines(self.tabular_targets, one_hot=False)\n\n    self.data_column_transformer = self.return_column_transformer(self.features_to_pipelines)\n    self.target_column_transformer = self.return_column_transformer(self.targets_to_pipelines)\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.get_default_numeric_pipeline","title":"<code>get_default_numeric_pipeline()</code>","text":"<p>Default numeric pipeline factory. Mean imputation and default min-max scaler.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Default numeric pipeline</p> Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def get_default_numeric_pipeline(self) -&gt; Pipeline:\n    \"\"\"\n    Default numeric pipeline factory. Mean imputation and default min-max scaler.\n\n    Returns:\n        (Pipeline): Default numeric pipeline\n    \"\"\"\n    return Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", MinMaxScaler())])\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.get_default_binary_pipeline","title":"<code>get_default_binary_pipeline()</code>","text":"<p>Default binary pipeline factor. Most frequent imputer and an ordinal encoder.</p> <p>Returns:</p> Type Description <code>Pipeline</code> <p>Default binary pipeline</p> Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def get_default_binary_pipeline(self) -&gt; Pipeline:\n    \"\"\"\n    Default binary pipeline factor. Most frequent imputer and an ordinal encoder.\n\n    Returns:\n        (Pipeline): Default binary pipeline\n    \"\"\"\n    return Pipeline(steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"encoder\", OrdinalEncoder())])\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.get_default_one_hot_pipeline","title":"<code>get_default_one_hot_pipeline(categories)</code>","text":"<p>Default one hot encoding pipeline. Unknowns are ignored, categories are provided as an input.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>MetaData</code> <p>Categories to be one hot encoded.</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Default one-hot encoding pipeline</p> Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def get_default_one_hot_pipeline(self, categories: MetaData) -&gt; Pipeline:\n    \"\"\"\n    Default one hot encoding pipeline. Unknowns are ignored, categories are provided as an input.\n\n    Args:\n        categories (MetaData): Categories to be one hot encoded.\n\n    Returns:\n        (Pipeline): Default one-hot encoding pipeline\n    \"\"\"\n    return Pipeline(steps=[(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", categories=[categories]))])\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.get_default_ordinal_pipeline","title":"<code>get_default_ordinal_pipeline(categories)</code>","text":"<p>Default ordinal pipeline. Unknowns have a category. Other categories are provided.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>MetaData</code> <p>Categories to be used in encoding</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Default ordinal pipeline</p> Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def get_default_ordinal_pipeline(self, categories: MetaData) -&gt; Pipeline:\n    \"\"\"\n    Default ordinal pipeline. Unknowns have a category. Other categories are provided.\n\n    Args:\n        categories (MetaData): Categories to be used in encoding\n\n    Returns:\n        (Pipeline): Default ordinal pipeline\n    \"\"\"\n    return Pipeline(\n        steps=[\n            (\n                \"encoder\",\n                OrdinalEncoder(\n                    unknown_value=len(categories) + 1,\n                    handle_unknown=\"use_encoded_value\",\n                    categories=[categories],\n                ),\n            )\n        ]\n    )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.get_default_string_pipeline","title":"<code>get_default_string_pipeline(vocabulary)</code>","text":"<p>Default string/text encoding pipeline. The vocabulary is provided and this is used to instantiate a default <code>TfidfVectorizer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>vocabulary</code> <code>MetaData</code> <p>Vocabulary to serve as the <code>TfidfVectorizer</code> vocab.</p> required <p>Returns:</p> Type Description <code>Pipeline</code> <p>Default string/text encoding pipeline.</p> Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def get_default_string_pipeline(self, vocabulary: MetaData) -&gt; Pipeline:\n    \"\"\"\n    Default string/text encoding pipeline. The vocabulary is provided and this is used to instantiate a default\n    ``TfidfVectorizer``.\n\n    Args:\n        vocabulary (MetaData): Vocabulary to serve as the ``TfidfVectorizer`` vocab.\n\n    Returns:\n        (Pipeline): Default string/text encoding pipeline.\n    \"\"\"\n    return Pipeline(steps=[(\"vectorizer\", TextColumnTransformer(TfidfVectorizer(vocabulary=vocabulary)))])\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.initialize_default_pipelines","title":"<code>initialize_default_pipelines(tabular_features, one_hot)</code>","text":"<p>Initialize a default Pipeline for every data column in <code>tabular_features</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_features</code> <code>list[TabularFeature]</code> <p>list of tabular features in the data columns.</p> required <code>one_hot</code> <code>bool</code> <p>Whether or not to apply a default one-hot pipeline.</p> required <p>Returns:</p> Type Description <code>dict[str, Pipeline]</code> <p>Default feature processing pipeline per feature in the list.</p> Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def initialize_default_pipelines(\n    self, tabular_features: list[TabularFeature], one_hot: bool\n) -&gt; dict[str, Pipeline]:\n    \"\"\"\n    Initialize a default Pipeline for every data column in ``tabular_features``.\n\n    Args:\n        tabular_features (list[TabularFeature]): list of tabular features in the data columns.\n        one_hot (bool): Whether or not to apply a default one-hot pipeline.\n\n    Returns:\n        (dict[str, Pipeline]): Default feature processing pipeline per feature in the list.\n    \"\"\"\n    columns_to_pipelines = {}\n    for tab_feature in tabular_features:\n        feature_type = tab_feature.get_feature_type()\n        feature_name = tab_feature.get_feature_name()\n        if feature_type == TabularType.NUMERIC:\n            feature_pipeline = self.get_default_numeric_pipeline()\n        elif feature_type == TabularType.BINARY:\n            feature_pipeline = self.get_default_binary_pipeline()\n        elif feature_type == TabularType.ORDINAL:\n            feature_categories = tab_feature.get_metadata()\n            if one_hot:\n                feature_pipeline = self.get_default_one_hot_pipeline(feature_categories)\n            else:\n                feature_pipeline = self.get_default_ordinal_pipeline(feature_categories)\n        else:\n            vocabulary = tab_feature.get_metadata()\n            feature_pipeline = self.get_default_string_pipeline(vocabulary)\n        columns_to_pipelines[feature_name] = feature_pipeline\n    return columns_to_pipelines\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.return_column_transformer","title":"<code>return_column_transformer(pipelines)</code>","text":"<p>Given a set of pipelines create a set of column transformations based on those pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>pipelines</code> <code>dict[str, Pipeline]</code> <p>Dictionary of pipelines for columns with the keys of the dictionary corresponding to the column names</p> required <p>Returns:</p> Type Description <code>ColumnTransformer</code> <p>Transformer for the specified columns. The unspecified columns are dropped.</p> Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def return_column_transformer(self, pipelines: dict[str, Pipeline]) -&gt; ColumnTransformer:\n    \"\"\"\n    Given a set of pipelines create a set of column transformations based on those pipelines.\n\n    Args:\n        pipelines (dict[str, Pipeline]): Dictionary of pipelines for columns with the keys of the dictionary\n            corresponding to the column names\n\n    Returns:\n        (ColumnTransformer): Transformer for the specified columns. The unspecified columns are dropped.\n    \"\"\"\n    transformers = [\n        (f\"{feature_name}_pipeline\", pipelines[feature_name], [feature_name])\n        for feature_name in sorted(pipelines.keys())\n    ]\n    # If a column does not have an associated transformer then it is dropped from the df.\n    return ColumnTransformer(\n        transformers=transformers,\n        remainder=\"drop\",\n    )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.set_feature_pipeline","title":"<code>set_feature_pipeline(feature_name, pipeline)</code>","text":"<p>This method allows the user to customize a specific pipeline to be applied to a specific feature. For example, the user may want to use different scalers for two distinct numerical features.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>target column name in the dataframe to apply the pipeline to</p> required <code>pipeline</code> <code>Pipeline</code> <p>Pipeline to apply to the associated column.</p> required Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def set_feature_pipeline(self, feature_name: str, pipeline: Pipeline) -&gt; None:\n    \"\"\"\n    This method allows the user to customize a specific pipeline to be applied to a specific feature.\n    For example, the user may want to use different scalers for two distinct numerical features.\n\n    Args:\n        feature_name (str): target column name in the dataframe to apply the pipeline to\n        pipeline (Pipeline): Pipeline to apply to the associated column.\n    \"\"\"\n    if feature_name in self.features_to_pipelines:\n        self.features_to_pipelines[feature_name] = pipeline\n        self.data_column_transformer = self.return_column_transformer(self.features_to_pipelines)\n    elif feature_name in self.targets_to_pipelines:\n        self.targets_to_pipelines[feature_name] = pipeline\n        self.target_column_transformer = self.return_column_transformer(self.targets_to_pipelines)\n    else:\n        log(WARNING, f\"{feature_name} is neither a feature nor target and the provided pipeline will be ignored.\")\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.preprocess_features","title":"<code>preprocess_features(df)</code>","text":"<p>Preprocess the provided dataframe with the specified pipelines.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to be processed.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray, NDArray]</code> <p>Resulting input and target numpy arrays after preprocessing.</p> Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def preprocess_features(self, df: pd.DataFrame) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"\n    Preprocess the provided dataframe with the specified pipelines.\n\n    Args:\n        df (pd.DataFrame): Dataframe to be processed.\n\n    Returns:\n        (tuple[NDArray, NDArray]): Resulting input and target numpy arrays after preprocessing.\n    \"\"\"\n    # If the dataframe has an entire column missing, we need to fill it with some default value first.\n    df_filled = self.fill_in_missing_columns(df)\n    # After filling in missing columns, apply the feature alignment transform.\n    return (\n        self.data_column_transformer.fit_transform(df_filled[self.feature_columns]),\n        self.target_column_transformer.fit_transform(df_filled[self.target_columns]),\n    )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tab_features_preprocessor.TabularFeaturesPreprocessor.fill_in_missing_columns","title":"<code>fill_in_missing_columns(df)</code>","text":"<p>Return a new DataFrame where entire missing columns are filled with values specified in each column's default fill value.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to be filled</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filled dataframe</p> Source code in <code>fl4health/feature_alignment/tab_features_preprocessor.py</code> <pre><code>def fill_in_missing_columns(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Return a new DataFrame where entire missing columns are filled with values specified in each column's\n    default fill value.\n\n    Args:\n        df (pd.DataFrame): Dataframe to be filled\n\n    Returns:\n        (pd.DataFrame): Filled dataframe\n    \"\"\"\n    df_new = df.copy(deep=True)\n    for tab_feature in self.tabular_features:\n        self._fill_in_missing_column(df_new, tab_feature.get_feature_name(), tab_feature.get_fill_value())\n    return df_new\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tabular_feature","title":"<code>tabular_feature</code>","text":""},{"location":"api/#fl4health.feature_alignment.tabular_feature.TabularFeature","title":"<code>TabularFeature</code>","text":"Source code in <code>fl4health/feature_alignment/tabular_feature.py</code> <pre><code>class TabularFeature:\n    def __init__(\n        self,\n        feature_name: str,\n        feature_type: TabularType,\n        fill_value: Scalar | None,\n        metadata: MetaData | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Information that represents a tabular feature.\n\n        Args:\n            feature_name (str): name of the feature.\n            feature_type (TabularType): data type of the feature.\n            fill_value (Scalar | None): the default fill value for this feature when it is missing in a dataframe.\n            metadata (MetaData, optional): metadata associated with this feature.\n                For example, if the feature is categorical, then metadata would be all the categories.\n                Defaults to None.\n        \"\"\"\n        self.feature_name = feature_name\n        self.feature_type = feature_type\n        # Each TabularType has its own default fill value, which is used\n        # when the feature does not have its default fill value specified.\n        if fill_value is None:\n            self.fill_value = TabularType.get_default_fill_value(self.feature_type)\n        else:\n            self.fill_value = fill_value\n        if metadata:\n            self.metadata = metadata\n        else:\n            self.metadata = []\n\n    def get_feature_name(self) -&gt; str:\n        return self.feature_name\n\n    def get_feature_type(self) -&gt; TabularType:\n        return self.feature_type\n\n    def get_fill_value(self) -&gt; Scalar:\n        return self.fill_value\n\n    def get_metadata(self) -&gt; MetaData:\n        return self.metadata\n\n    def get_metadata_dimension(self) -&gt; int:\n        if self.feature_type in {TabularType.BINARY, TabularType.ORDINAL}:\n            return len(self.metadata)\n        if self.feature_type == TabularType.NUMERIC:\n            return 1\n        raise ValueError(\"Metadata dimension is not supported when self.feature_type is TabularType.STRING.\")\n\n    def to_json(self) -&gt; str:\n        \"\"\"\n        Converge the information in this class to json format for serialization.\n\n        Returns:\n            (str): Json with all of the pieces of information in this class\n        \"\"\"\n        return json.dumps(\n            {\n                \"feature_name\": json.dumps(self.get_feature_name()),\n                \"feature_type\": json.dumps(self.get_feature_type()),\n                \"fill_value\": json.dumps(self.get_fill_value()),\n                \"metadata\": json.dumps(self.get_metadata()),\n            }\n        )\n\n    @staticmethod\n    def from_json(json_str: str) -&gt; TabularFeature:\n        \"\"\"\n        Provided a JSON string, this function reconstructs the ``TabularFeature`` class to which it corresponds.\n\n        Args:\n            json_str (str): json string with all of the information necessary to construct the ``TabularFeature``\n                object\n\n        Returns:\n            (TabularFeature): Reconstructed ``TabularFeature`` object from the provided JSON\n        \"\"\"\n        attributes = json.loads(json_str)\n        return TabularFeature(\n            json.loads(attributes[\"feature_name\"]),\n            TabularType(json.loads(attributes[\"feature_type\"])),\n            json.loads(attributes[\"fill_value\"]),\n            json.loads(attributes[\"metadata\"]),\n        )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tabular_feature.TabularFeature.__init__","title":"<code>__init__(feature_name, feature_type, fill_value, metadata=None)</code>","text":"<p>Information that represents a tabular feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>name of the feature.</p> required <code>feature_type</code> <code>TabularType</code> <p>data type of the feature.</p> required <code>fill_value</code> <code>Scalar | None</code> <p>the default fill value for this feature when it is missing in a dataframe.</p> required <code>metadata</code> <code>MetaData</code> <p>metadata associated with this feature. For example, if the feature is categorical, then metadata would be all the categories. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/feature_alignment/tabular_feature.py</code> <pre><code>def __init__(\n    self,\n    feature_name: str,\n    feature_type: TabularType,\n    fill_value: Scalar | None,\n    metadata: MetaData | None = None,\n) -&gt; None:\n    \"\"\"\n    Information that represents a tabular feature.\n\n    Args:\n        feature_name (str): name of the feature.\n        feature_type (TabularType): data type of the feature.\n        fill_value (Scalar | None): the default fill value for this feature when it is missing in a dataframe.\n        metadata (MetaData, optional): metadata associated with this feature.\n            For example, if the feature is categorical, then metadata would be all the categories.\n            Defaults to None.\n    \"\"\"\n    self.feature_name = feature_name\n    self.feature_type = feature_type\n    # Each TabularType has its own default fill value, which is used\n    # when the feature does not have its default fill value specified.\n    if fill_value is None:\n        self.fill_value = TabularType.get_default_fill_value(self.feature_type)\n    else:\n        self.fill_value = fill_value\n    if metadata:\n        self.metadata = metadata\n    else:\n        self.metadata = []\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tabular_feature.TabularFeature.to_json","title":"<code>to_json()</code>","text":"<p>Converge the information in this class to json format for serialization.</p> <p>Returns:</p> Type Description <code>str</code> <p>Json with all of the pieces of information in this class</p> Source code in <code>fl4health/feature_alignment/tabular_feature.py</code> <pre><code>def to_json(self) -&gt; str:\n    \"\"\"\n    Converge the information in this class to json format for serialization.\n\n    Returns:\n        (str): Json with all of the pieces of information in this class\n    \"\"\"\n    return json.dumps(\n        {\n            \"feature_name\": json.dumps(self.get_feature_name()),\n            \"feature_type\": json.dumps(self.get_feature_type()),\n            \"fill_value\": json.dumps(self.get_fill_value()),\n            \"metadata\": json.dumps(self.get_metadata()),\n        }\n    )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tabular_feature.TabularFeature.from_json","title":"<code>from_json(json_str)</code>  <code>staticmethod</code>","text":"<p>Provided a JSON string, this function reconstructs the <code>TabularFeature</code> class to which it corresponds.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>json string with all of the information necessary to construct the <code>TabularFeature</code> object</p> required <p>Returns:</p> Type Description <code>TabularFeature</code> <p>Reconstructed <code>TabularFeature</code> object from the provided JSON</p> Source code in <code>fl4health/feature_alignment/tabular_feature.py</code> <pre><code>@staticmethod\ndef from_json(json_str: str) -&gt; TabularFeature:\n    \"\"\"\n    Provided a JSON string, this function reconstructs the ``TabularFeature`` class to which it corresponds.\n\n    Args:\n        json_str (str): json string with all of the information necessary to construct the ``TabularFeature``\n            object\n\n    Returns:\n        (TabularFeature): Reconstructed ``TabularFeature`` object from the provided JSON\n    \"\"\"\n    attributes = json.loads(json_str)\n    return TabularFeature(\n        json.loads(attributes[\"feature_name\"]),\n        TabularType(json.loads(attributes[\"feature_type\"])),\n        json.loads(attributes[\"fill_value\"]),\n        json.loads(attributes[\"metadata\"]),\n    )\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tabular_type","title":"<code>tabular_type</code>","text":""},{"location":"api/#fl4health.feature_alignment.tabular_type.TabularType","title":"<code>TabularType</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> Source code in <code>fl4health/feature_alignment/tabular_type.py</code> <pre><code>class TabularType(str, Enum):\n    NUMERIC = \"numeric\"\n    BINARY = \"binary\"\n    ORDINAL = \"ordinal\"\n    STRING = \"string\"\n\n    @staticmethod\n    def get_default_fill_value(tabular_type: TabularType | str) -&gt; Scalar:\n        \"\"\"\n        Provided the tabular feature type as either a string or enum, this function returns the default value for\n        imputation to be used.\n\n        Args:\n            tabular_type (TabularType | str): Type of tabular feature to be imputed.\n\n        Raises:\n            ValueError: If the tabular type is unknown this will be thrown.\n\n        Returns:\n            (Scalar): Default imputation value for the specified ``TabularType``\n        \"\"\"\n        if tabular_type is TabularType.NUMERIC:\n            return 0.0\n        if tabular_type is TabularType.BINARY:\n            return 0\n        if tabular_type is TabularType.STRING:\n            return \"N/A\"\n        if tabular_type is TabularType.ORDINAL:\n            return \"UNKNOWN\"\n        raise ValueError(\"Invalid Tabular Data Type.\")\n</code></pre>"},{"location":"api/#fl4health.feature_alignment.tabular_type.TabularType.get_default_fill_value","title":"<code>get_default_fill_value(tabular_type)</code>  <code>staticmethod</code>","text":"<p>Provided the tabular feature type as either a string or enum, this function returns the default value for imputation to be used.</p> <p>Parameters:</p> Name Type Description Default <code>tabular_type</code> <code>TabularType | str</code> <p>Type of tabular feature to be imputed.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the tabular type is unknown this will be thrown.</p> <p>Returns:</p> Type Description <code>Scalar</code> <p>Default imputation value for the specified <code>TabularType</code></p> Source code in <code>fl4health/feature_alignment/tabular_type.py</code> <pre><code>@staticmethod\ndef get_default_fill_value(tabular_type: TabularType | str) -&gt; Scalar:\n    \"\"\"\n    Provided the tabular feature type as either a string or enum, this function returns the default value for\n    imputation to be used.\n\n    Args:\n        tabular_type (TabularType | str): Type of tabular feature to be imputed.\n\n    Raises:\n        ValueError: If the tabular type is unknown this will be thrown.\n\n    Returns:\n        (Scalar): Default imputation value for the specified ``TabularType``\n    \"\"\"\n    if tabular_type is TabularType.NUMERIC:\n        return 0.0\n    if tabular_type is TabularType.BINARY:\n        return 0\n    if tabular_type is TabularType.STRING:\n        return \"N/A\"\n    if tabular_type is TabularType.ORDINAL:\n        return \"UNKNOWN\"\n    raise ValueError(\"Invalid Tabular Data Type.\")\n</code></pre>"},{"location":"api/#fl4health.losses","title":"<code>losses</code>","text":""},{"location":"api/#fl4health.losses.contrastive_loss","title":"<code>contrastive_loss</code>","text":""},{"location":"api/#fl4health.losses.contrastive_loss.MoonContrastiveLoss","title":"<code>MoonContrastiveLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/losses/contrastive_loss.py</code> <pre><code>class MoonContrastiveLoss(nn.Module):\n    def __init__(\n        self,\n        device: torch.device,\n        temperature: float = 0.5,\n    ) -&gt; None:\n        \"\"\"\n        This contrastive loss is implemented based on https://github.com/QinbinLi/MOON.\n\n        Contrastive loss aims to enhance the similarity between the features and their positive pairs while reducing\n        the similarity between the features and their negative pairs.\n\n        Args:\n            device (torch.device): device to use for computation.\n            temperature (float): temperature to scale the logits.\n        \"\"\"\n        super().__init__()\n        self.device = device\n        self.temperature = temperature\n        self.cosine_similarity_function = torch.nn.CosineSimilarity(dim=-1).to(self.device)\n        self.cross_entropy_function = torch.nn.CrossEntropyLoss().to(self.device)\n\n    def compute_negative_similarities(self, features: torch.Tensor, negative_pairs: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        This function computes the cosine similarities of the batch of features provided with the set of batches of\n        negative pairs.\n\n        Args:\n            features (torch.Tensor): Main features, shape (``batch_size``, ``n_features``)\n            negative_pairs (torch.Tensor): Negative pairs of main features, shape\n                (``n_pairs``, ``batch_size``, ``n_features``)\n\n        Returns:\n            (torch.Tensor): Cosine similarities of the batch of features provided with the set of batches of negative\n                pairs. The shape is ``n_pairs`` x ``batch_size``.\n        \"\"\"\n        # Check that features and each of the negatives pairs have the same shape\n        assert features.shape == negative_pairs.shape[1:]\n        # Repeat the feature tensor to compute the similarity of the feature tensor with all negative pairs.\n        repeated_features = features.unsqueeze(0).repeat(len(negative_pairs), 1, 1)\n        return self.cosine_similarity_function(repeated_features, negative_pairs)\n\n    def forward(\n        self, features: torch.Tensor, positive_pairs: torch.Tensor, negative_pairs: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the contrastive loss based on the features, positive pair and negative pairs. While every feature\n        has a positive pair, it can have multiple negative pairs. The loss is computed based on the similarity\n        between the feature and its positive pair relative to negative pairs.\n\n        Args:\n            features (torch.Tensor): Main features, shape (``batch_size``, ``n_features``)\n            positive_pairs (torch.Tensor): Positive pair of main features, shape (1, ``batch_size``, ``n_features``)\n            negative_pairs (torch.Tensor): Negative pairs of main features,\n                shape (``n_pairs``, ``batch_size``, ``n_features``)\n\n        Returns:\n            (torch.Tensor): Contrastive loss value\n        \"\"\"\n        # TODO: We can extend it to support multiple positive pairs using multi-label classification\n\n        features = features.to(self.device)\n        positive_pairs = positive_pairs.to(self.device)\n        negative_pairs = negative_pairs.to(self.device)\n\n        if len(positive_pairs) != 1:\n            raise AssertionError(\n                \"Each feature can have only one positive pair. \",\n                \"Thus positive pairs should be a tensor of shape (1, batch_size, n_features) \",\n                f\"rather than {positive_pairs.shape}\",\n            )\n\n        positive_pair = positive_pairs[0]\n        assert len(features) == len(positive_pair)\n        # Compute similarity of the batch of features with the provided batch of positive pair features\n        positive_similarity = self.cosine_similarity_function(features, positive_pair)\n        # Store similarities with shape batch_size x 1\n        logits = positive_similarity.reshape(-1, 1)\n\n        # Compute the similarity of the batch of features with the collection of batches of negative pair features\n        # Shape of tensor coming out is n_pairs x batch_size\n        negative_pair_similarities = self.compute_negative_similarities(features, negative_pairs)\n        logits = torch.cat((logits, negative_pair_similarities.T), dim=1)\n        logits /= self.temperature\n        labels = torch.zeros(features.size(0)).to(self.device).long()\n\n        return self.cross_entropy_function(logits, labels)\n</code></pre>"},{"location":"api/#fl4health.losses.contrastive_loss.MoonContrastiveLoss.__init__","title":"<code>__init__(device, temperature=0.5)</code>","text":"<p>This contrastive loss is implemented based on https://github.com/QinbinLi/MOON.</p> <p>Contrastive loss aims to enhance the similarity between the features and their positive pairs while reducing the similarity between the features and their negative pairs.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>device to use for computation.</p> required <code>temperature</code> <code>float</code> <p>temperature to scale the logits.</p> <code>0.5</code> Source code in <code>fl4health/losses/contrastive_loss.py</code> <pre><code>def __init__(\n    self,\n    device: torch.device,\n    temperature: float = 0.5,\n) -&gt; None:\n    \"\"\"\n    This contrastive loss is implemented based on https://github.com/QinbinLi/MOON.\n\n    Contrastive loss aims to enhance the similarity between the features and their positive pairs while reducing\n    the similarity between the features and their negative pairs.\n\n    Args:\n        device (torch.device): device to use for computation.\n        temperature (float): temperature to scale the logits.\n    \"\"\"\n    super().__init__()\n    self.device = device\n    self.temperature = temperature\n    self.cosine_similarity_function = torch.nn.CosineSimilarity(dim=-1).to(self.device)\n    self.cross_entropy_function = torch.nn.CrossEntropyLoss().to(self.device)\n</code></pre>"},{"location":"api/#fl4health.losses.contrastive_loss.MoonContrastiveLoss.compute_negative_similarities","title":"<code>compute_negative_similarities(features, negative_pairs)</code>","text":"<p>This function computes the cosine similarities of the batch of features provided with the set of batches of negative pairs.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Main features, shape (<code>batch_size</code>, <code>n_features</code>)</p> required <code>negative_pairs</code> <code>Tensor</code> <p>Negative pairs of main features, shape (<code>n_pairs</code>, <code>batch_size</code>, <code>n_features</code>)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Cosine similarities of the batch of features provided with the set of batches of negative pairs. The shape is <code>n_pairs</code> x <code>batch_size</code>.</p> Source code in <code>fl4health/losses/contrastive_loss.py</code> <pre><code>def compute_negative_similarities(self, features: torch.Tensor, negative_pairs: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    This function computes the cosine similarities of the batch of features provided with the set of batches of\n    negative pairs.\n\n    Args:\n        features (torch.Tensor): Main features, shape (``batch_size``, ``n_features``)\n        negative_pairs (torch.Tensor): Negative pairs of main features, shape\n            (``n_pairs``, ``batch_size``, ``n_features``)\n\n    Returns:\n        (torch.Tensor): Cosine similarities of the batch of features provided with the set of batches of negative\n            pairs. The shape is ``n_pairs`` x ``batch_size``.\n    \"\"\"\n    # Check that features and each of the negatives pairs have the same shape\n    assert features.shape == negative_pairs.shape[1:]\n    # Repeat the feature tensor to compute the similarity of the feature tensor with all negative pairs.\n    repeated_features = features.unsqueeze(0).repeat(len(negative_pairs), 1, 1)\n    return self.cosine_similarity_function(repeated_features, negative_pairs)\n</code></pre>"},{"location":"api/#fl4health.losses.contrastive_loss.MoonContrastiveLoss.forward","title":"<code>forward(features, positive_pairs, negative_pairs)</code>","text":"<p>Compute the contrastive loss based on the features, positive pair and negative pairs. While every feature has a positive pair, it can have multiple negative pairs. The loss is computed based on the similarity between the feature and its positive pair relative to negative pairs.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Main features, shape (<code>batch_size</code>, <code>n_features</code>)</p> required <code>positive_pairs</code> <code>Tensor</code> <p>Positive pair of main features, shape (1, <code>batch_size</code>, <code>n_features</code>)</p> required <code>negative_pairs</code> <code>Tensor</code> <p>Negative pairs of main features, shape (<code>n_pairs</code>, <code>batch_size</code>, <code>n_features</code>)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Contrastive loss value</p> Source code in <code>fl4health/losses/contrastive_loss.py</code> <pre><code>def forward(\n    self, features: torch.Tensor, positive_pairs: torch.Tensor, negative_pairs: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the contrastive loss based on the features, positive pair and negative pairs. While every feature\n    has a positive pair, it can have multiple negative pairs. The loss is computed based on the similarity\n    between the feature and its positive pair relative to negative pairs.\n\n    Args:\n        features (torch.Tensor): Main features, shape (``batch_size``, ``n_features``)\n        positive_pairs (torch.Tensor): Positive pair of main features, shape (1, ``batch_size``, ``n_features``)\n        negative_pairs (torch.Tensor): Negative pairs of main features,\n            shape (``n_pairs``, ``batch_size``, ``n_features``)\n\n    Returns:\n        (torch.Tensor): Contrastive loss value\n    \"\"\"\n    # TODO: We can extend it to support multiple positive pairs using multi-label classification\n\n    features = features.to(self.device)\n    positive_pairs = positive_pairs.to(self.device)\n    negative_pairs = negative_pairs.to(self.device)\n\n    if len(positive_pairs) != 1:\n        raise AssertionError(\n            \"Each feature can have only one positive pair. \",\n            \"Thus positive pairs should be a tensor of shape (1, batch_size, n_features) \",\n            f\"rather than {positive_pairs.shape}\",\n        )\n\n    positive_pair = positive_pairs[0]\n    assert len(features) == len(positive_pair)\n    # Compute similarity of the batch of features with the provided batch of positive pair features\n    positive_similarity = self.cosine_similarity_function(features, positive_pair)\n    # Store similarities with shape batch_size x 1\n    logits = positive_similarity.reshape(-1, 1)\n\n    # Compute the similarity of the batch of features with the collection of batches of negative pair features\n    # Shape of tensor coming out is n_pairs x batch_size\n    negative_pair_similarities = self.compute_negative_similarities(features, negative_pairs)\n    logits = torch.cat((logits, negative_pair_similarities.T), dim=1)\n    logits /= self.temperature\n    labels = torch.zeros(features.size(0)).to(self.device).long()\n\n    return self.cross_entropy_function(logits, labels)\n</code></pre>"},{"location":"api/#fl4health.losses.contrastive_loss.NtXentLoss","title":"<code>NtXentLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/losses/contrastive_loss.py</code> <pre><code>class NtXentLoss(nn.Module):\n    def __init__(self, device: torch.device, temperature: float = 0.5) -&gt; None:\n        \"\"\"\n        Implementation of Normalized Temperature-Scaled Cross Entropy Loss (NT-Xent) proposed in\n        https://papers.nips.cc/paper_files/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html.\n\n        and notably used in:\n\n        - SimCLR (https://arxiv.org/pdf/2002.05709)\n        - FedSimCLR as proposed in Fed-X (https://arxiv.org/pdf/2207.09158).\n\n        NT-Xent is a contrastive loss in which each feature has a positive pair and the rest of the features\n        are considered negative. It is computed based on the similarity of positive pairs relative to negative\n        pairs.\n\n        Args:\n            device (torch.device): device to use for computation.\n            temperature (float): temperature to scale the logits.\n        \"\"\"\n        super().__init__()\n        self.device = device\n        self.temperature = temperature\n\n    def forward(self, features: torch.Tensor, transformed_features: torch.Tensor) -&gt; torch.Tensor:\n        r\"\"\"\n        Compute the contrastive loss based on the features and ``transformed_features``. Given N features\n        and N ``transformed_features`` per batch, ``features[i]`` and ``transformed_features[i]`` are positive pairs\n        and the remaining \\(2N - 2\\) are negative pairs.\n\n        Args:\n            features (torch.Tensor): Features of input without transformation applied.\n                Shaped (``batch_size``, ``feature_dimension``).\n            transformed_features (torch.Tensor): Features of input with transformation applied.\n                Shaped (``batch_size``, ``feature_dimension``).\n\n        Returns:\n            (torch.Tensor): Contrastive loss value.\n        \"\"\"\n        features.to(self.device)\n        transformed_features.to(self.device)\n\n        # Ensure features and transformed_features are same shape\n        assert features.shape == transformed_features.shape\n        batch_size = features.shape[0]\n\n        # Concatenate features and transformed features. Normalize each feature with euclidean norm.\n        all_features = torch.concatenate([features, transformed_features], dim=0).to(self.device)\n        all_features = F.normalize(all_features, dim=-1)\n\n        # Compute similarity of each features with other features\n        # Equivalent to Cosine Similarity since feature are normalized\n        similarity_matrix = torch.matmul(all_features, all_features.T)\n\n        # Extract positive pairs from similarity matrix\n        # Positive pairs are elements (i, j) offset from matrix by batch size\n        # As a result of stacking feature and transformed_features\n        similarity_ij = torch.diag(similarity_matrix, diagonal=batch_size)\n        similarity_ji = torch.diag(similarity_matrix, diagonal=-batch_size)\n        positives = torch.concatenate([similarity_ij, similarity_ji], dim=0)\n\n        # Numerator is the sum of the exponent of positive similarities\n        numerator = torch.exp(positives / self.temperature)\n\n        # Denominator is all pair combinations except for diagonal which corresponds to a features similarity to itself\n        mask = (torch.ones(2 * batch_size, 2 * batch_size) - torch.eye(2 * batch_size)).to(self.device)\n        similarity_matrix_without_diagonal = torch.mul(similarity_matrix, mask)\n        denominator = torch.exp(similarity_matrix_without_diagonal / self.temperature)\n\n        # Final loss negative log likelihood\n        losses = -torch.log(numerator / denominator.sum(dim=1))\n\n        # Divide by 2 * batch size because pairs are double counted due to the symmetry of the similarity matrix\n        return torch.sum(losses) / (2 * batch_size)\n</code></pre>"},{"location":"api/#fl4health.losses.contrastive_loss.NtXentLoss.__init__","title":"<code>__init__(device, temperature=0.5)</code>","text":"<p>Implementation of Normalized Temperature-Scaled Cross Entropy Loss (NT-Xent) proposed in https://papers.nips.cc/paper_files/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html.</p> <p>and notably used in:</p> <ul> <li>SimCLR (https://arxiv.org/pdf/2002.05709)</li> <li>FedSimCLR as proposed in Fed-X (https://arxiv.org/pdf/2207.09158).</li> </ul> <p>NT-Xent is a contrastive loss in which each feature has a positive pair and the rest of the features are considered negative. It is computed based on the similarity of positive pairs relative to negative pairs.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>device to use for computation.</p> required <code>temperature</code> <code>float</code> <p>temperature to scale the logits.</p> <code>0.5</code> Source code in <code>fl4health/losses/contrastive_loss.py</code> <pre><code>def __init__(self, device: torch.device, temperature: float = 0.5) -&gt; None:\n    \"\"\"\n    Implementation of Normalized Temperature-Scaled Cross Entropy Loss (NT-Xent) proposed in\n    https://papers.nips.cc/paper_files/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html.\n\n    and notably used in:\n\n    - SimCLR (https://arxiv.org/pdf/2002.05709)\n    - FedSimCLR as proposed in Fed-X (https://arxiv.org/pdf/2207.09158).\n\n    NT-Xent is a contrastive loss in which each feature has a positive pair and the rest of the features\n    are considered negative. It is computed based on the similarity of positive pairs relative to negative\n    pairs.\n\n    Args:\n        device (torch.device): device to use for computation.\n        temperature (float): temperature to scale the logits.\n    \"\"\"\n    super().__init__()\n    self.device = device\n    self.temperature = temperature\n</code></pre>"},{"location":"api/#fl4health.losses.contrastive_loss.NtXentLoss.forward","title":"<code>forward(features, transformed_features)</code>","text":"<p>Compute the contrastive loss based on the features and <code>transformed_features</code>. Given N features and N <code>transformed_features</code> per batch, <code>features[i]</code> and <code>transformed_features[i]</code> are positive pairs and the remaining \\(2N - 2\\) are negative pairs.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>Features of input without transformation applied. Shaped (<code>batch_size</code>, <code>feature_dimension</code>).</p> required <code>transformed_features</code> <code>Tensor</code> <p>Features of input with transformation applied. Shaped (<code>batch_size</code>, <code>feature_dimension</code>).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Contrastive loss value.</p> Source code in <code>fl4health/losses/contrastive_loss.py</code> <pre><code>def forward(self, features: torch.Tensor, transformed_features: torch.Tensor) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the contrastive loss based on the features and ``transformed_features``. Given N features\n    and N ``transformed_features`` per batch, ``features[i]`` and ``transformed_features[i]`` are positive pairs\n    and the remaining \\(2N - 2\\) are negative pairs.\n\n    Args:\n        features (torch.Tensor): Features of input without transformation applied.\n            Shaped (``batch_size``, ``feature_dimension``).\n        transformed_features (torch.Tensor): Features of input with transformation applied.\n            Shaped (``batch_size``, ``feature_dimension``).\n\n    Returns:\n        (torch.Tensor): Contrastive loss value.\n    \"\"\"\n    features.to(self.device)\n    transformed_features.to(self.device)\n\n    # Ensure features and transformed_features are same shape\n    assert features.shape == transformed_features.shape\n    batch_size = features.shape[0]\n\n    # Concatenate features and transformed features. Normalize each feature with euclidean norm.\n    all_features = torch.concatenate([features, transformed_features], dim=0).to(self.device)\n    all_features = F.normalize(all_features, dim=-1)\n\n    # Compute similarity of each features with other features\n    # Equivalent to Cosine Similarity since feature are normalized\n    similarity_matrix = torch.matmul(all_features, all_features.T)\n\n    # Extract positive pairs from similarity matrix\n    # Positive pairs are elements (i, j) offset from matrix by batch size\n    # As a result of stacking feature and transformed_features\n    similarity_ij = torch.diag(similarity_matrix, diagonal=batch_size)\n    similarity_ji = torch.diag(similarity_matrix, diagonal=-batch_size)\n    positives = torch.concatenate([similarity_ij, similarity_ji], dim=0)\n\n    # Numerator is the sum of the exponent of positive similarities\n    numerator = torch.exp(positives / self.temperature)\n\n    # Denominator is all pair combinations except for diagonal which corresponds to a features similarity to itself\n    mask = (torch.ones(2 * batch_size, 2 * batch_size) - torch.eye(2 * batch_size)).to(self.device)\n    similarity_matrix_without_diagonal = torch.mul(similarity_matrix, mask)\n    denominator = torch.exp(similarity_matrix_without_diagonal / self.temperature)\n\n    # Final loss negative log likelihood\n    losses = -torch.log(numerator / denominator.sum(dim=1))\n\n    # Divide by 2 * batch size because pairs are double counted due to the symmetry of the similarity matrix\n    return torch.sum(losses) / (2 * batch_size)\n</code></pre>"},{"location":"api/#fl4health.losses.cosine_similarity_loss","title":"<code>cosine_similarity_loss</code>","text":""},{"location":"api/#fl4health.losses.cosine_similarity_loss.CosineSimilarityLoss","title":"<code>CosineSimilarityLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/losses/cosine_similarity_loss.py</code> <pre><code>class CosineSimilarityLoss(nn.Module):\n    def __init__(self, device: torch.device, dim: int = -1) -&gt; None:\n        \"\"\"\n        Cosine similarity loss between two torch Tensors.\n\n        Args:\n            device (torch.device): Which device this loss should be computed on.\n            dim (int, optional): Dimension where cosine similarity is computed. Defaults to -1.\n        \"\"\"\n        super().__init__()\n        self.cosine_similarity_function = nn.CosineSimilarity(dim=dim).to(device)\n\n    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Assumes that the tensors are provided \"batch first\" and computes the mean (over the batch) of the absolute\n        value of the cosine similarity between features in x1 and x2.\n\n        Args:\n            x1 (torch.Tensor): First set of tensors to compute cosine sim, shape (``batch_size``, ``n_features``)\n            x2 (torch.Tensor): Second set of tensors to compute cosine sim, shape (``batch_size``, ``n_features``)\n\n        Returns:\n            (torch.Tensor): Mean absolute value of the cosine similarity between vectors across the mutual batch size.\n        \"\"\"\n        assert len(x1) == len(x2), \"Tensors have different batch sizes\"\n        return torch.abs(self.cosine_similarity_function(x1, x2)).mean()\n</code></pre>"},{"location":"api/#fl4health.losses.cosine_similarity_loss.CosineSimilarityLoss.__init__","title":"<code>__init__(device, dim=-1)</code>","text":"<p>Cosine similarity loss between two torch Tensors.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Which device this loss should be computed on.</p> required <code>dim</code> <code>int</code> <p>Dimension where cosine similarity is computed. Defaults to -1.</p> <code>-1</code> Source code in <code>fl4health/losses/cosine_similarity_loss.py</code> <pre><code>def __init__(self, device: torch.device, dim: int = -1) -&gt; None:\n    \"\"\"\n    Cosine similarity loss between two torch Tensors.\n\n    Args:\n        device (torch.device): Which device this loss should be computed on.\n        dim (int, optional): Dimension where cosine similarity is computed. Defaults to -1.\n    \"\"\"\n    super().__init__()\n    self.cosine_similarity_function = nn.CosineSimilarity(dim=dim).to(device)\n</code></pre>"},{"location":"api/#fl4health.losses.cosine_similarity_loss.CosineSimilarityLoss.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Assumes that the tensors are provided \"batch first\" and computes the mean (over the batch) of the absolute value of the cosine similarity between features in x1 and x2.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>Tensor</code> <p>First set of tensors to compute cosine sim, shape (<code>batch_size</code>, <code>n_features</code>)</p> required <code>x2</code> <code>Tensor</code> <p>Second set of tensors to compute cosine sim, shape (<code>batch_size</code>, <code>n_features</code>)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Mean absolute value of the cosine similarity between vectors across the mutual batch size.</p> Source code in <code>fl4health/losses/cosine_similarity_loss.py</code> <pre><code>def forward(self, x1: torch.Tensor, x2: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Assumes that the tensors are provided \"batch first\" and computes the mean (over the batch) of the absolute\n    value of the cosine similarity between features in x1 and x2.\n\n    Args:\n        x1 (torch.Tensor): First set of tensors to compute cosine sim, shape (``batch_size``, ``n_features``)\n        x2 (torch.Tensor): Second set of tensors to compute cosine sim, shape (``batch_size``, ``n_features``)\n\n    Returns:\n        (torch.Tensor): Mean absolute value of the cosine similarity between vectors across the mutual batch size.\n    \"\"\"\n    assert len(x1) == len(x2), \"Tensors have different batch sizes\"\n    return torch.abs(self.cosine_similarity_function(x1, x2)).mean()\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss","title":"<code>deep_mmd_loss</code>","text":""},{"location":"api/#fl4health.losses.deep_mmd_loss.ModelLatentF","title":"<code>ModelLatentF</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>class ModelLatentF(torch.nn.Module):\n    def __init__(self, x_in_dim: int, hidden_dim: int, x_out_dim: int):\n        \"\"\"\n        Deep network for learning the deep kernel over features.\n\n        Args:\n            x_in_dim (int): The input dimension of the deep network.\n            hidden_dim (int): The hidden dimension of the deep network.\n            x_out_dim (int): The output dimension of the deep network.\n        \"\"\"\n        super().__init__()\n        self.latent = torch.nn.Sequential(\n            torch.nn.Linear(x_in_dim, hidden_dim, bias=True),\n            torch.nn.Softplus(),\n            torch.nn.Linear(hidden_dim, hidden_dim, bias=True),\n            torch.nn.Softplus(),\n            torch.nn.Linear(hidden_dim, hidden_dim, bias=True),\n            torch.nn.Softplus(),\n            torch.nn.Linear(hidden_dim, x_out_dim, bias=True),\n        )\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the deep network.\n\n        Args:\n            input (torch.Tensor): The input tensor to the deep network.\n\n        Returns:\n            (torch.Tensor): The output tensor of the deep network.\n        \"\"\"\n        return self.latent(input)\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss.ModelLatentF.__init__","title":"<code>__init__(x_in_dim, hidden_dim, x_out_dim)</code>","text":"<p>Deep network for learning the deep kernel over features.</p> <p>Parameters:</p> Name Type Description Default <code>x_in_dim</code> <code>int</code> <p>The input dimension of the deep network.</p> required <code>hidden_dim</code> <code>int</code> <p>The hidden dimension of the deep network.</p> required <code>x_out_dim</code> <code>int</code> <p>The output dimension of the deep network.</p> required Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>def __init__(self, x_in_dim: int, hidden_dim: int, x_out_dim: int):\n    \"\"\"\n    Deep network for learning the deep kernel over features.\n\n    Args:\n        x_in_dim (int): The input dimension of the deep network.\n        hidden_dim (int): The hidden dimension of the deep network.\n        x_out_dim (int): The output dimension of the deep network.\n    \"\"\"\n    super().__init__()\n    self.latent = torch.nn.Sequential(\n        torch.nn.Linear(x_in_dim, hidden_dim, bias=True),\n        torch.nn.Softplus(),\n        torch.nn.Linear(hidden_dim, hidden_dim, bias=True),\n        torch.nn.Softplus(),\n        torch.nn.Linear(hidden_dim, hidden_dim, bias=True),\n        torch.nn.Softplus(),\n        torch.nn.Linear(hidden_dim, x_out_dim, bias=True),\n    )\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss.ModelLatentF.forward","title":"<code>forward(input)</code>","text":"<p>Forward pass of the deep network.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor to the deep network.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output tensor of the deep network.</p> Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the deep network.\n\n    Args:\n        input (torch.Tensor): The input tensor to the deep network.\n\n    Returns:\n        (torch.Tensor): The output tensor of the deep network.\n    \"\"\"\n    return self.latent(input)\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss.DeepMmdLoss","title":"<code>DeepMmdLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>class DeepMmdLoss(torch.nn.Module):\n    def __init__(\n        self,\n        device: torch.device,\n        input_size: int,\n        hidden_size: int = 10,\n        output_size: int = 50,\n        lr: float = 0.001,\n        is_unbiased: bool = True,\n        gaussian_degree: int = 1,\n        optimization_steps: int = 5,\n    ) -&gt; None:\n        \"\"\"\n        Compute the Deep MMD (Maximum Mean Discrepancy) loss, as proposed in the paper Learning Deep Kernels for\n        Non-Parametric Two-Sample Tests. This loss function uses a kernel-based approach to assess whether two\n        samples are drawn from the same distribution. By minimizing this loss, we can learn a deep kernel that\n        reduces the MMD distance between two distributions, ensuring that the input feature representations are\n        aligned. This implementation is inspired by the original code from the paper:\n        https://github.com/fengliu90/DK-for-TST.\n\n        Args:\n            device (torch.device): Device onto which tensors should be moved.\n            input_size (int): The length of the input feature representations of the deep network as the deep\n                kernel used to compute the MMD loss.\n            hidden_size (int, optional): The hidden size of the deep network as the deep kernel used to compute\n                the MMD loss. Defaults to 10.\n            output_size (int, optional): The output size of the deep network as the deep kernel used to compute\n                the MMD loss. Defaults to 50.\n            lr (float, optional): Learning rate for training the Deep Kernel. Defaults to 0.001.\n            is_unbiased (bool, optional): Whether to use the unbiased estimator for the MMD loss. Defaults to True.\n            gaussian_degree (int, optional): The degree of the generalized Gaussian kernel. Defaults to 1.\n            optimization_steps (int, optional): The number of optimization steps to train the Deep Kernel in each\n                forward pass. Defaults to 5.\n        \"\"\"\n        super().__init__()\n        self.device = device\n        self.lr = lr\n        self.is_unbiased = is_unbiased\n        self.gaussian_degree = gaussian_degree  # generalized Gaussian (if L&gt;1)\n        self.optimization_steps = optimization_steps\n\n        # Initialize the model\n        self.featurizer = ModelLatentF(input_size, hidden_size, output_size).to(self.device)\n        # Set the model to evaluation mode as default\n        self.featurizer.eval()\n\n        # Initialize parameters\n        self.epsilon_opt: torch.Tensor = torch.log(torch.from_numpy(np.random.rand(1) * 10 ** (-10)).to(self.device))\n        self.epsilon_opt.requires_grad = False\n        self.sigma_q_opt: torch.Tensor = torch.sqrt(torch.tensor(2 * 32 * 32, dtype=torch.float).to(self.device))\n        self.sigma_q_opt.requires_grad = False\n        self.sigma_phi_opt: torch.Tensor = torch.sqrt(torch.tensor(0.005, dtype=torch.float).to(self.device))\n        self.sigma_phi_opt.requires_grad = False\n\n        # Initialize optimizers\n        self.optimizer_F = torch.optim.AdamW(\n            list(self.featurizer.parameters()) + [self.epsilon_opt] + [self.sigma_q_opt] + [self.sigma_phi_opt],\n            lr=self.lr,\n        )\n\n        # Set the model to training mode if required to train the Deep Kernel\n        self.training = False\n\n    def pairwise_distance_squared(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the paired distance between x and y.\n\n        Args:\n            x (torch.Tensor): The input tensor x.\n            y (torch.Tensor): The input tensor y.\n\n        Returns:\n            (torch.Tensor): The paired distance between X and Y.\n        \"\"\"\n        x_norm = (x**2).sum(1).view(-1, 1)\n        y_norm = (y**2).sum(1).view(1, -1)\n        paired_distance = x_norm + y_norm - 2.0 * torch.mm(x, torch.transpose(y, 0, 1))\n        paired_distance[paired_distance &lt; 0] = 0\n        return paired_distance\n\n    def h1_mean_var_gram(\n        self,\n        k_x: torch.Tensor,\n        k_y: torch.Tensor,\n        k_xy: torch.Tensor,\n        is_var_computed: bool,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n        \"\"\"\n        Compute value of MMD and std of MMD using kernel matrix.\n\n        Args:\n            k_x (torch.Tensor): The kernel matrix of x.\n            k_y (torch.Tensor): The kernel matrix of y.\n            k_xy (torch.Tensor): The kernel matrix of x and y.\n            is_var_computed (bool): Whether to compute the variance of the MMD.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor | None]): The value of MMD and the variance of MMD if required to\n                compute.\n        \"\"\"\n        nx = k_x.shape[0]\n        ny = k_y.shape[0]\n\n        if self.is_unbiased:\n            # compute the unbiased MMD estimator (\\hat{\\text{MMD}}_u^2) defined in Eq. (2) of the paper\n            xx = torch.div((torch.sum(k_x) - torch.sum(torch.diag(k_x))), (nx * (nx - 1)))\n            yy = torch.div((torch.sum(k_y) - torch.sum(torch.diag(k_y))), (ny * (ny - 1)))\n            xy = torch.div((torch.sum(k_xy) - torch.sum(torch.diag(k_xy))), (nx * (ny - 1)))\n\n        else:\n            # compute the biased MMD estimator (\\hat{\\text{MMD}}_b^2) defined below Equation (2) of the paper\n            xx = torch.div((torch.sum(k_x)), (nx * nx))\n            yy = torch.div((torch.sum(k_y)), (ny * ny))\n            xy = torch.div((torch.sum(k_xy)), (nx * ny))\n\n        mmd2 = xx - 2 * xy + yy\n        if not is_var_computed:\n            return mmd2, None\n        h_ij = k_x + k_y - k_xy - k_xy.transpose(0, 1)\n\n        # Compute the variance estimate of MMD defined in Equation (5) of the paper\n        v1 = (4.0 / ny**3) * (torch.dot(h_ij.sum(1), h_ij.sum(1)))\n        v2 = (4.0 / nx**4) * (h_ij.sum() ** 2)\n        variance_estimate = v1 - v2 + (10 ** (-8))\n        return mmd2, variance_estimate\n\n    def mmdu(\n        self,\n        features: torch.Tensor,\n        len_s: int,\n        features_org: torch.Tensor,\n        sigma_q: torch.Tensor,\n        sigma_phi: torch.Tensor,\n        epsilon: torch.Tensor,\n        is_smooth: bool = True,\n        is_var_computed: bool = True,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n        \"\"\"\n        Compute value of deep-kernel MMD and std of deep-kernel MMD using merged data.\n\n        Args:\n            features (torch.Tensor): The output features of the deep network.\n            len_s (int): The length of the sample.\n            features_org (torch.Tensor): The original input features of the deep network.\n            sigma_q (torch.Tensor): The ``sigma_q`` parameter.\n            sigma_phi (torch.Tensor): The ``sigma_phi`` parameter.\n            epsilon (torch.Tensor): The epsilon parameter.\n            is_smooth (bool, optional): Whether to use the smooth version of the MMD. Defaults to True.\n            is_var_computed (bool, optional): Whether to compute the variance of the MMD. Defaults to True.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor | None]): The value of MMD and the variance of MMD if required to\n                compute.\n        \"\"\"\n        x = features[0:len_s, :]  # fetch the sample 1 (features of deep networks)\n        y = features[len_s:, :]  # fetch the sample 2 (features of deep networks)\n        distance_xx = self.pairwise_distance_squared(x, x)\n        distance_yy = self.pairwise_distance_squared(y, y)\n        distance_xy = self.pairwise_distance_squared(x, y)\n\n        if is_smooth:\n            x_original = features_org[0:len_s, :]  # fetch the original sample 1\n            y_original = features_org[len_s:, :]  # fetch the original sample 2\n            distance_xx_original = self.pairwise_distance_squared(x_original, x_original)\n            distance_yy_original = self.pairwise_distance_squared(y_original, y_original)\n            distance_xy_original = self.pairwise_distance_squared(x_original, y_original)\n\n            kernel_x = (1 - epsilon) * torch.exp(\n                -((distance_xx / sigma_phi) ** self.gaussian_degree) - distance_xx_original / sigma_q\n            ) + epsilon * torch.exp(-distance_xx_original / sigma_q)\n            kernel_y = (1 - epsilon) * torch.exp(\n                -((distance_yy / sigma_phi) ** self.gaussian_degree) - distance_yy_original / sigma_q\n            ) + epsilon * torch.exp(-distance_yy_original / sigma_q)\n            kernel_xy = (1 - epsilon) * torch.exp(\n                -((distance_xy / sigma_phi) ** self.gaussian_degree) - distance_xy_original / sigma_q\n            ) + epsilon * torch.exp(-distance_xy_original / sigma_q)\n\n        else:\n            kernel_x = torch.exp(-distance_xx / sigma_phi)\n            kernel_y = torch.exp(-distance_yy / sigma_phi)\n            kernel_xy = torch.exp(-distance_xy / sigma_phi)\n\n        # kernel_x represents k_w(x_i, x_j), kernel_y represents k_w(y_i, y_j), kernel_xy represents\n        # k_w(x_i, y_j) for all i, j in the sample X and sample Y defined in Equation (1) of the paper\n        return self.h1_mean_var_gram(kernel_x, kernel_y, kernel_xy, is_var_computed)\n\n    def train_kernel(self, x: torch.Tensor, y: torch.Tensor) -&gt; None:\n        \"\"\"\n        Train the Deep MMD kernel.\n\n        Args:\n            x (torch.Tensor): The input tensor x.\n            y (torch.Tensor): The input tensor y.\n        \"\"\"\n        self.featurizer.train()\n        self.sigma_q_opt.requires_grad = True\n        self.sigma_phi_opt.requires_grad = True\n        self.epsilon_opt.requires_grad = True\n\n        # Shuffle the data to ensure they are not always presented in the same order for training\n        # which might lead to overfitting\n        indices = torch.randperm(y.size(0))\n        y_shuffled = y[indices]\n\n        features = torch.cat([x, y_shuffled], 0)\n\n        # ------------------------------\n        #  Train deep network for MMD-D\n        # ------------------------------\n        # Zero gradients\n        self.optimizer_F.zero_grad()\n        # Compute output of deep network\n        model_output = self.featurizer(features)\n        # Compute epsilon, sigma_q and sigma_phi in \\kappa_w(x, y) in Equation (1) of the paper\n        epsilon = torch.exp(self.epsilon_opt) / (1 + torch.exp(self.epsilon_opt))\n        sigma_q = self.sigma_q_opt**2\n        sigma_phi = self.sigma_phi_opt**2\n        # Compute Deep MMD value and variance estimates\n        mmd_value_estimate, mmd_var_estimate = self.mmdu(\n            features=model_output,\n            len_s=x.shape[0],\n            features_org=features.view(features.shape[0], -1),\n            sigma_q=sigma_q,\n            sigma_phi=sigma_phi,\n            epsilon=epsilon,\n            is_var_computed=True,\n        )\n        if mmd_var_estimate is None:\n            raise AssertionError(\"Error: Variance of MMD is not computed. Please set is_var_computed=True.\")\n        mmd_std_estimate = torch.sqrt(mmd_var_estimate)\n        # Forming \\hat{J}_{\\lambda} defined in Equation (4) of the paper (STAT_u)\n        stat_u = torch.div(-1 * mmd_value_estimate, mmd_std_estimate)\n        # Compute gradient\n        stat_u.backward()\n        # Update weights using gradient descent\n        self.optimizer_F.step()\n\n    def compute_kernel(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the Deep MMD Loss.\n\n        Args:\n            x (torch.Tensor): The input tensor x.\n            y (torch.Tensor): The input tensor y.\n\n        Returns:\n            (torch.Tensor): The value of Deep MMD Loss.\n        \"\"\"\n        self.featurizer.eval()\n        self.sigma_q_opt.requires_grad = False\n        self.sigma_phi_opt.requires_grad = False\n        self.epsilon_opt.requires_grad = False\n\n        features = torch.cat([x, y], 0)\n\n        # Compute output of deep network\n        model_output = self.featurizer(features)\n        # Compute epsilon, sigma_q and sigma_phi in \\kappa_w(x, y) in Equation (1) of the paper\n        epsilon = torch.exp(self.epsilon_opt) / (1 + torch.exp(self.epsilon_opt))\n        sigma_q = self.sigma_q_opt**2\n        sigma_phi = self.sigma_phi_opt**2\n        # Compute Deep MMD value estimates\n        mmd_value_estimate, _ = self.mmdu(\n            features=model_output,\n            len_s=x.shape[0],\n            features_org=features.view(features.shape[0], -1),\n            sigma_q=sigma_q,\n            sigma_phi=sigma_phi,\n            epsilon=epsilon,\n            is_var_computed=False,\n        )\n\n        return mmd_value_estimate\n\n    def forward(self, x_s: torch.Tensor, x_t: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the Deep MMD Loss where it first trains the deep kernel for number of optimization\n        steps and then computes the MMD loss.\n\n        Args:\n            x_s (torch.Tensor): The source input tensor.\n            x_t (torch.Tensor): The target input tensor.\n\n        Returns:\n            (torch.Tensor): The value of Deep MMD Loss.\n        \"\"\"\n        if self.training:\n            for _ in range(self.optimization_steps):\n                self.train_kernel(x_s.clone().detach(), x_t.clone().detach())\n\n        return self.compute_kernel(x_s, x_t)\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss.DeepMmdLoss.__init__","title":"<code>__init__(device, input_size, hidden_size=10, output_size=50, lr=0.001, is_unbiased=True, gaussian_degree=1, optimization_steps=5)</code>","text":"<p>Compute the Deep MMD (Maximum Mean Discrepancy) loss, as proposed in the paper Learning Deep Kernels for Non-Parametric Two-Sample Tests. This loss function uses a kernel-based approach to assess whether two samples are drawn from the same distribution. By minimizing this loss, we can learn a deep kernel that reduces the MMD distance between two distributions, ensuring that the input feature representations are aligned. This implementation is inspired by the original code from the paper: https://github.com/fengliu90/DK-for-TST.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Device onto which tensors should be moved.</p> required <code>input_size</code> <code>int</code> <p>The length of the input feature representations of the deep network as the deep kernel used to compute the MMD loss.</p> required <code>hidden_size</code> <code>int</code> <p>The hidden size of the deep network as the deep kernel used to compute the MMD loss. Defaults to 10.</p> <code>10</code> <code>output_size</code> <code>int</code> <p>The output size of the deep network as the deep kernel used to compute the MMD loss. Defaults to 50.</p> <code>50</code> <code>lr</code> <code>float</code> <p>Learning rate for training the Deep Kernel. Defaults to 0.001.</p> <code>0.001</code> <code>is_unbiased</code> <code>bool</code> <p>Whether to use the unbiased estimator for the MMD loss. Defaults to True.</p> <code>True</code> <code>gaussian_degree</code> <code>int</code> <p>The degree of the generalized Gaussian kernel. Defaults to 1.</p> <code>1</code> <code>optimization_steps</code> <code>int</code> <p>The number of optimization steps to train the Deep Kernel in each forward pass. Defaults to 5.</p> <code>5</code> Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>def __init__(\n    self,\n    device: torch.device,\n    input_size: int,\n    hidden_size: int = 10,\n    output_size: int = 50,\n    lr: float = 0.001,\n    is_unbiased: bool = True,\n    gaussian_degree: int = 1,\n    optimization_steps: int = 5,\n) -&gt; None:\n    \"\"\"\n    Compute the Deep MMD (Maximum Mean Discrepancy) loss, as proposed in the paper Learning Deep Kernels for\n    Non-Parametric Two-Sample Tests. This loss function uses a kernel-based approach to assess whether two\n    samples are drawn from the same distribution. By minimizing this loss, we can learn a deep kernel that\n    reduces the MMD distance between two distributions, ensuring that the input feature representations are\n    aligned. This implementation is inspired by the original code from the paper:\n    https://github.com/fengliu90/DK-for-TST.\n\n    Args:\n        device (torch.device): Device onto which tensors should be moved.\n        input_size (int): The length of the input feature representations of the deep network as the deep\n            kernel used to compute the MMD loss.\n        hidden_size (int, optional): The hidden size of the deep network as the deep kernel used to compute\n            the MMD loss. Defaults to 10.\n        output_size (int, optional): The output size of the deep network as the deep kernel used to compute\n            the MMD loss. Defaults to 50.\n        lr (float, optional): Learning rate for training the Deep Kernel. Defaults to 0.001.\n        is_unbiased (bool, optional): Whether to use the unbiased estimator for the MMD loss. Defaults to True.\n        gaussian_degree (int, optional): The degree of the generalized Gaussian kernel. Defaults to 1.\n        optimization_steps (int, optional): The number of optimization steps to train the Deep Kernel in each\n            forward pass. Defaults to 5.\n    \"\"\"\n    super().__init__()\n    self.device = device\n    self.lr = lr\n    self.is_unbiased = is_unbiased\n    self.gaussian_degree = gaussian_degree  # generalized Gaussian (if L&gt;1)\n    self.optimization_steps = optimization_steps\n\n    # Initialize the model\n    self.featurizer = ModelLatentF(input_size, hidden_size, output_size).to(self.device)\n    # Set the model to evaluation mode as default\n    self.featurizer.eval()\n\n    # Initialize parameters\n    self.epsilon_opt: torch.Tensor = torch.log(torch.from_numpy(np.random.rand(1) * 10 ** (-10)).to(self.device))\n    self.epsilon_opt.requires_grad = False\n    self.sigma_q_opt: torch.Tensor = torch.sqrt(torch.tensor(2 * 32 * 32, dtype=torch.float).to(self.device))\n    self.sigma_q_opt.requires_grad = False\n    self.sigma_phi_opt: torch.Tensor = torch.sqrt(torch.tensor(0.005, dtype=torch.float).to(self.device))\n    self.sigma_phi_opt.requires_grad = False\n\n    # Initialize optimizers\n    self.optimizer_F = torch.optim.AdamW(\n        list(self.featurizer.parameters()) + [self.epsilon_opt] + [self.sigma_q_opt] + [self.sigma_phi_opt],\n        lr=self.lr,\n    )\n\n    # Set the model to training mode if required to train the Deep Kernel\n    self.training = False\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss.DeepMmdLoss.pairwise_distance_squared","title":"<code>pairwise_distance_squared(x, y)</code>","text":"<p>Compute the paired distance between x and y.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor x.</p> required <code>y</code> <code>Tensor</code> <p>The input tensor y.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The paired distance between X and Y.</p> Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>def pairwise_distance_squared(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the paired distance between x and y.\n\n    Args:\n        x (torch.Tensor): The input tensor x.\n        y (torch.Tensor): The input tensor y.\n\n    Returns:\n        (torch.Tensor): The paired distance between X and Y.\n    \"\"\"\n    x_norm = (x**2).sum(1).view(-1, 1)\n    y_norm = (y**2).sum(1).view(1, -1)\n    paired_distance = x_norm + y_norm - 2.0 * torch.mm(x, torch.transpose(y, 0, 1))\n    paired_distance[paired_distance &lt; 0] = 0\n    return paired_distance\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss.DeepMmdLoss.h1_mean_var_gram","title":"<code>h1_mean_var_gram(k_x, k_y, k_xy, is_var_computed)</code>","text":"<p>Compute value of MMD and std of MMD using kernel matrix.</p> <p>Parameters:</p> Name Type Description Default <code>k_x</code> <code>Tensor</code> <p>The kernel matrix of x.</p> required <code>k_y</code> <code>Tensor</code> <p>The kernel matrix of y.</p> required <code>k_xy</code> <code>Tensor</code> <p>The kernel matrix of x and y.</p> required <code>is_var_computed</code> <code>bool</code> <p>Whether to compute the variance of the MMD.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>The value of MMD and the variance of MMD if required to compute.</p> Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>def h1_mean_var_gram(\n    self,\n    k_x: torch.Tensor,\n    k_y: torch.Tensor,\n    k_xy: torch.Tensor,\n    is_var_computed: bool,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"\n    Compute value of MMD and std of MMD using kernel matrix.\n\n    Args:\n        k_x (torch.Tensor): The kernel matrix of x.\n        k_y (torch.Tensor): The kernel matrix of y.\n        k_xy (torch.Tensor): The kernel matrix of x and y.\n        is_var_computed (bool): Whether to compute the variance of the MMD.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor | None]): The value of MMD and the variance of MMD if required to\n            compute.\n    \"\"\"\n    nx = k_x.shape[0]\n    ny = k_y.shape[0]\n\n    if self.is_unbiased:\n        # compute the unbiased MMD estimator (\\hat{\\text{MMD}}_u^2) defined in Eq. (2) of the paper\n        xx = torch.div((torch.sum(k_x) - torch.sum(torch.diag(k_x))), (nx * (nx - 1)))\n        yy = torch.div((torch.sum(k_y) - torch.sum(torch.diag(k_y))), (ny * (ny - 1)))\n        xy = torch.div((torch.sum(k_xy) - torch.sum(torch.diag(k_xy))), (nx * (ny - 1)))\n\n    else:\n        # compute the biased MMD estimator (\\hat{\\text{MMD}}_b^2) defined below Equation (2) of the paper\n        xx = torch.div((torch.sum(k_x)), (nx * nx))\n        yy = torch.div((torch.sum(k_y)), (ny * ny))\n        xy = torch.div((torch.sum(k_xy)), (nx * ny))\n\n    mmd2 = xx - 2 * xy + yy\n    if not is_var_computed:\n        return mmd2, None\n    h_ij = k_x + k_y - k_xy - k_xy.transpose(0, 1)\n\n    # Compute the variance estimate of MMD defined in Equation (5) of the paper\n    v1 = (4.0 / ny**3) * (torch.dot(h_ij.sum(1), h_ij.sum(1)))\n    v2 = (4.0 / nx**4) * (h_ij.sum() ** 2)\n    variance_estimate = v1 - v2 + (10 ** (-8))\n    return mmd2, variance_estimate\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss.DeepMmdLoss.mmdu","title":"<code>mmdu(features, len_s, features_org, sigma_q, sigma_phi, epsilon, is_smooth=True, is_var_computed=True)</code>","text":"<p>Compute value of deep-kernel MMD and std of deep-kernel MMD using merged data.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>The output features of the deep network.</p> required <code>len_s</code> <code>int</code> <p>The length of the sample.</p> required <code>features_org</code> <code>Tensor</code> <p>The original input features of the deep network.</p> required <code>sigma_q</code> <code>Tensor</code> <p>The <code>sigma_q</code> parameter.</p> required <code>sigma_phi</code> <code>Tensor</code> <p>The <code>sigma_phi</code> parameter.</p> required <code>epsilon</code> <code>Tensor</code> <p>The epsilon parameter.</p> required <code>is_smooth</code> <code>bool</code> <p>Whether to use the smooth version of the MMD. Defaults to True.</p> <code>True</code> <code>is_var_computed</code> <code>bool</code> <p>Whether to compute the variance of the MMD. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | None]</code> <p>The value of MMD and the variance of MMD if required to compute.</p> Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>def mmdu(\n    self,\n    features: torch.Tensor,\n    len_s: int,\n    features_org: torch.Tensor,\n    sigma_q: torch.Tensor,\n    sigma_phi: torch.Tensor,\n    epsilon: torch.Tensor,\n    is_smooth: bool = True,\n    is_var_computed: bool = True,\n) -&gt; tuple[torch.Tensor, torch.Tensor | None]:\n    \"\"\"\n    Compute value of deep-kernel MMD and std of deep-kernel MMD using merged data.\n\n    Args:\n        features (torch.Tensor): The output features of the deep network.\n        len_s (int): The length of the sample.\n        features_org (torch.Tensor): The original input features of the deep network.\n        sigma_q (torch.Tensor): The ``sigma_q`` parameter.\n        sigma_phi (torch.Tensor): The ``sigma_phi`` parameter.\n        epsilon (torch.Tensor): The epsilon parameter.\n        is_smooth (bool, optional): Whether to use the smooth version of the MMD. Defaults to True.\n        is_var_computed (bool, optional): Whether to compute the variance of the MMD. Defaults to True.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor | None]): The value of MMD and the variance of MMD if required to\n            compute.\n    \"\"\"\n    x = features[0:len_s, :]  # fetch the sample 1 (features of deep networks)\n    y = features[len_s:, :]  # fetch the sample 2 (features of deep networks)\n    distance_xx = self.pairwise_distance_squared(x, x)\n    distance_yy = self.pairwise_distance_squared(y, y)\n    distance_xy = self.pairwise_distance_squared(x, y)\n\n    if is_smooth:\n        x_original = features_org[0:len_s, :]  # fetch the original sample 1\n        y_original = features_org[len_s:, :]  # fetch the original sample 2\n        distance_xx_original = self.pairwise_distance_squared(x_original, x_original)\n        distance_yy_original = self.pairwise_distance_squared(y_original, y_original)\n        distance_xy_original = self.pairwise_distance_squared(x_original, y_original)\n\n        kernel_x = (1 - epsilon) * torch.exp(\n            -((distance_xx / sigma_phi) ** self.gaussian_degree) - distance_xx_original / sigma_q\n        ) + epsilon * torch.exp(-distance_xx_original / sigma_q)\n        kernel_y = (1 - epsilon) * torch.exp(\n            -((distance_yy / sigma_phi) ** self.gaussian_degree) - distance_yy_original / sigma_q\n        ) + epsilon * torch.exp(-distance_yy_original / sigma_q)\n        kernel_xy = (1 - epsilon) * torch.exp(\n            -((distance_xy / sigma_phi) ** self.gaussian_degree) - distance_xy_original / sigma_q\n        ) + epsilon * torch.exp(-distance_xy_original / sigma_q)\n\n    else:\n        kernel_x = torch.exp(-distance_xx / sigma_phi)\n        kernel_y = torch.exp(-distance_yy / sigma_phi)\n        kernel_xy = torch.exp(-distance_xy / sigma_phi)\n\n    # kernel_x represents k_w(x_i, x_j), kernel_y represents k_w(y_i, y_j), kernel_xy represents\n    # k_w(x_i, y_j) for all i, j in the sample X and sample Y defined in Equation (1) of the paper\n    return self.h1_mean_var_gram(kernel_x, kernel_y, kernel_xy, is_var_computed)\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss.DeepMmdLoss.train_kernel","title":"<code>train_kernel(x, y)</code>","text":"<p>Train the Deep MMD kernel.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor x.</p> required <code>y</code> <code>Tensor</code> <p>The input tensor y.</p> required Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>def train_kernel(self, x: torch.Tensor, y: torch.Tensor) -&gt; None:\n    \"\"\"\n    Train the Deep MMD kernel.\n\n    Args:\n        x (torch.Tensor): The input tensor x.\n        y (torch.Tensor): The input tensor y.\n    \"\"\"\n    self.featurizer.train()\n    self.sigma_q_opt.requires_grad = True\n    self.sigma_phi_opt.requires_grad = True\n    self.epsilon_opt.requires_grad = True\n\n    # Shuffle the data to ensure they are not always presented in the same order for training\n    # which might lead to overfitting\n    indices = torch.randperm(y.size(0))\n    y_shuffled = y[indices]\n\n    features = torch.cat([x, y_shuffled], 0)\n\n    # ------------------------------\n    #  Train deep network for MMD-D\n    # ------------------------------\n    # Zero gradients\n    self.optimizer_F.zero_grad()\n    # Compute output of deep network\n    model_output = self.featurizer(features)\n    # Compute epsilon, sigma_q and sigma_phi in \\kappa_w(x, y) in Equation (1) of the paper\n    epsilon = torch.exp(self.epsilon_opt) / (1 + torch.exp(self.epsilon_opt))\n    sigma_q = self.sigma_q_opt**2\n    sigma_phi = self.sigma_phi_opt**2\n    # Compute Deep MMD value and variance estimates\n    mmd_value_estimate, mmd_var_estimate = self.mmdu(\n        features=model_output,\n        len_s=x.shape[0],\n        features_org=features.view(features.shape[0], -1),\n        sigma_q=sigma_q,\n        sigma_phi=sigma_phi,\n        epsilon=epsilon,\n        is_var_computed=True,\n    )\n    if mmd_var_estimate is None:\n        raise AssertionError(\"Error: Variance of MMD is not computed. Please set is_var_computed=True.\")\n    mmd_std_estimate = torch.sqrt(mmd_var_estimate)\n    # Forming \\hat{J}_{\\lambda} defined in Equation (4) of the paper (STAT_u)\n    stat_u = torch.div(-1 * mmd_value_estimate, mmd_std_estimate)\n    # Compute gradient\n    stat_u.backward()\n    # Update weights using gradient descent\n    self.optimizer_F.step()\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss.DeepMmdLoss.compute_kernel","title":"<code>compute_kernel(x, y)</code>","text":"<p>Compute the Deep MMD Loss.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor x.</p> required <code>y</code> <code>Tensor</code> <p>The input tensor y.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The value of Deep MMD Loss.</p> Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>def compute_kernel(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the Deep MMD Loss.\n\n    Args:\n        x (torch.Tensor): The input tensor x.\n        y (torch.Tensor): The input tensor y.\n\n    Returns:\n        (torch.Tensor): The value of Deep MMD Loss.\n    \"\"\"\n    self.featurizer.eval()\n    self.sigma_q_opt.requires_grad = False\n    self.sigma_phi_opt.requires_grad = False\n    self.epsilon_opt.requires_grad = False\n\n    features = torch.cat([x, y], 0)\n\n    # Compute output of deep network\n    model_output = self.featurizer(features)\n    # Compute epsilon, sigma_q and sigma_phi in \\kappa_w(x, y) in Equation (1) of the paper\n    epsilon = torch.exp(self.epsilon_opt) / (1 + torch.exp(self.epsilon_opt))\n    sigma_q = self.sigma_q_opt**2\n    sigma_phi = self.sigma_phi_opt**2\n    # Compute Deep MMD value estimates\n    mmd_value_estimate, _ = self.mmdu(\n        features=model_output,\n        len_s=x.shape[0],\n        features_org=features.view(features.shape[0], -1),\n        sigma_q=sigma_q,\n        sigma_phi=sigma_phi,\n        epsilon=epsilon,\n        is_var_computed=False,\n    )\n\n    return mmd_value_estimate\n</code></pre>"},{"location":"api/#fl4health.losses.deep_mmd_loss.DeepMmdLoss.forward","title":"<code>forward(x_s, x_t)</code>","text":"<p>Forward pass of the Deep MMD Loss where it first trains the deep kernel for number of optimization steps and then computes the MMD loss.</p> <p>Parameters:</p> Name Type Description Default <code>x_s</code> <code>Tensor</code> <p>The source input tensor.</p> required <code>x_t</code> <code>Tensor</code> <p>The target input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The value of Deep MMD Loss.</p> Source code in <code>fl4health/losses/deep_mmd_loss.py</code> <pre><code>def forward(self, x_s: torch.Tensor, x_t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass of the Deep MMD Loss where it first trains the deep kernel for number of optimization\n    steps and then computes the MMD loss.\n\n    Args:\n        x_s (torch.Tensor): The source input tensor.\n        x_t (torch.Tensor): The target input tensor.\n\n    Returns:\n        (torch.Tensor): The value of Deep MMD Loss.\n    \"\"\"\n    if self.training:\n        for _ in range(self.optimization_steps):\n            self.train_kernel(x_s.clone().detach(), x_t.clone().detach())\n\n    return self.compute_kernel(x_s, x_t)\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config","title":"<code>fenda_loss_config</code>","text":""},{"location":"api/#fl4health.losses.fenda_loss_config.PerFclLossContainer","title":"<code>PerFclLossContainer</code>","text":"Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>class PerFclLossContainer:\n    def __init__(\n        self,\n        device: torch.device,\n        global_feature_contrastive_loss_weight: float,\n        local_feature_contrastive_loss_weight: float,\n        global_feature_loss_temperature: float = 0.5,\n        local_feature_loss_temperature: float = 0.5,\n    ) -&gt; None:\n        \"\"\"\n        Container to hold the different pieces associated with PerFCL Loss.\n\n        Args:\n            device (torch.device): Device to which the loss will be sent and computed on.\n            global_feature_contrastive_loss_weight (float): Weight on the global contrastive loss function.\n            local_feature_contrastive_loss_weight (float): Weight on the local model contrastive loss function.\n            global_feature_loss_temperature (float, optional): Temperature parameter on the global contrastive loss\n                function. Defaults to 0.5.\n            local_feature_loss_temperature (float, optional): Temperature parameter on the local contrastive loss\n                function. Defaults to 0.5.\n        \"\"\"\n        self.global_feature_contrastive_loss_weight = global_feature_contrastive_loss_weight\n        self.local_feature_contrastive_loss_weight = local_feature_contrastive_loss_weight\n        self.perfcl_loss_function = PerFclLoss(device, global_feature_loss_temperature, local_feature_loss_temperature)\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config.PerFclLossContainer.__init__","title":"<code>__init__(device, global_feature_contrastive_loss_weight, local_feature_contrastive_loss_weight, global_feature_loss_temperature=0.5, local_feature_loss_temperature=0.5)</code>","text":"<p>Container to hold the different pieces associated with PerFCL Loss.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Device to which the loss will be sent and computed on.</p> required <code>global_feature_contrastive_loss_weight</code> <code>float</code> <p>Weight on the global contrastive loss function.</p> required <code>local_feature_contrastive_loss_weight</code> <code>float</code> <p>Weight on the local model contrastive loss function.</p> required <code>global_feature_loss_temperature</code> <code>float</code> <p>Temperature parameter on the global contrastive loss function. Defaults to 0.5.</p> <code>0.5</code> <code>local_feature_loss_temperature</code> <code>float</code> <p>Temperature parameter on the local contrastive loss function. Defaults to 0.5.</p> <code>0.5</code> Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>def __init__(\n    self,\n    device: torch.device,\n    global_feature_contrastive_loss_weight: float,\n    local_feature_contrastive_loss_weight: float,\n    global_feature_loss_temperature: float = 0.5,\n    local_feature_loss_temperature: float = 0.5,\n) -&gt; None:\n    \"\"\"\n    Container to hold the different pieces associated with PerFCL Loss.\n\n    Args:\n        device (torch.device): Device to which the loss will be sent and computed on.\n        global_feature_contrastive_loss_weight (float): Weight on the global contrastive loss function.\n        local_feature_contrastive_loss_weight (float): Weight on the local model contrastive loss function.\n        global_feature_loss_temperature (float, optional): Temperature parameter on the global contrastive loss\n            function. Defaults to 0.5.\n        local_feature_loss_temperature (float, optional): Temperature parameter on the local contrastive loss\n            function. Defaults to 0.5.\n    \"\"\"\n    self.global_feature_contrastive_loss_weight = global_feature_contrastive_loss_weight\n    self.local_feature_contrastive_loss_weight = local_feature_contrastive_loss_weight\n    self.perfcl_loss_function = PerFclLoss(device, global_feature_loss_temperature, local_feature_loss_temperature)\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config.CosineSimilarityLossContainer","title":"<code>CosineSimilarityLossContainer</code>","text":"Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>class CosineSimilarityLossContainer:\n    def __init__(self, device: torch.device, cos_sim_loss_weight: float) -&gt; None:\n        \"\"\"\n        Container to hold the different pieces associated with cosine similarity.\n\n        Args:\n            device (torch.device): Device to which the loss will be sent and computed on.\n            cos_sim_loss_weight (float): Weight associated with the cosine loss function in optimization.\n        \"\"\"\n        self.cos_sim_loss_weight = cos_sim_loss_weight\n        self.cos_sim_loss_function = CosineSimilarityLoss(device)\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config.CosineSimilarityLossContainer.__init__","title":"<code>__init__(device, cos_sim_loss_weight)</code>","text":"<p>Container to hold the different pieces associated with cosine similarity.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Device to which the loss will be sent and computed on.</p> required <code>cos_sim_loss_weight</code> <code>float</code> <p>Weight associated with the cosine loss function in optimization.</p> required Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>def __init__(self, device: torch.device, cos_sim_loss_weight: float) -&gt; None:\n    \"\"\"\n    Container to hold the different pieces associated with cosine similarity.\n\n    Args:\n        device (torch.device): Device to which the loss will be sent and computed on.\n        cos_sim_loss_weight (float): Weight associated with the cosine loss function in optimization.\n    \"\"\"\n    self.cos_sim_loss_weight = cos_sim_loss_weight\n    self.cos_sim_loss_function = CosineSimilarityLoss(device)\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config.MoonContrastiveLossContainer","title":"<code>MoonContrastiveLossContainer</code>","text":"Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>class MoonContrastiveLossContainer:\n    def __init__(self, device: torch.device, contrastive_loss_weight: float, temperature: float = 0.5) -&gt; None:\n        \"\"\"\n        Container to hold the different pieces associated with Moon Contrastive loss function.\n\n        Args:\n            device (torch.device): Device to which the loss will be sent and computed on.\n            contrastive_loss_weight (float): Weight associated with the contrastive loss function in optimization.\n            temperature (float, optional): Temperature parameter on the contrastive loss function.\n                Defaults to 0.5.\n        \"\"\"\n        self.contrastive_loss_weight = contrastive_loss_weight\n        self.contrastive_loss_function = MoonContrastiveLoss(device, temperature)\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config.MoonContrastiveLossContainer.__init__","title":"<code>__init__(device, contrastive_loss_weight, temperature=0.5)</code>","text":"<p>Container to hold the different pieces associated with Moon Contrastive loss function.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Device to which the loss will be sent and computed on.</p> required <code>contrastive_loss_weight</code> <code>float</code> <p>Weight associated with the contrastive loss function in optimization.</p> required <code>temperature</code> <code>float</code> <p>Temperature parameter on the contrastive loss function. Defaults to 0.5.</p> <code>0.5</code> Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>def __init__(self, device: torch.device, contrastive_loss_weight: float, temperature: float = 0.5) -&gt; None:\n    \"\"\"\n    Container to hold the different pieces associated with Moon Contrastive loss function.\n\n    Args:\n        device (torch.device): Device to which the loss will be sent and computed on.\n        contrastive_loss_weight (float): Weight associated with the contrastive loss function in optimization.\n        temperature (float, optional): Temperature parameter on the contrastive loss function.\n            Defaults to 0.5.\n    \"\"\"\n    self.contrastive_loss_weight = contrastive_loss_weight\n    self.contrastive_loss_function = MoonContrastiveLoss(device, temperature)\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config.ConstrainedFendaLossContainer","title":"<code>ConstrainedFendaLossContainer</code>","text":"Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>class ConstrainedFendaLossContainer:\n    def __init__(\n        self,\n        perfcl_loss_config: PerFclLossContainer | None,\n        cosine_similarity_loss_config: CosineSimilarityLossContainer | None,\n        contrastive_loss_config: MoonContrastiveLossContainer | None,\n    ) -&gt; None:\n        \"\"\"\n        Container to gather all of the possible loss functions used in constrained FENDA model optimization.\n\n        Args:\n            perfcl_loss_config (PerFclLossContainer | None): PerFCL loss container. If none, the loss isn not used.\n            cosine_similarity_loss_config (CosineSimilarityLossContainer | None): Cosine similarity loss container.\n                If none the loss is not used.\n            contrastive_loss_config (MoonContrastiveLossContainer | None): Contrastive loss container. If none, the\n                loss is not used.\n        \"\"\"\n        self.perfcl_loss_config = perfcl_loss_config\n        self.cos_sim_loss_config = cosine_similarity_loss_config\n        self.contrastive_loss_config = contrastive_loss_config\n\n    def has_perfcl_loss(self) -&gt; bool:\n        return self.perfcl_loss_config is not None\n\n    def has_cosine_similarity_loss(self) -&gt; bool:\n        return self.cos_sim_loss_config is not None\n\n    def has_contrastive_loss(self) -&gt; bool:\n        return self.contrastive_loss_config is not None\n\n    def compute_contrastive_loss(\n        self, features: torch.Tensor, positive_pairs: torch.Tensor, negative_pairs: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the contrastive loss, if it exists, using the configuration.\n\n        Args:\n            features (torch.Tensor): features from the model.\n            positive_pairs (torch.Tensor): positive pair features to compare to.\n            negative_pairs (torch.Tensor): negative pair features to compare to.\n\n        Returns:\n            (torch.Tensor): loss function\n        \"\"\"\n        assert self.contrastive_loss_config is not None\n        contrastive_loss = self.contrastive_loss_config.contrastive_loss_function(\n            features, positive_pairs, negative_pairs\n        )\n        return self.contrastive_loss_config.contrastive_loss_weight * contrastive_loss\n\n    def compute_cosine_similarity_loss(\n        self, first_features: torch.Tensor, second_features: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the cosine loss, if it exists, using the configuration.\n\n        Args:\n            first_features (torch.Tensor): first set of features in the cosine comparison\n            second_features (torch.Tensor): second set of features in the cosine comparison\n\n        Returns:\n            (torch.Tensor): cosine similarity loss between the provided features.\n        \"\"\"\n        assert self.cos_sim_loss_config is not None\n        cosine_similarity_loss = self.cos_sim_loss_config.cos_sim_loss_function(first_features, second_features)\n        return self.cos_sim_loss_config.cos_sim_loss_weight * cosine_similarity_loss\n\n    def compute_perfcl_loss(\n        self,\n        local_features: torch.Tensor,\n        old_local_features: torch.Tensor,\n        global_features: torch.Tensor,\n        old_global_features: torch.Tensor,\n        initial_global_features: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Compute the PerFCL loss, if it exists, using the configuration.\n\n        Args:\n            local_features (torch.Tensor): See PerFCL loss documentation\n            old_local_features (torch.Tensor): See PerFCL loss documentation\n            global_features (torch.Tensor): See PerFCL loss documentation\n            old_global_features (torch.Tensor): See PerFCL loss documentation\n            initial_global_features (torch.Tensor): See PerFCL loss documentation\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): PerFCL loss based on the input values\n        \"\"\"\n        assert self.perfcl_loss_config is not None\n        global_feature_contrastive_loss, local_feature_contrastive_loss = self.perfcl_loss_config.perfcl_loss_function(\n            local_features, old_local_features, global_features, old_global_features, initial_global_features\n        )\n        return (\n            self.perfcl_loss_config.global_feature_contrastive_loss_weight * global_feature_contrastive_loss,\n            self.perfcl_loss_config.local_feature_contrastive_loss_weight * local_feature_contrastive_loss,\n        )\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config.ConstrainedFendaLossContainer.__init__","title":"<code>__init__(perfcl_loss_config, cosine_similarity_loss_config, contrastive_loss_config)</code>","text":"<p>Container to gather all of the possible loss functions used in constrained FENDA model optimization.</p> <p>Parameters:</p> Name Type Description Default <code>perfcl_loss_config</code> <code>PerFclLossContainer | None</code> <p>PerFCL loss container. If none, the loss isn not used.</p> required <code>cosine_similarity_loss_config</code> <code>CosineSimilarityLossContainer | None</code> <p>Cosine similarity loss container. If none the loss is not used.</p> required <code>contrastive_loss_config</code> <code>MoonContrastiveLossContainer | None</code> <p>Contrastive loss container. If none, the loss is not used.</p> required Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>def __init__(\n    self,\n    perfcl_loss_config: PerFclLossContainer | None,\n    cosine_similarity_loss_config: CosineSimilarityLossContainer | None,\n    contrastive_loss_config: MoonContrastiveLossContainer | None,\n) -&gt; None:\n    \"\"\"\n    Container to gather all of the possible loss functions used in constrained FENDA model optimization.\n\n    Args:\n        perfcl_loss_config (PerFclLossContainer | None): PerFCL loss container. If none, the loss isn not used.\n        cosine_similarity_loss_config (CosineSimilarityLossContainer | None): Cosine similarity loss container.\n            If none the loss is not used.\n        contrastive_loss_config (MoonContrastiveLossContainer | None): Contrastive loss container. If none, the\n            loss is not used.\n    \"\"\"\n    self.perfcl_loss_config = perfcl_loss_config\n    self.cos_sim_loss_config = cosine_similarity_loss_config\n    self.contrastive_loss_config = contrastive_loss_config\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config.ConstrainedFendaLossContainer.compute_contrastive_loss","title":"<code>compute_contrastive_loss(features, positive_pairs, negative_pairs)</code>","text":"<p>Compute the contrastive loss, if it exists, using the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>features from the model.</p> required <code>positive_pairs</code> <code>Tensor</code> <p>positive pair features to compare to.</p> required <code>negative_pairs</code> <code>Tensor</code> <p>negative pair features to compare to.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>loss function</p> Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>def compute_contrastive_loss(\n    self, features: torch.Tensor, positive_pairs: torch.Tensor, negative_pairs: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the contrastive loss, if it exists, using the configuration.\n\n    Args:\n        features (torch.Tensor): features from the model.\n        positive_pairs (torch.Tensor): positive pair features to compare to.\n        negative_pairs (torch.Tensor): negative pair features to compare to.\n\n    Returns:\n        (torch.Tensor): loss function\n    \"\"\"\n    assert self.contrastive_loss_config is not None\n    contrastive_loss = self.contrastive_loss_config.contrastive_loss_function(\n        features, positive_pairs, negative_pairs\n    )\n    return self.contrastive_loss_config.contrastive_loss_weight * contrastive_loss\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config.ConstrainedFendaLossContainer.compute_cosine_similarity_loss","title":"<code>compute_cosine_similarity_loss(first_features, second_features)</code>","text":"<p>Compute the cosine loss, if it exists, using the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>first_features</code> <code>Tensor</code> <p>first set of features in the cosine comparison</p> required <code>second_features</code> <code>Tensor</code> <p>second set of features in the cosine comparison</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>cosine similarity loss between the provided features.</p> Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>def compute_cosine_similarity_loss(\n    self, first_features: torch.Tensor, second_features: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the cosine loss, if it exists, using the configuration.\n\n    Args:\n        first_features (torch.Tensor): first set of features in the cosine comparison\n        second_features (torch.Tensor): second set of features in the cosine comparison\n\n    Returns:\n        (torch.Tensor): cosine similarity loss between the provided features.\n    \"\"\"\n    assert self.cos_sim_loss_config is not None\n    cosine_similarity_loss = self.cos_sim_loss_config.cos_sim_loss_function(first_features, second_features)\n    return self.cos_sim_loss_config.cos_sim_loss_weight * cosine_similarity_loss\n</code></pre>"},{"location":"api/#fl4health.losses.fenda_loss_config.ConstrainedFendaLossContainer.compute_perfcl_loss","title":"<code>compute_perfcl_loss(local_features, old_local_features, global_features, old_global_features, initial_global_features)</code>","text":"<p>Compute the PerFCL loss, if it exists, using the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>local_features</code> <code>Tensor</code> <p>See PerFCL loss documentation</p> required <code>old_local_features</code> <code>Tensor</code> <p>See PerFCL loss documentation</p> required <code>global_features</code> <code>Tensor</code> <p>See PerFCL loss documentation</p> required <code>old_global_features</code> <code>Tensor</code> <p>See PerFCL loss documentation</p> required <code>initial_global_features</code> <code>Tensor</code> <p>See PerFCL loss documentation</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>PerFCL loss based on the input values</p> Source code in <code>fl4health/losses/fenda_loss_config.py</code> <pre><code>def compute_perfcl_loss(\n    self,\n    local_features: torch.Tensor,\n    old_local_features: torch.Tensor,\n    global_features: torch.Tensor,\n    old_global_features: torch.Tensor,\n    initial_global_features: torch.Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Compute the PerFCL loss, if it exists, using the configuration.\n\n    Args:\n        local_features (torch.Tensor): See PerFCL loss documentation\n        old_local_features (torch.Tensor): See PerFCL loss documentation\n        global_features (torch.Tensor): See PerFCL loss documentation\n        old_global_features (torch.Tensor): See PerFCL loss documentation\n        initial_global_features (torch.Tensor): See PerFCL loss documentation\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): PerFCL loss based on the input values\n    \"\"\"\n    assert self.perfcl_loss_config is not None\n    global_feature_contrastive_loss, local_feature_contrastive_loss = self.perfcl_loss_config.perfcl_loss_function(\n        local_features, old_local_features, global_features, old_global_features, initial_global_features\n    )\n    return (\n        self.perfcl_loss_config.global_feature_contrastive_loss_weight * global_feature_contrastive_loss,\n        self.perfcl_loss_config.local_feature_contrastive_loss_weight * local_feature_contrastive_loss,\n    )\n</code></pre>"},{"location":"api/#fl4health.losses.mkmmd_loss","title":"<code>mkmmd_loss</code>","text":""},{"location":"api/#fl4health.losses.mkmmd_loss.MkMmdLoss","title":"<code>MkMmdLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/losses/mkmmd_loss.py</code> <pre><code>class MkMmdLoss(torch.nn.Module):\n    def __init__(\n        self,\n        device: torch.device,\n        gammas: torch.Tensor | None = None,\n        betas: torch.Tensor | None = None,\n        minimize_type_two_error: bool = True,\n        normalize_features: bool = False,\n        layer_name: str | None = None,\n        perform_linear_approximation: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Compute the multi-kernel maximum mean discrepancy (MK-MMD) between the source and target domains. Also allows\n        for optimization of the coefficients, beta.\n\n        Args:\n            device (torch.device): Device onto which tensors should be moved.\n            gammas (torch.Tensor | None, optional): These are known as the length-scales of the RBF functions used\n                to compute the MK-MMD distances. The length of this list defines the number of kernels used in the\n                norm measurement. If none, a default of 19 kernels is used. Defaults to None.\n            betas (torch.Tensor | None, optional): These are the linear coefficients used on the basis of kernels\n                to compute the MK-MMD measure. If not provided, a unit-length, random default is constructed. These\n                can be optimized using the functions of this class. Defaults to None.\n            minimize_type_two_error (bool | None, optional): Whether we're aiming to minimize the type II error in\n                optimizing the betas or maximize it. The first coincides with trying to minimize feature distance. The\n                second coincides with trying to maximize their feature distance. Defaults to True.\n            normalize_features (bool | None, optional): Whether to normalize the features to have unit length before\n                computing the MK-MMD and optimizing betas. Defaults to False.\n            layer_name (str | None, optional): The name of the layer to extract features from. Defaults to None.\n            perform_linear_approximation (bool | None, optional): Whether to use linear approximations for the\n                estimates of the mean and covariance of the kernel values. Experimentally, we have found that the\n                linear approximations largely hinder the statistical power of MK-MMD. Defaults to False\n        \"\"\"\n        super().__init__()\n        self.device = device\n        if gammas is None:\n            # Note arange is not inclusive, so this ends up being [-3.5, 1] in steps of 0.25\n            default_gamma_powers = torch.arange(-3.5, 1.25, 0.25, device=device)\n            self.gammas = torch.pow(2.0, default_gamma_powers)\n        else:\n            self.gammas = gammas.to(self.device)\n        self.kernel_num = len(self.gammas)\n\n        if betas is None:\n            rand_coefficients = torch.rand((self.kernel_num, 1)).to(self.device)\n            # normalize the coefficients to sum to 1\n            self.betas = (1 / torch.sum(rand_coefficients)) * rand_coefficients\n        else:\n            assert betas.shape == (self.kernel_num, 1)\n            self.betas = betas.to(self.device)\n        assert torch.abs(torch.sum(self.betas) - 1) &lt; BETA_CONSTRAINT_EPSILON\n\n        self.minimize_type_two_error = minimize_type_two_error\n        self.normalize_features = normalize_features\n        self.layer_name = layer_name\n        self.perform_linear_approximation = perform_linear_approximation\n\n    def normalize(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return torch.div(x, torch.linalg.norm(x, dim=1, keepdim=True))\n\n    def construct_quadruples(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        In this function, we assume that ``X``, ``Y``: ``n_samples``, ``n_features`` are the same size. We construct\n        the quadruples.\n\n        \\\\[v_i = [x_{2i-1}, x_{2i}, y_{2i-1}, y_{2i}]\\\\]\n\n        forming a matrix of dimension (``n_samples/2``, ``4``, ``n_features``).\n\n        **NOTE**: that if ``n_samples`` is not divisible by 2, we leave off the modulus\n\n        Args:\n            x (torch.Tensor): First set of feature tensors\n            y (torch.Tensor): Second set of feature tensors\n\n        Returns:\n            (torch.Tensor): Quadruples of the form described above.\n        \"\"\"\n        n_samples, n_features = x.shape\n        # truncate if not divisible by 2\n        if n_samples % 2 == 1:\n            x = x[:-1, :]\n            y = y[:-1, :]\n        return torch.cat((x.reshape(n_samples // 2, 2, n_features), y.reshape(n_samples // 2, 2, n_features)), dim=1)\n\n    def compute_euclidean_inner_products(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        # In this function, we assume that X, Y: n_samples, n_features\n        # We want to compute estimates of the expectation for each RBF kernel WITHOUT using a linear approximation.\n        # So we need to compute ||x - y||^2 for all pairs (x_j, x_k), (x_j, y_k), (x_k, y_j), and (y_j, y_k) for all\n        # j, k in range(n_samples).\n        # NOTE: ||x - y||^2 = &lt;x - y, x - y&gt; = &lt;x, x&gt; + &lt;y, y&gt; - 2&lt;x, y&gt;\n        x_x_prime = (\n            torch.sum((x**2), dim=1).reshape(-1, 1)\n            + torch.sum((x**2), dim=1).reshape(1, -1)\n            - 2.0 * torch.mm(x, torch.transpose(x, 0, 1))\n        )\n        y_y_prime = (\n            torch.sum((y**2), dim=1).reshape(-1, 1)\n            + torch.sum((y**2), dim=1).reshape(1, -1)\n            - 2.0 * torch.mm(y, torch.transpose(y, 0, 1))\n        )\n        x_y_prime = (\n            torch.sum((x**2), dim=1).reshape(-1, 1)\n            + torch.sum((y**2), dim=1).reshape(1, -1)\n            - 2.0 * torch.mm(x, torch.transpose(y, 0, 1))\n        )\n        x_prime_y = (\n            torch.sum((y**2), dim=1).reshape(-1, 1)\n            + torch.sum((x**2), dim=1).reshape(1, -1)\n            - 2.0 * torch.mm(y, torch.transpose(x, 0, 1))\n        )\n\n        # Correct any values that ended up nearly but not identically zero (||x-y||^2 should be semi-definite)\n        x_x_prime[x_x_prime &lt; 0] = 0\n        y_y_prime[y_y_prime &lt; 0] = 0\n        x_y_prime[x_y_prime &lt; 0] = 0\n        x_prime_y[x_prime_y &lt; 0] = 0\n\n        # each inner product is a tensor of dimension n_samples x n_samples, we return a\n        # tensor of shape 4 x len(X) x len(Y)\n        return torch.cat(\n            [x_x_prime.unsqueeze(0), y_y_prime.unsqueeze(0), x_y_prime.unsqueeze(0), x_prime_y.unsqueeze(0)]\n        )\n\n    def compute_euclidean_inner_products_linear(self, v_i_quadruples: torch.Tensor) -&gt; torch.Tensor:\n        # Shape of v_i_quadruples is n_samples/2 x 4 x n_features\n        # v_i = [x_{2i-1}, x_{2i}, y_{2i-1}, y_{2i}]\n        #\n        # We want to compute the RBF kernel values. To do this, we need to compute ||x - y||^2 for the relevant pairs\n        # x and y. That is the inner product. Note that ||x - y||^2 = &lt;x - y, x - y&gt; = (x-y)^T(x-y)\n        # For the quadruples of the form (x,  x', y, y') we need distances for pairings (x, x'), (y, y'), (x, y'),\n        # (x, y')\n        x_x_prime = torch.sum((v_i_quadruples[:, 0, :] - v_i_quadruples[:, 1, :]) ** 2, dim=1, keepdim=True)\n        y_y_prime = torch.sum((v_i_quadruples[:, 2, :] - v_i_quadruples[:, 3, :]) ** 2, dim=1, keepdim=True)\n        x_y_prime = torch.sum((v_i_quadruples[:, 0, :] - v_i_quadruples[:, 3, :]) ** 2, dim=1, keepdim=True)\n        x_prime_y = torch.sum((v_i_quadruples[:, 1, :] - v_i_quadruples[:, 2, :]) ** 2, dim=1, keepdim=True)\n\n        # each inner product is a tensor of dimension len(v_i_quadruples), we return a tensor of shape\n        # len(v_i_quadruples) x 4\n        return torch.cat([x_x_prime, y_y_prime, x_y_prime, x_prime_y], dim=1)\n\n    def compute_h_u_from_inner_products(self, inner_products: torch.Tensor, gamma: torch.Tensor) -&gt; torch.Tensor:\n        # Gamma should be of shape torch.Tensor([gamma])\n        assert gamma.shape == (1,)\n        # inner_products has shape of 4 x n_samples x n_samples\n        # h_u_components should have the same shape\n        h_u_components = torch.exp((-1 * inner_products) / gamma)\n        # Each first dimension of h_u_components should now be u(x_j, x_k), u(y_j, y_k), u(x_j, y_k), and u(y_j, x_k),\n        # where u is the kernel_index^th kernel and j, k index over samples\n        # So we compute:\n        #   h_u[x_j, x_k,y_j, y_k] =  u(x_j, x_k) + u(y_j, y_k) - u(x_j, y_k) - u(y_j, x_k)\n        h_u = h_u_components[0, :] + h_u_components[1, :] - h_u_components[2, :] - h_u_components[3, :]\n        # this results in a matrix of shape 1 x n_samples x n_samples\n        return h_u.unsqueeze(0)\n\n    def compute_h_u_from_inner_products_linear(\n        self, inner_products: torch.Tensor, gamma: torch.Tensor\n    ) -&gt; torch.Tensor:\n        # Gamma should be of shape torch.Tensor([gamma])\n        assert gamma.shape == (1,)\n        # inner_products has shape number of len(v_i_quadruples) x 4 since this is the linear approximation strategy\n        # h_u_components should have the same shape\n        h_u_components = torch.exp((-1 * inner_products) / gamma)\n        # Each column of h_u_components should now be u(x_{2i-1}, x_{2i}), u(y_{2i-1}, y_{2i}), u(x_{2i-1}, y_{2i}),\n        # and u(x_{2i}, y_{2i-1}), where u is the kernel_index^th kernel and i indexes over quadruples\n        # So we compute:\n        #   h_u[x_{2i-1}, x_{2i},y_{2i-1}, y_{2i}] =  u(x_{2i-1}, x_{2i}) + u(y_{2i-1}, y_{2i})\n        #       - u(x_{2i-1}, y_{2i}) - u(x_{2i}, y_{2i-1})\n        h_u = h_u_components[:, 0] + h_u_components[:, 1] - h_u_components[:, 2] - h_u_components[:, 3]\n        # this results in a matrix of shape 1 x number of v_i_quadruples\n        return h_u.unsqueeze(0)\n\n    def compute_all_h_u_from_inner_products(self, inner_product_all_samples: torch.Tensor) -&gt; torch.Tensor:\n        k_list = [\n            self.compute_h_u_from_inner_products(inner_product_all_samples, gamma.reshape(1)) for gamma in self.gammas\n        ]\n        # Matrix should be of shape number of kernels x n_samples x n_samples, since we compute the kernel value on all\n        # possible combinations of pairs (x_j, y_j) (x_k, y_k) for every kernel.\n        return torch.cat(k_list)\n\n    def compute_all_h_u_from_inner_products_linear(self, inner_product_quadruples: torch.Tensor) -&gt; torch.Tensor:\n        # For the linear approximation version the shape of inner_product_quadruples is len(v_i_quadruples) x 4\n        k_list = [\n            self.compute_h_u_from_inner_products_linear(inner_product_quadruples, gamma.reshape(1))\n            for gamma in self.gammas\n        ]\n        # Matrix should be of shape number of kernels x number of quadruples, since we compute the kernel value on all\n        # quadruples for every kernel\n        return torch.cat(k_list)\n\n    def compute_all_h_u_linear(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        # In this function, we assume that X, Y: n_samples, n_features\n        # v_i = [x_{2i-1}, x_{2i}, y_{2i-1}, y_{2i}]\n        v_is = self.construct_quadruples(x, y)\n        # For the quadruples of the form (x,  x', y, y') we need distances for pairs (x, x'), (y, y'), (x, y'), (x, y')\n        inner_product_quadruples = self.compute_euclidean_inner_products_linear(v_is)\n        # all_h_u has shape number of kernels x number of quadruples\n        return self.compute_all_h_u_from_inner_products_linear(inner_product_quadruples)\n\n    def compute_all_h_u_all_samples(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        # In this function, we assume that X, Y: n_samples, n_features\n        # We don't need to construct the quadruples here, we can just compute the inner products directly\n        inner_product = self.compute_euclidean_inner_products(x, y)\n        # all_h_u has shape number of kernels x n_samples x n_samples\n        return self.compute_all_h_u_from_inner_products(inner_product)\n\n    def compute_hat_d_per_kernel(self, all_h_u_per_sample: torch.Tensor) -&gt; torch.Tensor:\n        # all_h_u_per_sample has two possible shapes.\n        # If we're using a linear approximation for the stats it has shape (n kernels, number of quadruples)\n        # If we're using a full approximation for the stats it has shape (n kernels, n_samples, n_samples)\n        # The data corresponding to the first dimension is values for a single kernel. For either shape\n        # we want the mean value of all entries per kernel\n        dim_to_reduce = tuple(range(1, all_h_u_per_sample.dim()))\n        # Taking the mean across kernel entries, output shape is number of kernels x 1\n        return torch.mean(all_h_u_per_sample, dim=dim_to_reduce).unsqueeze(1)\n\n    def compute_mkmmd(self, x: torch.Tensor, y: torch.Tensor, beta: torch.Tensor) -&gt; torch.Tensor:\n        # Normalize the features if necessary to have unit length\n        if self.normalize_features:\n            x = self.normalize(x)\n            y = self.normalize(y)\n\n        # In this function, we assume that X, Y: n_samples, n_features are the same size and that beta is a tensor of\n        # shape number of kernels x 1\n        if self.perform_linear_approximation:\n            all_h_u_per_vi = self.compute_all_h_u_linear(x, y)\n            hat_d_per_kernel = self.compute_hat_d_per_kernel(all_h_u_per_vi)\n        else:\n            all_h_u_per_sample = self.compute_all_h_u_all_samples(x, y)\n            hat_d_per_kernel = self.compute_hat_d_per_kernel(all_h_u_per_sample)\n        # Take the dot product between the individual kernel hat_d values to scale by the basis coefficients\n        return torch.mm(beta.t(), hat_d_per_kernel)[0][0]\n\n    def form_h_u_delta_w_i(self, all_h_u_per_v_i: torch.Tensor) -&gt; torch.Tensor:\n        # all_h_u_per_v_i has dimension number kernels x number of v_i quadruples\n        n_kernels, n_v_i_s = all_h_u_per_v_i.shape\n        # if the number of v_is is not divisible by two just drop the final ones\n        if n_v_i_s % 2 == 1:\n            all_h_u_per_v_i = all_h_u_per_v_i[:, :-1]\n        # pairing the h_u(v_i) values as h_u(v_{2i-1}), h_u(v_{2i}), think w_is from the paper\n        h_u_v_i_pairs = all_h_u_per_v_i.reshape(n_kernels, n_v_i_s // 2, 2)\n        return h_u_v_i_pairs[:, :, 0] - h_u_v_i_pairs[:, :, 1]\n\n    def compute_hat_q_k_linear(self, all_h_u_per_v_i: torch.Tensor) -&gt; torch.Tensor:\n        # all_h_u_per_v_i has dimension number kernels x number of v_i quadruples\n        h_u_delta_w_i = self.form_h_u_delta_w_i(all_h_u_per_v_i)\n\n        q_k_matrix: torch.Tensor = torch.zeros((self.kernel_num, self.kernel_num)).to(self.device)\n        len_w_is = h_u_delta_w_i.shape[1]\n        # For each basis function we're adding in the value of h_{j, \\Delta}(w_i)*h_{k, \\Delta}(w_i) from the\n        # construction above to the proper entry in Q. Note that Q is symmetric. So we can construct the symmetric\n        # entries at the same time.\n        for j in range(self.kernel_num):\n            for k in range(j + 1):\n                q_k_matrix[j][k] += torch.sum(h_u_delta_w_i[j] * h_u_delta_w_i[k])\n                if j != k:\n                    q_k_matrix[k][j] += torch.sum(h_u_delta_w_i[j] * h_u_delta_w_i[k])\n        # Q_k_matrix has shape number of kernels x number of kernels\n        return q_k_matrix / len_w_is\n\n    def form_kernel_samples_minus_expectation(\n        self, all_h_u_per_sample: torch.Tensor, hat_d_per_kernel: torch.Tensor\n    ) -&gt; torch.Tensor:\n        # all_h_u_per_sample has shape num kernels x n_samples x n_samples, for each index in the first dimension,\n        # the n_samples x n_samples represent the values of the kernel evaluated on samples (x_j, y_j) x (x_k, y_k)\n        # hat_d_per_kernel has shape num kernels x 1. This is the expectation of each kernel over all those samples\n        _, rows, columns = all_h_u_per_sample.shape\n        # We want to subtract the expectation for the kernel from all sample values for the kernel in\n        # all_h_u_per_sample, so we repeat the expectations to produce the right shape.\n        repeated_hat_d_per_kernel = hat_d_per_kernel.unsqueeze(1).repeat(1, rows, columns)\n        return all_h_u_per_sample - repeated_hat_d_per_kernel\n\n    def compute_hat_q_k(self, all_h_u_per_sample: torch.Tensor, hat_d_per_kernel: torch.Tensor) -&gt; torch.Tensor:\n        # all_h_u_per_sample has dimension num kernels x n_samples x n_samples\n        n_samples = all_h_u_per_sample.shape[1]\n        kernel_samples_minus_kernel_expectation = self.form_kernel_samples_minus_expectation(\n            all_h_u_per_sample, hat_d_per_kernel\n        )\n        q_k_matrix: torch.Tensor = torch.zeros((self.kernel_num, self.kernel_num)).to(self.device)\n        # For each basis function we need to compute the covariance between the kernels where\n        # Cov(X, Y) = E[(X - E[X])(Y - E[Y])], and X and Y are random variables representing the kernel values over\n        # distributions P, Q. For kernels h_{k_i} and h_{k_j} this can be estimated as\n        # \\frac{1}{n^2 -1} \\sum_{s, t} (h_{k_i}(x_s, x_t, y_s, y_t) - \\hat{d}_{k_i}(p, q))\n        #     \\cdot (h_{k_j}(x_s, x_t, y_s, y_t) - \\hat{d}_{k_j}(p, q))\n        # So we loop over the different kernel combinations\n        for i in range(self.kernel_num):\n            for j in range(self.kernel_num):\n                product_of_variances = (\n                    kernel_samples_minus_kernel_expectation[i, :, :] * kernel_samples_minus_kernel_expectation[j, :, :]\n                )\n                # Compute the expectation to get Cov(h_{k_i}, h_{k_j}).\n                # NOTE: the n^2-1 correction is because we're using expectation estimates\n                q_k_matrix[i][j] = (1.0 / (n_samples**2 - 1.0)) * torch.sum(product_of_variances)\n        return q_k_matrix\n\n    def beta_with_extreme_kernel_base_values(\n        self, hat_d_per_kernel: torch.Tensor, hat_q_k: torch.Tensor, minimize_type_two_error: bool = True\n    ) -&gt; torch.Tensor:\n        kernel_base_values = torch.tensor(\n            [hat_d_per_kernel[i] / hat_q_k[i][i] for i in range(len(hat_d_per_kernel))]\n        ).to(self.device)\n        if minimize_type_two_error:\n            log(\n                INFO,\n                \"Rather than optimizing, we select a single kernel with largest hat_d_k/hat_Q_k_lambda\",\n            )\n            largest_kernel_index = torch.argmax(kernel_base_values)\n        else:\n            log(\n                INFO,\n                \"Rather than optimizing, we select a single kernel with smallest hat_d_k/hat_Q_k_lambda\",\n            )\n            largest_kernel_index = torch.argmin(kernel_base_values)\n        beta_one_hot = torch.zeros_like(hat_d_per_kernel)\n        beta_one_hot[largest_kernel_index] = 1.0\n        return beta_one_hot\n\n    def compute_vertices(self, hat_d_per_kernel: torch.Tensor) -&gt; torch.Tensor:\n        return 1.0 / hat_d_per_kernel\n\n    def get_best_vertex_for_objective_function(\n        self, hat_d_per_kernel: torch.Tensor, hat_q_k: torch.Tensor\n    ) -&gt; torch.Tensor:\n        # vertices_weights have shape num kernels x 1\n        vertices_weights = self.compute_vertices(hat_d_per_kernel)\n        maximum_value = -torch.inf\n        best_index = 0\n        best_vertex = torch.zeros_like(hat_d_per_kernel).to(self.device)\n        for i in range(self.kernel_num):\n            vertices = torch.zeros_like(hat_d_per_kernel).to(self.device)\n            vertices[i, 0] = vertices_weights[i, 0]\n            objective_value = torch.mm(torch.mm(vertices.t(), hat_q_k), vertices).item()\n            if objective_value &gt; maximum_value:\n                maximum_value = objective_value\n                best_index = i\n        best_vertex[best_index, 0] = vertices_weights[best_index, 0]\n        return best_vertex\n\n    def form_and_solve_qp(self, hat_d_per_kernel: torch.Tensor, regularized_q_k: torch.Tensor) -&gt; torch.Tensor:\n        # p = \\vec{0} of shape (number of kernels, 1). It is only used for the QP setup\n        p = torch.zeros(self.kernel_num).to(self.device)\n\n        # We want each beta &gt;= 0, the QP defines the constraint as G\\beta &lt;= h. So h is a vector of zeros (see below)\n        # and we want each -1*beta &lt;=0 to guarantee that beta&gt;=0. So G is an identify matrix scaled by -1\n        g = -1 * torch.eye(self.kernel_num).to(self.device)\n\n        # This is the RHS of the inequality constraint on the beta coefficients. We want them to be &gt;= 0\n        h = torch.zeros(self.kernel_num).to(self.device)\n\n        # We want the beta^T * hat_d_per_kernel = 1. Translated into the form of the quadratic program below,\n        # this means A is essentially n_p_k\n        b = torch.tensor([1.0]).to(self.device)\n\n        # Solve a batch of QPs.\n\n        #   This function solves a batch of QPs, each optimizing over\n        #   `nz` variables and having `nineq` inequality constraints\n        #   and `neq` equality constraints.\n        #   The optimization problem for each instance in the batch\n        #   (dropping indexing from the notation) is of the form\n\n        #     \\hat z =   argmin_z 1/2 z^T Q z + p^T z\n        #             subject to Gz &lt;= h\n        #                         Az  = b\n        # Dimensions are as follows:\n        #   Q is (kernel_num, kernel_num)\n        #   p is (kernel_num, 1)\n        #   G is (kernel_num, kernel_num)\n        #   h is (kernel_num, 1)\n        #   A is (1, kernel_num)\n        #   b is a scalar\n\n        # QPFunction returns betas in the shape 1 x num_kernels to we take the transpose for consistency\n        return QPFunction(verbose=False, solver=QPSolvers.CVXPY, check_Q_spd=False)(\n            regularized_q_k, p, g, h, hat_d_per_kernel.t(), b\n        ).t()\n\n    def optimize_betas(self, x: torch.Tensor, y: torch.Tensor, lambda_m: float = 1e-5) -&gt; torch.Tensor:\n        # In this function, we assume that X, Y: n_samples, n_features\n\n        # Normalize the features if necessary to have unit length\n        if self.normalize_features:\n            x = self.normalize(x)\n            y = self.normalize(y)\n\n        if self.perform_linear_approximation:\n            all_h_u_per_v_i = self.compute_all_h_u_linear(x, y)\n            # shape of hat_d_per_kernel is number of kernels x 1\n            hat_d_per_kernel = self.compute_hat_d_per_kernel(all_h_u_per_v_i)\n            # shape of hat_Q_k is number of kernels x number of kernels\n            hat_q_k = self.compute_hat_q_k_linear(all_h_u_per_v_i)\n        else:\n            all_h_u_per_sample = self.compute_all_h_u_all_samples(x, y)\n            # shape of hat_d_per_kernel is number of kernels x 1\n            hat_d_per_kernel = self.compute_hat_d_per_kernel(all_h_u_per_sample)\n            # shape of hat_Q_k is number of kernels x number of kernels\n            hat_q_k = self.compute_hat_q_k(all_h_u_per_sample, hat_d_per_kernel)\n\n        # Eigen shift hat_Q_k and scale by 2 as the QP setup scales by 1/2\n        regularized_q_k = 2 * hat_q_k + lambda_m * torch.eye(self.kernel_num).to(self.device)\n\n        # check to see that at least one of hat_d_per_kernel is positive. If none of them are positive, then select a\n        # single kernel with largest hat_d, similar to the suggestion of Gretton et al. in \"Optimal Kernel Choice for\n        # Large-Scale Two-Sample Tests\", 2012\n        if not torch.any(hat_d_per_kernel &gt; 0):\n            log(INFO, f\"None of the estimates for hat_d are positive: {hat_d_per_kernel.squeeze()}.\")\n            return self.beta_with_extreme_kernel_base_values(\n                hat_d_per_kernel, regularized_q_k, minimize_type_two_error=True\n            )\n\n        if self.minimize_type_two_error:\n            try:\n                raw_betas = self.form_and_solve_qp(hat_d_per_kernel, regularized_q_k).detach()\n            except Exception as e:\n                # If we can't solve the QP due to infeasibility, then we keep the previous betas\n                if self.layer_name is not None:\n                    log(INFO, f\"{e} We keep previous betas for layer {self.layer_name}.\")\n                else:\n                    log(INFO, f\"{e} We keep previous betas.\")\n                raw_betas = self.betas.detach()\n        else:\n            # If we're trying to maximize the type II error, then we are trying to maximize a convex function over a\n            # convex polygon of beta values. So the maximum is found at one of the vertices\n            raw_betas = self.get_best_vertex_for_objective_function(hat_d_per_kernel, regularized_q_k)\n\n        # We want to ensure that the betas are non-negative\n        raw_betas = torch.clamp(raw_betas, min=0)\n        return (1.0 / torch.sum(raw_betas)) * raw_betas\n\n    def forward(self, x_s: torch.Tensor, x_t: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the multi-kernel maximum mean discrepancy (MK-MMD) between the source and target domains.\n\n        Args:\n            x_s (torch.Tensor): Source domain data, shape (``n_samples``, ``n_features``).\n            x_t (torch.Tensor): Target domain data, shape (``n_samples``, ``n_features``).\n\n        Returns:\n            (torch.Tensor): MK-MMD value.\n        \"\"\"\n        return self.compute_mkmmd(x_s, x_t, self.betas)\n</code></pre>"},{"location":"api/#fl4health.losses.mkmmd_loss.MkMmdLoss.__init__","title":"<code>__init__(device, gammas=None, betas=None, minimize_type_two_error=True, normalize_features=False, layer_name=None, perform_linear_approximation=False)</code>","text":"<p>Compute the multi-kernel maximum mean discrepancy (MK-MMD) between the source and target domains. Also allows for optimization of the coefficients, beta.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Device onto which tensors should be moved.</p> required <code>gammas</code> <code>Tensor | None</code> <p>These are known as the length-scales of the RBF functions used to compute the MK-MMD distances. The length of this list defines the number of kernels used in the norm measurement. If none, a default of 19 kernels is used. Defaults to None.</p> <code>None</code> <code>betas</code> <code>Tensor | None</code> <p>These are the linear coefficients used on the basis of kernels to compute the MK-MMD measure. If not provided, a unit-length, random default is constructed. These can be optimized using the functions of this class. Defaults to None.</p> <code>None</code> <code>minimize_type_two_error</code> <code>bool | None</code> <p>Whether we're aiming to minimize the type II error in optimizing the betas or maximize it. The first coincides with trying to minimize feature distance. The second coincides with trying to maximize their feature distance. Defaults to True.</p> <code>True</code> <code>normalize_features</code> <code>bool | None</code> <p>Whether to normalize the features to have unit length before computing the MK-MMD and optimizing betas. Defaults to False.</p> <code>False</code> <code>layer_name</code> <code>str | None</code> <p>The name of the layer to extract features from. Defaults to None.</p> <code>None</code> <code>perform_linear_approximation</code> <code>bool | None</code> <p>Whether to use linear approximations for the estimates of the mean and covariance of the kernel values. Experimentally, we have found that the linear approximations largely hinder the statistical power of MK-MMD. Defaults to False</p> <code>False</code> Source code in <code>fl4health/losses/mkmmd_loss.py</code> <pre><code>def __init__(\n    self,\n    device: torch.device,\n    gammas: torch.Tensor | None = None,\n    betas: torch.Tensor | None = None,\n    minimize_type_two_error: bool = True,\n    normalize_features: bool = False,\n    layer_name: str | None = None,\n    perform_linear_approximation: bool = False,\n) -&gt; None:\n    \"\"\"\n    Compute the multi-kernel maximum mean discrepancy (MK-MMD) between the source and target domains. Also allows\n    for optimization of the coefficients, beta.\n\n    Args:\n        device (torch.device): Device onto which tensors should be moved.\n        gammas (torch.Tensor | None, optional): These are known as the length-scales of the RBF functions used\n            to compute the MK-MMD distances. The length of this list defines the number of kernels used in the\n            norm measurement. If none, a default of 19 kernels is used. Defaults to None.\n        betas (torch.Tensor | None, optional): These are the linear coefficients used on the basis of kernels\n            to compute the MK-MMD measure. If not provided, a unit-length, random default is constructed. These\n            can be optimized using the functions of this class. Defaults to None.\n        minimize_type_two_error (bool | None, optional): Whether we're aiming to minimize the type II error in\n            optimizing the betas or maximize it. The first coincides with trying to minimize feature distance. The\n            second coincides with trying to maximize their feature distance. Defaults to True.\n        normalize_features (bool | None, optional): Whether to normalize the features to have unit length before\n            computing the MK-MMD and optimizing betas. Defaults to False.\n        layer_name (str | None, optional): The name of the layer to extract features from. Defaults to None.\n        perform_linear_approximation (bool | None, optional): Whether to use linear approximations for the\n            estimates of the mean and covariance of the kernel values. Experimentally, we have found that the\n            linear approximations largely hinder the statistical power of MK-MMD. Defaults to False\n    \"\"\"\n    super().__init__()\n    self.device = device\n    if gammas is None:\n        # Note arange is not inclusive, so this ends up being [-3.5, 1] in steps of 0.25\n        default_gamma_powers = torch.arange(-3.5, 1.25, 0.25, device=device)\n        self.gammas = torch.pow(2.0, default_gamma_powers)\n    else:\n        self.gammas = gammas.to(self.device)\n    self.kernel_num = len(self.gammas)\n\n    if betas is None:\n        rand_coefficients = torch.rand((self.kernel_num, 1)).to(self.device)\n        # normalize the coefficients to sum to 1\n        self.betas = (1 / torch.sum(rand_coefficients)) * rand_coefficients\n    else:\n        assert betas.shape == (self.kernel_num, 1)\n        self.betas = betas.to(self.device)\n    assert torch.abs(torch.sum(self.betas) - 1) &lt; BETA_CONSTRAINT_EPSILON\n\n    self.minimize_type_two_error = minimize_type_two_error\n    self.normalize_features = normalize_features\n    self.layer_name = layer_name\n    self.perform_linear_approximation = perform_linear_approximation\n</code></pre>"},{"location":"api/#fl4health.losses.mkmmd_loss.MkMmdLoss.construct_quadruples","title":"<code>construct_quadruples(x, y)</code>","text":"<p>In this function, we assume that <code>X</code>, <code>Y</code>: <code>n_samples</code>, <code>n_features</code> are the same size. We construct the quadruples.</p> \\[v_i = [x_{2i-1}, x_{2i}, y_{2i-1}, y_{2i}]\\] <p>forming a matrix of dimension (<code>n_samples/2</code>, <code>4</code>, <code>n_features</code>).</p> <p>NOTE: that if <code>n_samples</code> is not divisible by 2, we leave off the modulus</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>First set of feature tensors</p> required <code>y</code> <code>Tensor</code> <p>Second set of feature tensors</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Quadruples of the form described above.</p> Source code in <code>fl4health/losses/mkmmd_loss.py</code> <pre><code>def construct_quadruples(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    In this function, we assume that ``X``, ``Y``: ``n_samples``, ``n_features`` are the same size. We construct\n    the quadruples.\n\n    \\\\[v_i = [x_{2i-1}, x_{2i}, y_{2i-1}, y_{2i}]\\\\]\n\n    forming a matrix of dimension (``n_samples/2``, ``4``, ``n_features``).\n\n    **NOTE**: that if ``n_samples`` is not divisible by 2, we leave off the modulus\n\n    Args:\n        x (torch.Tensor): First set of feature tensors\n        y (torch.Tensor): Second set of feature tensors\n\n    Returns:\n        (torch.Tensor): Quadruples of the form described above.\n    \"\"\"\n    n_samples, n_features = x.shape\n    # truncate if not divisible by 2\n    if n_samples % 2 == 1:\n        x = x[:-1, :]\n        y = y[:-1, :]\n    return torch.cat((x.reshape(n_samples // 2, 2, n_features), y.reshape(n_samples // 2, 2, n_features)), dim=1)\n</code></pre>"},{"location":"api/#fl4health.losses.mkmmd_loss.MkMmdLoss.forward","title":"<code>forward(x_s, x_t)</code>","text":"<p>Compute the multi-kernel maximum mean discrepancy (MK-MMD) between the source and target domains.</p> <p>Parameters:</p> Name Type Description Default <code>x_s</code> <code>Tensor</code> <p>Source domain data, shape (<code>n_samples</code>, <code>n_features</code>).</p> required <code>x_t</code> <code>Tensor</code> <p>Target domain data, shape (<code>n_samples</code>, <code>n_features</code>).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>MK-MMD value.</p> Source code in <code>fl4health/losses/mkmmd_loss.py</code> <pre><code>def forward(self, x_s: torch.Tensor, x_t: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the multi-kernel maximum mean discrepancy (MK-MMD) between the source and target domains.\n\n    Args:\n        x_s (torch.Tensor): Source domain data, shape (``n_samples``, ``n_features``).\n        x_t (torch.Tensor): Target domain data, shape (``n_samples``, ``n_features``).\n\n    Returns:\n        (torch.Tensor): MK-MMD value.\n    \"\"\"\n    return self.compute_mkmmd(x_s, x_t, self.betas)\n</code></pre>"},{"location":"api/#fl4health.losses.perfcl_loss","title":"<code>perfcl_loss</code>","text":""},{"location":"api/#fl4health.losses.perfcl_loss.PerFclLoss","title":"<code>PerFclLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/losses/perfcl_loss.py</code> <pre><code>class PerFclLoss(nn.Module):\n    def __init__(\n        self,\n        device: torch.device,\n        global_feature_loss_temperature: float = 0.5,\n        local_feature_loss_temperature: float = 0.5,\n    ) -&gt; None:\n        \"\"\"\n        Loss function for local model training with the PerFCL Method: https://ieeexplore.ieee.org/document/10020518/.\n\n        It is essentially a combination of two separate MOON contrastive losses.\n\n        Args:\n            device (torch.device): Device onto which this loss should be transferred.\n            global_feature_loss_temperature (float, optional): Temperature for the contrastive loss associated with\n                the global features. Defaults to 0.5.\n            local_feature_loss_temperature (float, optional): Temperature for the contrastive loss associated with\n                the local features. Defaults to 0.5.\n        \"\"\"\n        super().__init__()\n        self.global_feature_contrastive_loss = MoonContrastiveLoss(device, global_feature_loss_temperature)\n        self.local_feature_contrastive_loss = MoonContrastiveLoss(device, local_feature_loss_temperature)\n\n    def forward(\n        self,\n        local_features: torch.Tensor,\n        old_local_features: torch.Tensor,\n        global_features: torch.Tensor,\n        old_global_features: torch.Tensor,\n        initial_global_features: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        PerFCL loss implemented based on https://www.sciencedirect.com/science/article/pii/S0031320323002078.\n\n        This paper introduced two contrastive loss functions:\n\n        - First one aims to enhance the similarity between the current global features (\\\\(z_s\\\\)) and aggregated\n          global features (\\\\(z_g\\\\)) (saved at the start of client-side training) as positive pairs while reducing\n          the similarity between the current global features (\\\\(z_s\\\\)) and old global features\n          (\\\\(\\\\hat{z}_s\\\\)) from the end of the previous client-side training as negative pairs.\n        - Second one aims to enhance the similarity between the current local features (\\\\(z_p\\\\)) and old local\n          features (\\\\(\\\\hat{z}_p\\\\)) from the end of the previous client-side training as positive pairs while\n          reducing the similarity between the current local features (\\\\(z_p\\\\)) and aggregated global features\n          (\\\\(z_g\\\\)) (saved at the start of client-side training) as negative pairs.\n\n        Args:\n            local_features (torch.Tensor): Features produced by the local feature extractor of the model during the\n                client-side training. Denoted as \\\\(z_p\\\\) in the original paper. Shape\n                (``batch_size``, ``n_features``)\n            old_local_features (torch.Tensor): Features produced by the FINAL local feature extractor of the model\n                from the PREVIOUS server round. Denoted as \\\\(\\\\hat{z}_p\\\\) in the original paper. Shape\n                (``batch_size``, ``n_features``)\n            global_features (torch.Tensor): Features produced by the global feature extractor of the model during the\n                client-side training. Denoted as \\\\(z_s\\\\) in the original paper. Shape\n                (``batch_size``, ``n_features``)\n            old_global_features (torch.Tensor): Features produced by the FINAL global feature extractor of the model\n                from the PREVIOUS server round. Denoted as \\\\(\\\\hat{z}_s\\\\) in the original paper. Shape\n                (``batch_size``, ``n_features``)\n            initial_global_features (torch.Tensor): Features produced by the **INITIAL** global feature extractor of\n                the model at the start of client-side training. This feature extractor is the **AGGREGATED** weights\n                across clients. Shape (``batch_size``, ``n_features``)\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Tuple containing the two components of the PerFCL loss function to\n                be weighted and summed. The first tensor corresponds to the global feature loss, the second is\n                associated with the local feature loss.\n        \"\"\"\n        # The more general contrastive loss function requires 3D tensors, where the first dimension is potentially\n        # greater than 1. For the PerFCL loss, this will always just be 1.\n        old_local_features = old_local_features.unsqueeze(0)\n        old_global_features = old_global_features.unsqueeze(0)\n        initial_global_features = initial_global_features.unsqueeze(0)\n\n        global_feature_loss = self.global_feature_contrastive_loss(\n            features=global_features,  # (z_s)\n            positive_pairs=initial_global_features,  # (z_g)\n            negative_pairs=old_global_features,  # (\\hat{z_s})\n        )\n        local_feature_loss = self.local_feature_contrastive_loss(\n            features=local_features,  # (z_p)\n            positive_pairs=old_local_features,  # (\\hat{z_p})\n            negative_pairs=initial_global_features,  # (z_g)\n        )\n\n        return global_feature_loss, local_feature_loss\n</code></pre>"},{"location":"api/#fl4health.losses.perfcl_loss.PerFclLoss.__init__","title":"<code>__init__(device, global_feature_loss_temperature=0.5, local_feature_loss_temperature=0.5)</code>","text":"<p>Loss function for local model training with the PerFCL Method: https://ieeexplore.ieee.org/document/10020518/.</p> <p>It is essentially a combination of two separate MOON contrastive losses.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Device onto which this loss should be transferred.</p> required <code>global_feature_loss_temperature</code> <code>float</code> <p>Temperature for the contrastive loss associated with the global features. Defaults to 0.5.</p> <code>0.5</code> <code>local_feature_loss_temperature</code> <code>float</code> <p>Temperature for the contrastive loss associated with the local features. Defaults to 0.5.</p> <code>0.5</code> Source code in <code>fl4health/losses/perfcl_loss.py</code> <pre><code>def __init__(\n    self,\n    device: torch.device,\n    global_feature_loss_temperature: float = 0.5,\n    local_feature_loss_temperature: float = 0.5,\n) -&gt; None:\n    \"\"\"\n    Loss function for local model training with the PerFCL Method: https://ieeexplore.ieee.org/document/10020518/.\n\n    It is essentially a combination of two separate MOON contrastive losses.\n\n    Args:\n        device (torch.device): Device onto which this loss should be transferred.\n        global_feature_loss_temperature (float, optional): Temperature for the contrastive loss associated with\n            the global features. Defaults to 0.5.\n        local_feature_loss_temperature (float, optional): Temperature for the contrastive loss associated with\n            the local features. Defaults to 0.5.\n    \"\"\"\n    super().__init__()\n    self.global_feature_contrastive_loss = MoonContrastiveLoss(device, global_feature_loss_temperature)\n    self.local_feature_contrastive_loss = MoonContrastiveLoss(device, local_feature_loss_temperature)\n</code></pre>"},{"location":"api/#fl4health.losses.perfcl_loss.PerFclLoss.forward","title":"<code>forward(local_features, old_local_features, global_features, old_global_features, initial_global_features)</code>","text":"<p>PerFCL loss implemented based on https://www.sciencedirect.com/science/article/pii/S0031320323002078.</p> <p>This paper introduced two contrastive loss functions:</p> <ul> <li>First one aims to enhance the similarity between the current global features (\\(z_s\\)) and aggregated   global features (\\(z_g\\)) (saved at the start of client-side training) as positive pairs while reducing   the similarity between the current global features (\\(z_s\\)) and old global features   (\\(\\hat{z}_s\\)) from the end of the previous client-side training as negative pairs.</li> <li>Second one aims to enhance the similarity between the current local features (\\(z_p\\)) and old local   features (\\(\\hat{z}_p\\)) from the end of the previous client-side training as positive pairs while   reducing the similarity between the current local features (\\(z_p\\)) and aggregated global features   (\\(z_g\\)) (saved at the start of client-side training) as negative pairs.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>local_features</code> <code>Tensor</code> <p>Features produced by the local feature extractor of the model during the client-side training. Denoted as \\(z_p\\) in the original paper. Shape (<code>batch_size</code>, <code>n_features</code>)</p> required <code>old_local_features</code> <code>Tensor</code> <p>Features produced by the FINAL local feature extractor of the model from the PREVIOUS server round. Denoted as \\(\\hat{z}_p\\) in the original paper. Shape (<code>batch_size</code>, <code>n_features</code>)</p> required <code>global_features</code> <code>Tensor</code> <p>Features produced by the global feature extractor of the model during the client-side training. Denoted as \\(z_s\\) in the original paper. Shape (<code>batch_size</code>, <code>n_features</code>)</p> required <code>old_global_features</code> <code>Tensor</code> <p>Features produced by the FINAL global feature extractor of the model from the PREVIOUS server round. Denoted as \\(\\hat{z}_s\\) in the original paper. Shape (<code>batch_size</code>, <code>n_features</code>)</p> required <code>initial_global_features</code> <code>Tensor</code> <p>Features produced by the INITIAL global feature extractor of the model at the start of client-side training. This feature extractor is the AGGREGATED weights across clients. Shape (<code>batch_size</code>, <code>n_features</code>)</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Tuple containing the two components of the PerFCL loss function to be weighted and summed. The first tensor corresponds to the global feature loss, the second is associated with the local feature loss.</p> Source code in <code>fl4health/losses/perfcl_loss.py</code> <pre><code>def forward(\n    self,\n    local_features: torch.Tensor,\n    old_local_features: torch.Tensor,\n    global_features: torch.Tensor,\n    old_global_features: torch.Tensor,\n    initial_global_features: torch.Tensor,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    PerFCL loss implemented based on https://www.sciencedirect.com/science/article/pii/S0031320323002078.\n\n    This paper introduced two contrastive loss functions:\n\n    - First one aims to enhance the similarity between the current global features (\\\\(z_s\\\\)) and aggregated\n      global features (\\\\(z_g\\\\)) (saved at the start of client-side training) as positive pairs while reducing\n      the similarity between the current global features (\\\\(z_s\\\\)) and old global features\n      (\\\\(\\\\hat{z}_s\\\\)) from the end of the previous client-side training as negative pairs.\n    - Second one aims to enhance the similarity between the current local features (\\\\(z_p\\\\)) and old local\n      features (\\\\(\\\\hat{z}_p\\\\)) from the end of the previous client-side training as positive pairs while\n      reducing the similarity between the current local features (\\\\(z_p\\\\)) and aggregated global features\n      (\\\\(z_g\\\\)) (saved at the start of client-side training) as negative pairs.\n\n    Args:\n        local_features (torch.Tensor): Features produced by the local feature extractor of the model during the\n            client-side training. Denoted as \\\\(z_p\\\\) in the original paper. Shape\n            (``batch_size``, ``n_features``)\n        old_local_features (torch.Tensor): Features produced by the FINAL local feature extractor of the model\n            from the PREVIOUS server round. Denoted as \\\\(\\\\hat{z}_p\\\\) in the original paper. Shape\n            (``batch_size``, ``n_features``)\n        global_features (torch.Tensor): Features produced by the global feature extractor of the model during the\n            client-side training. Denoted as \\\\(z_s\\\\) in the original paper. Shape\n            (``batch_size``, ``n_features``)\n        old_global_features (torch.Tensor): Features produced by the FINAL global feature extractor of the model\n            from the PREVIOUS server round. Denoted as \\\\(\\\\hat{z}_s\\\\) in the original paper. Shape\n            (``batch_size``, ``n_features``)\n        initial_global_features (torch.Tensor): Features produced by the **INITIAL** global feature extractor of\n            the model at the start of client-side training. This feature extractor is the **AGGREGATED** weights\n            across clients. Shape (``batch_size``, ``n_features``)\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Tuple containing the two components of the PerFCL loss function to\n            be weighted and summed. The first tensor corresponds to the global feature loss, the second is\n            associated with the local feature loss.\n    \"\"\"\n    # The more general contrastive loss function requires 3D tensors, where the first dimension is potentially\n    # greater than 1. For the PerFCL loss, this will always just be 1.\n    old_local_features = old_local_features.unsqueeze(0)\n    old_global_features = old_global_features.unsqueeze(0)\n    initial_global_features = initial_global_features.unsqueeze(0)\n\n    global_feature_loss = self.global_feature_contrastive_loss(\n        features=global_features,  # (z_s)\n        positive_pairs=initial_global_features,  # (z_g)\n        negative_pairs=old_global_features,  # (\\hat{z_s})\n    )\n    local_feature_loss = self.local_feature_contrastive_loss(\n        features=local_features,  # (z_p)\n        positive_pairs=old_local_features,  # (\\hat{z_p})\n        negative_pairs=initial_global_features,  # (z_g)\n    )\n\n    return global_feature_loss, local_feature_loss\n</code></pre>"},{"location":"api/#fl4health.losses.weight_drift_loss","title":"<code>weight_drift_loss</code>","text":""},{"location":"api/#fl4health.losses.weight_drift_loss.WeightDriftLoss","title":"<code>WeightDriftLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/losses/weight_drift_loss.py</code> <pre><code>class WeightDriftLoss(nn.Module):\n    def __init__(\n        self,\n        device: torch.device,\n    ) -&gt; None:\n        r\"\"\"\n        Used to compute the \\(l_2\\)-inner product between a Torch model and a reference set of weights\n        corresponding to a past version of that model.\n\n        Args:\n            device (torch.device): Device on which the loss should be computed.\n        \"\"\"\n        super().__init__()\n        self.device = device\n\n    def _compute_weight_difference_inner_product(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the \\\\(l_2\\\\)-inner product between two tensors. This amounts to the Frobenius norm of the\n        difference between the tensors \\\\(\\\\Vert x - y \\\\Vert_F\\\\).\n\n        Args:\n            x (torch.Tensor): first tensor\n            y (torch.Tensor): second tensor\n\n        Returns:\n            (torch.Tensor): Frobenius norm of their difference\n        \"\"\"\n        return torch.pow(torch.linalg.norm(x - y), 2.0)\n\n    def forward(self, target_model: nn.Module, constraint_tensors: list[torch.Tensor], weight: float) -&gt; torch.Tensor:\n        r\"\"\"\n        Compute the \\(l_2\\)-inner product between a Torch model and a reference set of weights in a differentiable\n        way. The ```constraint_tensors``` are frozen.\n\n        Args:\n            target_model (nn.Module): Model being constrained by the ``constraint_tensors``. Weights are\n                differentiable.\n            constraint_tensors (list[torch.Tensor]): Tensors corresponding to a previous version of the\n                ``target_model``.\n            weight (float): Weight with which to scale the loss.\n\n        Returns:\n            (torch.Tensor): Loss value.\n        \"\"\"\n        # move model and tensors to device if needed\n        target_model = target_model.to(self.device)\n        constraint_tensors = [constraint_tensor.to(self.device) for constraint_tensor in constraint_tensors]\n\n        model_weights = list(target_model.parameters())\n        assert len(constraint_tensors) == len(model_weights)\n        assert len(model_weights) &gt; 0\n\n        layer_inner_products: list[torch.Tensor] = [\n            self._compute_weight_difference_inner_product(constraint_layer_weights, model_layer_weights)\n            for constraint_layer_weights, model_layer_weights in zip(constraint_tensors, model_weights)\n        ]\n\n        # Network l2 inner product tensor\n        # NOTE: Scaling by 1/2 is for grad consistency.\n        return (weight / 2.0) * torch.stack(layer_inner_products).sum()\n</code></pre>"},{"location":"api/#fl4health.losses.weight_drift_loss.WeightDriftLoss.__init__","title":"<code>__init__(device)</code>","text":"<p>Used to compute the \\(l_2\\)-inner product between a Torch model and a reference set of weights corresponding to a past version of that model.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>device</code> <p>Device on which the loss should be computed.</p> required Source code in <code>fl4health/losses/weight_drift_loss.py</code> <pre><code>def __init__(\n    self,\n    device: torch.device,\n) -&gt; None:\n    r\"\"\"\n    Used to compute the \\(l_2\\)-inner product between a Torch model and a reference set of weights\n    corresponding to a past version of that model.\n\n    Args:\n        device (torch.device): Device on which the loss should be computed.\n    \"\"\"\n    super().__init__()\n    self.device = device\n</code></pre>"},{"location":"api/#fl4health.losses.weight_drift_loss.WeightDriftLoss.forward","title":"<code>forward(target_model, constraint_tensors, weight)</code>","text":"<p>Compute the \\(l_2\\)-inner product between a Torch model and a reference set of weights in a differentiable way. The <code>constraint_tensors</code> are frozen.</p> <p>Parameters:</p> Name Type Description Default <code>target_model</code> <code>Module</code> <p>Model being constrained by the <code>constraint_tensors</code>. Weights are differentiable.</p> required <code>constraint_tensors</code> <code>list[Tensor]</code> <p>Tensors corresponding to a previous version of the <code>target_model</code>.</p> required <code>weight</code> <code>float</code> <p>Weight with which to scale the loss.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value.</p> Source code in <code>fl4health/losses/weight_drift_loss.py</code> <pre><code>def forward(self, target_model: nn.Module, constraint_tensors: list[torch.Tensor], weight: float) -&gt; torch.Tensor:\n    r\"\"\"\n    Compute the \\(l_2\\)-inner product between a Torch model and a reference set of weights in a differentiable\n    way. The ```constraint_tensors``` are frozen.\n\n    Args:\n        target_model (nn.Module): Model being constrained by the ``constraint_tensors``. Weights are\n            differentiable.\n        constraint_tensors (list[torch.Tensor]): Tensors corresponding to a previous version of the\n            ``target_model``.\n        weight (float): Weight with which to scale the loss.\n\n    Returns:\n        (torch.Tensor): Loss value.\n    \"\"\"\n    # move model and tensors to device if needed\n    target_model = target_model.to(self.device)\n    constraint_tensors = [constraint_tensor.to(self.device) for constraint_tensor in constraint_tensors]\n\n    model_weights = list(target_model.parameters())\n    assert len(constraint_tensors) == len(model_weights)\n    assert len(model_weights) &gt; 0\n\n    layer_inner_products: list[torch.Tensor] = [\n        self._compute_weight_difference_inner_product(constraint_layer_weights, model_layer_weights)\n        for constraint_layer_weights, model_layer_weights in zip(constraint_tensors, model_weights)\n    ]\n\n    # Network l2 inner product tensor\n    # NOTE: Scaling by 1/2 is for grad consistency.\n    return (weight / 2.0) * torch.stack(layer_inner_products).sum()\n</code></pre>"},{"location":"api/#fl4health.metrics","title":"<code>metrics</code>","text":""},{"location":"api/#fl4health.metrics.F1","title":"<code>F1</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class F1(SimpleMetric):\n    def __init__(\n        self,\n        name: str = \"F1 score\",\n        average: str | None = \"weighted\",\n    ):\n        \"\"\"\n        Computes the F1 score using the ``sklearn f1_score`` function. As such, the values of average correspond to\n        those of that function.\n\n        Args:\n            name (str, optional): Name of the metric. Defaults to \"F1 score\".\n            average (str | None, optional): Whether to perform averaging of the F1 scores and how. The values of this\n                string corresponds to those of the ``sklearn f1_score function``. See:\n\n                https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n\n                Defaults to \"weighted\".\n        \"\"\"\n        super().__init__(name)\n        self.average = average\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        assert logits.shape[0] == target.shape[0]\n        target = target.cpu().detach()\n        logits = logits.cpu().detach()\n        y_true = target.reshape(-1)\n        preds = np.argmax(logits, axis=1)\n        return sklearn_metrics.f1_score(y_true, preds, average=self.average)\n</code></pre>"},{"location":"api/#fl4health.metrics.F1.__init__","title":"<code>__init__(name='F1 score', average='weighted')</code>","text":"<p>Computes the F1 score using the <code>sklearn f1_score</code> function. As such, the values of average correspond to those of that function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric. Defaults to \"F1 score\".</p> <code>'F1 score'</code> <code>average</code> <code>str | None</code> <p>Whether to perform averaging of the F1 scores and how. The values of this string corresponds to those of the <code>sklearn f1_score function</code>. See:</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html</p> <p>Defaults to \"weighted\".</p> <code>'weighted'</code> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"F1 score\",\n    average: str | None = \"weighted\",\n):\n    \"\"\"\n    Computes the F1 score using the ``sklearn f1_score`` function. As such, the values of average correspond to\n    those of that function.\n\n    Args:\n        name (str, optional): Name of the metric. Defaults to \"F1 score\".\n        average (str | None, optional): Whether to perform averaging of the F1 scores and how. The values of this\n            string corresponds to those of the ``sklearn f1_score function``. See:\n\n            https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n\n            Defaults to \"weighted\".\n    \"\"\"\n    super().__init__(name)\n    self.average = average\n</code></pre>"},{"location":"api/#fl4health.metrics.Accuracy","title":"<code>Accuracy</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class Accuracy(SimpleMetric):\n    def __init__(self, name: str = \"accuracy\"):\n        \"\"\"\n        Accuracy metric for classification tasks.\n\n        Args:\n            name (str): The name of the metric.\n\n        \"\"\"\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor, threshold: float = 0.5) -&gt; Scalar:\n        # assuming batch first\n        assert logits.shape[0] == target.shape[0]\n        # Single value output, assume binary logits\n        preds = (\n            (logits &gt; threshold).int() if len(logits.shape) == 1 or logits.shape[1] == 1 else torch.argmax(logits, 1)\n        )\n        target = target.cpu().detach()\n        preds = preds.cpu().detach()\n        return sklearn_metrics.accuracy_score(target, preds)\n</code></pre>"},{"location":"api/#fl4health.metrics.Accuracy.__init__","title":"<code>__init__(name='accuracy')</code>","text":"<p>Accuracy metric for classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>'accuracy'</code> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str = \"accuracy\"):\n    \"\"\"\n    Accuracy metric for classification tasks.\n\n    Args:\n        name (str): The name of the metric.\n\n    \"\"\"\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.metrics.BalancedAccuracy","title":"<code>BalancedAccuracy</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class BalancedAccuracy(SimpleMetric):\n    def __init__(self, name: str = \"balanced_accuracy\"):\n        \"\"\"\n        Balanced accuracy metric for classification tasks. Used for the evaluation of imbalanced datasets.\n\n        For more information:\n\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n        \"\"\"\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        # assuming batch first\n        assert logits.shape[0] == target.shape[0]\n        target = target.cpu().detach()\n        logits = logits.cpu().detach()\n        y_true = target.reshape(-1)\n        preds = np.argmax(logits, axis=1)\n        return sklearn_metrics.balanced_accuracy_score(y_true, preds)\n</code></pre>"},{"location":"api/#fl4health.metrics.BalancedAccuracy.__init__","title":"<code>__init__(name='balanced_accuracy')</code>","text":"<p>Balanced accuracy metric for classification tasks. Used for the evaluation of imbalanced datasets.</p> <p>For more information:</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str = \"balanced_accuracy\"):\n    \"\"\"\n    Balanced accuracy metric for classification tasks. Used for the evaluation of imbalanced datasets.\n\n    For more information:\n\n    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n    \"\"\"\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.metrics.BinarySoftDiceCoefficient","title":"<code>BinarySoftDiceCoefficient</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class BinarySoftDiceCoefficient(SimpleMetric):\n    def __init__(\n        self,\n        name: str = \"BinarySoftDiceCoefficient\",\n        epsilon: float = 1.0e-7,\n        spatial_dimensions: tuple[int, ...] = (2, 3, 4),\n        logits_threshold: float | None = 0.5,\n    ):\n        \"\"\"\n        Binary DICE Coefficient Metric with configurable spatial dimensions and logits threshold.\n\n        Args:\n            name (str): Name of the metric.\n            epsilon (float): Small float to add to denominator of DICE calculation to avoid divide by 0.\n            spatial_dimensions (tuple[int, ...]): The spatial dimensions of the image within the prediction tensors.\n                The default assumes that the images are 3D and have shape:\n                (``batch_size``, ``channel``, ``spatial``, ``spatial``, ``spatial``)\n            logits_threshold: This is a threshold value where values above are classified as 1 and those below are\n                mapped to 0. If the threshold is None, then no thresholding is performed and a continuous or \"soft\"\n                DICE coefficient is computed.\n        \"\"\"\n        self.epsilon = epsilon\n        self.spatial_dimensions = spatial_dimensions\n\n        self.logits_threshold = logits_threshold\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        # Assuming the logits are to be mapped to binary. Note that this assumes the logits have already been\n        # constrained to [0, 1]. The metric still functions if not, but results will be unpredictable.\n        y_pred = (logits &gt; self.logits_threshold).int() if self.logits_threshold else logits\n        intersection = (y_pred * target).sum(dim=self.spatial_dimensions)\n        union = (0.5 * (y_pred + target)).sum(dim=self.spatial_dimensions)\n        dice = intersection / (union + self.epsilon)\n        # If both inputs are empty the dice coefficient should be equal 1\n        dice[union == 0] = 1\n        return torch.mean(dice).item()\n</code></pre>"},{"location":"api/#fl4health.metrics.BinarySoftDiceCoefficient.__init__","title":"<code>__init__(name='BinarySoftDiceCoefficient', epsilon=1e-07, spatial_dimensions=(2, 3, 4), logits_threshold=0.5)</code>","text":"<p>Binary DICE Coefficient Metric with configurable spatial dimensions and logits threshold.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> <code>'BinarySoftDiceCoefficient'</code> <code>epsilon</code> <code>float</code> <p>Small float to add to denominator of DICE calculation to avoid divide by 0.</p> <code>1e-07</code> <code>spatial_dimensions</code> <code>tuple[int, ...]</code> <p>The spatial dimensions of the image within the prediction tensors. The default assumes that the images are 3D and have shape: (<code>batch_size</code>, <code>channel</code>, <code>spatial</code>, <code>spatial</code>, <code>spatial</code>)</p> <code>(2, 3, 4)</code> <code>logits_threshold</code> <code>float | None</code> <p>This is a threshold value where values above are classified as 1 and those below are mapped to 0. If the threshold is None, then no thresholding is performed and a continuous or \"soft\" DICE coefficient is computed.</p> <code>0.5</code> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"BinarySoftDiceCoefficient\",\n    epsilon: float = 1.0e-7,\n    spatial_dimensions: tuple[int, ...] = (2, 3, 4),\n    logits_threshold: float | None = 0.5,\n):\n    \"\"\"\n    Binary DICE Coefficient Metric with configurable spatial dimensions and logits threshold.\n\n    Args:\n        name (str): Name of the metric.\n        epsilon (float): Small float to add to denominator of DICE calculation to avoid divide by 0.\n        spatial_dimensions (tuple[int, ...]): The spatial dimensions of the image within the prediction tensors.\n            The default assumes that the images are 3D and have shape:\n            (``batch_size``, ``channel``, ``spatial``, ``spatial``, ``spatial``)\n        logits_threshold: This is a threshold value where values above are classified as 1 and those below are\n            mapped to 0. If the threshold is None, then no thresholding is performed and a continuous or \"soft\"\n            DICE coefficient is computed.\n    \"\"\"\n    self.epsilon = epsilon\n    self.spatial_dimensions = spatial_dimensions\n\n    self.logits_threshold = logits_threshold\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.metrics.RocAuc","title":"<code>RocAuc</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class RocAuc(SimpleMetric):\n    def __init__(self, name: str = \"ROC_AUC score\"):\n        \"\"\"\n        Area under the Receiver Operator Curve (AUCROC) metric for classification.\n\n        For more information:\n\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n        \"\"\"\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        assert logits.shape[0] == target.shape[0]\n        prob = torch.nn.functional.softmax(logits, dim=1)\n        prob = prob.cpu().detach()\n        target = target.cpu().detach()\n        y_true = target.reshape(-1)\n        return sklearn_metrics.roc_auc_score(y_true, prob, average=\"weighted\", multi_class=\"ovr\")\n</code></pre>"},{"location":"api/#fl4health.metrics.RocAuc.__init__","title":"<code>__init__(name='ROC_AUC score')</code>","text":"<p>Area under the Receiver Operator Curve (AUCROC) metric for classification.</p> <p>For more information:</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str = \"ROC_AUC score\"):\n    \"\"\"\n    Area under the Receiver Operator Curve (AUCROC) metric for classification.\n\n    For more information:\n\n    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n    \"\"\"\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.metrics.SimpleMetric","title":"<code>SimpleMetric</code>","text":"<p>               Bases: <code>Metric</code>, <code>ABC</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class SimpleMetric(Metric, ABC):\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"\n        Abstract metric class with base functionality to update, compute and clear metrics. User needs to define\n        ``__call__`` method which returns metric given inputs and target.\n\n        Args:\n            name (str): Name of the metric.\n        \"\"\"\n        super().__init__(name)\n        self.accumulated_inputs: list[torch.Tensor] = []\n        self.accumulated_targets: list[torch.Tensor] = []\n\n    def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n        \"\"\"\n        This method updates the state of the metric by appending the passed input and target pairing to their\n        respective list.\n\n        Args:\n            input (torch.Tensor): The predictions of the model to be evaluated.\n            target (torch.Tensor): The ground truth target to evaluate predictions against.\n        \"\"\"\n        self.accumulated_inputs.append(input)\n        self.accumulated_targets.append(target)\n\n    def compute(self, name: str | None = None) -&gt; Metrics:\n        \"\"\"\n        Compute metric on accumulated input and output over updates.\n\n        Args:\n            name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n                dictionary.\n\n        Raises:\n            AssertionError: Input and target lists must be non empty.\n\n        Returns:\n            (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n        \"\"\"\n        assert len(self.accumulated_inputs) &gt; 0 and len(self.accumulated_targets) &gt; 0\n        stacked_inputs = torch.cat(self.accumulated_inputs)\n        stacked_targets = torch.cat(self.accumulated_targets)\n        result = self.__call__(stacked_inputs, stacked_targets)\n        result_key = f\"{name} - {self.name}\" if name is not None else self.name\n\n        return {result_key: result}\n\n    def clear(self) -&gt; None:\n        \"\"\"Resets metrics by clearing input and target lists.\"\"\"\n        self.accumulated_inputs = []\n        self.accumulated_targets = []\n\n    @abstractmethod\n    def __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        \"\"\"\n        User defined method that calculates the desired metric given the predictions and target.\n\n        Raises:\n            NotImplementedError: User must define this method.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.SimpleMetric.__init__","title":"<code>__init__(name)</code>","text":"<p>Abstract metric class with base functionality to update, compute and clear metrics. User needs to define <code>__call__</code> method which returns metric given inputs and target.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"\n    Abstract metric class with base functionality to update, compute and clear metrics. User needs to define\n    ``__call__`` method which returns metric given inputs and target.\n\n    Args:\n        name (str): Name of the metric.\n    \"\"\"\n    super().__init__(name)\n    self.accumulated_inputs: list[torch.Tensor] = []\n    self.accumulated_targets: list[torch.Tensor] = []\n</code></pre>"},{"location":"api/#fl4health.metrics.SimpleMetric.update","title":"<code>update(input, target)</code>","text":"<p>This method updates the state of the metric by appending the passed input and target pairing to their respective list.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The predictions of the model to be evaluated.</p> required <code>target</code> <code>Tensor</code> <p>The ground truth target to evaluate predictions against.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n    \"\"\"\n    This method updates the state of the metric by appending the passed input and target pairing to their\n    respective list.\n\n    Args:\n        input (torch.Tensor): The predictions of the model to be evaluated.\n        target (torch.Tensor): The ground truth target to evaluate predictions against.\n    \"\"\"\n    self.accumulated_inputs.append(input)\n    self.accumulated_targets.append(target)\n</code></pre>"},{"location":"api/#fl4health.metrics.SimpleMetric.compute","title":"<code>compute(name=None)</code>","text":"<p>Compute metric on accumulated input and output over updates.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Optional name used in conjunction with class attribute name to define key in metrics dictionary.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>Input and target lists must be non empty.</p> <p>Returns:</p> Type Description <code>Metrics</code> <p>A dictionary of string and <code>Scalar</code> representing the computed metric and its associated key.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def compute(self, name: str | None = None) -&gt; Metrics:\n    \"\"\"\n    Compute metric on accumulated input and output over updates.\n\n    Args:\n        name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n            dictionary.\n\n    Raises:\n        AssertionError: Input and target lists must be non empty.\n\n    Returns:\n        (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n    \"\"\"\n    assert len(self.accumulated_inputs) &gt; 0 and len(self.accumulated_targets) &gt; 0\n    stacked_inputs = torch.cat(self.accumulated_inputs)\n    stacked_targets = torch.cat(self.accumulated_targets)\n    result = self.__call__(stacked_inputs, stacked_targets)\n    result_key = f\"{name} - {self.name}\" if name is not None else self.name\n\n    return {result_key: result}\n</code></pre>"},{"location":"api/#fl4health.metrics.SimpleMetric.clear","title":"<code>clear()</code>","text":"<p>Resets metrics by clearing input and target lists.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Resets metrics by clearing input and target lists.\"\"\"\n    self.accumulated_inputs = []\n    self.accumulated_targets = []\n</code></pre>"},{"location":"api/#fl4health.metrics.SimpleMetric.__call__","title":"<code>__call__(input, target)</code>  <code>abstractmethod</code>","text":"<p>User defined method that calculates the desired metric given the predictions and target.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>User must define this method.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>@abstractmethod\ndef __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n    \"\"\"\n    User defined method that calculates the desired metric given the predictions and target.\n\n    Raises:\n        NotImplementedError: User must define this method.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.TorchMetric","title":"<code>TorchMetric</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class TorchMetric(Metric):\n    def __init__(self, name: str, metric: TMetric) -&gt; None:\n        \"\"\"\n        Thin wrapper on ``TorchMetric`` to make it compatible with our ``Metric`` interface.\n\n        Args:\n            name (str): The name of the metric.\n            metric (TMetric): ``TorchMetric`` class based metric.\n        \"\"\"\n        super().__init__(name)\n        self.metric = metric\n\n    def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n        \"\"\"\n        Updates the state of the underlying ``TorchMetric``.\n\n        Args:\n            input (torch.Tensor): The predictions of the model to be evaluated.\n            target (torch.Tensor): The ground truth target to evaluate predictions against.\n        \"\"\"\n        self.metric.update(input, target.long())\n\n    def compute(self, name: str | None = None) -&gt; Metrics:\n        \"\"\"\n        Compute value of underlying ``TorchMetric``.\n\n        Args:\n            name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n                dictionary.\n\n        Returns:\n           (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n        \"\"\"\n        result_key = f\"{name} - {self.name}\" if name is not None else self.name\n        result = self.metric.compute().item()\n        return {result_key: result}\n\n    def clear(self) -&gt; None:\n        self.metric.reset()\n</code></pre>"},{"location":"api/#fl4health.metrics.TorchMetric.__init__","title":"<code>__init__(name, metric)</code>","text":"<p>Thin wrapper on <code>TorchMetric</code> to make it compatible with our <code>Metric</code> interface.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> required <code>metric</code> <code>Metric</code> <p><code>TorchMetric</code> class based metric.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str, metric: TMetric) -&gt; None:\n    \"\"\"\n    Thin wrapper on ``TorchMetric`` to make it compatible with our ``Metric`` interface.\n\n    Args:\n        name (str): The name of the metric.\n        metric (TMetric): ``TorchMetric`` class based metric.\n    \"\"\"\n    super().__init__(name)\n    self.metric = metric\n</code></pre>"},{"location":"api/#fl4health.metrics.TorchMetric.update","title":"<code>update(input, target)</code>","text":"<p>Updates the state of the underlying <code>TorchMetric</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The predictions of the model to be evaluated.</p> required <code>target</code> <code>Tensor</code> <p>The ground truth target to evaluate predictions against.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n    \"\"\"\n    Updates the state of the underlying ``TorchMetric``.\n\n    Args:\n        input (torch.Tensor): The predictions of the model to be evaluated.\n        target (torch.Tensor): The ground truth target to evaluate predictions against.\n    \"\"\"\n    self.metric.update(input, target.long())\n</code></pre>"},{"location":"api/#fl4health.metrics.TorchMetric.compute","title":"<code>compute(name=None)</code>","text":"<p>Compute value of underlying <code>TorchMetric</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Optional name used in conjunction with class attribute name to define key in metrics dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>Metrics</code> <p>A dictionary of string and <code>Scalar</code> representing the computed metric and its associated key.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def compute(self, name: str | None = None) -&gt; Metrics:\n    \"\"\"\n    Compute value of underlying ``TorchMetric``.\n\n    Args:\n        name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n            dictionary.\n\n    Returns:\n       (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n    \"\"\"\n    result_key = f\"{name} - {self.name}\" if name is not None else self.name\n    result = self.metric.compute().item()\n    return {result_key: result}\n</code></pre>"},{"location":"api/#fl4health.metrics.base_metrics","title":"<code>base_metrics</code>","text":""},{"location":"api/#fl4health.metrics.base_metrics.Metric","title":"<code>Metric</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>fl4health/metrics/base_metrics.py</code> <pre><code>class Metric(ABC):\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"\n        Base abstract Metric class to extend for metric accumulation and computation.\n\n        Args:\n            name (str): Name of the metric.\n        \"\"\"\n        self.name = name\n\n    @abstractmethod\n    def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n        \"\"\"\n        This method updates the state of the metric by appending the passed input and target pairing to their\n        respective list.\n\n        Args:\n            input (torch.Tensor): The predictions of the model to be evaluated.\n            target (torch.Tensor): The ground truth target to evaluate predictions against.\n\n        Raises:\n            NotImplementedError: To be defined in the classes extending this class.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def compute(self, name: str | None = None) -&gt; Metrics:\n        \"\"\"\n        Compute metric on state accumulated over updates.\n\n        Args:\n            name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n                dictionary.\n\n        Raises:\n            NotImplementedError: To be defined in the classes extending this class.\n\n        Returns:\n           (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def clear(self) -&gt; None:\n        \"\"\"\n        Resets metric.\n\n        Raises:\n            NotImplementedError: To be defined in the classes extending this class.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.base_metrics.Metric.__init__","title":"<code>__init__(name)</code>","text":"<p>Base abstract Metric class to extend for metric accumulation and computation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> required Source code in <code>fl4health/metrics/base_metrics.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"\n    Base abstract Metric class to extend for metric accumulation and computation.\n\n    Args:\n        name (str): Name of the metric.\n    \"\"\"\n    self.name = name\n</code></pre>"},{"location":"api/#fl4health.metrics.base_metrics.Metric.update","title":"<code>update(input, target)</code>  <code>abstractmethod</code>","text":"<p>This method updates the state of the metric by appending the passed input and target pairing to their respective list.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The predictions of the model to be evaluated.</p> required <code>target</code> <code>Tensor</code> <p>The ground truth target to evaluate predictions against.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in the classes extending this class.</p> Source code in <code>fl4health/metrics/base_metrics.py</code> <pre><code>@abstractmethod\ndef update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n    \"\"\"\n    This method updates the state of the metric by appending the passed input and target pairing to their\n    respective list.\n\n    Args:\n        input (torch.Tensor): The predictions of the model to be evaluated.\n        target (torch.Tensor): The ground truth target to evaluate predictions against.\n\n    Raises:\n        NotImplementedError: To be defined in the classes extending this class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.base_metrics.Metric.compute","title":"<code>compute(name=None)</code>  <code>abstractmethod</code>","text":"<p>Compute metric on state accumulated over updates.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Optional name used in conjunction with class attribute name to define key in metrics dictionary.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in the classes extending this class.</p> <p>Returns:</p> Type Description <code>Metrics</code> <p>A dictionary of string and <code>Scalar</code> representing the computed metric and its associated key.</p> Source code in <code>fl4health/metrics/base_metrics.py</code> <pre><code>@abstractmethod\ndef compute(self, name: str | None = None) -&gt; Metrics:\n    \"\"\"\n    Compute metric on state accumulated over updates.\n\n    Args:\n        name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n            dictionary.\n\n    Raises:\n        NotImplementedError: To be defined in the classes extending this class.\n\n    Returns:\n       (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.base_metrics.Metric.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Resets metric.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be defined in the classes extending this class.</p> Source code in <code>fl4health/metrics/base_metrics.py</code> <pre><code>@abstractmethod\ndef clear(self) -&gt; None:\n    \"\"\"\n    Resets metric.\n\n    Raises:\n        NotImplementedError: To be defined in the classes extending this class.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.compound_metrics","title":"<code>compound_metrics</code>","text":""},{"location":"api/#fl4health.metrics.compound_metrics.EmaMetric","title":"<code>EmaMetric</code>","text":"<p>               Bases: <code>Metric</code>, <code>Generic[T]</code></p> Source code in <code>fl4health/metrics/compound_metrics.py</code> <pre><code>class EmaMetric(Metric, Generic[T]):\n    def __init__(self, metric: T, smoothing_factor: float = 0.1, name: str | None = None):\n        \"\"\"\n        Exponential Moving Average (EMA) metric wrapper to apply EMA to the underlying metric.\n\n        **NOTE**: If the underlying metric accumulates batches during update, then updating this metric without\n        clearing in between will result in previously seen inputs and targets being a part of subsequent computations.\n        For example, if we use ``Accuracy`` from ``fl4health.metrics``, which accumulates batches, we get the following\n        behavior in the code block below.\n\n        ```python\n        from fl4health.metrics import Accuracy\n\n        ema = EmaMetric(Accuracy(), 0.1)\n\n        preds_1 = torch.Tensor([1, 0, 1]), targets_1 = torch.Tensor([1, 1, 1])\n\n        ema.update(preds_1, targets_1)\n\n        ema.compute() -&gt; 0.667\n\n        preds_2 = torch.Tensor([0, 0, 1]), targets_2 = torch.Tensor([1, 1, 1])\n\n        # If no clear before update (new accuracy is computed using both pred_1 and pred_2)\n\n        ema.update(preds_2, targets_2) = 0.9(0.667) + 0.1 (0.5)\n\n        # If there were a clear before update (new accuracy is computed using pred_2)\n\n        ema.clear()\n\n        ema.update(preds_2, targets_2 = 0.9(0.667) + 0.1(0.333)\n        ```\n\n        Args:\n            metric (T): An FL4Health compatible metric.\n            smoothing_factor (float, optional): Smoothing factor in range [0, 1] for the EMA. Smaller values increase\n                smoothing by weighting previous scores more heavily. Defaults to 0.1.\n            name (str | None, optional): Name of the ``EMAMetric``. If left as None will default to\n                'EMA_{metric.name}'.\n\n        \"\"\"\n        # Create a copy of the metrics object so that we do not inadvertently change the provided object elsewhere\n        self.metric = copy.deepcopy(metric)\n        assert 0.0 &lt;= smoothing_factor &lt;= 1.0, f\"smoothing_factor should be in [0, 1] but was {smoothing_factor}\"\n        self.smoothing_factor = smoothing_factor\n        self.previous_score: Metrics | None = None\n        self.name = f\"EMA_{self.metric.name}\" if name is None else name\n\n    def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n        return self.metric.update(input, target)\n\n    def compute(self, name: str | None = None) -&gt; Metrics:\n        \"\"\"\n        Compute metric on state accumulated over updates.\n\n        This computation considers the exponential moving average with respect to previous scores. For time step\n        \\\\(t\\\\), and metric score \\\\(m_t\\\\), the EMA score is computed\n\n        \\\\[\n        \\\\text{smoothing_factor} \\\\cdot m_t + (1-\\\\text{smoothing_factor}) \\\\cdot (m_{t-1}).\n        \\\\]\n\n        The very first score is stored as is.\n\n        Args:\n            name (str | None, optional): Optional name used in conjunction with class attribute name to define key in\n                metrics dictionary. Defaults to None.\n\n        Returns:\n            (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n        \"\"\"\n        # Temporarily change name of the underlying metric so that we get the EMAMetric name in keys of metrics_dict\n        metric_name = self.metric.name\n        self.metric.name = self.name\n        metrics_dict = self.metric.compute(name)\n        self.metric.name = metric_name\n\n        # Check if this is the first score\n        if self.previous_score is None:\n            self._drop_str_or_bytes_scores_and_store(metrics_dict)\n            assert self.previous_score is not None\n            return copy.deepcopy(self.previous_score)\n\n        # Otherwise compute EMA score for each 'metric' in Metrics dict\n        for key, previous_score in self.previous_score.items():\n            current_score = metrics_dict[key]\n            if not isinstance(current_score, (str, bytes)) and not isinstance(previous_score, (str, bytes)):\n                self.previous_score[key] = (\n                    self.smoothing_factor * current_score + (1 - self.smoothing_factor) * previous_score\n                )\n\n        return copy.deepcopy(self.previous_score)\n\n    def _drop_str_or_bytes_scores_and_store(self, metrics_dict: Metrics) -&gt; None:\n        self.previous_score = {}\n        for key, score in metrics_dict.items():\n            if not isinstance(score, (int, float)):\n                log(\n                    WARNING,\n                    \"EMAMetric is only compatible with float or int metrics, but metrics contains a value with \"\n                    f\"type: {type(score)} at key: {key}. These values will be ignored in subsequent computations.\",\n                )\n            else:\n                self.previous_score[key] = score\n\n    def clear(self) -&gt; None:\n        # Clear accumulated inputs and targets but not the previous score\n        return self.metric.clear()\n</code></pre>"},{"location":"api/#fl4health.metrics.compound_metrics.EmaMetric.__init__","title":"<code>__init__(metric, smoothing_factor=0.1, name=None)</code>","text":"<p>Exponential Moving Average (EMA) metric wrapper to apply EMA to the underlying metric.</p> <p>NOTE: If the underlying metric accumulates batches during update, then updating this metric without clearing in between will result in previously seen inputs and targets being a part of subsequent computations. For example, if we use <code>Accuracy</code> from <code>fl4health.metrics</code>, which accumulates batches, we get the following behavior in the code block below.</p> <pre><code>from fl4health.metrics import Accuracy\n\nema = EmaMetric(Accuracy(), 0.1)\n\npreds_1 = torch.Tensor([1, 0, 1]), targets_1 = torch.Tensor([1, 1, 1])\n\nema.update(preds_1, targets_1)\n\nema.compute() -&gt; 0.667\n\npreds_2 = torch.Tensor([0, 0, 1]), targets_2 = torch.Tensor([1, 1, 1])\n\n# If no clear before update (new accuracy is computed using both pred_1 and pred_2)\n\nema.update(preds_2, targets_2) = 0.9(0.667) + 0.1 (0.5)\n\n# If there were a clear before update (new accuracy is computed using pred_2)\n\nema.clear()\n\nema.update(preds_2, targets_2 = 0.9(0.667) + 0.1(0.333)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>T</code> <p>An FL4Health compatible metric.</p> required <code>smoothing_factor</code> <code>float</code> <p>Smoothing factor in range [0, 1] for the EMA. Smaller values increase smoothing by weighting previous scores more heavily. Defaults to 0.1.</p> <code>0.1</code> <code>name</code> <code>str | None</code> <p>Name of the <code>EMAMetric</code>. If left as None will default to 'EMA_{metric.name}'.</p> <code>None</code> Source code in <code>fl4health/metrics/compound_metrics.py</code> <pre><code>def __init__(self, metric: T, smoothing_factor: float = 0.1, name: str | None = None):\n    \"\"\"\n    Exponential Moving Average (EMA) metric wrapper to apply EMA to the underlying metric.\n\n    **NOTE**: If the underlying metric accumulates batches during update, then updating this metric without\n    clearing in between will result in previously seen inputs and targets being a part of subsequent computations.\n    For example, if we use ``Accuracy`` from ``fl4health.metrics``, which accumulates batches, we get the following\n    behavior in the code block below.\n\n    ```python\n    from fl4health.metrics import Accuracy\n\n    ema = EmaMetric(Accuracy(), 0.1)\n\n    preds_1 = torch.Tensor([1, 0, 1]), targets_1 = torch.Tensor([1, 1, 1])\n\n    ema.update(preds_1, targets_1)\n\n    ema.compute() -&gt; 0.667\n\n    preds_2 = torch.Tensor([0, 0, 1]), targets_2 = torch.Tensor([1, 1, 1])\n\n    # If no clear before update (new accuracy is computed using both pred_1 and pred_2)\n\n    ema.update(preds_2, targets_2) = 0.9(0.667) + 0.1 (0.5)\n\n    # If there were a clear before update (new accuracy is computed using pred_2)\n\n    ema.clear()\n\n    ema.update(preds_2, targets_2 = 0.9(0.667) + 0.1(0.333)\n    ```\n\n    Args:\n        metric (T): An FL4Health compatible metric.\n        smoothing_factor (float, optional): Smoothing factor in range [0, 1] for the EMA. Smaller values increase\n            smoothing by weighting previous scores more heavily. Defaults to 0.1.\n        name (str | None, optional): Name of the ``EMAMetric``. If left as None will default to\n            'EMA_{metric.name}'.\n\n    \"\"\"\n    # Create a copy of the metrics object so that we do not inadvertently change the provided object elsewhere\n    self.metric = copy.deepcopy(metric)\n    assert 0.0 &lt;= smoothing_factor &lt;= 1.0, f\"smoothing_factor should be in [0, 1] but was {smoothing_factor}\"\n    self.smoothing_factor = smoothing_factor\n    self.previous_score: Metrics | None = None\n    self.name = f\"EMA_{self.metric.name}\" if name is None else name\n</code></pre>"},{"location":"api/#fl4health.metrics.compound_metrics.EmaMetric.compute","title":"<code>compute(name=None)</code>","text":"<p>Compute metric on state accumulated over updates.</p> <p>This computation considers the exponential moving average with respect to previous scores. For time step \\(t\\), and metric score \\(m_t\\), the EMA score is computed</p> \\[ \\text{smoothing_factor} \\cdot m_t + (1-\\text{smoothing_factor}) \\cdot (m_{t-1}). \\] <p>The very first score is stored as is.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Optional name used in conjunction with class attribute name to define key in metrics dictionary. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Metrics</code> <p>A dictionary of string and <code>Scalar</code> representing the computed metric and its associated key.</p> Source code in <code>fl4health/metrics/compound_metrics.py</code> <pre><code>def compute(self, name: str | None = None) -&gt; Metrics:\n    \"\"\"\n    Compute metric on state accumulated over updates.\n\n    This computation considers the exponential moving average with respect to previous scores. For time step\n    \\\\(t\\\\), and metric score \\\\(m_t\\\\), the EMA score is computed\n\n    \\\\[\n    \\\\text{smoothing_factor} \\\\cdot m_t + (1-\\\\text{smoothing_factor}) \\\\cdot (m_{t-1}).\n    \\\\]\n\n    The very first score is stored as is.\n\n    Args:\n        name (str | None, optional): Optional name used in conjunction with class attribute name to define key in\n            metrics dictionary. Defaults to None.\n\n    Returns:\n        (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n    \"\"\"\n    # Temporarily change name of the underlying metric so that we get the EMAMetric name in keys of metrics_dict\n    metric_name = self.metric.name\n    self.metric.name = self.name\n    metrics_dict = self.metric.compute(name)\n    self.metric.name = metric_name\n\n    # Check if this is the first score\n    if self.previous_score is None:\n        self._drop_str_or_bytes_scores_and_store(metrics_dict)\n        assert self.previous_score is not None\n        return copy.deepcopy(self.previous_score)\n\n    # Otherwise compute EMA score for each 'metric' in Metrics dict\n    for key, previous_score in self.previous_score.items():\n        current_score = metrics_dict[key]\n        if not isinstance(current_score, (str, bytes)) and not isinstance(previous_score, (str, bytes)):\n            self.previous_score[key] = (\n                self.smoothing_factor * current_score + (1 - self.smoothing_factor) * previous_score\n            )\n\n    return copy.deepcopy(self.previous_score)\n</code></pre>"},{"location":"api/#fl4health.metrics.compound_metrics.TransformsMetric","title":"<code>TransformsMetric</code>","text":"<p>               Bases: <code>Metric</code>, <code>Generic[T]</code></p> Source code in <code>fl4health/metrics/compound_metrics.py</code> <pre><code>class TransformsMetric(Metric, Generic[T]):\n    def __init__(\n        self,\n        metric: T,\n        pred_transforms: Sequence[TorchTransformFunction] | None = None,\n        target_transforms: Sequence[TorchTransformFunction] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        A thin wrapper class to allow transforms to be applied to preds and targets prior to calculating metrics.\n        Transforms are applied in the order given.\n\n        Args:\n            metric (Metric): A FL4Health compatible metric\n            pred_transforms (Sequence[TorchTransformFunction] | None, optional): A list of transform functions to\n                apply to the model predictions before computing the metrics. Each callable must accept and return a\n                ``torch.Tensor``. Use partial to set other arguments.\n            target_transforms (Sequence[TorchTransformFunction] | None, optional): A list of transform functions to\n                apply to the targets before computing the metrics. Each callable must accept and return a\n                ``torch.Tensor``. Use partial to set other arguments.\n        \"\"\"\n        # Create a copy of the metrics object so that we do not inadvertently change the provided object elsewhere\n        self.metric = copy.deepcopy(metric)\n        self.pred_transforms = [] if pred_transforms is None else pred_transforms\n        self.target_transforms = [] if target_transforms is None else target_transforms\n        super().__init__(name=self.metric.name)\n\n    def update(self, pred: torch.Tensor, target: torch.Tensor) -&gt; None:\n        for transform in self.pred_transforms:\n            pred = transform(pred)\n\n        for transform in self.target_transforms:\n            target = transform(target)\n\n        self.metric.update(pred, target)\n\n    def compute(self, name: str | None = None) -&gt; Metrics:\n        return self.metric.compute(name)\n\n    def clear(self) -&gt; None:\n        return self.metric.clear()\n</code></pre>"},{"location":"api/#fl4health.metrics.compound_metrics.TransformsMetric.__init__","title":"<code>__init__(metric, pred_transforms=None, target_transforms=None)</code>","text":"<p>A thin wrapper class to allow transforms to be applied to preds and targets prior to calculating metrics. Transforms are applied in the order given.</p> <p>Parameters:</p> Name Type Description Default <code>metric</code> <code>Metric</code> <p>A FL4Health compatible metric</p> required <code>pred_transforms</code> <code>Sequence[TorchTransformFunction] | None</code> <p>A list of transform functions to apply to the model predictions before computing the metrics. Each callable must accept and return a <code>torch.Tensor</code>. Use partial to set other arguments.</p> <code>None</code> <code>target_transforms</code> <code>Sequence[TorchTransformFunction] | None</code> <p>A list of transform functions to apply to the targets before computing the metrics. Each callable must accept and return a <code>torch.Tensor</code>. Use partial to set other arguments.</p> <code>None</code> Source code in <code>fl4health/metrics/compound_metrics.py</code> <pre><code>def __init__(\n    self,\n    metric: T,\n    pred_transforms: Sequence[TorchTransformFunction] | None = None,\n    target_transforms: Sequence[TorchTransformFunction] | None = None,\n) -&gt; None:\n    \"\"\"\n    A thin wrapper class to allow transforms to be applied to preds and targets prior to calculating metrics.\n    Transforms are applied in the order given.\n\n    Args:\n        metric (Metric): A FL4Health compatible metric\n        pred_transforms (Sequence[TorchTransformFunction] | None, optional): A list of transform functions to\n            apply to the model predictions before computing the metrics. Each callable must accept and return a\n            ``torch.Tensor``. Use partial to set other arguments.\n        target_transforms (Sequence[TorchTransformFunction] | None, optional): A list of transform functions to\n            apply to the targets before computing the metrics. Each callable must accept and return a\n            ``torch.Tensor``. Use partial to set other arguments.\n    \"\"\"\n    # Create a copy of the metrics object so that we do not inadvertently change the provided object elsewhere\n    self.metric = copy.deepcopy(metric)\n    self.pred_transforms = [] if pred_transforms is None else pred_transforms\n    self.target_transforms = [] if target_transforms is None else target_transforms\n    super().__init__(name=self.metric.name)\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics","title":"<code>efficient_metrics</code>","text":""},{"location":"api/#fl4health.metrics.efficient_metrics.MultiClassDice","title":"<code>MultiClassDice</code>","text":"<p>               Bases: <code>MultiClassificationMetric</code></p> Source code in <code>fl4health/metrics/efficient_metrics.py</code> <pre><code>class MultiClassDice(MultiClassificationMetric):\n    def __init__(\n        self,\n        batch_dim: int | None,\n        label_dim: int,\n        name: str = \"MultiClassDice\",\n        dtype: torch.dtype = torch.float32,\n        threshold: float | int | None = None,\n        ignore_background: int | None = None,\n        zero_division: float | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Computes the Mean Dice Coefficient between class predictions and targets with multiple classes.\n\n        **NOTE**: The default behavior for Dice Scores is to compute the mean over each **SAMPLE** of the dataset being\n        measured. In the image domain, for example, this means that the Dice score is computed for each image\n        separately and then averaged across images (then classes) to produce a single score. This is accomplished\n        by specifying the ``batch_dim`` here. If, however, you would like to compute the Dice score over ALL TP, FP,\n        FNs across all samples (then classes) as a single count, ``batch_dim = None`` is appropriate.\n\n        **NOTE**: Preds and targets are expected to have elements in the interval ``[0, 1]`` or to be thresholded,\n        using that argument to be as such.\n\n        **NOTE**: If preds and targets passed to the update method have different shapes, this class will attempt to\n        align the shapes by one-hot-encoding one (but not both) of the tensors if possible.\n\n        **NOTE**: In the case of **BINARY** predictions/targets with 2 labels, the result will be the **AVERAGE** of\n        the Dice score for the two labels. If you want a single score associated with one of the binary labels, use\n        ``BinaryDice``.\n\n        Args:\n            batch_dim (int | None, optional): If None, the counts along the specified dimension (i.e. for each sample)\n                are aggregated and the batch dimension is reduced. If specified, counts will be computed along the\n                dimension specified. That is, counts are maintained for each training sample **INDIVIDUALLY**.\n                **NOTE**: If ``batch_dim`` is specified, then counts will be presented batch dimension\n                first, then label dimension. For example, if ``batch_dim = 1`` and ``label_dim = 0``, then\n\n                ```python\n\n                p = torch.tensor(\n\n                ```\n                        [[[1.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0]]]\n                    )  # Size([2, 2, 4])\n\n                    t = torch.tensor(\n                        [[[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]]]\n                    )  # Size([2, 2, 4])\n\n                    self.tp = torch.Tensor([[2, 1], [0, 4]])  # Size([2, 2])\n\n                    self.tn = torch.Tensor([[1, 2], [4, 0]])  # Size([2, 2])\n\n                    self.fp = torch.Tensor([[1, 0], [0, 0]])  # Size([2, 2])\n\n                    self.fn = torch.Tensor([[0, 1], [0, 0]])  # Size([2, 2])\n\n                In computing the Dice score ``(2*tp/(2*tp + fp + fn))``, we get scores for each sample/label pair as\n                    ``[[2*2/(2*2+1+0), 2*1/(2*1+0+1)], [0*2/(0*2+0+0), 2*4/(2*4+0+0)]]``.\n                Assuming ``zero_division = None``, the undefined calculation at ``(1, 0)`` is dropped and the\n                remainder of the individual scores are averaged to be ``(1/3)*(4/5 + 2/3 + 8/8) = 0.8222``.\n            label_dim (int): Specifies which dimension in the provided tensors corresponds to the label\n                dimension. During metric computation, this dimension must have size of **AT LEAST 2**. Counts are\n                always computed along the label dimension. That is, counts are maintained for each output label\n                **INDIVIDUALLY**.\n            name (str): Name of the metric. Defaults to 'MultiClassDice'\n            dtype (torch.dtype): The dtype to store the counts as. If preds or targets can be continuous, specify a\n                float type. Otherwise specify an integer type to prevent overflow. Defaults to ``torch.float32``.\n            threshold (float | int | None, optional): A float for thresholding values or an integer specifying the\n                index of the label dimension. If a float is given, predictions below the threshold are mapped\n                to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class\n                with the highest prediction where the specified axis is assumed to contain a prediction for each class\n                (where its index along that dimension is the class label). Default of None leaves preds unchanged.\n            ignore_background (int | None): If specified, the **FIRST** channel of the specified axis is removed prior\n                to computing the counts. Useful for removing background classes. Defaults to None.\n            zero_division (float | None, optional): Set what the individual Dice coefficients should be when there is\n                a zero division (only true negatives present). How this argument affects the final Dice score will vary\n                depending on the Dice scores for other labels. If left as None, the resultant Dice coefficients will\n                be excluded from the average/final Dice score.\n        \"\"\"\n        super().__init__(\n            name=name,\n            batch_dim=batch_dim,\n            label_dim=label_dim,\n            dtype=dtype,\n            threshold=threshold,\n            ignore_background=ignore_background,\n            discard={ClassificationOutcome.TRUE_NEGATIVE},\n        )\n        self.zero_division = zero_division\n\n    def compute_from_counts(\n        self,\n        true_positives: torch.Tensor,\n        false_positives: torch.Tensor,\n        true_negatives: torch.Tensor,\n        false_negatives: torch.Tensor,\n    ) -&gt; Metrics:\n        \"\"\"\n        Computes a multi-class Dice score, defined to be the mean Dice score across all labels in the multi-class\n        problem. This score is computed relative the outcome counts provided in the form of true positives (TP),\n        false positives (FP), and false negatives (FN). Because Dice scores don't factor in true negatives, this\n        argument is unused. For a set of counts, the Dice score for a particular label is...\n\n        \\\\[\n\n\n        \\\\]\n            \\\\frac{2 \\\\cdot TP}{2 \\\\cdot TP + FP + FN}.  2*TP/(2*TP + FP + FN).\n\n        For this class, counts are assumed to have shape ``(num_labels,)`` or ``(num_samples, num_labels)``. In the\n        former, a single Dice score is computed relative to the counts for each label and then **AVERAGED**. In the\n        latter, an **AVERAGE** dice score over both the samples AND labels computed. The second setting is useful, for\n        example, if you are computing the Dice score per image and then averaging. The first setting is useful, for\n        example, if you want to treat all examples as a **SINGLE** image.\n\n        Args:\n            true_positives (torch.Tensor): Counts associated with positive predictions and positive labels.\n            false_positives (torch.Tensor): Counts associated with positive predictions and negative labels.\n            true_negatives (torch.Tensor): Counts associated with negative predictions and negative labels.\n            false_negatives (torch.Tensor): Counts associated with negative predictions and positive labels.\n\n        Returns:\n            (Metrics): A mean dice score associated with the counts.\n        \"\"\"\n        # compute dice coefficients and return mean\n        dice = compute_dice_on_count_tensors(true_positives, false_positives, false_negatives, self.zero_division)\n        if dice.numel() == 0:\n            log(WARNING, \"Currently, Dice score is undefined due to only true negatives present\")\n        return {self.name: torch.mean(dice).item()}\n\n    def __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        \"\"\"\n        Computes the Dice score relative to the single input and target tensors provided.\n\n        Args:\n            input (torch.Tensor): predictions tensor.\n            target (torch.Tensor): target tensor.\n\n        Returns:\n            (Scalar): Mean dice score for the provided tensors.\n        \"\"\"\n        true_positives, false_positives, true_negatives, false_negatives = self.count_tp_fp_tn_fn(input, target)\n        dice_metric = self.compute_from_counts(true_positives, false_positives, true_negatives, false_negatives)\n        # Extract the scalar from the dictionary.\n        return dice_metric[self.name]\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics.MultiClassDice.__init__","title":"<code>__init__(batch_dim, label_dim, name='MultiClassDice', dtype=torch.float32, threshold=None, ignore_background=None, zero_division=None)</code>","text":"<p>Computes the Mean Dice Coefficient between class predictions and targets with multiple classes.</p> <p>NOTE: The default behavior for Dice Scores is to compute the mean over each SAMPLE of the dataset being measured. In the image domain, for example, this means that the Dice score is computed for each image separately and then averaged across images (then classes) to produce a single score. This is accomplished by specifying the <code>batch_dim</code> here. If, however, you would like to compute the Dice score over ALL TP, FP, FNs across all samples (then classes) as a single count, <code>batch_dim = None</code> is appropriate.</p> <p>NOTE: Preds and targets are expected to have elements in the interval <code>[0, 1]</code> or to be thresholded, using that argument to be as such.</p> <p>NOTE: If preds and targets passed to the update method have different shapes, this class will attempt to align the shapes by one-hot-encoding one (but not both) of the tensors if possible.</p> <p>NOTE: In the case of BINARY predictions/targets with 2 labels, the result will be the AVERAGE of the Dice score for the two labels. If you want a single score associated with one of the binary labels, use <code>BinaryDice</code>.</p> <p>Parameters:</p> Name Type Description Default <code>batch_dim</code> <code>int | None</code> <p>If None, the counts along the specified dimension (i.e. for each sample) are aggregated and the batch dimension is reduced. If specified, counts will be computed along the dimension specified. That is, counts are maintained for each training sample INDIVIDUALLY. NOTE: If <code>batch_dim</code> is specified, then counts will be presented batch dimension first, then label dimension. For example, if <code>batch_dim = 1</code> and <code>label_dim = 0</code>, then</p> <p><pre><code>p = torch.tensor(\n</code></pre>         [[[1.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0]]]     )  # Size([2, 2, 4])</p> <pre><code>t = torch.tensor(\n    [[[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]]]\n)  # Size([2, 2, 4])\n\nself.tp = torch.Tensor([[2, 1], [0, 4]])  # Size([2, 2])\n\nself.tn = torch.Tensor([[1, 2], [4, 0]])  # Size([2, 2])\n\nself.fp = torch.Tensor([[1, 0], [0, 0]])  # Size([2, 2])\n\nself.fn = torch.Tensor([[0, 1], [0, 0]])  # Size([2, 2])\n</code></pre> <p>In computing the Dice score <code>(2*tp/(2*tp + fp + fn))</code>, we get scores for each sample/label pair as     <code>[[2*2/(2*2+1+0), 2*1/(2*1+0+1)], [0*2/(0*2+0+0), 2*4/(2*4+0+0)]]</code>. Assuming <code>zero_division = None</code>, the undefined calculation at <code>(1, 0)</code> is dropped and the remainder of the individual scores are averaged to be <code>(1/3)*(4/5 + 2/3 + 8/8) = 0.8222</code>.</p> required <code>label_dim</code> <code>int</code> <p>Specifies which dimension in the provided tensors corresponds to the label dimension. During metric computation, this dimension must have size of AT LEAST 2. Counts are always computed along the label dimension. That is, counts are maintained for each output label INDIVIDUALLY.</p> required <code>name</code> <code>str</code> <p>Name of the metric. Defaults to 'MultiClassDice'</p> <code>'MultiClassDice'</code> <code>dtype</code> <code>dtype</code> <p>The dtype to store the counts as. If preds or targets can be continuous, specify a float type. Otherwise specify an integer type to prevent overflow. Defaults to <code>torch.float32</code>.</p> <code>float32</code> <code>threshold</code> <code>float | int | None</code> <p>A float for thresholding values or an integer specifying the index of the label dimension. If a float is given, predictions below the threshold are mapped to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class with the highest prediction where the specified axis is assumed to contain a prediction for each class (where its index along that dimension is the class label). Default of None leaves preds unchanged.</p> <code>None</code> <code>ignore_background</code> <code>int | None</code> <p>If specified, the FIRST channel of the specified axis is removed prior to computing the counts. Useful for removing background classes. Defaults to None.</p> <code>None</code> <code>zero_division</code> <code>float | None</code> <p>Set what the individual Dice coefficients should be when there is a zero division (only true negatives present). How this argument affects the final Dice score will vary depending on the Dice scores for other labels. If left as None, the resultant Dice coefficients will be excluded from the average/final Dice score.</p> <code>None</code> Source code in <code>fl4health/metrics/efficient_metrics.py</code> <pre><code>def __init__(\n    self,\n    batch_dim: int | None,\n    label_dim: int,\n    name: str = \"MultiClassDice\",\n    dtype: torch.dtype = torch.float32,\n    threshold: float | int | None = None,\n    ignore_background: int | None = None,\n    zero_division: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Computes the Mean Dice Coefficient between class predictions and targets with multiple classes.\n\n    **NOTE**: The default behavior for Dice Scores is to compute the mean over each **SAMPLE** of the dataset being\n    measured. In the image domain, for example, this means that the Dice score is computed for each image\n    separately and then averaged across images (then classes) to produce a single score. This is accomplished\n    by specifying the ``batch_dim`` here. If, however, you would like to compute the Dice score over ALL TP, FP,\n    FNs across all samples (then classes) as a single count, ``batch_dim = None`` is appropriate.\n\n    **NOTE**: Preds and targets are expected to have elements in the interval ``[0, 1]`` or to be thresholded,\n    using that argument to be as such.\n\n    **NOTE**: If preds and targets passed to the update method have different shapes, this class will attempt to\n    align the shapes by one-hot-encoding one (but not both) of the tensors if possible.\n\n    **NOTE**: In the case of **BINARY** predictions/targets with 2 labels, the result will be the **AVERAGE** of\n    the Dice score for the two labels. If you want a single score associated with one of the binary labels, use\n    ``BinaryDice``.\n\n    Args:\n        batch_dim (int | None, optional): If None, the counts along the specified dimension (i.e. for each sample)\n            are aggregated and the batch dimension is reduced. If specified, counts will be computed along the\n            dimension specified. That is, counts are maintained for each training sample **INDIVIDUALLY**.\n            **NOTE**: If ``batch_dim`` is specified, then counts will be presented batch dimension\n            first, then label dimension. For example, if ``batch_dim = 1`` and ``label_dim = 0``, then\n\n            ```python\n\n            p = torch.tensor(\n\n            ```\n                    [[[1.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0]]]\n                )  # Size([2, 2, 4])\n\n                t = torch.tensor(\n                    [[[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]]]\n                )  # Size([2, 2, 4])\n\n                self.tp = torch.Tensor([[2, 1], [0, 4]])  # Size([2, 2])\n\n                self.tn = torch.Tensor([[1, 2], [4, 0]])  # Size([2, 2])\n\n                self.fp = torch.Tensor([[1, 0], [0, 0]])  # Size([2, 2])\n\n                self.fn = torch.Tensor([[0, 1], [0, 0]])  # Size([2, 2])\n\n            In computing the Dice score ``(2*tp/(2*tp + fp + fn))``, we get scores for each sample/label pair as\n                ``[[2*2/(2*2+1+0), 2*1/(2*1+0+1)], [0*2/(0*2+0+0), 2*4/(2*4+0+0)]]``.\n            Assuming ``zero_division = None``, the undefined calculation at ``(1, 0)`` is dropped and the\n            remainder of the individual scores are averaged to be ``(1/3)*(4/5 + 2/3 + 8/8) = 0.8222``.\n        label_dim (int): Specifies which dimension in the provided tensors corresponds to the label\n            dimension. During metric computation, this dimension must have size of **AT LEAST 2**. Counts are\n            always computed along the label dimension. That is, counts are maintained for each output label\n            **INDIVIDUALLY**.\n        name (str): Name of the metric. Defaults to 'MultiClassDice'\n        dtype (torch.dtype): The dtype to store the counts as. If preds or targets can be continuous, specify a\n            float type. Otherwise specify an integer type to prevent overflow. Defaults to ``torch.float32``.\n        threshold (float | int | None, optional): A float for thresholding values or an integer specifying the\n            index of the label dimension. If a float is given, predictions below the threshold are mapped\n            to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class\n            with the highest prediction where the specified axis is assumed to contain a prediction for each class\n            (where its index along that dimension is the class label). Default of None leaves preds unchanged.\n        ignore_background (int | None): If specified, the **FIRST** channel of the specified axis is removed prior\n            to computing the counts. Useful for removing background classes. Defaults to None.\n        zero_division (float | None, optional): Set what the individual Dice coefficients should be when there is\n            a zero division (only true negatives present). How this argument affects the final Dice score will vary\n            depending on the Dice scores for other labels. If left as None, the resultant Dice coefficients will\n            be excluded from the average/final Dice score.\n    \"\"\"\n    super().__init__(\n        name=name,\n        batch_dim=batch_dim,\n        label_dim=label_dim,\n        dtype=dtype,\n        threshold=threshold,\n        ignore_background=ignore_background,\n        discard={ClassificationOutcome.TRUE_NEGATIVE},\n    )\n    self.zero_division = zero_division\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics.MultiClassDice.compute_from_counts","title":"<code>compute_from_counts(true_positives, false_positives, true_negatives, false_negatives)</code>","text":"<p>Computes a multi-class Dice score, defined to be the mean Dice score across all labels in the multi-class problem. This score is computed relative the outcome counts provided in the form of true positives (TP), false positives (FP), and false negatives (FN). Because Dice scores don't factor in true negatives, this argument is unused. For a set of counts, the Dice score for a particular label is...</p> <p>[</p> <p>]     \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}.  2TP/(2TP + FP + FN).</p> <p>For this class, counts are assumed to have shape <code>(num_labels,)</code> or <code>(num_samples, num_labels)</code>. In the former, a single Dice score is computed relative to the counts for each label and then AVERAGED. In the latter, an AVERAGE dice score over both the samples AND labels computed. The second setting is useful, for example, if you are computing the Dice score per image and then averaging. The first setting is useful, for example, if you want to treat all examples as a SINGLE image.</p> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>Tensor</code> <p>Counts associated with positive predictions and positive labels.</p> required <code>false_positives</code> <code>Tensor</code> <p>Counts associated with positive predictions and negative labels.</p> required <code>true_negatives</code> <code>Tensor</code> <p>Counts associated with negative predictions and negative labels.</p> required <code>false_negatives</code> <code>Tensor</code> <p>Counts associated with negative predictions and positive labels.</p> required <p>Returns:</p> Type Description <code>Metrics</code> <p>A mean dice score associated with the counts.</p> Source code in <code>fl4health/metrics/efficient_metrics.py</code> <pre><code>def compute_from_counts(\n    self,\n    true_positives: torch.Tensor,\n    false_positives: torch.Tensor,\n    true_negatives: torch.Tensor,\n    false_negatives: torch.Tensor,\n) -&gt; Metrics:\n    \"\"\"\n    Computes a multi-class Dice score, defined to be the mean Dice score across all labels in the multi-class\n    problem. This score is computed relative the outcome counts provided in the form of true positives (TP),\n    false positives (FP), and false negatives (FN). Because Dice scores don't factor in true negatives, this\n    argument is unused. For a set of counts, the Dice score for a particular label is...\n\n    \\\\[\n\n\n    \\\\]\n        \\\\frac{2 \\\\cdot TP}{2 \\\\cdot TP + FP + FN}.  2*TP/(2*TP + FP + FN).\n\n    For this class, counts are assumed to have shape ``(num_labels,)`` or ``(num_samples, num_labels)``. In the\n    former, a single Dice score is computed relative to the counts for each label and then **AVERAGED**. In the\n    latter, an **AVERAGE** dice score over both the samples AND labels computed. The second setting is useful, for\n    example, if you are computing the Dice score per image and then averaging. The first setting is useful, for\n    example, if you want to treat all examples as a **SINGLE** image.\n\n    Args:\n        true_positives (torch.Tensor): Counts associated with positive predictions and positive labels.\n        false_positives (torch.Tensor): Counts associated with positive predictions and negative labels.\n        true_negatives (torch.Tensor): Counts associated with negative predictions and negative labels.\n        false_negatives (torch.Tensor): Counts associated with negative predictions and positive labels.\n\n    Returns:\n        (Metrics): A mean dice score associated with the counts.\n    \"\"\"\n    # compute dice coefficients and return mean\n    dice = compute_dice_on_count_tensors(true_positives, false_positives, false_negatives, self.zero_division)\n    if dice.numel() == 0:\n        log(WARNING, \"Currently, Dice score is undefined due to only true negatives present\")\n    return {self.name: torch.mean(dice).item()}\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics.MultiClassDice.__call__","title":"<code>__call__(input, target)</code>","text":"<p>Computes the Dice score relative to the single input and target tensors provided.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>predictions tensor.</p> required <code>target</code> <code>Tensor</code> <p>target tensor.</p> required <p>Returns:</p> Type Description <code>Scalar</code> <p>Mean dice score for the provided tensors.</p> Source code in <code>fl4health/metrics/efficient_metrics.py</code> <pre><code>def __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n    \"\"\"\n    Computes the Dice score relative to the single input and target tensors provided.\n\n    Args:\n        input (torch.Tensor): predictions tensor.\n        target (torch.Tensor): target tensor.\n\n    Returns:\n        (Scalar): Mean dice score for the provided tensors.\n    \"\"\"\n    true_positives, false_positives, true_negatives, false_negatives = self.count_tp_fp_tn_fn(input, target)\n    dice_metric = self.compute_from_counts(true_positives, false_positives, true_negatives, false_negatives)\n    # Extract the scalar from the dictionary.\n    return dice_metric[self.name]\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics.BinaryDice","title":"<code>BinaryDice</code>","text":"<p>               Bases: <code>BinaryClassificationMetric</code></p> Source code in <code>fl4health/metrics/efficient_metrics.py</code> <pre><code>class BinaryDice(BinaryClassificationMetric):\n    def __init__(\n        self,\n        batch_dim: int | None,\n        name: str = \"BinaryDice\",\n        label_dim: int | None = None,\n        dtype: torch.dtype = torch.float32,\n        pos_label: int = 1,\n        threshold: float | int | None = None,\n        zero_division: float | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Computes the Dice Coefficient between binary predictions and targets. These can be vector encoded or\n        just single elements values with an implicit positive class. That is, predictions might be vectorized\n        where a single predictions is a 2D vector ``[0.2, 0.8]`` or a float 0.8 (with the complement implied).\n\n        Regardless of how the input is structured, the provided score will be provided with respect to the value of the\n        ``pos_label`` variable, which defaults to 1 (and can only have values {0, 1}). That is, the reported score\n        will correspond to the score from the perspective of the specified label. For additional documentation see\n        that of the parent class ``BinaryClassificationMetric`` and the function ``_post_process_count_tensor``\n        therein.\n\n        **NOTE**: For this class, the predictions and targets passed to the update function **MUST** have the same\n        shape.\n\n        **NOTE**: The default behavior for Dice Scores is to compute the mean over each **SAMPLE** of the dataset being\n        measured. In the image domain, for example, this means that the Dice score is computed for each image\n        separately and then averaged across images (then classes) to produce a single score. This is accomplished\n        by specifying the ``batch_dim`` here. If, however, you would like to compute the Dice score over ALL TP, FP,\n        FNs across all samples (then classes) as a single count, ``batch_dim = None`` is appropriate.\n\n        **NOTE**: Preds and targets are expected to have elements in the interval [0, 1] or to be thresholded, using\n        the argument of this class to be as such.\n\n        Args:\n            batch_dim (int | None, optional): If None, then counts are aggregated across the batch dimension. If\n                specified, counts will be computed along the dimension specified. That is, counts are maintained for\n                each training sample **INDIVIDUALLY**. For example, if ``batch_dim = 1`` and ``label_dim = 0``, then\n\n                ```python\n                predictions = torch.tensor([[[0, 0, 0, 1], [1, 1, 1, 1]]])  # shape (1, 2, 4)\n\n                targets = torch.tensor([[[0, 0, 1, 0], [1, 1, 1, 1]]])  # shape (1, 2, 4)\n\n                self.true_positives = torch.Tensor([[0], [4]])\n\n                self.true_negatives = torch.Tensor([[2], [0]])\n\n                self.false_positives = torch.Tensor([[1], [0]])\n\n                self.false_negatives = torch.Tensor([[1], [0]])\n                ```\n\n                In computing the Dice score, we get scores for each sample ``[[2*0/(2*0 +1+1)]``,\n                ``[2*4/(2*4+0+0)]]``. These are then averaged to get 0.5.\n            name (str): Name of the metric. Defaults to 'BinaryDice'\n            label_dim (int | None, optional): Specifies which dimension in the provided tensors corresponds to the\n                label dimension. During metric computation, this dimension must have size of **AT MOST 2**. If left as\n                None, this class will assume that each entry in the tensor corresponds to a prediction/target, with\n                the positive class indicated by predictions of 1. Defaults to None.\n            dtype (torch.dtype): The dtype to store the counts as. If preds or targets can be continuous, specify a\n                float type. Otherwise specify an integer type to prevent overflow. Defaults to ``torch.float32``\n            pos_label (int, optional): The label relative to which to report the Dice. Must be either 0 or 1.\n                Defaults to 1.\n            threshold (float | int | None, optional): A float for thresholding values or an integer specifying the\n                index of the label dimension. If a float is given, predictions below the threshold are mapped\n                to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class\n                with the highest prediction where the specified axis is assumed to contain a prediction for each class\n                (where its index along that dimension is the class label). Value of None leaves preds unchanged.\n                Defaults to None.\n            zero_division (float | None, optional): Set what the individual dice coefficients should be when there is\n                a zero division (only true negatives present). If None, these examples will be dropped. If all\n                components are only TNs, then NaN will be returned.\n        \"\"\"\n        # The right set of counts that can be ignored for Dice computation depends on which label relative to which\n        # we're reporting the score. If reporting relative to the positive label, then we need not track\n        # True Negatives, as they don't factor into the standard Dice score. On the other hand, if reporting relative\n        # to the negative class, we need not keep True Positives around, for the same reason.\n        discard = {ClassificationOutcome.TRUE_NEGATIVE} if pos_label == 1 else {ClassificationOutcome.TRUE_POSITIVE}\n        super().__init__(\n            name=name,\n            batch_dim=batch_dim,\n            label_dim=label_dim,\n            dtype=dtype,\n            threshold=threshold,\n            pos_label=pos_label,\n            discard=discard,\n        )\n        self.zero_division = zero_division\n\n    def compute_from_counts(\n        self,\n        true_positives: torch.Tensor,\n        false_positives: torch.Tensor,\n        true_negatives: torch.Tensor,\n        false_negatives: torch.Tensor,\n    ) -&gt; Metrics:\n        \"\"\"\n        Computes a binary Dice score associated with the outcome counts provided in the form of true positives (TP),\n        false positives (FP), and false negatives (FN). Because Dice scores don't factor in true negatives, this\n        argument is unused. For a set of counts, the binary Dice score is...\n\n        \\\\[\n\n\n        \\\\]\n            \\\\frac{2 \\\\cdot TP}{2 \\\\cdot TP + FP + FN}.\n\n        For this class it is assumed that all counts are presented relative to the class indicated by the\n        ``pos_label`` index. Moreover, they are assumed to either have a single entry or have shape\n        ``(num_samples, 1)``. In the former, a single Dice score is computed relative to the counts. In the latter, a\n        **MEAN** dice score over the samples is computed. The second setting is useful, for example, if you are\n        computing the Dice score per image and then averaging. The first setting is useful, for example, if you want\n        to treat all examples as a **SINGLE** image.\n\n        Args:\n            true_positives (torch.Tensor): Counts associated with positive predictions and positive labels.\n            false_positives (torch.Tensor): Counts associated with positive predictions and negative labels.\n            true_negatives (torch.Tensor): Counts associated with negative predictions and negative labels.\n            false_negatives (torch.Tensor): Counts associated with negative predictions and positive labels.\n\n        Returns:\n            (Metrics): A mean dice score associated with the counts.\n        \"\"\"\n        # compute dice coefficients and return mean\n        dice = compute_dice_on_count_tensors(true_positives, false_positives, false_negatives, self.zero_division)\n        if dice.numel() == 0:\n            log(WARNING, \"Currently, Dice score is undefined due to only true negatives present\")\n        return {self.name: torch.mean(dice).item()}\n\n    def __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        \"\"\"\n        Computes the Dice score relative to the single input and target tensors provided.\n\n        Args:\n            input (torch.Tensor): predictions tensor.\n            target (torch.Tensor): target tensor.\n\n        Returns:\n            (Scalar): Mean dice score for the provided tensors.\n        \"\"\"\n        true_positives, false_positives, true_negatives, false_negatives = self.count_tp_fp_tn_fn(input, target)\n        dice_metric = self.compute_from_counts(true_positives, false_positives, true_negatives, false_negatives)\n        # Extract the scalar from the dictionary.\n        return dice_metric[self.name]\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics.BinaryDice.__init__","title":"<code>__init__(batch_dim, name='BinaryDice', label_dim=None, dtype=torch.float32, pos_label=1, threshold=None, zero_division=None)</code>","text":"<p>Computes the Dice Coefficient between binary predictions and targets. These can be vector encoded or just single elements values with an implicit positive class. That is, predictions might be vectorized where a single predictions is a 2D vector <code>[0.2, 0.8]</code> or a float 0.8 (with the complement implied).</p> <p>Regardless of how the input is structured, the provided score will be provided with respect to the value of the <code>pos_label</code> variable, which defaults to 1 (and can only have values {0, 1}). That is, the reported score will correspond to the score from the perspective of the specified label. For additional documentation see that of the parent class <code>BinaryClassificationMetric</code> and the function <code>_post_process_count_tensor</code> therein.</p> <p>NOTE: For this class, the predictions and targets passed to the update function MUST have the same shape.</p> <p>NOTE: The default behavior for Dice Scores is to compute the mean over each SAMPLE of the dataset being measured. In the image domain, for example, this means that the Dice score is computed for each image separately and then averaged across images (then classes) to produce a single score. This is accomplished by specifying the <code>batch_dim</code> here. If, however, you would like to compute the Dice score over ALL TP, FP, FNs across all samples (then classes) as a single count, <code>batch_dim = None</code> is appropriate.</p> <p>NOTE: Preds and targets are expected to have elements in the interval [0, 1] or to be thresholded, using the argument of this class to be as such.</p> <p>Parameters:</p> Name Type Description Default <code>batch_dim</code> <code>int | None</code> <p>If None, then counts are aggregated across the batch dimension. If specified, counts will be computed along the dimension specified. That is, counts are maintained for each training sample INDIVIDUALLY. For example, if <code>batch_dim = 1</code> and <code>label_dim = 0</code>, then</p> <pre><code>predictions = torch.tensor([[[0, 0, 0, 1], [1, 1, 1, 1]]])  # shape (1, 2, 4)\n\ntargets = torch.tensor([[[0, 0, 1, 0], [1, 1, 1, 1]]])  # shape (1, 2, 4)\n\nself.true_positives = torch.Tensor([[0], [4]])\n\nself.true_negatives = torch.Tensor([[2], [0]])\n\nself.false_positives = torch.Tensor([[1], [0]])\n\nself.false_negatives = torch.Tensor([[1], [0]])\n</code></pre> <p>In computing the Dice score, we get scores for each sample <code>[[2*0/(2*0 +1+1)]</code>, <code>[2*4/(2*4+0+0)]]</code>. These are then averaged to get 0.5.</p> required <code>name</code> <code>str</code> <p>Name of the metric. Defaults to 'BinaryDice'</p> <code>'BinaryDice'</code> <code>label_dim</code> <code>int | None</code> <p>Specifies which dimension in the provided tensors corresponds to the label dimension. During metric computation, this dimension must have size of AT MOST 2. If left as None, this class will assume that each entry in the tensor corresponds to a prediction/target, with the positive class indicated by predictions of 1. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The dtype to store the counts as. If preds or targets can be continuous, specify a float type. Otherwise specify an integer type to prevent overflow. Defaults to <code>torch.float32</code></p> <code>float32</code> <code>pos_label</code> <code>int</code> <p>The label relative to which to report the Dice. Must be either 0 or 1. Defaults to 1.</p> <code>1</code> <code>threshold</code> <code>float | int | None</code> <p>A float for thresholding values or an integer specifying the index of the label dimension. If a float is given, predictions below the threshold are mapped to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class with the highest prediction where the specified axis is assumed to contain a prediction for each class (where its index along that dimension is the class label). Value of None leaves preds unchanged. Defaults to None.</p> <code>None</code> <code>zero_division</code> <code>float | None</code> <p>Set what the individual dice coefficients should be when there is a zero division (only true negatives present). If None, these examples will be dropped. If all components are only TNs, then NaN will be returned.</p> <code>None</code> Source code in <code>fl4health/metrics/efficient_metrics.py</code> <pre><code>def __init__(\n    self,\n    batch_dim: int | None,\n    name: str = \"BinaryDice\",\n    label_dim: int | None = None,\n    dtype: torch.dtype = torch.float32,\n    pos_label: int = 1,\n    threshold: float | int | None = None,\n    zero_division: float | None = None,\n) -&gt; None:\n    \"\"\"\n    Computes the Dice Coefficient between binary predictions and targets. These can be vector encoded or\n    just single elements values with an implicit positive class. That is, predictions might be vectorized\n    where a single predictions is a 2D vector ``[0.2, 0.8]`` or a float 0.8 (with the complement implied).\n\n    Regardless of how the input is structured, the provided score will be provided with respect to the value of the\n    ``pos_label`` variable, which defaults to 1 (and can only have values {0, 1}). That is, the reported score\n    will correspond to the score from the perspective of the specified label. For additional documentation see\n    that of the parent class ``BinaryClassificationMetric`` and the function ``_post_process_count_tensor``\n    therein.\n\n    **NOTE**: For this class, the predictions and targets passed to the update function **MUST** have the same\n    shape.\n\n    **NOTE**: The default behavior for Dice Scores is to compute the mean over each **SAMPLE** of the dataset being\n    measured. In the image domain, for example, this means that the Dice score is computed for each image\n    separately and then averaged across images (then classes) to produce a single score. This is accomplished\n    by specifying the ``batch_dim`` here. If, however, you would like to compute the Dice score over ALL TP, FP,\n    FNs across all samples (then classes) as a single count, ``batch_dim = None`` is appropriate.\n\n    **NOTE**: Preds and targets are expected to have elements in the interval [0, 1] or to be thresholded, using\n    the argument of this class to be as such.\n\n    Args:\n        batch_dim (int | None, optional): If None, then counts are aggregated across the batch dimension. If\n            specified, counts will be computed along the dimension specified. That is, counts are maintained for\n            each training sample **INDIVIDUALLY**. For example, if ``batch_dim = 1`` and ``label_dim = 0``, then\n\n            ```python\n            predictions = torch.tensor([[[0, 0, 0, 1], [1, 1, 1, 1]]])  # shape (1, 2, 4)\n\n            targets = torch.tensor([[[0, 0, 1, 0], [1, 1, 1, 1]]])  # shape (1, 2, 4)\n\n            self.true_positives = torch.Tensor([[0], [4]])\n\n            self.true_negatives = torch.Tensor([[2], [0]])\n\n            self.false_positives = torch.Tensor([[1], [0]])\n\n            self.false_negatives = torch.Tensor([[1], [0]])\n            ```\n\n            In computing the Dice score, we get scores for each sample ``[[2*0/(2*0 +1+1)]``,\n            ``[2*4/(2*4+0+0)]]``. These are then averaged to get 0.5.\n        name (str): Name of the metric. Defaults to 'BinaryDice'\n        label_dim (int | None, optional): Specifies which dimension in the provided tensors corresponds to the\n            label dimension. During metric computation, this dimension must have size of **AT MOST 2**. If left as\n            None, this class will assume that each entry in the tensor corresponds to a prediction/target, with\n            the positive class indicated by predictions of 1. Defaults to None.\n        dtype (torch.dtype): The dtype to store the counts as. If preds or targets can be continuous, specify a\n            float type. Otherwise specify an integer type to prevent overflow. Defaults to ``torch.float32``\n        pos_label (int, optional): The label relative to which to report the Dice. Must be either 0 or 1.\n            Defaults to 1.\n        threshold (float | int | None, optional): A float for thresholding values or an integer specifying the\n            index of the label dimension. If a float is given, predictions below the threshold are mapped\n            to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class\n            with the highest prediction where the specified axis is assumed to contain a prediction for each class\n            (where its index along that dimension is the class label). Value of None leaves preds unchanged.\n            Defaults to None.\n        zero_division (float | None, optional): Set what the individual dice coefficients should be when there is\n            a zero division (only true negatives present). If None, these examples will be dropped. If all\n            components are only TNs, then NaN will be returned.\n    \"\"\"\n    # The right set of counts that can be ignored for Dice computation depends on which label relative to which\n    # we're reporting the score. If reporting relative to the positive label, then we need not track\n    # True Negatives, as they don't factor into the standard Dice score. On the other hand, if reporting relative\n    # to the negative class, we need not keep True Positives around, for the same reason.\n    discard = {ClassificationOutcome.TRUE_NEGATIVE} if pos_label == 1 else {ClassificationOutcome.TRUE_POSITIVE}\n    super().__init__(\n        name=name,\n        batch_dim=batch_dim,\n        label_dim=label_dim,\n        dtype=dtype,\n        threshold=threshold,\n        pos_label=pos_label,\n        discard=discard,\n    )\n    self.zero_division = zero_division\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics.BinaryDice.compute_from_counts","title":"<code>compute_from_counts(true_positives, false_positives, true_negatives, false_negatives)</code>","text":"<p>Computes a binary Dice score associated with the outcome counts provided in the form of true positives (TP), false positives (FP), and false negatives (FN). Because Dice scores don't factor in true negatives, this argument is unused. For a set of counts, the binary Dice score is...</p> <p>[</p> <p>]     \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}.</p> <p>For this class it is assumed that all counts are presented relative to the class indicated by the <code>pos_label</code> index. Moreover, they are assumed to either have a single entry or have shape <code>(num_samples, 1)</code>. In the former, a single Dice score is computed relative to the counts. In the latter, a MEAN dice score over the samples is computed. The second setting is useful, for example, if you are computing the Dice score per image and then averaging. The first setting is useful, for example, if you want to treat all examples as a SINGLE image.</p> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>Tensor</code> <p>Counts associated with positive predictions and positive labels.</p> required <code>false_positives</code> <code>Tensor</code> <p>Counts associated with positive predictions and negative labels.</p> required <code>true_negatives</code> <code>Tensor</code> <p>Counts associated with negative predictions and negative labels.</p> required <code>false_negatives</code> <code>Tensor</code> <p>Counts associated with negative predictions and positive labels.</p> required <p>Returns:</p> Type Description <code>Metrics</code> <p>A mean dice score associated with the counts.</p> Source code in <code>fl4health/metrics/efficient_metrics.py</code> <pre><code>def compute_from_counts(\n    self,\n    true_positives: torch.Tensor,\n    false_positives: torch.Tensor,\n    true_negatives: torch.Tensor,\n    false_negatives: torch.Tensor,\n) -&gt; Metrics:\n    \"\"\"\n    Computes a binary Dice score associated with the outcome counts provided in the form of true positives (TP),\n    false positives (FP), and false negatives (FN). Because Dice scores don't factor in true negatives, this\n    argument is unused. For a set of counts, the binary Dice score is...\n\n    \\\\[\n\n\n    \\\\]\n        \\\\frac{2 \\\\cdot TP}{2 \\\\cdot TP + FP + FN}.\n\n    For this class it is assumed that all counts are presented relative to the class indicated by the\n    ``pos_label`` index. Moreover, they are assumed to either have a single entry or have shape\n    ``(num_samples, 1)``. In the former, a single Dice score is computed relative to the counts. In the latter, a\n    **MEAN** dice score over the samples is computed. The second setting is useful, for example, if you are\n    computing the Dice score per image and then averaging. The first setting is useful, for example, if you want\n    to treat all examples as a **SINGLE** image.\n\n    Args:\n        true_positives (torch.Tensor): Counts associated with positive predictions and positive labels.\n        false_positives (torch.Tensor): Counts associated with positive predictions and negative labels.\n        true_negatives (torch.Tensor): Counts associated with negative predictions and negative labels.\n        false_negatives (torch.Tensor): Counts associated with negative predictions and positive labels.\n\n    Returns:\n        (Metrics): A mean dice score associated with the counts.\n    \"\"\"\n    # compute dice coefficients and return mean\n    dice = compute_dice_on_count_tensors(true_positives, false_positives, false_negatives, self.zero_division)\n    if dice.numel() == 0:\n        log(WARNING, \"Currently, Dice score is undefined due to only true negatives present\")\n    return {self.name: torch.mean(dice).item()}\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics.BinaryDice.__call__","title":"<code>__call__(input, target)</code>","text":"<p>Computes the Dice score relative to the single input and target tensors provided.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>predictions tensor.</p> required <code>target</code> <code>Tensor</code> <p>target tensor.</p> required <p>Returns:</p> Type Description <code>Scalar</code> <p>Mean dice score for the provided tensors.</p> Source code in <code>fl4health/metrics/efficient_metrics.py</code> <pre><code>def __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n    \"\"\"\n    Computes the Dice score relative to the single input and target tensors provided.\n\n    Args:\n        input (torch.Tensor): predictions tensor.\n        target (torch.Tensor): target tensor.\n\n    Returns:\n        (Scalar): Mean dice score for the provided tensors.\n    \"\"\"\n    true_positives, false_positives, true_negatives, false_negatives = self.count_tp_fp_tn_fn(input, target)\n    dice_metric = self.compute_from_counts(true_positives, false_positives, true_negatives, false_negatives)\n    # Extract the scalar from the dictionary.\n    return dice_metric[self.name]\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base","title":"<code>efficient_metrics_base</code>","text":""},{"location":"api/#fl4health.metrics.efficient_metrics_base.ClassificationMetric","title":"<code>ClassificationMetric</code>","text":"<p>               Bases: <code>Metric</code>, <code>ABC</code></p> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>class ClassificationMetric(Metric, ABC):\n    def __init__(\n        self,\n        name: str,\n        label_dim: int | None,\n        batch_dim: int | None,\n        dtype: torch.dtype,\n        threshold: float | int | None,\n        discard: set[ClassificationOutcome] | None,\n    ) -&gt; None:\n        \"\"\"\n        A Base class for efficiently computing classification metrics that can be calculated using the true positives\n        (tp), false positive (fp), false negative (fn) and true negative (tn) counts.\n\n        How these values are counted is left to the inheriting class along with how they are composed together for the\n        final metric score. There are two classes inheriting from this class to form the basis of efficient\n        classification metrics: ``BinaryClassificationMetric`` and ``MultiClassificationMetric``. These handle\n        implementation of the ``count_tp_fp_tn_fn`` method.\n\n        On each update, the ``true_positives``, ``false_positives``, ``false_negatives`` and ``true_negatives`` counts\n        for the provided predictions and targets are accumulated into ``self.true_positives``,\n        ``self.false_positives``, ``self.false_negatives`` and ``self.true_negatives``, respectively, and reduced\n        along all unspecified dimensions. This reduces the memory footprint required to compute metrics across rounds.\n        The user needs to define the ``compute_from_counts`` method which returns a dictionary of ``Scalar`` metrics\n        given the ``true_positives``, ``false_positives``, ``false_negatives``, and  ``true_negatives`` counts. The\n        accumulated counts are reset by the ``clear`` method. If your subclass returns multiple metrics you may need\n        to also override the ``__call__`` method.\n\n        If the predictions provided are continuous in value, then the associated counts will also be continuous\n        (\"soft\"). For example, with a target of 1, a prediction of 0.8 contributes 0.8 to the ``true_positives`` count\n        and 0.2 to the ``false_negatives``.\n\n        **NOTE**: Preds and targets are expected to have elements in the interval ``[0, 1]`` or to be thresholded,\n        using the argument of this class to be as such.\n\n        Args:\n            name (str): The name of the metric.\n            label_dim (int | None, optional): Specifies which dimension in the provided tensors corresponds to the\n                label dimension. If None, the counts along the specified dimension (i.e. for each output label) are\n                aggregated and the label dimension is reduced. If specified, counts will be computed along the\n                specified dimensions. That is, counts are maintained for each output label **INDIVIDUALLY**.\n\n                **NOTE**: If both ``label_dim`` and ``batch_dim`` are specified, then counts will be presented batch\n                dimension first, then label dimension. If neither are specified, each count is a global scalar.\n            batch_dim (int | None, optional): If None, the counts along the specified dimension (i.e. for each sample)\n                are aggregated and the batch dimension is reduced. If specified, counts will be computed along the\n                dimension specified. That is, counts are maintained for each training sample **INDIVIDUALLY**.\n\n                **NOTE**: If both ```label_dim``` and `batch_dim` are specified, then counts will be presented batch\n                dimension first, then label dimension. If neither are specified, each count is a global scalar.\n            dtype (torch.dtype): The dtype to store the counts as. If preds or targets can be continuous, specify a\n                float type. Otherwise specify an integer type to prevent overflow.\n            threshold (float | int | None, optional): A float for thresholding values or an integer specifying the\n                index of the label dimension. If a float is given, predictions below the threshold are mapped\n                to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class\n                with the highest prediction where the specified axis is assumed to contain a prediction for each class\n                (where its index along that dimension is the class label). Setting to None leaves preds unchanged.\n            discard (set[ClassificationOutcome] | None, optional): One or several of ``ClassificationOutcome`` values.\n                Specified counts will not be accumulated. Their associated attribute will remain as an empty pytorch\n                tensor. Useful for reducing the memory footprint of metrics that do not use all of the counts in their\n                computation.\n        \"\"\"\n        self.name = name\n        self.dtype = dtype\n        self.threshold = threshold\n        self.label_dim = label_dim\n        self.batch_dim = batch_dim\n\n        if self.label_dim is not None:\n            if isinstance(self.threshold, int) and self.threshold != self.label_dim:\n                log(\n                    WARNING,\n                    f\"Specified threshold dimension: {threshold} is not the same as the label_dim: \"\n                    f\"{label_dim}. This is atypical and may produce undesired behavior\",\n                )\n            if self.batch_dim is not None and self.label_dim == self.batch_dim:\n                raise ValueError(f\"The label and batch dimensions must differ but got {self.label_dim}\")\n\n        # Parse discard argument\n        discard = set() if discard is None else discard\n\n        self.discard_tp = ClassificationOutcome.TRUE_POSITIVE in discard\n        self.discard_fp = ClassificationOutcome.FALSE_POSITIVE in discard\n        self.discard_fn = ClassificationOutcome.FALSE_NEGATIVE in discard\n        self.discard_tn = ClassificationOutcome.TRUE_NEGATIVE in discard\n\n        # Create intermediate tensors. Will be initialized with tensors of correct shape on first update.\n        self.counts_initialized = False\n        self.true_positives, self.false_positives, self.false_negatives, self.true_negatives = (\n            torch.tensor([]),\n            torch.tensor([]),\n            torch.tensor([]),\n            torch.tensor([]),\n        )\n\n    def _transform_tensors(self, preds: torch.Tensor, targets: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Given the predictions and targets this function performs two possible transformations. The first is to map\n        boolean tensors to integers for computation. The second is to potentially threshold the predictions if\n        ``self.threshold`` is not None.\n\n        **NOTE**: This is a common implementation meant to be called (or overridden) by inheriting classes.\n\n        Args:\n            preds (torch.Tensor): Predictions tensor.\n            targets (torch.Tensor): Targets tensor.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Potentially transformed predictions and targets tensors, in that\n                order.\n        \"\"\"\n        # On the off chance were given booleans convert them to integers\n        preds = preds.to(torch.uint8) if preds.dtype == torch.bool else preds\n        targets = targets.to(torch.uint8) if targets.dtype == torch.bool else targets\n\n        # Maybe threshold predictions into 'hard' predictions.\n        preds = preds if self.threshold is None else threshold_tensor(preds, self.threshold)\n        return preds, targets\n\n    def _assert_correct_ranges_and_shape(self, preds: torch.Tensor, targets: torch.Tensor) -&gt; None:\n        \"\"\"\n        Ensures that the prediction and target tensors are in the expected form for computation.\n\n        **NOTE**: This is a common implementation meant to be called (or overridden) by inheriting classes.\n\n        Args:\n            preds (torch.Tensor): Predictions tensor.\n            targets (torch.Tensor): Targets tensor.\n        \"\"\"\n        assert torch.min(preds) &gt;= 0 and torch.max(preds) &lt;= 1, \"Expected preds to be in range [0, 1].\"\n        assert torch.min(targets) &gt;= 0 and torch.max(targets) &lt;= 1, \"Expected targets to be in range [0, 1].\"\n\n    def _prepare_counts_from_preds_and_targets(\n        self, preds: torch.Tensor, targets: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Compute counts. If were ignoring a count, set it as an empty tensor to avoid downstream errors.\n\n        **NOTE**: preds and targets are assumed to be in range ``[0, 1]``. Otherwise the computations below may produce\n        unexpected results.\n\n        Args:\n            preds (torch.Tensor): Predictions tensor.\n            targets (torch.Tensor): Targets tensor.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]): True positive, false positive,\n                true negative, and false negative indications for each of the predictions in the provided tensors.\n        \"\"\"\n        true_positives = (preds * targets) if not self.discard_tp else torch.tensor([])\n        false_positives = (preds * (1 - targets)) if not self.discard_fp else torch.tensor([])\n        false_negatives = ((1 - preds) * targets) if not self.discard_fn else torch.tensor([])\n        true_negatives = ((1 - preds) * (1 - targets)) if not self.discard_tn else torch.tensor([])\n        return true_positives, false_positives, true_negatives, false_negatives\n\n    def _sum_along_axes_or_discard(\n        self,\n        sum_axes: tuple[int, ...],\n        true_positives: torch.Tensor,\n        false_positives: torch.Tensor,\n        true_negatives: torch.Tensor,\n        false_negatives: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Provided a set of axes over which to reduce by summation. These sums are applied to each of the\n        provided tensors.\n\n        Args:\n            sum_axes (tuple[int, ...]): The dimension or dimensions to reduce. If empty, all dimensions are reduced.\n            true_positives (torch.Tensor): Tensor with entry of 1 indicating a true positive for a pred/target pair.\n            false_positives (torch.Tensor): Tensor with entry of 1 indicating a false positive for a pred/target pair.\n            true_negatives (torch.Tensor): Tensor with entry of 1 indicating a true negative for a pred/target pair.\n            false_negatives (torch.Tensor): Tensor with entry of 1 indicating a false negative for a pred/target pair.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]): Tensors reduced over the specified axes\n                in the order ``true_positives``, ``false_positives``, ``true_negatives``, ``false_negatives``.\n        \"\"\"\n        true_positives = true_positives.sum(sum_axes, dtype=self.dtype) if not self.discard_tp else true_positives\n        false_positives = false_positives.sum(sum_axes, dtype=self.dtype) if not self.discard_fp else false_positives\n        false_negatives = false_negatives.sum(sum_axes, dtype=self.dtype) if not self.discard_fn else false_negatives\n        true_negatives = true_negatives.sum(sum_axes, dtype=self.dtype) if not self.discard_tn else true_negatives\n        return true_positives, false_positives, true_negatives, false_negatives\n\n    def _maybe_transpose_count_tensor(self, count_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Ensures that the batch dimension always appears first in the count tensor.\n\n        If the count tensor does not have two dimensions, or the batch dimension is already in front of the label\n        dimension, then it is returned unchanged. Otherwise it is transposed.\n\n        Args:\n            count_tensor (torch.Tensor): The count tensor after reducing dimensions.\n\n        Returns:\n            (torch.Tensor): If ``count_tensor`` was 1D then it is returned unchanged. Otherwise this is the\n                ``count_tensor`` with shape (``batch_size``, ``num_labels``).\n        \"\"\"\n        if count_tensor.ndim != MAX_COUNT_TENSOR_DIMS:\n            return count_tensor\n\n        assert self.batch_dim is not None and self.label_dim is not None, (\n            \"Was expecting batch_dim and label_dim to both be defined if count_tensor has 2 unreduced dimensions.\"\n        )\n\n        if self.batch_dim &gt; self.label_dim:\n            return count_tensor.transpose(0, 1)\n\n        return count_tensor\n\n    def update(self, preds: torch.Tensor, targets: torch.Tensor) -&gt; None:\n        \"\"\"\n        Updates the existing ``self.true_positive``, ``self.false_positive``, ``self.false_negative`` and\n        ``self.true_negative`` counts with new counts computed from preds and targets.\n\n        **NOTE**: This function assumes that if ``self.batch_dim`` is not ``None``, the counts are returned with\n        shapes such that the batch dimension comes **FIRST** for the counts. If ``self.count_tp_fp_tn_fn`` is\n        overridden it must ensure that this remains the case.\n\n        Args:\n            preds (torch.Tensor): Predictions tensor.\n            targets (torch.Tensor): Targets tensor.\n        \"\"\"\n        # Get tp, fp, tn and fn counts for current update. If a count is discarded, then an empty tensor is returned.\n        tp, fp, tn, fn = self.count_tp_fp_tn_fn(preds, targets)\n\n        # If this is first update since init or clear, initialize counts attributes and exit function\n        if not self.counts_initialized:\n            self.true_positives, self.false_positives, self.true_negatives, self.false_negatives = tp, fp, tn, fn\n            self.counts_initialized = True\n            return\n\n        # If batch_dim has been specified, we accumulate counts for EACH INSTANCE seen throughout the updates, such\n        # that each of the counts is a tensor of row length equal to the samples seen.\n        # NOTE: This ASSUMES the batch dimension comes FIRST for the counts.\n        # Otherwise, the counts are 1D tensors with length equal to the number of classes (or possibly 1 if using the\n        # BinaryClassificationMetric)\n        self.true_positives = (\n            torch.cat([self.true_positives, tp], dim=0) if self.batch_dim is not None else self.true_positives + tp\n        )\n        self.false_positives = (\n            torch.cat([self.false_positives, fp], dim=0) if self.batch_dim is not None else self.false_positives + fp\n        )\n        self.true_negatives = (\n            torch.cat([self.true_negatives, tn], dim=0) if self.batch_dim is not None else self.true_negatives + tn\n        )\n        self.false_negatives = (\n            torch.cat([self.false_negatives, fn], dim=0) if self.batch_dim is not None else self.false_negatives + fn\n        )\n\n    def clear(self) -&gt; None:\n        \"\"\"Reset accumulated tp, fp, fn and tn's. They will be initialized with correct shape on next update.\"\"\"\n        self.true_positives = torch.tensor([])\n        self.false_positives = torch.tensor([])\n        self.false_negatives = torch.tensor([])\n        self.true_negatives = torch.tensor([])\n        self.counts_initialized = False\n\n    def compute(self, name: str | None = None) -&gt; Metrics:\n        \"\"\"\n        Computes the metrics from the currently saved counts using the ``compute_from_counts`` function defined in\n        inheriting classes.\n\n        Args:\n            name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n                dictionary.\n\n        Returns:\n            (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n        \"\"\"\n        metrics = self.compute_from_counts(\n            true_positives=self.true_positives,\n            false_positives=self.false_positives,\n            true_negatives=self.true_negatives,\n            false_negatives=self.false_negatives,\n        )\n        if name is not None:\n            metrics = {f\"{name} - {k}\": v for k, v in metrics.items()}\n        return metrics\n\n    def count_tp_fp_tn_fn(\n        self, preds: torch.Tensor, targets: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Given two tensors containing model predictions and targets, returns the number of true positives (tp), false\n        positives (fp), true negatives (tn), and false negatives (fn).\n\n        The shape of these counts depends on if ``self.batch_dim`` and ``self.label_dim`` are specified and the\n        implementation of the inheriting class. If the batch dimension appears after the label dimension in the input\n        tensors this method will transpose the count tensors to ensure the batch dimension comes first.\n\n        If any of the true positives, false positives, true negative, or false negative counts were specified to be\n        discarded during initialization of the class, then that count will not be computed and an empty tensor will be\n        returned in its place.\n\n        **NOTE**: Inheriting classes may implement additional functionality on top of this class. For example, any\n        preprocessing that needs to be done to preds and targets should be done in the inheriting function. Any post\n        processing should also be done there. See implementations in the ``BinaryClassificationMetric`` or\n        ``MultiClassificationMetric`` class for examples.\n\n        Args:\n            preds (torch.Tensor): Tensor containing model predictions. Must be the same shape as targets\n            targets (torch.Tensor): Tensor containing prediction targets. Must be same shape as preds.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]): Tensors containing the counts along the\n                specified dimensions for each of true positives, false positives, true negatives, and false negatives\n                respectively. The output shape of these tensors depends on if ``self.batch_dim`` and ``self.label_dim``\n                are specified. The batch dimension, if it exists in the output, will always come first. For example,\n                if the batch and label dimensions have sizes 2 and 3 respectively:\n\n                - Both ``batch_dim`` and ``label_dim`` are specified: ``Size([2, 3])``\n                - Only ``batch_dim`` is specified: ``Size([2])``\n                - Only ``label_dim`` is specified: ``Size([3])``\n                - Neither specified: ``Size([])``\n        \"\"\"\n        # Transform predictions and targets to get them ready for computation\n        preds, targets = self._transform_tensors(preds, targets)\n\n        # Assert that we are ready for computation\n        self._assert_correct_ranges_and_shape(preds, targets)\n\n        # Compute counts. If we're ignoring a count, set it as an empty tensor to avoid downstream errors.\n        true_positives, false_positives, true_negatives, false_negatives = self._prepare_counts_from_preds_and_targets(\n            preds, targets\n        )\n\n        axes_to_ignore: set[int] = {self.label_dim} if self.label_dim is not None else set()\n        if self.batch_dim is not None:\n            axes_to_ignore.add(self.batch_dim)\n        sum_axes = tuple([i for i in range(preds.ndim) if i not in axes_to_ignore])\n\n        # If sum_axes is empty, then we have nothing to do.\n        if len(sum_axes) != 0:\n            true_positives, false_positives, true_negatives, false_negatives = self._sum_along_axes_or_discard(\n                sum_axes, true_positives, false_positives, true_negatives, false_negatives\n            )\n\n        # Ensure that batch dimension appears first if count tensors are 2D\n        true_positives = self._maybe_transpose_count_tensor(true_positives)\n        false_positives = self._maybe_transpose_count_tensor(false_positives)\n        true_negatives = self._maybe_transpose_count_tensor(true_negatives)\n        false_negatives = self._maybe_transpose_count_tensor(false_negatives)\n\n        return true_positives, false_positives, true_negatives, false_negatives\n\n    @abstractmethod\n    def compute_from_counts(\n        self,\n        true_positives: torch.Tensor,\n        false_positives: torch.Tensor,\n        true_negatives: torch.Tensor,\n        false_negatives: torch.Tensor,\n    ) -&gt; Metrics:\n        \"\"\"\n        Provided tensors associated with the various classification outcomes from predictions compared to targets in\n        the form of true positives, false positives, true negatives, and false negatives, returns a dictionary of\n        ``Scalar`` metrics. For example, one might compute recall as ``true_positives/(true_positives +\n        false_negatives)``.\n\n        The count tensors will all have the same shape. This shape depends on whether ``batch_dim`` and or\n        ``label_dim`` were specified. For example, if the batch and label dimensions have sizes 2 and 3 respectively,\n        then:\n\n        - Both ``batch_dim`` and ``label_dim`` are specified: ``Size([2, 3])``\n        - Only ``batch_dim`` is specified: ``Size([2])``\n        - Only ``label_dim`` is specified: ``Size([3])``\n        - Neither specified: ``Size([])``\n\n        Inheriting classes may further modify the shapes of the count tensors that are provided as arguments depending\n        on the kind of classification being done (eg. multi-class vs. binary).\n\n        Args:\n            true_positives (torch.Tensor): Counts associated with positive predictions of a class and true positives\n                for that class.\n            false_positives (torch.Tensor): Counts associated with positive predictions of a class and true negatives\n                for that class.\n            true_negatives (torch.Tensor): Counts associated with negative predictions of a class and true negatives\n                for that class.\n            false_negatives (torch.Tensor): Counts associated with negative predictions of a class and true positives\n                for that class.\n\n        Raises:\n            NotImplementedError: Must be implemented by the inheriting class.\n\n        Returns:\n            (Metrics): Metrics computed from the provided outcome counts.\n        \"\"\"\n        raise NotImplementedError\n\n    def __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        \"\"\"\n        User defined convenience method that calculates the desired metric given the predictions and target without\n        accumulating it into the class itself.\n\n        Raises:\n            NotImplementedError: User must define this method.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.ClassificationMetric.__init__","title":"<code>__init__(name, label_dim, batch_dim, dtype, threshold, discard)</code>","text":"<p>A Base class for efficiently computing classification metrics that can be calculated using the true positives (tp), false positive (fp), false negative (fn) and true negative (tn) counts.</p> <p>How these values are counted is left to the inheriting class along with how they are composed together for the final metric score. There are two classes inheriting from this class to form the basis of efficient classification metrics: <code>BinaryClassificationMetric</code> and <code>MultiClassificationMetric</code>. These handle implementation of the <code>count_tp_fp_tn_fn</code> method.</p> <p>On each update, the <code>true_positives</code>, <code>false_positives</code>, <code>false_negatives</code> and <code>true_negatives</code> counts for the provided predictions and targets are accumulated into <code>self.true_positives</code>, <code>self.false_positives</code>, <code>self.false_negatives</code> and <code>self.true_negatives</code>, respectively, and reduced along all unspecified dimensions. This reduces the memory footprint required to compute metrics across rounds. The user needs to define the <code>compute_from_counts</code> method which returns a dictionary of <code>Scalar</code> metrics given the <code>true_positives</code>, <code>false_positives</code>, <code>false_negatives</code>, and  <code>true_negatives</code> counts. The accumulated counts are reset by the <code>clear</code> method. If your subclass returns multiple metrics you may need to also override the <code>__call__</code> method.</p> <p>If the predictions provided are continuous in value, then the associated counts will also be continuous (\"soft\"). For example, with a target of 1, a prediction of 0.8 contributes 0.8 to the <code>true_positives</code> count and 0.2 to the <code>false_negatives</code>.</p> <p>NOTE: Preds and targets are expected to have elements in the interval <code>[0, 1]</code> or to be thresholded, using the argument of this class to be as such.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> required <code>label_dim</code> <code>int | None</code> <p>Specifies which dimension in the provided tensors corresponds to the label dimension. If None, the counts along the specified dimension (i.e. for each output label) are aggregated and the label dimension is reduced. If specified, counts will be computed along the specified dimensions. That is, counts are maintained for each output label INDIVIDUALLY.</p> <p>NOTE: If both <code>label_dim</code> and <code>batch_dim</code> are specified, then counts will be presented batch dimension first, then label dimension. If neither are specified, each count is a global scalar.</p> required <code>batch_dim</code> <code>int | None</code> <p>If None, the counts along the specified dimension (i.e. for each sample) are aggregated and the batch dimension is reduced. If specified, counts will be computed along the dimension specified. That is, counts are maintained for each training sample INDIVIDUALLY.</p> <p>NOTE: If both <code>label_dim</code> and <code>batch_dim</code> are specified, then counts will be presented batch dimension first, then label dimension. If neither are specified, each count is a global scalar.</p> required <code>dtype</code> <code>dtype</code> <p>The dtype to store the counts as. If preds or targets can be continuous, specify a float type. Otherwise specify an integer type to prevent overflow.</p> required <code>threshold</code> <code>float | int | None</code> <p>A float for thresholding values or an integer specifying the index of the label dimension. If a float is given, predictions below the threshold are mapped to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class with the highest prediction where the specified axis is assumed to contain a prediction for each class (where its index along that dimension is the class label). Setting to None leaves preds unchanged.</p> required <code>discard</code> <code>set[ClassificationOutcome] | None</code> <p>One or several of <code>ClassificationOutcome</code> values. Specified counts will not be accumulated. Their associated attribute will remain as an empty pytorch tensor. Useful for reducing the memory footprint of metrics that do not use all of the counts in their computation.</p> required Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    label_dim: int | None,\n    batch_dim: int | None,\n    dtype: torch.dtype,\n    threshold: float | int | None,\n    discard: set[ClassificationOutcome] | None,\n) -&gt; None:\n    \"\"\"\n    A Base class for efficiently computing classification metrics that can be calculated using the true positives\n    (tp), false positive (fp), false negative (fn) and true negative (tn) counts.\n\n    How these values are counted is left to the inheriting class along with how they are composed together for the\n    final metric score. There are two classes inheriting from this class to form the basis of efficient\n    classification metrics: ``BinaryClassificationMetric`` and ``MultiClassificationMetric``. These handle\n    implementation of the ``count_tp_fp_tn_fn`` method.\n\n    On each update, the ``true_positives``, ``false_positives``, ``false_negatives`` and ``true_negatives`` counts\n    for the provided predictions and targets are accumulated into ``self.true_positives``,\n    ``self.false_positives``, ``self.false_negatives`` and ``self.true_negatives``, respectively, and reduced\n    along all unspecified dimensions. This reduces the memory footprint required to compute metrics across rounds.\n    The user needs to define the ``compute_from_counts`` method which returns a dictionary of ``Scalar`` metrics\n    given the ``true_positives``, ``false_positives``, ``false_negatives``, and  ``true_negatives`` counts. The\n    accumulated counts are reset by the ``clear`` method. If your subclass returns multiple metrics you may need\n    to also override the ``__call__`` method.\n\n    If the predictions provided are continuous in value, then the associated counts will also be continuous\n    (\"soft\"). For example, with a target of 1, a prediction of 0.8 contributes 0.8 to the ``true_positives`` count\n    and 0.2 to the ``false_negatives``.\n\n    **NOTE**: Preds and targets are expected to have elements in the interval ``[0, 1]`` or to be thresholded,\n    using the argument of this class to be as such.\n\n    Args:\n        name (str): The name of the metric.\n        label_dim (int | None, optional): Specifies which dimension in the provided tensors corresponds to the\n            label dimension. If None, the counts along the specified dimension (i.e. for each output label) are\n            aggregated and the label dimension is reduced. If specified, counts will be computed along the\n            specified dimensions. That is, counts are maintained for each output label **INDIVIDUALLY**.\n\n            **NOTE**: If both ``label_dim`` and ``batch_dim`` are specified, then counts will be presented batch\n            dimension first, then label dimension. If neither are specified, each count is a global scalar.\n        batch_dim (int | None, optional): If None, the counts along the specified dimension (i.e. for each sample)\n            are aggregated and the batch dimension is reduced. If specified, counts will be computed along the\n            dimension specified. That is, counts are maintained for each training sample **INDIVIDUALLY**.\n\n            **NOTE**: If both ```label_dim``` and `batch_dim` are specified, then counts will be presented batch\n            dimension first, then label dimension. If neither are specified, each count is a global scalar.\n        dtype (torch.dtype): The dtype to store the counts as. If preds or targets can be continuous, specify a\n            float type. Otherwise specify an integer type to prevent overflow.\n        threshold (float | int | None, optional): A float for thresholding values or an integer specifying the\n            index of the label dimension. If a float is given, predictions below the threshold are mapped\n            to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class\n            with the highest prediction where the specified axis is assumed to contain a prediction for each class\n            (where its index along that dimension is the class label). Setting to None leaves preds unchanged.\n        discard (set[ClassificationOutcome] | None, optional): One or several of ``ClassificationOutcome`` values.\n            Specified counts will not be accumulated. Their associated attribute will remain as an empty pytorch\n            tensor. Useful for reducing the memory footprint of metrics that do not use all of the counts in their\n            computation.\n    \"\"\"\n    self.name = name\n    self.dtype = dtype\n    self.threshold = threshold\n    self.label_dim = label_dim\n    self.batch_dim = batch_dim\n\n    if self.label_dim is not None:\n        if isinstance(self.threshold, int) and self.threshold != self.label_dim:\n            log(\n                WARNING,\n                f\"Specified threshold dimension: {threshold} is not the same as the label_dim: \"\n                f\"{label_dim}. This is atypical and may produce undesired behavior\",\n            )\n        if self.batch_dim is not None and self.label_dim == self.batch_dim:\n            raise ValueError(f\"The label and batch dimensions must differ but got {self.label_dim}\")\n\n    # Parse discard argument\n    discard = set() if discard is None else discard\n\n    self.discard_tp = ClassificationOutcome.TRUE_POSITIVE in discard\n    self.discard_fp = ClassificationOutcome.FALSE_POSITIVE in discard\n    self.discard_fn = ClassificationOutcome.FALSE_NEGATIVE in discard\n    self.discard_tn = ClassificationOutcome.TRUE_NEGATIVE in discard\n\n    # Create intermediate tensors. Will be initialized with tensors of correct shape on first update.\n    self.counts_initialized = False\n    self.true_positives, self.false_positives, self.false_negatives, self.true_negatives = (\n        torch.tensor([]),\n        torch.tensor([]),\n        torch.tensor([]),\n        torch.tensor([]),\n    )\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.ClassificationMetric.update","title":"<code>update(preds, targets)</code>","text":"<p>Updates the existing <code>self.true_positive</code>, <code>self.false_positive</code>, <code>self.false_negative</code> and <code>self.true_negative</code> counts with new counts computed from preds and targets.</p> <p>NOTE: This function assumes that if <code>self.batch_dim</code> is not <code>None</code>, the counts are returned with shapes such that the batch dimension comes FIRST for the counts. If <code>self.count_tp_fp_tn_fn</code> is overridden it must ensure that this remains the case.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Predictions tensor.</p> required <code>targets</code> <code>Tensor</code> <p>Targets tensor.</p> required Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def update(self, preds: torch.Tensor, targets: torch.Tensor) -&gt; None:\n    \"\"\"\n    Updates the existing ``self.true_positive``, ``self.false_positive``, ``self.false_negative`` and\n    ``self.true_negative`` counts with new counts computed from preds and targets.\n\n    **NOTE**: This function assumes that if ``self.batch_dim`` is not ``None``, the counts are returned with\n    shapes such that the batch dimension comes **FIRST** for the counts. If ``self.count_tp_fp_tn_fn`` is\n    overridden it must ensure that this remains the case.\n\n    Args:\n        preds (torch.Tensor): Predictions tensor.\n        targets (torch.Tensor): Targets tensor.\n    \"\"\"\n    # Get tp, fp, tn and fn counts for current update. If a count is discarded, then an empty tensor is returned.\n    tp, fp, tn, fn = self.count_tp_fp_tn_fn(preds, targets)\n\n    # If this is first update since init or clear, initialize counts attributes and exit function\n    if not self.counts_initialized:\n        self.true_positives, self.false_positives, self.true_negatives, self.false_negatives = tp, fp, tn, fn\n        self.counts_initialized = True\n        return\n\n    # If batch_dim has been specified, we accumulate counts for EACH INSTANCE seen throughout the updates, such\n    # that each of the counts is a tensor of row length equal to the samples seen.\n    # NOTE: This ASSUMES the batch dimension comes FIRST for the counts.\n    # Otherwise, the counts are 1D tensors with length equal to the number of classes (or possibly 1 if using the\n    # BinaryClassificationMetric)\n    self.true_positives = (\n        torch.cat([self.true_positives, tp], dim=0) if self.batch_dim is not None else self.true_positives + tp\n    )\n    self.false_positives = (\n        torch.cat([self.false_positives, fp], dim=0) if self.batch_dim is not None else self.false_positives + fp\n    )\n    self.true_negatives = (\n        torch.cat([self.true_negatives, tn], dim=0) if self.batch_dim is not None else self.true_negatives + tn\n    )\n    self.false_negatives = (\n        torch.cat([self.false_negatives, fn], dim=0) if self.batch_dim is not None else self.false_negatives + fn\n    )\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.ClassificationMetric.clear","title":"<code>clear()</code>","text":"<p>Reset accumulated tp, fp, fn and tn's. They will be initialized with correct shape on next update.</p> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Reset accumulated tp, fp, fn and tn's. They will be initialized with correct shape on next update.\"\"\"\n    self.true_positives = torch.tensor([])\n    self.false_positives = torch.tensor([])\n    self.false_negatives = torch.tensor([])\n    self.true_negatives = torch.tensor([])\n    self.counts_initialized = False\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.ClassificationMetric.compute","title":"<code>compute(name=None)</code>","text":"<p>Computes the metrics from the currently saved counts using the <code>compute_from_counts</code> function defined in inheriting classes.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Optional name used in conjunction with class attribute name to define key in metrics dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>Metrics</code> <p>A dictionary of string and <code>Scalar</code> representing the computed metric and its associated key.</p> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def compute(self, name: str | None = None) -&gt; Metrics:\n    \"\"\"\n    Computes the metrics from the currently saved counts using the ``compute_from_counts`` function defined in\n    inheriting classes.\n\n    Args:\n        name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n            dictionary.\n\n    Returns:\n        (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n    \"\"\"\n    metrics = self.compute_from_counts(\n        true_positives=self.true_positives,\n        false_positives=self.false_positives,\n        true_negatives=self.true_negatives,\n        false_negatives=self.false_negatives,\n    )\n    if name is not None:\n        metrics = {f\"{name} - {k}\": v for k, v in metrics.items()}\n    return metrics\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.ClassificationMetric.count_tp_fp_tn_fn","title":"<code>count_tp_fp_tn_fn(preds, targets)</code>","text":"<p>Given two tensors containing model predictions and targets, returns the number of true positives (tp), false positives (fp), true negatives (tn), and false negatives (fn).</p> <p>The shape of these counts depends on if <code>self.batch_dim</code> and <code>self.label_dim</code> are specified and the implementation of the inheriting class. If the batch dimension appears after the label dimension in the input tensors this method will transpose the count tensors to ensure the batch dimension comes first.</p> <p>If any of the true positives, false positives, true negative, or false negative counts were specified to be discarded during initialization of the class, then that count will not be computed and an empty tensor will be returned in its place.</p> <p>NOTE: Inheriting classes may implement additional functionality on top of this class. For example, any preprocessing that needs to be done to preds and targets should be done in the inheriting function. Any post processing should also be done there. See implementations in the <code>BinaryClassificationMetric</code> or <code>MultiClassificationMetric</code> class for examples.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Tensor containing model predictions. Must be the same shape as targets</p> required <code>targets</code> <code>Tensor</code> <p>Tensor containing prediction targets. Must be same shape as preds.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>Tensors containing the counts along the specified dimensions for each of true positives, false positives, true negatives, and false negatives respectively. The output shape of these tensors depends on if <code>self.batch_dim</code> and <code>self.label_dim</code> are specified. The batch dimension, if it exists in the output, will always come first. For example, if the batch and label dimensions have sizes 2 and 3 respectively:</p> <ul> <li>Both <code>batch_dim</code> and <code>label_dim</code> are specified: <code>Size([2, 3])</code></li> <li>Only <code>batch_dim</code> is specified: <code>Size([2])</code></li> <li>Only <code>label_dim</code> is specified: <code>Size([3])</code></li> <li>Neither specified: <code>Size([])</code></li> </ul> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def count_tp_fp_tn_fn(\n    self, preds: torch.Tensor, targets: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Given two tensors containing model predictions and targets, returns the number of true positives (tp), false\n    positives (fp), true negatives (tn), and false negatives (fn).\n\n    The shape of these counts depends on if ``self.batch_dim`` and ``self.label_dim`` are specified and the\n    implementation of the inheriting class. If the batch dimension appears after the label dimension in the input\n    tensors this method will transpose the count tensors to ensure the batch dimension comes first.\n\n    If any of the true positives, false positives, true negative, or false negative counts were specified to be\n    discarded during initialization of the class, then that count will not be computed and an empty tensor will be\n    returned in its place.\n\n    **NOTE**: Inheriting classes may implement additional functionality on top of this class. For example, any\n    preprocessing that needs to be done to preds and targets should be done in the inheriting function. Any post\n    processing should also be done there. See implementations in the ``BinaryClassificationMetric`` or\n    ``MultiClassificationMetric`` class for examples.\n\n    Args:\n        preds (torch.Tensor): Tensor containing model predictions. Must be the same shape as targets\n        targets (torch.Tensor): Tensor containing prediction targets. Must be same shape as preds.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]): Tensors containing the counts along the\n            specified dimensions for each of true positives, false positives, true negatives, and false negatives\n            respectively. The output shape of these tensors depends on if ``self.batch_dim`` and ``self.label_dim``\n            are specified. The batch dimension, if it exists in the output, will always come first. For example,\n            if the batch and label dimensions have sizes 2 and 3 respectively:\n\n            - Both ``batch_dim`` and ``label_dim`` are specified: ``Size([2, 3])``\n            - Only ``batch_dim`` is specified: ``Size([2])``\n            - Only ``label_dim`` is specified: ``Size([3])``\n            - Neither specified: ``Size([])``\n    \"\"\"\n    # Transform predictions and targets to get them ready for computation\n    preds, targets = self._transform_tensors(preds, targets)\n\n    # Assert that we are ready for computation\n    self._assert_correct_ranges_and_shape(preds, targets)\n\n    # Compute counts. If we're ignoring a count, set it as an empty tensor to avoid downstream errors.\n    true_positives, false_positives, true_negatives, false_negatives = self._prepare_counts_from_preds_and_targets(\n        preds, targets\n    )\n\n    axes_to_ignore: set[int] = {self.label_dim} if self.label_dim is not None else set()\n    if self.batch_dim is not None:\n        axes_to_ignore.add(self.batch_dim)\n    sum_axes = tuple([i for i in range(preds.ndim) if i not in axes_to_ignore])\n\n    # If sum_axes is empty, then we have nothing to do.\n    if len(sum_axes) != 0:\n        true_positives, false_positives, true_negatives, false_negatives = self._sum_along_axes_or_discard(\n            sum_axes, true_positives, false_positives, true_negatives, false_negatives\n        )\n\n    # Ensure that batch dimension appears first if count tensors are 2D\n    true_positives = self._maybe_transpose_count_tensor(true_positives)\n    false_positives = self._maybe_transpose_count_tensor(false_positives)\n    true_negatives = self._maybe_transpose_count_tensor(true_negatives)\n    false_negatives = self._maybe_transpose_count_tensor(false_negatives)\n\n    return true_positives, false_positives, true_negatives, false_negatives\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.ClassificationMetric.compute_from_counts","title":"<code>compute_from_counts(true_positives, false_positives, true_negatives, false_negatives)</code>  <code>abstractmethod</code>","text":"<p>Provided tensors associated with the various classification outcomes from predictions compared to targets in the form of true positives, false positives, true negatives, and false negatives, returns a dictionary of <code>Scalar</code> metrics. For example, one might compute recall as <code>true_positives/(true_positives + false_negatives)</code>.</p> <p>The count tensors will all have the same shape. This shape depends on whether <code>batch_dim</code> and or <code>label_dim</code> were specified. For example, if the batch and label dimensions have sizes 2 and 3 respectively, then:</p> <ul> <li>Both <code>batch_dim</code> and <code>label_dim</code> are specified: <code>Size([2, 3])</code></li> <li>Only <code>batch_dim</code> is specified: <code>Size([2])</code></li> <li>Only <code>label_dim</code> is specified: <code>Size([3])</code></li> <li>Neither specified: <code>Size([])</code></li> </ul> <p>Inheriting classes may further modify the shapes of the count tensors that are provided as arguments depending on the kind of classification being done (eg. multi-class vs. binary).</p> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>Tensor</code> <p>Counts associated with positive predictions of a class and true positives for that class.</p> required <code>false_positives</code> <code>Tensor</code> <p>Counts associated with positive predictions of a class and true negatives for that class.</p> required <code>true_negatives</code> <code>Tensor</code> <p>Counts associated with negative predictions of a class and true negatives for that class.</p> required <code>false_negatives</code> <code>Tensor</code> <p>Counts associated with negative predictions of a class and true positives for that class.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by the inheriting class.</p> <p>Returns:</p> Type Description <code>Metrics</code> <p>Metrics computed from the provided outcome counts.</p> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>@abstractmethod\ndef compute_from_counts(\n    self,\n    true_positives: torch.Tensor,\n    false_positives: torch.Tensor,\n    true_negatives: torch.Tensor,\n    false_negatives: torch.Tensor,\n) -&gt; Metrics:\n    \"\"\"\n    Provided tensors associated with the various classification outcomes from predictions compared to targets in\n    the form of true positives, false positives, true negatives, and false negatives, returns a dictionary of\n    ``Scalar`` metrics. For example, one might compute recall as ``true_positives/(true_positives +\n    false_negatives)``.\n\n    The count tensors will all have the same shape. This shape depends on whether ``batch_dim`` and or\n    ``label_dim`` were specified. For example, if the batch and label dimensions have sizes 2 and 3 respectively,\n    then:\n\n    - Both ``batch_dim`` and ``label_dim`` are specified: ``Size([2, 3])``\n    - Only ``batch_dim`` is specified: ``Size([2])``\n    - Only ``label_dim`` is specified: ``Size([3])``\n    - Neither specified: ``Size([])``\n\n    Inheriting classes may further modify the shapes of the count tensors that are provided as arguments depending\n    on the kind of classification being done (eg. multi-class vs. binary).\n\n    Args:\n        true_positives (torch.Tensor): Counts associated with positive predictions of a class and true positives\n            for that class.\n        false_positives (torch.Tensor): Counts associated with positive predictions of a class and true negatives\n            for that class.\n        true_negatives (torch.Tensor): Counts associated with negative predictions of a class and true negatives\n            for that class.\n        false_negatives (torch.Tensor): Counts associated with negative predictions of a class and true positives\n            for that class.\n\n    Raises:\n        NotImplementedError: Must be implemented by the inheriting class.\n\n    Returns:\n        (Metrics): Metrics computed from the provided outcome counts.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.ClassificationMetric.__call__","title":"<code>__call__(input, target)</code>","text":"<p>User defined convenience method that calculates the desired metric given the predictions and target without accumulating it into the class itself.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>User must define this method.</p> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n    \"\"\"\n    User defined convenience method that calculates the desired metric given the predictions and target without\n    accumulating it into the class itself.\n\n    Raises:\n        NotImplementedError: User must define this method.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.BinaryClassificationMetric","title":"<code>BinaryClassificationMetric</code>","text":"<p>               Bases: <code>ClassificationMetric</code></p> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>class BinaryClassificationMetric(ClassificationMetric):\n    def __init__(\n        self,\n        name: str,\n        label_dim: int | None = None,\n        batch_dim: int | None = None,\n        dtype: torch.dtype = torch.float32,\n        pos_label: int = 1,\n        threshold: float | int | None = None,\n        discard: set[ClassificationOutcome] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        A Base class for **BINARY** classification metrics that can be computed using the true positives (tp),\n        false positive (fp), false negative (fn) and true negative (tn) counts. These counts are computed for\n        each class independently. How they are composed together for the metric is left to inheriting classes.\n\n        On each update, the ``true_positives``, ``false_positives``, ``false_negatives`` and ``true_negatives`` counts\n        for the provided predictions and targets are accumulated into ``self.true_positives``,\n        ``self.false_positives``, ``self.false_negatives`` and ``self.true_negatives``, respectively, for each label\n        type. This reduces the memory footprint required to compute metrics across rounds. The user needs to define the\n        ``compute_from_counts`` method which returns a dictionary of Scalar metrics given the ``true_positives``,\n        ``false_positives``, ``false_negatives``, and ``true_negatives`` counts. The accumulated counts are reset by\n        the ``clear`` method. If your subclass returns multiple metrics you may need to also override the\n        ``__call__`` method.\n\n        If the predictions provided are continuous in value, then the associated counts will also be continuous\n        (\"soft\"). For example, with a target of 1, a prediction of 0.8 contributes 0.8 to the ``true_positives`` count\n        and 0.2 to the ``false_negatives``.\n\n        **NOTE**: For this class, the predictions and targets passed to the ``update`` function **MUST** have the same\n        shape.\n\n        **NOTE**: Preds and targets are expected to have elements in the interval ``[0, 1]`` or to be thresholded,\n        using that argument to be as such.\n\n        **NOTE**: For this class, only the counts for the positive label are accumulated.\n\n        Args:\n            name (str): The name of the metric.\n            label_dim (int | None, optional): Specifies which dimension in the provided tensors corresponds to the\n                label dimension. During metric computation, this dimension must have size of **AT MOST 2**. If left as\n                None, this class will assume that there is no label dimension and that each entry in the tensor\n                corresponds to a prediction/target, with the positive class label indicated by ``pos_label``. In both\n                cases, only the counts for the positive class label are accumulated and any counts/predictions for the\n                negative class label are discarded. Defaults to None.\n            batch_dim (int | None, optional): If None, the counts along the specified dimension (i.e. for each sample)\n                are aggregated and the batch dimension is reduced. If specified, counts will be computed along the\n                dimension specified. That is, counts are maintained for each training sample **INDIVIDUALLY**. For\n                example, if ``batch_dim = 1`` and ``label_dim = 0``, then\n\n                ```python\n                p = torch.tensor([[[0, 0, 0, 1], [1, 1, 1, 1]]])  # Size([1, 2, 4])\n\n                t = torch.tensor([[[0, 0, 1, 0], [1, 1, 1, 1]]])  # Size([1, 2, 4])\n\n                self.tp = torch.Tensor([[0], [4]])  # Size([2, 1])\n\n                self.tn = torch.Tensor([[2], [0]])  # Size([2, 1])\n\n                self.fp = torch.Tensor([[1], [0]])  # Size([2, 1])\n\n                self.fn = torch.Tensor([[1], [0]])  # Size([2, 1])\n                ```\n\n                **NOTE**: The resulting counts will always be presented batch dimension first, then label dimension,\n                regardless of input shape. Defaults to None.\n            dtype (torch.dtype): The ``dtype`` to store the counts as. If preds or targets can be continuous, specify a\n                ``float`` type. Otherwise specify an integer type to prevent overflow. Defaults to ``torch.float32``.\n            pos_label (int, optional): The label relative to which to report the counts. Must be either 0 or 1.\n                Defaults to 1.\n            threshold (float | int | None, optional): A float for thresholding values or an integer specifying the\n                index of the label dimension. If a float is given, predictions below the threshold are mapped\n                to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class\n                with the highest prediction where the specified axis is assumed to contain a prediction for each class\n                (where its index along that dimension is the class label). Value of None leaves preds unchanged.\n                Defaults to None.\n            discard (set[ClassificationOutcome] | None, optional): One or several of ``ClassificationOutcome`` values.\n                Specified outcome counts will not be accumulated. Their associated attribute will remain as an empty\n                pytorch tensor. Useful for reducing the memory footprint of metrics that do not use all of the counts\n                in their computation. Defaults to None.\n        \"\"\"\n        super().__init__(\n            name=name,\n            dtype=dtype,\n            label_dim=label_dim,\n            batch_dim=batch_dim,\n            threshold=threshold,\n            discard=discard,\n        )\n        assert pos_label in {0, 1}, \"pos_label must be either 0 or 1\"\n        self.pos_label = pos_label\n\n    def _postprocess_count_tensor(self, count_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Given a count tensor, in the various forms that it might appear, we need to post process these so that they\n        can be returned to the user in the appropriate format. The structure of ``count_tensor`` after processing\n        a batch of data will differ depending on whether the ``label_dim`` and ``batch_dim`` have been specified.\n\n        That is, the shape of ``count_tensor`` will be:\n\n        - If both ``label_dim`` and ``batch_dim`` have been provided, the count tensor will be 2D with the batch\n          dimension first, followed by the label dimension.\n        - If ``batch_dim`` has been provided, but not ``label_dim``, the count tensor will be 1D with a single count\n          associated with each sample in the batch.\n        - If ``label_dim`` has been provided, but not ``batch_dim``, the count tensor will have 1 or 2 elements,\n          because it's a binary problem.\n        - If neither has been provided then the tensor will have 1 element corresponding to the count over all\n          tensor elements.\n\n        **NOTE**: If the count has been specified as discarded, it will always simply be an empty tensor\n\n        After postprocessing, the count tensor will always be **RELATIVE** to the class associated with the class at\n        index 1 or the implicit class associated with targets 1.0 (when preds/targets are not vector encoded).\n        Returning values relative to the opposite class is handled elsewhere.\n\n        Args:\n            count_tensor (torch.Tensor): Count tensor with the correct shape and meaning.\n\n        Raises:\n            ValueError: Raises errors if the tensor does not have the right shape for the expected setting\n\n        Returns:\n            (torch.Tensor): Count tensor of the appropriate shape and structure. If ``self.batch_dim`` is not None\n                then the postprocessed ``count_tensor`` will have shape ``(batch size, 1)``, Otherwise, it will have\n                shape ``(1,)``. In both settings, the count is relative to the label at index 1 (implied or explicit).\n        \"\"\"\n        # If tensor is empty, we do nothing\n        if count_tensor.numel() == 0:\n            return count_tensor\n\n        count_ndims = count_tensor.ndim\n\n        if self.batch_dim is not None and self.label_dim is not None:\n            # Both a batch and label dim have been specified. So tensor should be 2D and either have 1 or 2 columns\n            assert count_ndims == MAX_COUNT_TENSOR_DIMS, (\n                f\"Batch and label dims have been specified, tensor should be 2D, but got {count_ndims}\"\n            )\n\n            # Batch dimension is always first, so count_tensor shape is (batch_size, n_labels)\n            if count_tensor.shape[1] == MAX_COUNT_TENSOR_DIMS:\n                # Always return the class with label 1 (pos_label 0 will be handled by rearranging counts if necessary)\n                return count_tensor[:, 1].unsqueeze(1)\n            if count_tensor.shape[1] == 1:\n                return count_tensor\n            raise ValueError(f\"Label dimension has unexpected size of {count_tensor.shape[1]}\")\n        if self.batch_dim is not None:\n            # Batch dim has been specified but label dim has not, Tensor should be 1D equivalent to size of the batch\n            assert count_ndims == 1, (\n                f\"Batch dim has been specified but not label dim, tensor should be 1D but got {count_ndims}D\"\n            )\n            return count_tensor.unsqueeze(1)\n        # Tensor should be 1D equivalent to size of labels dimension if it has been specified or a single element\n        # if label dims has not be specified. The label dimension is of size at most 2, but can be of size 1 if\n        # there is a dimension for an \"implied\" label (i.e. 0.8 representing vector predictions [0.2, 0.8]). If\n        # there is no label dimension specified, there should only be a single element as well.\n        assert count_ndims &lt;= 1, f\"Batch dim has not been specified, tensor should be 0 or 1D but got {count_ndims}\"\n        if count_tensor.numel() == MAX_COUNT_TENSOR_DIMS:\n            assert self.label_dim is not None, \"self.label_dim is None but got two elements in the count_tensor\"\n            # Always return the class with label 1\n            return count_tensor[1].unsqueeze(0)\n        if count_tensor.numel() == 1:\n            return count_tensor.unsqueeze(0)\n        raise ValueError(f\"Too many elements in the count tensor, expected 2 or less and got {count_tensor.numel()}\")\n\n    def _assert_correct_ranges_and_shape(self, preds: torch.Tensor, targets: torch.Tensor) -&gt; None:\n        \"\"\"\n        Ensures that the prediction and target tensors are in the expected form for computation.\n\n        This class adds assertions specific to **BINARY** classification.\n\n        Args:\n            preds (torch.Tensor): Predictions tensor.\n            targets (torch.Tensor): Targets tensor.\n        \"\"\"\n        super()._assert_correct_ranges_and_shape(preds, targets)\n\n        # Make sure after transformation the preds and targets have the right shape\n        assert preds.shape == targets.shape, (\n            f\"Preds and targets must have the same shape but got {preds.shape} and {targets.shape} respectively.\"\n        )\n\n        # Assert that the label dimension for these tensors is of size 2 at most.\n        if self.label_dim is not None:\n            assert preds.shape[self.label_dim] &lt;= N_LABELS_BINARY, (\n                f\"Label dimension for preds tensor is greater than 2 {preds.shape[self.label_dim]}. This class is \"\n                \"meant for binary metric computation only\"\n            )\n            assert targets.shape[self.label_dim] &lt;= N_LABELS_BINARY, (\n                f\"Label dimension for targets tensor is greater than 2 {targets.shape[self.label_dim]}. This class is \"\n                \"meant for binary metric computation only\"\n            )\n\n    def count_tp_fp_tn_fn(\n        self, preds: torch.Tensor, targets: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Given two tensors containing model predictions and targets, returns the number of true positives (tp), false\n        positives (fp), true negatives (tn), and false negatives (fn).\n\n        This class overrides the base method to ensure that only the counts with respect to the positive label are\n        returned.\n\n        The shape of these counts depends on if the values of ``self.batch_dim`` and ``self.label_dim`` are specified.\n        If any of the true positive, false positive, true negative, or false negative counts were specified to be\n        discarded during initialization of the class, then that count will not be computed and an empty tensor will be\n        returned in its place.\n\n        Args:\n             preds (torch.Tensor): Tensor containing model predictions. Must be the same shape as targets.\n             targets (torch.Tensor): Tensor containing prediction targets. Must be same shape as preds.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]): Tensors containing the counts along the\n                specified dimensions for each of true positives, false positives, true negatives, and false negatives,\n                respectively. If ``self.batch_dim`` is not None then these tensors will have shape\n                ``(batch_size, 1)``, Otherwise, it will have shape ``(1,)``. The counts will be relative to the\n                index of ``self.pos_label``.\n        \"\"\"\n        true_positives, false_positives, true_negatives, false_negatives = super().count_tp_fp_tn_fn(preds, targets)\n\n        true_positives = self._postprocess_count_tensor(true_positives)\n        false_positives = self._postprocess_count_tensor(false_positives)\n        true_negatives = self._postprocess_count_tensor(true_negatives)\n        false_negatives = self._postprocess_count_tensor(false_negatives)\n\n        # Need to flip the label interpretations\n        if self.pos_label == 0:\n            return true_negatives, false_negatives, true_positives, false_positives\n\n        return true_positives, false_positives, true_negatives, false_negatives\n\n    def compute_from_counts(\n        self,\n        true_positives: torch.Tensor,\n        false_positives: torch.Tensor,\n        true_negatives: torch.Tensor,\n        false_negatives: torch.Tensor,\n    ) -&gt; Metrics:\n        \"\"\"\n        Provided tensors associated with the various outcomes from predictions compared to targets in the form of\n        true positives, false positives, true negatives, and false negatives, returns a dictionary of ``Scalar``\n        metrics. For example, one might compute recall as ``true_positives/(true_positives + false_negatives)``. The\n        shape of  these tensors are specific to how this object is configured, see class documentation above.\n\n        For this class it is assumed that all counts are presented relative to the class indicated by the ``pos_label``\n        index, counts for the negative label are discarded. Moreover, they are assumed to either have a shape ``(1,)``\n        or have shape ``(num_samples, 1)`` if ``batch_dim`` was specified. In the former, a single count is\n        presented **ACROSS** all samples relative to the ``pos_label`` specified. In the latter, counts are computed\n        **WITHIN** each sample, but held separate across samples. A concrete setting where this makes sense is\n        binary image segmentation. You can have such counts summed for all pixels within an image, but separate per\n        image. A metric could then be computed for each image and then averaged.\n\n        Args:\n            true_positives (torch.Tensor): Counts associated with positive predictions and positive labels.\n            false_positives (torch.Tensor): Counts associated with positive predictions and negative labels.\n            true_negatives (torch.Tensor): Counts associated with negative predictions and negative labels.\n            false_negatives (torch.Tensor): Counts associated with negative predictions and positive labels.\n\n        Raises:\n            NotImplementedError: Must be implemented by the inheriting class.\n\n        Returns:\n            (Metrics): Metrics computed from the provided outcome counts.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.BinaryClassificationMetric.__init__","title":"<code>__init__(name, label_dim=None, batch_dim=None, dtype=torch.float32, pos_label=1, threshold=None, discard=None)</code>","text":"<p>A Base class for BINARY classification metrics that can be computed using the true positives (tp), false positive (fp), false negative (fn) and true negative (tn) counts. These counts are computed for each class independently. How they are composed together for the metric is left to inheriting classes.</p> <p>On each update, the <code>true_positives</code>, <code>false_positives</code>, <code>false_negatives</code> and <code>true_negatives</code> counts for the provided predictions and targets are accumulated into <code>self.true_positives</code>, <code>self.false_positives</code>, <code>self.false_negatives</code> and <code>self.true_negatives</code>, respectively, for each label type. This reduces the memory footprint required to compute metrics across rounds. The user needs to define the <code>compute_from_counts</code> method which returns a dictionary of Scalar metrics given the <code>true_positives</code>, <code>false_positives</code>, <code>false_negatives</code>, and <code>true_negatives</code> counts. The accumulated counts are reset by the <code>clear</code> method. If your subclass returns multiple metrics you may need to also override the <code>__call__</code> method.</p> <p>If the predictions provided are continuous in value, then the associated counts will also be continuous (\"soft\"). For example, with a target of 1, a prediction of 0.8 contributes 0.8 to the <code>true_positives</code> count and 0.2 to the <code>false_negatives</code>.</p> <p>NOTE: For this class, the predictions and targets passed to the <code>update</code> function MUST have the same shape.</p> <p>NOTE: Preds and targets are expected to have elements in the interval <code>[0, 1]</code> or to be thresholded, using that argument to be as such.</p> <p>NOTE: For this class, only the counts for the positive label are accumulated.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> required <code>label_dim</code> <code>int | None</code> <p>Specifies which dimension in the provided tensors corresponds to the label dimension. During metric computation, this dimension must have size of AT MOST 2. If left as None, this class will assume that there is no label dimension and that each entry in the tensor corresponds to a prediction/target, with the positive class label indicated by <code>pos_label</code>. In both cases, only the counts for the positive class label are accumulated and any counts/predictions for the negative class label are discarded. Defaults to None.</p> <code>None</code> <code>batch_dim</code> <code>int | None</code> <p>If None, the counts along the specified dimension (i.e. for each sample) are aggregated and the batch dimension is reduced. If specified, counts will be computed along the dimension specified. That is, counts are maintained for each training sample INDIVIDUALLY. For example, if <code>batch_dim = 1</code> and <code>label_dim = 0</code>, then</p> <pre><code>p = torch.tensor([[[0, 0, 0, 1], [1, 1, 1, 1]]])  # Size([1, 2, 4])\n\nt = torch.tensor([[[0, 0, 1, 0], [1, 1, 1, 1]]])  # Size([1, 2, 4])\n\nself.tp = torch.Tensor([[0], [4]])  # Size([2, 1])\n\nself.tn = torch.Tensor([[2], [0]])  # Size([2, 1])\n\nself.fp = torch.Tensor([[1], [0]])  # Size([2, 1])\n\nself.fn = torch.Tensor([[1], [0]])  # Size([2, 1])\n</code></pre> <p>NOTE: The resulting counts will always be presented batch dimension first, then label dimension, regardless of input shape. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The <code>dtype</code> to store the counts as. If preds or targets can be continuous, specify a <code>float</code> type. Otherwise specify an integer type to prevent overflow. Defaults to <code>torch.float32</code>.</p> <code>float32</code> <code>pos_label</code> <code>int</code> <p>The label relative to which to report the counts. Must be either 0 or 1. Defaults to 1.</p> <code>1</code> <code>threshold</code> <code>float | int | None</code> <p>A float for thresholding values or an integer specifying the index of the label dimension. If a float is given, predictions below the threshold are mapped to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class with the highest prediction where the specified axis is assumed to contain a prediction for each class (where its index along that dimension is the class label). Value of None leaves preds unchanged. Defaults to None.</p> <code>None</code> <code>discard</code> <code>set[ClassificationOutcome] | None</code> <p>One or several of <code>ClassificationOutcome</code> values. Specified outcome counts will not be accumulated. Their associated attribute will remain as an empty pytorch tensor. Useful for reducing the memory footprint of metrics that do not use all of the counts in their computation. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    label_dim: int | None = None,\n    batch_dim: int | None = None,\n    dtype: torch.dtype = torch.float32,\n    pos_label: int = 1,\n    threshold: float | int | None = None,\n    discard: set[ClassificationOutcome] | None = None,\n) -&gt; None:\n    \"\"\"\n    A Base class for **BINARY** classification metrics that can be computed using the true positives (tp),\n    false positive (fp), false negative (fn) and true negative (tn) counts. These counts are computed for\n    each class independently. How they are composed together for the metric is left to inheriting classes.\n\n    On each update, the ``true_positives``, ``false_positives``, ``false_negatives`` and ``true_negatives`` counts\n    for the provided predictions and targets are accumulated into ``self.true_positives``,\n    ``self.false_positives``, ``self.false_negatives`` and ``self.true_negatives``, respectively, for each label\n    type. This reduces the memory footprint required to compute metrics across rounds. The user needs to define the\n    ``compute_from_counts`` method which returns a dictionary of Scalar metrics given the ``true_positives``,\n    ``false_positives``, ``false_negatives``, and ``true_negatives`` counts. The accumulated counts are reset by\n    the ``clear`` method. If your subclass returns multiple metrics you may need to also override the\n    ``__call__`` method.\n\n    If the predictions provided are continuous in value, then the associated counts will also be continuous\n    (\"soft\"). For example, with a target of 1, a prediction of 0.8 contributes 0.8 to the ``true_positives`` count\n    and 0.2 to the ``false_negatives``.\n\n    **NOTE**: For this class, the predictions and targets passed to the ``update`` function **MUST** have the same\n    shape.\n\n    **NOTE**: Preds and targets are expected to have elements in the interval ``[0, 1]`` or to be thresholded,\n    using that argument to be as such.\n\n    **NOTE**: For this class, only the counts for the positive label are accumulated.\n\n    Args:\n        name (str): The name of the metric.\n        label_dim (int | None, optional): Specifies which dimension in the provided tensors corresponds to the\n            label dimension. During metric computation, this dimension must have size of **AT MOST 2**. If left as\n            None, this class will assume that there is no label dimension and that each entry in the tensor\n            corresponds to a prediction/target, with the positive class label indicated by ``pos_label``. In both\n            cases, only the counts for the positive class label are accumulated and any counts/predictions for the\n            negative class label are discarded. Defaults to None.\n        batch_dim (int | None, optional): If None, the counts along the specified dimension (i.e. for each sample)\n            are aggregated and the batch dimension is reduced. If specified, counts will be computed along the\n            dimension specified. That is, counts are maintained for each training sample **INDIVIDUALLY**. For\n            example, if ``batch_dim = 1`` and ``label_dim = 0``, then\n\n            ```python\n            p = torch.tensor([[[0, 0, 0, 1], [1, 1, 1, 1]]])  # Size([1, 2, 4])\n\n            t = torch.tensor([[[0, 0, 1, 0], [1, 1, 1, 1]]])  # Size([1, 2, 4])\n\n            self.tp = torch.Tensor([[0], [4]])  # Size([2, 1])\n\n            self.tn = torch.Tensor([[2], [0]])  # Size([2, 1])\n\n            self.fp = torch.Tensor([[1], [0]])  # Size([2, 1])\n\n            self.fn = torch.Tensor([[1], [0]])  # Size([2, 1])\n            ```\n\n            **NOTE**: The resulting counts will always be presented batch dimension first, then label dimension,\n            regardless of input shape. Defaults to None.\n        dtype (torch.dtype): The ``dtype`` to store the counts as. If preds or targets can be continuous, specify a\n            ``float`` type. Otherwise specify an integer type to prevent overflow. Defaults to ``torch.float32``.\n        pos_label (int, optional): The label relative to which to report the counts. Must be either 0 or 1.\n            Defaults to 1.\n        threshold (float | int | None, optional): A float for thresholding values or an integer specifying the\n            index of the label dimension. If a float is given, predictions below the threshold are mapped\n            to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class\n            with the highest prediction where the specified axis is assumed to contain a prediction for each class\n            (where its index along that dimension is the class label). Value of None leaves preds unchanged.\n            Defaults to None.\n        discard (set[ClassificationOutcome] | None, optional): One or several of ``ClassificationOutcome`` values.\n            Specified outcome counts will not be accumulated. Their associated attribute will remain as an empty\n            pytorch tensor. Useful for reducing the memory footprint of metrics that do not use all of the counts\n            in their computation. Defaults to None.\n    \"\"\"\n    super().__init__(\n        name=name,\n        dtype=dtype,\n        label_dim=label_dim,\n        batch_dim=batch_dim,\n        threshold=threshold,\n        discard=discard,\n    )\n    assert pos_label in {0, 1}, \"pos_label must be either 0 or 1\"\n    self.pos_label = pos_label\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.BinaryClassificationMetric.count_tp_fp_tn_fn","title":"<code>count_tp_fp_tn_fn(preds, targets)</code>","text":"<p>Given two tensors containing model predictions and targets, returns the number of true positives (tp), false positives (fp), true negatives (tn), and false negatives (fn).</p> <p>This class overrides the base method to ensure that only the counts with respect to the positive label are returned.</p> <p>The shape of these counts depends on if the values of <code>self.batch_dim</code> and <code>self.label_dim</code> are specified. If any of the true positive, false positive, true negative, or false negative counts were specified to be discarded during initialization of the class, then that count will not be computed and an empty tensor will be returned in its place.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Tensor containing model predictions. Must be the same shape as targets.</p> required <code>targets</code> <code>Tensor</code> <p>Tensor containing prediction targets. Must be same shape as preds.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>Tensors containing the counts along the specified dimensions for each of true positives, false positives, true negatives, and false negatives, respectively. If <code>self.batch_dim</code> is not None then these tensors will have shape <code>(batch_size, 1)</code>, Otherwise, it will have shape <code>(1,)</code>. The counts will be relative to the index of <code>self.pos_label</code>.</p> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def count_tp_fp_tn_fn(\n    self, preds: torch.Tensor, targets: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Given two tensors containing model predictions and targets, returns the number of true positives (tp), false\n    positives (fp), true negatives (tn), and false negatives (fn).\n\n    This class overrides the base method to ensure that only the counts with respect to the positive label are\n    returned.\n\n    The shape of these counts depends on if the values of ``self.batch_dim`` and ``self.label_dim`` are specified.\n    If any of the true positive, false positive, true negative, or false negative counts were specified to be\n    discarded during initialization of the class, then that count will not be computed and an empty tensor will be\n    returned in its place.\n\n    Args:\n         preds (torch.Tensor): Tensor containing model predictions. Must be the same shape as targets.\n         targets (torch.Tensor): Tensor containing prediction targets. Must be same shape as preds.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]): Tensors containing the counts along the\n            specified dimensions for each of true positives, false positives, true negatives, and false negatives,\n            respectively. If ``self.batch_dim`` is not None then these tensors will have shape\n            ``(batch_size, 1)``, Otherwise, it will have shape ``(1,)``. The counts will be relative to the\n            index of ``self.pos_label``.\n    \"\"\"\n    true_positives, false_positives, true_negatives, false_negatives = super().count_tp_fp_tn_fn(preds, targets)\n\n    true_positives = self._postprocess_count_tensor(true_positives)\n    false_positives = self._postprocess_count_tensor(false_positives)\n    true_negatives = self._postprocess_count_tensor(true_negatives)\n    false_negatives = self._postprocess_count_tensor(false_negatives)\n\n    # Need to flip the label interpretations\n    if self.pos_label == 0:\n        return true_negatives, false_negatives, true_positives, false_positives\n\n    return true_positives, false_positives, true_negatives, false_negatives\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.BinaryClassificationMetric.compute_from_counts","title":"<code>compute_from_counts(true_positives, false_positives, true_negatives, false_negatives)</code>","text":"<p>Provided tensors associated with the various outcomes from predictions compared to targets in the form of true positives, false positives, true negatives, and false negatives, returns a dictionary of <code>Scalar</code> metrics. For example, one might compute recall as <code>true_positives/(true_positives + false_negatives)</code>. The shape of  these tensors are specific to how this object is configured, see class documentation above.</p> <p>For this class it is assumed that all counts are presented relative to the class indicated by the <code>pos_label</code> index, counts for the negative label are discarded. Moreover, they are assumed to either have a shape <code>(1,)</code> or have shape <code>(num_samples, 1)</code> if <code>batch_dim</code> was specified. In the former, a single count is presented ACROSS all samples relative to the <code>pos_label</code> specified. In the latter, counts are computed WITHIN each sample, but held separate across samples. A concrete setting where this makes sense is binary image segmentation. You can have such counts summed for all pixels within an image, but separate per image. A metric could then be computed for each image and then averaged.</p> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>Tensor</code> <p>Counts associated with positive predictions and positive labels.</p> required <code>false_positives</code> <code>Tensor</code> <p>Counts associated with positive predictions and negative labels.</p> required <code>true_negatives</code> <code>Tensor</code> <p>Counts associated with negative predictions and negative labels.</p> required <code>false_negatives</code> <code>Tensor</code> <p>Counts associated with negative predictions and positive labels.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by the inheriting class.</p> <p>Returns:</p> Type Description <code>Metrics</code> <p>Metrics computed from the provided outcome counts.</p> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def compute_from_counts(\n    self,\n    true_positives: torch.Tensor,\n    false_positives: torch.Tensor,\n    true_negatives: torch.Tensor,\n    false_negatives: torch.Tensor,\n) -&gt; Metrics:\n    \"\"\"\n    Provided tensors associated with the various outcomes from predictions compared to targets in the form of\n    true positives, false positives, true negatives, and false negatives, returns a dictionary of ``Scalar``\n    metrics. For example, one might compute recall as ``true_positives/(true_positives + false_negatives)``. The\n    shape of  these tensors are specific to how this object is configured, see class documentation above.\n\n    For this class it is assumed that all counts are presented relative to the class indicated by the ``pos_label``\n    index, counts for the negative label are discarded. Moreover, they are assumed to either have a shape ``(1,)``\n    or have shape ``(num_samples, 1)`` if ``batch_dim`` was specified. In the former, a single count is\n    presented **ACROSS** all samples relative to the ``pos_label`` specified. In the latter, counts are computed\n    **WITHIN** each sample, but held separate across samples. A concrete setting where this makes sense is\n    binary image segmentation. You can have such counts summed for all pixels within an image, but separate per\n    image. A metric could then be computed for each image and then averaged.\n\n    Args:\n        true_positives (torch.Tensor): Counts associated with positive predictions and positive labels.\n        false_positives (torch.Tensor): Counts associated with positive predictions and negative labels.\n        true_negatives (torch.Tensor): Counts associated with negative predictions and negative labels.\n        false_negatives (torch.Tensor): Counts associated with negative predictions and positive labels.\n\n    Raises:\n        NotImplementedError: Must be implemented by the inheriting class.\n\n    Returns:\n        (Metrics): Metrics computed from the provided outcome counts.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.MultiClassificationMetric","title":"<code>MultiClassificationMetric</code>","text":"<p>               Bases: <code>ClassificationMetric</code></p> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>class MultiClassificationMetric(ClassificationMetric):\n    def __init__(\n        self,\n        name: str,\n        label_dim: int,\n        batch_dim: int | None = None,\n        dtype: torch.dtype = torch.float32,\n        threshold: float | int | None = None,\n        ignore_background: int | None = None,\n        discard: set[ClassificationOutcome] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        A Base class for multi-class, multi-label classification metrics that can be computed using the true\n        positives (tp), false positive (fp), false negative (fn) and true negative (tn) counts. These counts are\n        computed for each class independently. How they are composed together for the metric is left to inheriting\n        classes.\n\n        On each update, the ``true_positives``, ``false_positives``, ``false_negatives`` and ``true_negatives`` counts\n        for the provided predictions and targets are accumulated into ``self.true_positives``,\n        ``self.false_positives``, ``self.false_negatives`` and ``self.true_negatives``, respectively, for each label\n        type. This reduces the memory footprint required to compute metrics across rounds. The user needs to define the\n        ``compute_from_counts`` method which returns a dictionary of ``Scalar`` metrics given the ``true_positives``,\n        ``false_positives``, ``false_negatives``, and ``true_negatives`` counts. The accumulated counts are reset by\n        the ``clear`` method. If your subclass returns multiple metrics you may need to also override the ``__call__``\n        method.\n\n        If the predictions provided are continuous in value, then the associated counts will also be continuous\n        (\"soft\"). For example, with a target of 1, a prediction of 0.8 contributes 0.8 to the ``true_positives``\n        count and 0.2 to the ``false_negatives``.\n\n        **NOTE**: Preds and targets are expected to have elements in the interval ``[0, 1]`` or to be thresholded,\n        using that argument to be as such.\n\n        **NOTE**: If preds and targets passed to update method have different shapes, or end up with different shapes\n        after thresholding, this class will attempt to align the shapes by one-hot-encoding one of the tensors in the\n        label dimension, if possible.\n\n        Args:\n            name (str): The name of the metric.\n            label_dim (int): Specifies which dimension in the provided tensors corresponds to the label dimension.\n                During metric computation, this dimension must have size of **AT LEAST 2**. Counts are always computed\n                along the label dimension. That is, counts are maintained for each output label **INDIVIDUALLY**.\n            batch_dim (int | None, optional): If None, the counts along the specified dimension (i.e. for each sample)\n                are aggregated and the batch dimension is reduced. If specified, counts will be computed along the\n                dimension specified. That is, counts are maintained for each training sample **INDIVIDUALLY**.\n\n                **NOTE**: If ``batch_dim`` is specified, then counts will be presented batch dimension\n                first, then label dimension. For example, if ``batch_dim = 1`` and ``label_dim = 0``, then\n\n                ```python\n\n                p = torch.tensor([[[1.0, 1.0, 1.0, 0.0]], [[0.0, 0.0, 0.0, 1.0]]])  # Size([2, 1, 4])\n\n                t = torch.tensor([[[1.0, 1.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 1.0]]])  # Size([2, 1, 4])\n\n                self.tp = torch.Tensor([[2, 1]]]) # Size([1, 2])\n\n                self.tn = torch.Tensor([[1, 2]])  # Size([1, 2])\n\n                self.fp = torch.Tensor([[1, 0]])  # Size([1, 2])\n\n                self.fn = torch.Tensor([[0, 1]])  # Size([1, 2])\n\n                ```\n\n                **NOTE**: The resulting counts will always be presented batch dimension first, then label dimension,\n                regardless of input shape. Defaults to None.\n            dtype (torch.dtype): The dtype to store the counts as. If preds or targets can be continuous, specify a\n                float type. Otherwise specify an integer type to prevent overflow. Defaults to ``torch.float32``.\n            threshold (float | int | None, optional): A float for thresholding values or an integer specifying the\n                index of the label dimension. If a float is given, predictions below the threshold are mapped\n                to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class\n                with the highest prediction where the specified axis is assumed to contain a prediction for each class\n                (where its index along that dimension is the class label). Value of None leaves preds unchanged.\n                Defaults to None.\n            ignore_background (int | None): If specified, the **FIRST** channel of the specified axis is removed prior\n                to computing the counts. Useful for removing background classes. Defaults to None.\n            discard (set[ClassificationOutcome] | None, optional): One or several of ``ClassificationOutcome`` values.\n                Specified outcome counts will not be accumulated. Their associated attribute will remain as an empty\n                pytorch tensor. Useful for reducing the memory footprint of metrics that do not use all of the counts\n                in their computation.\n        \"\"\"\n        super().__init__(\n            name=name,\n            dtype=dtype,\n            label_dim=label_dim,\n            batch_dim=batch_dim,\n            threshold=threshold,\n            discard=discard,\n        )\n        if ignore_background is not None:\n            log(\n                INFO,\n                f\"ignore_background has been specified. The first channel of dimension {ignore_background} \"\n                \"will be removed from both predictions and targets\",\n            )\n        self.ignore_background = ignore_background\n\n    @classmethod\n    def _remove_background(\n        cls, ignore_background: int, preds: torch.Tensor, targets: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Removes the first element (channel) from the dimension specified in ``ignore_background`` for both the preds\n        and targets tensors. For example, if preds and targets have shape ``(64, 10, 10, 3)`` and\n        ``ignore_background`` is 3, then after removing the specified background, the shapes will be\n        ``(64, 10, 10, 2)``.\n\n        The tensors should have the same shape before and after removing the background.\n\n        Args:\n            ignore_background (int): Which dimension should have the first element (channel) removed.\n            preds (torch.Tensor): predictions tensor.\n            targets (torch.Tensor): targets tensor.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): predictions and targets tensor with the appropriate background\n                removed.\n        \"\"\"\n        assert preds.shape == targets.shape, (\n            f\"Preds ({preds.shape}) and targets ({targets.shape}) should have the same shape but do not.\"\n        )\n\n        indices = torch.arange(1, preds.shape[ignore_background], device=preds.device)\n        preds = torch.index_select(preds, ignore_background, indices)\n        targets = torch.index_select(targets, ignore_background, indices.to(targets.device))\n        return preds, targets\n\n    def _transform_tensors(self, preds: torch.Tensor, targets: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Given the predictions and targets this function performs a few possible transformations. The first is to map\n        boolean tensors to integers for computation. The second is to potentially threshold the predictions if\n        ``self.threshold`` is not None. Both are facilitated by the base class.\n\n        Thereafter, we attempt to align the tensors if they aren't already of the same shape. This is done by\n        attempting to expand the label encodings for class indices to one-hot vectors. See documentation of\n        ``align_pred_and_target_shapes`` for more details.\n\n        As a last possible transformation, the background is removed if ``self.ignore_background`` is defined.\n\n        Args:\n            preds (torch.Tensor): Predictions tensor.\n            targets (torch.Tensor): Targets tensor.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Potentially transformed predictions and targets tensors, in that\n                order.\n        \"\"\"\n        preds, targets = super()._transform_tensors(preds, targets)\n\n        # Attempt to automatically match pred and target shape.\n        # This supports any combination of hard/soft, vector/not-vector encoded\n        preds, targets = align_pred_and_target_shapes(preds, targets, self.label_dim)\n\n        # Remove the background channel from the axis specified by ignore_background_axis\n        if self.ignore_background is not None:\n            preds, targets = self._remove_background(self.ignore_background, preds, targets)\n\n        return preds, targets\n\n    def _assert_correct_ranges_and_shape(self, preds: torch.Tensor, targets: torch.Tensor) -&gt; None:\n        \"\"\"\n        Ensures that the prediction and target tensors are in the expected form for computation.\n\n        This class overrides this method to ensure that the label dimension has size of at least 2.\n\n        Args:\n            preds (torch.Tensor): Predictions tensor.\n            targets (torch.Tensor): Targets tensor.\n        \"\"\"\n        super()._assert_correct_ranges_and_shape(preds, targets)\n\n        # Assert that the label dimension for these tensors is not of size 1. This occurs either when considering\n        # binary predictions or when both the preds and targets are label index encoded, which is not admissible for\n        # this function\n        assert self.label_dim is not None\n        assert preds.shape[self.label_dim] &gt;= N_LABELS_BINARY, (\n            \"Label dimension for preds tensor is less than 2. Either your label dimension is a single float value \"\n            \"corresponding to a binary prediction or it is a class label that needs to be vector encoded.\"\n        )\n        assert targets.shape[self.label_dim] &gt;= N_LABELS_BINARY, (\n            \"Label dimension for targets tensor is less than 2. Either your label dimension is a single float value \"\n            \"corresponding to a binary prediction or it is a class label that needs to be vector encoded.\"\n        )\n\n    def compute_from_counts(\n        self,\n        true_positives: torch.Tensor,\n        false_positives: torch.Tensor,\n        true_negatives: torch.Tensor,\n        false_negatives: torch.Tensor,\n    ) -&gt; Metrics:\n        \"\"\"\n        Provided tensors associated with the various outcomes from predictions compared to targets in the form of\n        true positives, false positives, true negatives, and false negatives, returns a dictionary of ``Scalar``\n        metrics. For example, one might compute recall as ``true_positives/(true_positives + false_negatives)``. The\n        shape of these tensors is The shape of these tensors are specific to how this object is configured, see class\n        documentation above.\n\n        For this class, counts are assumed to have shape ``(num_labels,)`` or ``(num_samples, num_labels)``. In the\n        former, counts have been aggregated **ACROSS** samples into single count values for each possible label. In\n        the later, counts have been aggregated **WITHIN** each sample and remain separate across examples. A concrete\n        setting where this makes sense is image segmentation. You can have such counts summed for all pixels within an\n        image, but separate per image. A metric could then be computed for each image and then averaged.\n\n        **NOTE**: A user can implement further reduction along the label dimension (summing TPs across labels for\n        example), if desired. It just needs to be handled in the implementation of this function.\n\n        Args:\n            true_positives (torch.Tensor): Counts associated with positive predictions of a class and true positives\n                for that class.\n            false_positives (torch.Tensor): Counts associated with positive predictions of a class and true negatives\n                for that class.\n            true_negatives (torch.Tensor): Counts associated with negative predictions of a class and true negatives\n                for that class.\n            false_negatives (torch.Tensor): Counts associated with negative predictions of a class and true positives\n                for that class.\n\n        Raises:\n            NotImplementedError: Must be implemented by the inheriting class.\n\n        Returns:\n            (Metrics): Metrics computed from the provided outcome counts.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.MultiClassificationMetric.__init__","title":"<code>__init__(name, label_dim, batch_dim=None, dtype=torch.float32, threshold=None, ignore_background=None, discard=None)</code>","text":"<p>A Base class for multi-class, multi-label classification metrics that can be computed using the true positives (tp), false positive (fp), false negative (fn) and true negative (tn) counts. These counts are computed for each class independently. How they are composed together for the metric is left to inheriting classes.</p> <p>On each update, the <code>true_positives</code>, <code>false_positives</code>, <code>false_negatives</code> and <code>true_negatives</code> counts for the provided predictions and targets are accumulated into <code>self.true_positives</code>, <code>self.false_positives</code>, <code>self.false_negatives</code> and <code>self.true_negatives</code>, respectively, for each label type. This reduces the memory footprint required to compute metrics across rounds. The user needs to define the <code>compute_from_counts</code> method which returns a dictionary of <code>Scalar</code> metrics given the <code>true_positives</code>, <code>false_positives</code>, <code>false_negatives</code>, and <code>true_negatives</code> counts. The accumulated counts are reset by the <code>clear</code> method. If your subclass returns multiple metrics you may need to also override the <code>__call__</code> method.</p> <p>If the predictions provided are continuous in value, then the associated counts will also be continuous (\"soft\"). For example, with a target of 1, a prediction of 0.8 contributes 0.8 to the <code>true_positives</code> count and 0.2 to the <code>false_negatives</code>.</p> <p>NOTE: Preds and targets are expected to have elements in the interval <code>[0, 1]</code> or to be thresholded, using that argument to be as such.</p> <p>NOTE: If preds and targets passed to update method have different shapes, or end up with different shapes after thresholding, this class will attempt to align the shapes by one-hot-encoding one of the tensors in the label dimension, if possible.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> required <code>label_dim</code> <code>int</code> <p>Specifies which dimension in the provided tensors corresponds to the label dimension. During metric computation, this dimension must have size of AT LEAST 2. Counts are always computed along the label dimension. That is, counts are maintained for each output label INDIVIDUALLY.</p> required <code>batch_dim</code> <code>int | None</code> <p>If None, the counts along the specified dimension (i.e. for each sample) are aggregated and the batch dimension is reduced. If specified, counts will be computed along the dimension specified. That is, counts are maintained for each training sample INDIVIDUALLY.</p> <p>NOTE: If <code>batch_dim</code> is specified, then counts will be presented batch dimension first, then label dimension. For example, if <code>batch_dim = 1</code> and <code>label_dim = 0</code>, then</p> <pre><code>p = torch.tensor([[[1.0, 1.0, 1.0, 0.0]], [[0.0, 0.0, 0.0, 1.0]]])  # Size([2, 1, 4])\n\nt = torch.tensor([[[1.0, 1.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 1.0]]])  # Size([2, 1, 4])\n\nself.tp = torch.Tensor([[2, 1]]]) # Size([1, 2])\n\nself.tn = torch.Tensor([[1, 2]])  # Size([1, 2])\n\nself.fp = torch.Tensor([[1, 0]])  # Size([1, 2])\n\nself.fn = torch.Tensor([[0, 1]])  # Size([1, 2])\n</code></pre> <p>NOTE: The resulting counts will always be presented batch dimension first, then label dimension, regardless of input shape. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>The dtype to store the counts as. If preds or targets can be continuous, specify a float type. Otherwise specify an integer type to prevent overflow. Defaults to <code>torch.float32</code>.</p> <code>float32</code> <code>threshold</code> <code>float | int | None</code> <p>A float for thresholding values or an integer specifying the index of the label dimension. If a float is given, predictions below the threshold are mapped to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class with the highest prediction where the specified axis is assumed to contain a prediction for each class (where its index along that dimension is the class label). Value of None leaves preds unchanged. Defaults to None.</p> <code>None</code> <code>ignore_background</code> <code>int | None</code> <p>If specified, the FIRST channel of the specified axis is removed prior to computing the counts. Useful for removing background classes. Defaults to None.</p> <code>None</code> <code>discard</code> <code>set[ClassificationOutcome] | None</code> <p>One or several of <code>ClassificationOutcome</code> values. Specified outcome counts will not be accumulated. Their associated attribute will remain as an empty pytorch tensor. Useful for reducing the memory footprint of metrics that do not use all of the counts in their computation.</p> <code>None</code> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    label_dim: int,\n    batch_dim: int | None = None,\n    dtype: torch.dtype = torch.float32,\n    threshold: float | int | None = None,\n    ignore_background: int | None = None,\n    discard: set[ClassificationOutcome] | None = None,\n) -&gt; None:\n    \"\"\"\n    A Base class for multi-class, multi-label classification metrics that can be computed using the true\n    positives (tp), false positive (fp), false negative (fn) and true negative (tn) counts. These counts are\n    computed for each class independently. How they are composed together for the metric is left to inheriting\n    classes.\n\n    On each update, the ``true_positives``, ``false_positives``, ``false_negatives`` and ``true_negatives`` counts\n    for the provided predictions and targets are accumulated into ``self.true_positives``,\n    ``self.false_positives``, ``self.false_negatives`` and ``self.true_negatives``, respectively, for each label\n    type. This reduces the memory footprint required to compute metrics across rounds. The user needs to define the\n    ``compute_from_counts`` method which returns a dictionary of ``Scalar`` metrics given the ``true_positives``,\n    ``false_positives``, ``false_negatives``, and ``true_negatives`` counts. The accumulated counts are reset by\n    the ``clear`` method. If your subclass returns multiple metrics you may need to also override the ``__call__``\n    method.\n\n    If the predictions provided are continuous in value, then the associated counts will also be continuous\n    (\"soft\"). For example, with a target of 1, a prediction of 0.8 contributes 0.8 to the ``true_positives``\n    count and 0.2 to the ``false_negatives``.\n\n    **NOTE**: Preds and targets are expected to have elements in the interval ``[0, 1]`` or to be thresholded,\n    using that argument to be as such.\n\n    **NOTE**: If preds and targets passed to update method have different shapes, or end up with different shapes\n    after thresholding, this class will attempt to align the shapes by one-hot-encoding one of the tensors in the\n    label dimension, if possible.\n\n    Args:\n        name (str): The name of the metric.\n        label_dim (int): Specifies which dimension in the provided tensors corresponds to the label dimension.\n            During metric computation, this dimension must have size of **AT LEAST 2**. Counts are always computed\n            along the label dimension. That is, counts are maintained for each output label **INDIVIDUALLY**.\n        batch_dim (int | None, optional): If None, the counts along the specified dimension (i.e. for each sample)\n            are aggregated and the batch dimension is reduced. If specified, counts will be computed along the\n            dimension specified. That is, counts are maintained for each training sample **INDIVIDUALLY**.\n\n            **NOTE**: If ``batch_dim`` is specified, then counts will be presented batch dimension\n            first, then label dimension. For example, if ``batch_dim = 1`` and ``label_dim = 0``, then\n\n            ```python\n\n            p = torch.tensor([[[1.0, 1.0, 1.0, 0.0]], [[0.0, 0.0, 0.0, 1.0]]])  # Size([2, 1, 4])\n\n            t = torch.tensor([[[1.0, 1.0, 0.0, 0.0]], [[0.0, 0.0, 1.0, 1.0]]])  # Size([2, 1, 4])\n\n            self.tp = torch.Tensor([[2, 1]]]) # Size([1, 2])\n\n            self.tn = torch.Tensor([[1, 2]])  # Size([1, 2])\n\n            self.fp = torch.Tensor([[1, 0]])  # Size([1, 2])\n\n            self.fn = torch.Tensor([[0, 1]])  # Size([1, 2])\n\n            ```\n\n            **NOTE**: The resulting counts will always be presented batch dimension first, then label dimension,\n            regardless of input shape. Defaults to None.\n        dtype (torch.dtype): The dtype to store the counts as. If preds or targets can be continuous, specify a\n            float type. Otherwise specify an integer type to prevent overflow. Defaults to ``torch.float32``.\n        threshold (float | int | None, optional): A float for thresholding values or an integer specifying the\n            index of the label dimension. If a float is given, predictions below the threshold are mapped\n            to 0 and above are mapped to 1. If an integer is given, predictions are binarized based on the class\n            with the highest prediction where the specified axis is assumed to contain a prediction for each class\n            (where its index along that dimension is the class label). Value of None leaves preds unchanged.\n            Defaults to None.\n        ignore_background (int | None): If specified, the **FIRST** channel of the specified axis is removed prior\n            to computing the counts. Useful for removing background classes. Defaults to None.\n        discard (set[ClassificationOutcome] | None, optional): One or several of ``ClassificationOutcome`` values.\n            Specified outcome counts will not be accumulated. Their associated attribute will remain as an empty\n            pytorch tensor. Useful for reducing the memory footprint of metrics that do not use all of the counts\n            in their computation.\n    \"\"\"\n    super().__init__(\n        name=name,\n        dtype=dtype,\n        label_dim=label_dim,\n        batch_dim=batch_dim,\n        threshold=threshold,\n        discard=discard,\n    )\n    if ignore_background is not None:\n        log(\n            INFO,\n            f\"ignore_background has been specified. The first channel of dimension {ignore_background} \"\n            \"will be removed from both predictions and targets\",\n        )\n    self.ignore_background = ignore_background\n</code></pre>"},{"location":"api/#fl4health.metrics.efficient_metrics_base.MultiClassificationMetric.compute_from_counts","title":"<code>compute_from_counts(true_positives, false_positives, true_negatives, false_negatives)</code>","text":"<p>Provided tensors associated with the various outcomes from predictions compared to targets in the form of true positives, false positives, true negatives, and false negatives, returns a dictionary of <code>Scalar</code> metrics. For example, one might compute recall as <code>true_positives/(true_positives + false_negatives)</code>. The shape of these tensors is The shape of these tensors are specific to how this object is configured, see class documentation above.</p> <p>For this class, counts are assumed to have shape <code>(num_labels,)</code> or <code>(num_samples, num_labels)</code>. In the former, counts have been aggregated ACROSS samples into single count values for each possible label. In the later, counts have been aggregated WITHIN each sample and remain separate across examples. A concrete setting where this makes sense is image segmentation. You can have such counts summed for all pixels within an image, but separate per image. A metric could then be computed for each image and then averaged.</p> <p>NOTE: A user can implement further reduction along the label dimension (summing TPs across labels for example), if desired. It just needs to be handled in the implementation of this function.</p> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>Tensor</code> <p>Counts associated with positive predictions of a class and true positives for that class.</p> required <code>false_positives</code> <code>Tensor</code> <p>Counts associated with positive predictions of a class and true negatives for that class.</p> required <code>true_negatives</code> <code>Tensor</code> <p>Counts associated with negative predictions of a class and true negatives for that class.</p> required <code>false_negatives</code> <code>Tensor</code> <p>Counts associated with negative predictions of a class and true positives for that class.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by the inheriting class.</p> <p>Returns:</p> Type Description <code>Metrics</code> <p>Metrics computed from the provided outcome counts.</p> Source code in <code>fl4health/metrics/efficient_metrics_base.py</code> <pre><code>def compute_from_counts(\n    self,\n    true_positives: torch.Tensor,\n    false_positives: torch.Tensor,\n    true_negatives: torch.Tensor,\n    false_negatives: torch.Tensor,\n) -&gt; Metrics:\n    \"\"\"\n    Provided tensors associated with the various outcomes from predictions compared to targets in the form of\n    true positives, false positives, true negatives, and false negatives, returns a dictionary of ``Scalar``\n    metrics. For example, one might compute recall as ``true_positives/(true_positives + false_negatives)``. The\n    shape of these tensors is The shape of these tensors are specific to how this object is configured, see class\n    documentation above.\n\n    For this class, counts are assumed to have shape ``(num_labels,)`` or ``(num_samples, num_labels)``. In the\n    former, counts have been aggregated **ACROSS** samples into single count values for each possible label. In\n    the later, counts have been aggregated **WITHIN** each sample and remain separate across examples. A concrete\n    setting where this makes sense is image segmentation. You can have such counts summed for all pixels within an\n    image, but separate per image. A metric could then be computed for each image and then averaged.\n\n    **NOTE**: A user can implement further reduction along the label dimension (summing TPs across labels for\n    example), if desired. It just needs to be handled in the implementation of this function.\n\n    Args:\n        true_positives (torch.Tensor): Counts associated with positive predictions of a class and true positives\n            for that class.\n        false_positives (torch.Tensor): Counts associated with positive predictions of a class and true negatives\n            for that class.\n        true_negatives (torch.Tensor): Counts associated with negative predictions of a class and true negatives\n            for that class.\n        false_negatives (torch.Tensor): Counts associated with negative predictions of a class and true positives\n            for that class.\n\n    Raises:\n        NotImplementedError: Must be implemented by the inheriting class.\n\n    Returns:\n        (Metrics): Metrics computed from the provided outcome counts.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_aggregation","title":"<code>metric_aggregation</code>","text":""},{"location":"api/#fl4health.metrics.metric_aggregation.uniform_metric_aggregation","title":"<code>uniform_metric_aggregation(all_client_metrics)</code>","text":"<p>Function that aggregates client metrics and divides by the number of clients that contributed to metric.</p> <p>Parameters:</p> Name Type Description Default <code>all_client_metrics</code> <code>list[tuple[int, Metrics]]</code> <p>A list of tuples with the sample counts and metrics for each client.</p> required <p>Returns:</p> Type Description <code>tuple[defaultdict[str, int], Metrics]</code> <p>Client counts per metric and the uniformly aggregated metrics.</p> Source code in <code>fl4health/metrics/metric_aggregation.py</code> <pre><code>def uniform_metric_aggregation(\n    all_client_metrics: list[tuple[int, Metrics]],\n) -&gt; tuple[defaultdict[str, int], Metrics]:\n    \"\"\"\n    Function that aggregates client metrics and divides by the number of clients that contributed to metric.\n\n    Args:\n        all_client_metrics (list[tuple[int, Metrics]]): A list of tuples with the sample counts and metrics for each\n            client.\n\n    Returns:\n        (tuple[defaultdict[str, int], Metrics]): Client counts per metric and the uniformly aggregated metrics.\n    \"\"\"\n    aggregated_metrics: Metrics = {}\n    total_client_count_by_metric: defaultdict[str, int] = defaultdict(int)\n    # Run through all of the metrics\n    for _, client_metrics in all_client_metrics:\n        for metric_name, metric_value in client_metrics.items():\n            if isinstance(metric_value, float):\n                current_metric_value = aggregated_metrics.get(metric_name, 0.0)\n                assert isinstance(current_metric_value, float)\n                aggregated_metrics[metric_name] = current_metric_value + metric_value\n                total_client_count_by_metric[metric_name] += 1\n            elif isinstance(metric_value, int):\n                current_metric_value = aggregated_metrics.get(metric_name, 0)\n                assert isinstance(current_metric_value, int)\n                aggregated_metrics[metric_name] = current_metric_value + metric_value\n                total_client_count_by_metric[metric_name] += 1\n            else:\n                raise ValueError(\"Metric type is not supported\")\n    # Compute average of each metric by dividing by number of clients contributing\n    uniform_normalize_metrics(total_client_count_by_metric, aggregated_metrics)\n    return total_client_count_by_metric, aggregated_metrics\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_aggregation.metric_aggregation","title":"<code>metric_aggregation(all_client_metrics)</code>","text":"<p>Function that computes a weighted aggregation of metrics normalized by the total number of samples.</p> <p>Parameters:</p> Name Type Description Default <code>all_client_metrics</code> <code>list[tuple[int, Metrics]]</code> <p>A list of tuples with the sample counts and metrics for each client.</p> required <p>Returns:</p> Type Description <code>tuple[int, Metrics]</code> <p>The total number of examples along with aggregated metrics.</p> Source code in <code>fl4health/metrics/metric_aggregation.py</code> <pre><code>def metric_aggregation(\n    all_client_metrics: list[tuple[int, Metrics]],\n) -&gt; tuple[int, Metrics]:\n    \"\"\"\n    Function that computes a weighted aggregation of metrics normalized by the total number of samples.\n\n    Args:\n        all_client_metrics (list[tuple[int, Metrics]]): A list of tuples with the sample counts and metrics for each\n            client.\n\n    Returns:\n        (tuple[int, Metrics]): The total number of examples along with aggregated metrics.\n    \"\"\"\n    aggregated_metrics: Metrics = {}\n    total_examples = 0\n    # Run through all of the metrics\n    for num_examples_on_client, client_metrics in all_client_metrics:\n        total_examples += num_examples_on_client\n        for metric_name, metric_value in client_metrics.items():\n            # Here we assume each metric is normalized by the number of examples on the client. So we scale up to\n            # get the \"raw\" value\n            if isinstance(metric_value, float):\n                current_metric_value = aggregated_metrics.get(metric_name, 0.0)\n                assert isinstance(current_metric_value, float)\n                aggregated_metrics[metric_name] = current_metric_value + num_examples_on_client * metric_value\n            elif isinstance(metric_value, int):\n                current_metric_value = aggregated_metrics.get(metric_name, 0)\n                assert isinstance(current_metric_value, int)\n                aggregated_metrics[metric_name] = current_metric_value + num_examples_on_client * metric_value\n            else:\n                raise ValueError(\"Metric type is not supported\")\n    return total_examples, aggregated_metrics\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_aggregation.normalize_metrics","title":"<code>normalize_metrics(total_examples, aggregated_metrics)</code>","text":"<p>Function that normalizes metrics by provided sample count.</p> <p>Parameters:</p> Name Type Description Default <code>total_examples</code> <code>int</code> <p>The total number of samples across all client datasets.</p> required <code>aggregated_metrics</code> <code>Metrics</code> <p>Metrics that have been aggregated across clients.</p> required <p>Returns:</p> Type Description <code>Metrics</code> <p>The metrics normalized by <code>total_examples</code>.</p> Source code in <code>fl4health/metrics/metric_aggregation.py</code> <pre><code>def normalize_metrics(total_examples: int, aggregated_metrics: Metrics) -&gt; Metrics:\n    \"\"\"\n    Function that normalizes metrics by provided sample count.\n\n    Args:\n        total_examples (int): The total number of samples across all client datasets.\n        aggregated_metrics (Metrics): Metrics that have been aggregated across clients.\n\n    Returns:\n        (Metrics): The metrics normalized by ``total_examples``.\n    \"\"\"\n    # Normalize all metric values by the total count of examples seen.\n    normalized_metrics: Metrics = {}\n    for metric_name, metric_value in aggregated_metrics.items():\n        if isinstance(metric_value, (float, int)):\n            normalized_metrics[metric_name] = metric_value / total_examples\n    return normalized_metrics\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_aggregation.uniform_normalize_metrics","title":"<code>uniform_normalize_metrics(total_client_count_by_metric, aggregated_metrics)</code>","text":"<p>Function that normalizes metrics based on how many clients contributed to the metric.</p> <p>Parameters:</p> Name Type Description Default <code>total_client_count_by_metric</code> <code>defaultdict[str, int]</code> <p>The count of clients that contributed to each metric.</p> required <code>aggregated_metrics</code> <code>Metrics</code> <p>Metrics that have been aggregated across clients.</p> required <p>Returns:</p> Type Description <code>Metrics</code> <p>The normalized metrics.</p> Source code in <code>fl4health/metrics/metric_aggregation.py</code> <pre><code>def uniform_normalize_metrics(\n    total_client_count_by_metric: defaultdict[str, int], aggregated_metrics: Metrics\n) -&gt; Metrics:\n    \"\"\"\n    Function that normalizes metrics based on how many clients contributed to the metric.\n\n    Args:\n        total_client_count_by_metric (defaultdict[str, int]): The count of clients that contributed to each metric.\n        aggregated_metrics (Metrics): Metrics that have been aggregated across clients.\n\n    Returns:\n        (Metrics): The normalized metrics.\n    \"\"\"\n    # Normalize all metric values by the total count of clients that contributed to the metric.\n    normalized_metrics: Metrics = {}\n    for metric_name, metric_value in aggregated_metrics.items():\n        if isinstance(metric_value, (float, int)):\n            normalized_metrics[metric_name] = metric_value / total_client_count_by_metric[metric_name]\n    return normalized_metrics\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_aggregation.fit_metrics_aggregation_fn","title":"<code>fit_metrics_aggregation_fn(all_client_metrics)</code>","text":"<p>Function for fit that computes a weighted aggregation of the client metrics and normalizes by the total number of samples.</p> <p>Parameters:</p> Name Type Description Default <code>all_client_metrics</code> <code>list[tuple[int, Metrics]]</code> <p>A list of tuples with the sample counts and metrics for each client.</p> required <p>Returns:</p> Type Description <code>Metrics</code> <p>The aggregated normalized metrics.</p> Source code in <code>fl4health/metrics/metric_aggregation.py</code> <pre><code>def fit_metrics_aggregation_fn(\n    all_client_metrics: list[tuple[int, Metrics]],\n) -&gt; Metrics:\n    \"\"\"\n    Function for fit that computes a weighted aggregation of the client metrics and normalizes by the total number of\n    samples.\n\n    Args:\n        all_client_metrics (list[tuple[int, Metrics]]): A list of tuples with the sample counts and metrics for each\n            client.\n\n    Returns:\n        (Metrics): The aggregated normalized metrics.\n    \"\"\"\n    # This function is run by the server to aggregate metrics returned by each clients fit function\n    # NOTE: The first value of the tuple is number of examples for FedAvg\n    total_examples, aggregated_metrics = metric_aggregation(all_client_metrics)\n    return normalize_metrics(total_examples, aggregated_metrics)\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_aggregation.evaluate_metrics_aggregation_fn","title":"<code>evaluate_metrics_aggregation_fn(all_client_metrics)</code>","text":"<p>Function for evaluate that computes a weighted aggregation of the client metrics and normalizes by the total number of samples.</p> <p>Parameters:</p> Name Type Description Default <code>all_client_metrics</code> <code>list[tuple[int, Metrics]]</code> <p>A list of tuples with the sample counts and metrics for each client.</p> required <p>Returns:</p> Type Description <code>Metrics</code> <p>The aggregated normalized metrics.</p> Source code in <code>fl4health/metrics/metric_aggregation.py</code> <pre><code>def evaluate_metrics_aggregation_fn(\n    all_client_metrics: list[tuple[int, Metrics]],\n) -&gt; Metrics:\n    \"\"\"\n    Function for evaluate that computes a weighted aggregation of the client metrics and normalizes by the total\n    number of samples.\n\n    Args:\n        all_client_metrics (list[tuple[int, Metrics]]): A list of tuples with the sample counts and metrics for each\n            client.\n\n    Returns:\n        (Metrics): The aggregated normalized metrics.\n    \"\"\"\n    # This function is run by the server to aggregate metrics returned by each clients evaluate function\n    # NOTE: The first value of the tuple is number of examples for FedAvg\n    total_examples, aggregated_metrics = metric_aggregation(all_client_metrics)\n    return normalize_metrics(total_examples, aggregated_metrics)\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_aggregation.uniform_evaluate_metrics_aggregation_fn","title":"<code>uniform_evaluate_metrics_aggregation_fn(all_client_metrics)</code>","text":"<p>Function for evaluate that computes aggregation of the client metrics and normalizes by the number of clients that contributed to the metric.</p> <p>Parameters:</p> Name Type Description Default <code>all_client_metrics</code> <code>list[tuple[int, Metrics]]</code> <p>A list of tuples with the sample counts and metrics for each client.</p> required <p>Returns:</p> Type Description <code>Metrics</code> <p>The aggregated normalized metrics.</p> Source code in <code>fl4health/metrics/metric_aggregation.py</code> <pre><code>def uniform_evaluate_metrics_aggregation_fn(\n    all_client_metrics: list[tuple[int, Metrics]],\n) -&gt; Metrics:\n    \"\"\"\n    Function for evaluate that computes aggregation of the client metrics and normalizes by the number of clients that\n    contributed to the metric.\n\n    Args:\n        all_client_metrics (list[tuple[int, Metrics]]): A list of tuples with the sample counts and metrics for each\n            client.\n\n    Returns:\n        (Metrics): The aggregated normalized metrics.\n    \"\"\"\n    # This function is run by the server to aggregate metrics returned by each clients evaluate function\n    # NOTE: The first value of the tuple is number of examples for FedAvg, but it is not used here.\n    total_client_count_by_metric, aggregated_metrics = uniform_metric_aggregation(all_client_metrics)\n    return uniform_normalize_metrics(total_client_count_by_metric, aggregated_metrics)\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_managers","title":"<code>metric_managers</code>","text":""},{"location":"api/#fl4health.metrics.metric_managers.MetricManager","title":"<code>MetricManager</code>","text":"Source code in <code>fl4health/metrics/metric_managers.py</code> <pre><code>class MetricManager:\n    def __init__(self, metrics: Sequence[Metric], metric_manager_name: str) -&gt; None:\n        \"\"\"\n        Class to manage a set of metrics associated to a given prediction type.\n\n        Args:\n            metrics (Sequence[Metric]): List of metric to evaluate predictions on.\n            metric_manager_name (str): Name of the metric manager (i.e. train, val, test)\n        \"\"\"\n        self.original_metrics = metrics\n        self.metric_manager_name = metric_manager_name\n        self.metrics_per_prediction_type: dict[str, Sequence[Metric]] = {}\n\n    def update(self, preds: TorchPredType, target: TorchTargetType) -&gt; None:\n        \"\"\"\n        Updates (or creates then updates) a list of metrics for each prediction type.\n\n        Args:\n            preds (TorchPredType): A dictionary of preds from the model.\n            target (TorchTargetType): The ground truth labels for the data. If target is a dictionary with more than\n                one item, then each value in the preds dictionary is evaluated with the value that has the same key in\n                the target dictionary. If target has only one item or is a ``torch.Tensor``, then the same target is\n                used for all predictions.\n        \"\"\"\n        if not self.metrics_per_prediction_type:\n            self.metrics_per_prediction_type = {key: copy.deepcopy(self.original_metrics) for key in preds}\n\n        # Check if there are multiple targets\n        if isinstance(target, dict):\n            if len(target.keys()) &gt; 1:\n                self.check_target_prediction_keys_equal(preds, target)\n            else:  # There is only one target, get tensor from dict\n                target = list(target.values())[0]\n        for prediction_key, pred in preds.items():\n            metrics_for_prediction_type = self.metrics_per_prediction_type[prediction_key]\n            assert len(preds) == len(self.metrics_per_prediction_type)\n            for metric_for_prediction_type in metrics_for_prediction_type:\n                if isinstance(target, torch.Tensor):\n                    metric_for_prediction_type.update(pred, target)\n                else:\n                    metric_for_prediction_type.update(pred, target[prediction_key])\n\n    def compute(self) -&gt; Metrics:\n        \"\"\"\n        Computes set of metrics for each prediction type.\n\n        Returns:\n            (Metrics): dictionary containing computed metrics along with string identifiers for each prediction type.\n        \"\"\"\n        all_results = {}\n        for metrics_key, metrics in self.metrics_per_prediction_type.items():\n            for metric in metrics:\n                result = metric.compute(f\"{self.metric_manager_name} - {metrics_key}\")\n                all_results.update(result)\n\n        return all_results\n\n    def clear(self) -&gt; None:\n        \"\"\"Clears data accumulated in each metric for each of the prediction type.\"\"\"\n        for metrics_for_prediction_type in self.metrics_per_prediction_type.values():\n            for metric in metrics_for_prediction_type:\n                metric.clear()\n\n    def reset(self) -&gt; None:\n        \"\"\"Resets the metrics to their initial state.\"\"\"\n        # On next update, metrics will be recopied from self.original_metrics which are still in their initial state\n        self.metrics_per_prediction_type = {}\n\n    def check_target_prediction_keys_equal(\n        self, preds: dict[str, torch.Tensor], target: dict[str, torch.Tensor]\n    ) -&gt; None:\n        assert target.keys() == preds.keys(), (\n            \"Received a dict with multiple targets, but the keys of the \"\n            \"targets do not match the keys of the predictions. Please pass a \"\n            \"single target or ensure the keys between preds and target are the same\"\n        )\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_managers.MetricManager.__init__","title":"<code>__init__(metrics, metric_manager_name)</code>","text":"<p>Class to manage a set of metrics associated to a given prediction type.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>Sequence[Metric]</code> <p>List of metric to evaluate predictions on.</p> required <code>metric_manager_name</code> <code>str</code> <p>Name of the metric manager (i.e. train, val, test)</p> required Source code in <code>fl4health/metrics/metric_managers.py</code> <pre><code>def __init__(self, metrics: Sequence[Metric], metric_manager_name: str) -&gt; None:\n    \"\"\"\n    Class to manage a set of metrics associated to a given prediction type.\n\n    Args:\n        metrics (Sequence[Metric]): List of metric to evaluate predictions on.\n        metric_manager_name (str): Name of the metric manager (i.e. train, val, test)\n    \"\"\"\n    self.original_metrics = metrics\n    self.metric_manager_name = metric_manager_name\n    self.metrics_per_prediction_type: dict[str, Sequence[Metric]] = {}\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_managers.MetricManager.update","title":"<code>update(preds, target)</code>","text":"<p>Updates (or creates then updates) a list of metrics for each prediction type.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>A dictionary of preds from the model.</p> required <code>target</code> <code>TorchTargetType</code> <p>The ground truth labels for the data. If target is a dictionary with more than one item, then each value in the preds dictionary is evaluated with the value that has the same key in the target dictionary. If target has only one item or is a <code>torch.Tensor</code>, then the same target is used for all predictions.</p> required Source code in <code>fl4health/metrics/metric_managers.py</code> <pre><code>def update(self, preds: TorchPredType, target: TorchTargetType) -&gt; None:\n    \"\"\"\n    Updates (or creates then updates) a list of metrics for each prediction type.\n\n    Args:\n        preds (TorchPredType): A dictionary of preds from the model.\n        target (TorchTargetType): The ground truth labels for the data. If target is a dictionary with more than\n            one item, then each value in the preds dictionary is evaluated with the value that has the same key in\n            the target dictionary. If target has only one item or is a ``torch.Tensor``, then the same target is\n            used for all predictions.\n    \"\"\"\n    if not self.metrics_per_prediction_type:\n        self.metrics_per_prediction_type = {key: copy.deepcopy(self.original_metrics) for key in preds}\n\n    # Check if there are multiple targets\n    if isinstance(target, dict):\n        if len(target.keys()) &gt; 1:\n            self.check_target_prediction_keys_equal(preds, target)\n        else:  # There is only one target, get tensor from dict\n            target = list(target.values())[0]\n    for prediction_key, pred in preds.items():\n        metrics_for_prediction_type = self.metrics_per_prediction_type[prediction_key]\n        assert len(preds) == len(self.metrics_per_prediction_type)\n        for metric_for_prediction_type in metrics_for_prediction_type:\n            if isinstance(target, torch.Tensor):\n                metric_for_prediction_type.update(pred, target)\n            else:\n                metric_for_prediction_type.update(pred, target[prediction_key])\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_managers.MetricManager.compute","title":"<code>compute()</code>","text":"<p>Computes set of metrics for each prediction type.</p> <p>Returns:</p> Type Description <code>Metrics</code> <p>dictionary containing computed metrics along with string identifiers for each prediction type.</p> Source code in <code>fl4health/metrics/metric_managers.py</code> <pre><code>def compute(self) -&gt; Metrics:\n    \"\"\"\n    Computes set of metrics for each prediction type.\n\n    Returns:\n        (Metrics): dictionary containing computed metrics along with string identifiers for each prediction type.\n    \"\"\"\n    all_results = {}\n    for metrics_key, metrics in self.metrics_per_prediction_type.items():\n        for metric in metrics:\n            result = metric.compute(f\"{self.metric_manager_name} - {metrics_key}\")\n            all_results.update(result)\n\n    return all_results\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_managers.MetricManager.clear","title":"<code>clear()</code>","text":"<p>Clears data accumulated in each metric for each of the prediction type.</p> Source code in <code>fl4health/metrics/metric_managers.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clears data accumulated in each metric for each of the prediction type.\"\"\"\n    for metrics_for_prediction_type in self.metrics_per_prediction_type.values():\n        for metric in metrics_for_prediction_type:\n            metric.clear()\n</code></pre>"},{"location":"api/#fl4health.metrics.metric_managers.MetricManager.reset","title":"<code>reset()</code>","text":"<p>Resets the metrics to their initial state.</p> Source code in <code>fl4health/metrics/metric_managers.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Resets the metrics to their initial state.\"\"\"\n    # On next update, metrics will be recopied from self.original_metrics which are still in their initial state\n    self.metrics_per_prediction_type = {}\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics","title":"<code>metrics</code>","text":""},{"location":"api/#fl4health.metrics.metrics.TorchMetric","title":"<code>TorchMetric</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class TorchMetric(Metric):\n    def __init__(self, name: str, metric: TMetric) -&gt; None:\n        \"\"\"\n        Thin wrapper on ``TorchMetric`` to make it compatible with our ``Metric`` interface.\n\n        Args:\n            name (str): The name of the metric.\n            metric (TMetric): ``TorchMetric`` class based metric.\n        \"\"\"\n        super().__init__(name)\n        self.metric = metric\n\n    def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n        \"\"\"\n        Updates the state of the underlying ``TorchMetric``.\n\n        Args:\n            input (torch.Tensor): The predictions of the model to be evaluated.\n            target (torch.Tensor): The ground truth target to evaluate predictions against.\n        \"\"\"\n        self.metric.update(input, target.long())\n\n    def compute(self, name: str | None = None) -&gt; Metrics:\n        \"\"\"\n        Compute value of underlying ``TorchMetric``.\n\n        Args:\n            name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n                dictionary.\n\n        Returns:\n           (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n        \"\"\"\n        result_key = f\"{name} - {self.name}\" if name is not None else self.name\n        result = self.metric.compute().item()\n        return {result_key: result}\n\n    def clear(self) -&gt; None:\n        self.metric.reset()\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.TorchMetric.__init__","title":"<code>__init__(name, metric)</code>","text":"<p>Thin wrapper on <code>TorchMetric</code> to make it compatible with our <code>Metric</code> interface.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> required <code>metric</code> <code>Metric</code> <p><code>TorchMetric</code> class based metric.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str, metric: TMetric) -&gt; None:\n    \"\"\"\n    Thin wrapper on ``TorchMetric`` to make it compatible with our ``Metric`` interface.\n\n    Args:\n        name (str): The name of the metric.\n        metric (TMetric): ``TorchMetric`` class based metric.\n    \"\"\"\n    super().__init__(name)\n    self.metric = metric\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.TorchMetric.update","title":"<code>update(input, target)</code>","text":"<p>Updates the state of the underlying <code>TorchMetric</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The predictions of the model to be evaluated.</p> required <code>target</code> <code>Tensor</code> <p>The ground truth target to evaluate predictions against.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n    \"\"\"\n    Updates the state of the underlying ``TorchMetric``.\n\n    Args:\n        input (torch.Tensor): The predictions of the model to be evaluated.\n        target (torch.Tensor): The ground truth target to evaluate predictions against.\n    \"\"\"\n    self.metric.update(input, target.long())\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.TorchMetric.compute","title":"<code>compute(name=None)</code>","text":"<p>Compute value of underlying <code>TorchMetric</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Optional name used in conjunction with class attribute name to define key in metrics dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>Metrics</code> <p>A dictionary of string and <code>Scalar</code> representing the computed metric and its associated key.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def compute(self, name: str | None = None) -&gt; Metrics:\n    \"\"\"\n    Compute value of underlying ``TorchMetric``.\n\n    Args:\n        name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n            dictionary.\n\n    Returns:\n       (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n    \"\"\"\n    result_key = f\"{name} - {self.name}\" if name is not None else self.name\n    result = self.metric.compute().item()\n    return {result_key: result}\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.SimpleMetric","title":"<code>SimpleMetric</code>","text":"<p>               Bases: <code>Metric</code>, <code>ABC</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class SimpleMetric(Metric, ABC):\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"\n        Abstract metric class with base functionality to update, compute and clear metrics. User needs to define\n        ``__call__`` method which returns metric given inputs and target.\n\n        Args:\n            name (str): Name of the metric.\n        \"\"\"\n        super().__init__(name)\n        self.accumulated_inputs: list[torch.Tensor] = []\n        self.accumulated_targets: list[torch.Tensor] = []\n\n    def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n        \"\"\"\n        This method updates the state of the metric by appending the passed input and target pairing to their\n        respective list.\n\n        Args:\n            input (torch.Tensor): The predictions of the model to be evaluated.\n            target (torch.Tensor): The ground truth target to evaluate predictions against.\n        \"\"\"\n        self.accumulated_inputs.append(input)\n        self.accumulated_targets.append(target)\n\n    def compute(self, name: str | None = None) -&gt; Metrics:\n        \"\"\"\n        Compute metric on accumulated input and output over updates.\n\n        Args:\n            name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n                dictionary.\n\n        Raises:\n            AssertionError: Input and target lists must be non empty.\n\n        Returns:\n            (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n        \"\"\"\n        assert len(self.accumulated_inputs) &gt; 0 and len(self.accumulated_targets) &gt; 0\n        stacked_inputs = torch.cat(self.accumulated_inputs)\n        stacked_targets = torch.cat(self.accumulated_targets)\n        result = self.__call__(stacked_inputs, stacked_targets)\n        result_key = f\"{name} - {self.name}\" if name is not None else self.name\n\n        return {result_key: result}\n\n    def clear(self) -&gt; None:\n        \"\"\"Resets metrics by clearing input and target lists.\"\"\"\n        self.accumulated_inputs = []\n        self.accumulated_targets = []\n\n    @abstractmethod\n    def __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        \"\"\"\n        User defined method that calculates the desired metric given the predictions and target.\n\n        Raises:\n            NotImplementedError: User must define this method.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.SimpleMetric.__init__","title":"<code>__init__(name)</code>","text":"<p>Abstract metric class with base functionality to update, compute and clear metrics. User needs to define <code>__call__</code> method which returns metric given inputs and target.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"\n    Abstract metric class with base functionality to update, compute and clear metrics. User needs to define\n    ``__call__`` method which returns metric given inputs and target.\n\n    Args:\n        name (str): Name of the metric.\n    \"\"\"\n    super().__init__(name)\n    self.accumulated_inputs: list[torch.Tensor] = []\n    self.accumulated_targets: list[torch.Tensor] = []\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.SimpleMetric.update","title":"<code>update(input, target)</code>","text":"<p>This method updates the state of the metric by appending the passed input and target pairing to their respective list.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The predictions of the model to be evaluated.</p> required <code>target</code> <code>Tensor</code> <p>The ground truth target to evaluate predictions against.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n    \"\"\"\n    This method updates the state of the metric by appending the passed input and target pairing to their\n    respective list.\n\n    Args:\n        input (torch.Tensor): The predictions of the model to be evaluated.\n        target (torch.Tensor): The ground truth target to evaluate predictions against.\n    \"\"\"\n    self.accumulated_inputs.append(input)\n    self.accumulated_targets.append(target)\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.SimpleMetric.compute","title":"<code>compute(name=None)</code>","text":"<p>Compute metric on accumulated input and output over updates.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Optional name used in conjunction with class attribute name to define key in metrics dictionary.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>Input and target lists must be non empty.</p> <p>Returns:</p> Type Description <code>Metrics</code> <p>A dictionary of string and <code>Scalar</code> representing the computed metric and its associated key.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def compute(self, name: str | None = None) -&gt; Metrics:\n    \"\"\"\n    Compute metric on accumulated input and output over updates.\n\n    Args:\n        name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n            dictionary.\n\n    Raises:\n        AssertionError: Input and target lists must be non empty.\n\n    Returns:\n        (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n    \"\"\"\n    assert len(self.accumulated_inputs) &gt; 0 and len(self.accumulated_targets) &gt; 0\n    stacked_inputs = torch.cat(self.accumulated_inputs)\n    stacked_targets = torch.cat(self.accumulated_targets)\n    result = self.__call__(stacked_inputs, stacked_targets)\n    result_key = f\"{name} - {self.name}\" if name is not None else self.name\n\n    return {result_key: result}\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.SimpleMetric.clear","title":"<code>clear()</code>","text":"<p>Resets metrics by clearing input and target lists.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Resets metrics by clearing input and target lists.\"\"\"\n    self.accumulated_inputs = []\n    self.accumulated_targets = []\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.SimpleMetric.__call__","title":"<code>__call__(input, target)</code>  <code>abstractmethod</code>","text":"<p>User defined method that calculates the desired metric given the predictions and target.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>User must define this method.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>@abstractmethod\ndef __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n    \"\"\"\n    User defined method that calculates the desired metric given the predictions and target.\n\n    Raises:\n        NotImplementedError: User must define this method.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.BinarySoftDiceCoefficient","title":"<code>BinarySoftDiceCoefficient</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class BinarySoftDiceCoefficient(SimpleMetric):\n    def __init__(\n        self,\n        name: str = \"BinarySoftDiceCoefficient\",\n        epsilon: float = 1.0e-7,\n        spatial_dimensions: tuple[int, ...] = (2, 3, 4),\n        logits_threshold: float | None = 0.5,\n    ):\n        \"\"\"\n        Binary DICE Coefficient Metric with configurable spatial dimensions and logits threshold.\n\n        Args:\n            name (str): Name of the metric.\n            epsilon (float): Small float to add to denominator of DICE calculation to avoid divide by 0.\n            spatial_dimensions (tuple[int, ...]): The spatial dimensions of the image within the prediction tensors.\n                The default assumes that the images are 3D and have shape:\n                (``batch_size``, ``channel``, ``spatial``, ``spatial``, ``spatial``)\n            logits_threshold: This is a threshold value where values above are classified as 1 and those below are\n                mapped to 0. If the threshold is None, then no thresholding is performed and a continuous or \"soft\"\n                DICE coefficient is computed.\n        \"\"\"\n        self.epsilon = epsilon\n        self.spatial_dimensions = spatial_dimensions\n\n        self.logits_threshold = logits_threshold\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        # Assuming the logits are to be mapped to binary. Note that this assumes the logits have already been\n        # constrained to [0, 1]. The metric still functions if not, but results will be unpredictable.\n        y_pred = (logits &gt; self.logits_threshold).int() if self.logits_threshold else logits\n        intersection = (y_pred * target).sum(dim=self.spatial_dimensions)\n        union = (0.5 * (y_pred + target)).sum(dim=self.spatial_dimensions)\n        dice = intersection / (union + self.epsilon)\n        # If both inputs are empty the dice coefficient should be equal 1\n        dice[union == 0] = 1\n        return torch.mean(dice).item()\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.BinarySoftDiceCoefficient.__init__","title":"<code>__init__(name='BinarySoftDiceCoefficient', epsilon=1e-07, spatial_dimensions=(2, 3, 4), logits_threshold=0.5)</code>","text":"<p>Binary DICE Coefficient Metric with configurable spatial dimensions and logits threshold.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> <code>'BinarySoftDiceCoefficient'</code> <code>epsilon</code> <code>float</code> <p>Small float to add to denominator of DICE calculation to avoid divide by 0.</p> <code>1e-07</code> <code>spatial_dimensions</code> <code>tuple[int, ...]</code> <p>The spatial dimensions of the image within the prediction tensors. The default assumes that the images are 3D and have shape: (<code>batch_size</code>, <code>channel</code>, <code>spatial</code>, <code>spatial</code>, <code>spatial</code>)</p> <code>(2, 3, 4)</code> <code>logits_threshold</code> <code>float | None</code> <p>This is a threshold value where values above are classified as 1 and those below are mapped to 0. If the threshold is None, then no thresholding is performed and a continuous or \"soft\" DICE coefficient is computed.</p> <code>0.5</code> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"BinarySoftDiceCoefficient\",\n    epsilon: float = 1.0e-7,\n    spatial_dimensions: tuple[int, ...] = (2, 3, 4),\n    logits_threshold: float | None = 0.5,\n):\n    \"\"\"\n    Binary DICE Coefficient Metric with configurable spatial dimensions and logits threshold.\n\n    Args:\n        name (str): Name of the metric.\n        epsilon (float): Small float to add to denominator of DICE calculation to avoid divide by 0.\n        spatial_dimensions (tuple[int, ...]): The spatial dimensions of the image within the prediction tensors.\n            The default assumes that the images are 3D and have shape:\n            (``batch_size``, ``channel``, ``spatial``, ``spatial``, ``spatial``)\n        logits_threshold: This is a threshold value where values above are classified as 1 and those below are\n            mapped to 0. If the threshold is None, then no thresholding is performed and a continuous or \"soft\"\n            DICE coefficient is computed.\n    \"\"\"\n    self.epsilon = epsilon\n    self.spatial_dimensions = spatial_dimensions\n\n    self.logits_threshold = logits_threshold\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.Accuracy","title":"<code>Accuracy</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class Accuracy(SimpleMetric):\n    def __init__(self, name: str = \"accuracy\"):\n        \"\"\"\n        Accuracy metric for classification tasks.\n\n        Args:\n            name (str): The name of the metric.\n\n        \"\"\"\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor, threshold: float = 0.5) -&gt; Scalar:\n        # assuming batch first\n        assert logits.shape[0] == target.shape[0]\n        # Single value output, assume binary logits\n        preds = (\n            (logits &gt; threshold).int() if len(logits.shape) == 1 or logits.shape[1] == 1 else torch.argmax(logits, 1)\n        )\n        target = target.cpu().detach()\n        preds = preds.cpu().detach()\n        return sklearn_metrics.accuracy_score(target, preds)\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.Accuracy.__init__","title":"<code>__init__(name='accuracy')</code>","text":"<p>Accuracy metric for classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>'accuracy'</code> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str = \"accuracy\"):\n    \"\"\"\n    Accuracy metric for classification tasks.\n\n    Args:\n        name (str): The name of the metric.\n\n    \"\"\"\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.BalancedAccuracy","title":"<code>BalancedAccuracy</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class BalancedAccuracy(SimpleMetric):\n    def __init__(self, name: str = \"balanced_accuracy\"):\n        \"\"\"\n        Balanced accuracy metric for classification tasks. Used for the evaluation of imbalanced datasets.\n\n        For more information:\n\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n        \"\"\"\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        # assuming batch first\n        assert logits.shape[0] == target.shape[0]\n        target = target.cpu().detach()\n        logits = logits.cpu().detach()\n        y_true = target.reshape(-1)\n        preds = np.argmax(logits, axis=1)\n        return sklearn_metrics.balanced_accuracy_score(y_true, preds)\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.BalancedAccuracy.__init__","title":"<code>__init__(name='balanced_accuracy')</code>","text":"<p>Balanced accuracy metric for classification tasks. Used for the evaluation of imbalanced datasets.</p> <p>For more information:</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str = \"balanced_accuracy\"):\n    \"\"\"\n    Balanced accuracy metric for classification tasks. Used for the evaluation of imbalanced datasets.\n\n    For more information:\n\n    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n    \"\"\"\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.RocAuc","title":"<code>RocAuc</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class RocAuc(SimpleMetric):\n    def __init__(self, name: str = \"ROC_AUC score\"):\n        \"\"\"\n        Area under the Receiver Operator Curve (AUCROC) metric for classification.\n\n        For more information:\n\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n        \"\"\"\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        assert logits.shape[0] == target.shape[0]\n        prob = torch.nn.functional.softmax(logits, dim=1)\n        prob = prob.cpu().detach()\n        target = target.cpu().detach()\n        y_true = target.reshape(-1)\n        return sklearn_metrics.roc_auc_score(y_true, prob, average=\"weighted\", multi_class=\"ovr\")\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.RocAuc.__init__","title":"<code>__init__(name='ROC_AUC score')</code>","text":"<p>Area under the Receiver Operator Curve (AUCROC) metric for classification.</p> <p>For more information:</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str = \"ROC_AUC score\"):\n    \"\"\"\n    Area under the Receiver Operator Curve (AUCROC) metric for classification.\n\n    For more information:\n\n    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n    \"\"\"\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.F1","title":"<code>F1</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class F1(SimpleMetric):\n    def __init__(\n        self,\n        name: str = \"F1 score\",\n        average: str | None = \"weighted\",\n    ):\n        \"\"\"\n        Computes the F1 score using the ``sklearn f1_score`` function. As such, the values of average correspond to\n        those of that function.\n\n        Args:\n            name (str, optional): Name of the metric. Defaults to \"F1 score\".\n            average (str | None, optional): Whether to perform averaging of the F1 scores and how. The values of this\n                string corresponds to those of the ``sklearn f1_score function``. See:\n\n                https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n\n                Defaults to \"weighted\".\n        \"\"\"\n        super().__init__(name)\n        self.average = average\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        assert logits.shape[0] == target.shape[0]\n        target = target.cpu().detach()\n        logits = logits.cpu().detach()\n        y_true = target.reshape(-1)\n        preds = np.argmax(logits, axis=1)\n        return sklearn_metrics.f1_score(y_true, preds, average=self.average)\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics.F1.__init__","title":"<code>__init__(name='F1 score', average='weighted')</code>","text":"<p>Computes the F1 score using the <code>sklearn f1_score</code> function. As such, the values of average correspond to those of that function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric. Defaults to \"F1 score\".</p> <code>'F1 score'</code> <code>average</code> <code>str | None</code> <p>Whether to perform averaging of the F1 scores and how. The values of this string corresponds to those of the <code>sklearn f1_score function</code>. See:</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html</p> <p>Defaults to \"weighted\".</p> <code>'weighted'</code> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"F1 score\",\n    average: str | None = \"weighted\",\n):\n    \"\"\"\n    Computes the F1 score using the ``sklearn f1_score`` function. As such, the values of average correspond to\n    those of that function.\n\n    Args:\n        name (str, optional): Name of the metric. Defaults to \"F1 score\".\n        average (str | None, optional): Whether to perform averaging of the F1 scores and how. The values of this\n            string corresponds to those of the ``sklearn f1_score function``. See:\n\n            https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n\n            Defaults to \"weighted\".\n    \"\"\"\n    super().__init__(name)\n    self.average = average\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics_utils","title":"<code>metrics_utils</code>","text":""},{"location":"api/#fl4health.metrics.metrics_utils.compute_dice_on_count_tensors","title":"<code>compute_dice_on_count_tensors(true_positives, false_positives, false_negatives, zero_division)</code>","text":"<p>Given a set of count tensors representing true positives (TP), false positives (FP), and false negatives (FN), compute the  Dice score as...</p> <p>[</p> <p>]     \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}</p> <p>ELEMENTWISE. The zero division argument determines how to deal with examples with all true negatives, which implies that <code>TP + FP + FN = 0</code> and an undefined value.</p> <p>Parameters:</p> Name Type Description Default <code>true_positives</code> <code>Tensor</code> <p>count of true positives in each entry.</p> required <code>false_positives</code> <code>Tensor</code> <p>count of false positives in each entry.</p> required <code>false_negatives</code> <code>Tensor</code> <p>count of false negatives in each entry.</p> required <code>zero_division</code> <code>float | None</code> <p>How to deal with zero division. If None, the values with zero division are simply dropped. If a float is specified, this value is injected into each Dice score that would have been undefined.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Dice scores computed for each element in the TP, FP, FN tensors computed ELEMENTWISE with</p> <code>Tensor</code> <p>replacement or dropping of undefined entries. The tensor returned is flattened to be 1D.</p> Source code in <code>fl4health/metrics/metrics_utils.py</code> <pre><code>def compute_dice_on_count_tensors(\n    true_positives: torch.Tensor,\n    false_positives: torch.Tensor,\n    false_negatives: torch.Tensor,\n    zero_division: float | None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Given a set of count tensors representing true positives (TP), false positives (FP), and false negatives (FN),\n    compute the  Dice score as...\n\n    \\\\[\n\n\n    \\\\]\n        \\\\frac{2 \\\\cdot TP}{2 \\\\cdot TP + FP + FN}\n\n    **ELEMENTWISE**. The zero division argument determines how to deal with examples with all true negatives, which\n    implies that ``TP + FP + FN = 0`` and an undefined value.\n\n    Args:\n        true_positives (torch.Tensor): count of true positives in each entry.\n        false_positives (torch.Tensor): count of false positives in each entry.\n        false_negatives (torch.Tensor): count of false negatives in each entry.\n        zero_division (float | None): How to deal with zero division. If None, the values with zero division are\n            simply dropped. If a float is specified, this value is injected into each Dice score that would have\n            been undefined.\n\n    Returns:\n        (torch.Tensor): Dice scores computed for each element in the TP, FP, FN tensors computed **ELEMENTWISE** with\n        replacement or dropping of undefined entries. The tensor returned is flattened to be 1D.\n    \"\"\"\n    # Compute union and intersection\n    numerator = 2 * true_positives  # Equivalent to 2 times the intersection\n    denominator = 2 * true_positives + false_positives + false_negatives  # Equivalent to the union\n\n    # Remove or replace dice score that will be null due to zero division\n    if zero_division is None:\n        numerator = numerator[denominator != 0]\n        denominator = denominator[denominator != 0]\n    else:\n        numerator[denominator == 0] = zero_division\n        denominator[denominator == 0] = 1\n\n    # Return individual dice coefficients\n    return numerator / denominator\n</code></pre>"},{"location":"api/#fl4health.metrics.metrics_utils.threshold_tensor","title":"<code>threshold_tensor(input, threshold)</code>","text":"<p>Converts continuous 'soft' tensors into categorical 'hard' ones.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The tensor to threshold.</p> required <code>threshold</code> <code>float | int</code> <p>A float for thresholding values or an integer specifying the index of the label dimension. If a float is given, elements below the threshold are mapped to 0 and above are mapped to 1. If an integer is given, elements are thresholded based on the class with the highest prediction.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Thresholded tensor.</p> Source code in <code>fl4health/metrics/metrics_utils.py</code> <pre><code>def threshold_tensor(input: torch.Tensor, threshold: float | int) -&gt; torch.Tensor:\n    \"\"\"\n    Converts continuous 'soft' tensors into categorical 'hard' ones.\n\n    Args:\n        input (torch.Tensor): The tensor to threshold.\n        threshold (float | int): A float for thresholding values or an integer specifying the index of the\n            label dimension. If a float is given, elements below the threshold are mapped to 0 and above are\n            mapped to 1. If an integer is given, elements are thresholded based on the class with the highest\n            prediction.\n\n    Returns:\n        (torch.Tensor): Thresholded tensor.\n    \"\"\"\n    if isinstance(threshold, float):\n        thresholded_tensor = torch.zeros_like(input)\n        mask_1 = input &gt; threshold\n        thresholded_tensor[mask_1] = 1\n        return thresholded_tensor\n    if isinstance(threshold, int):\n        # Use argmax to get predicted class labels (hard_preds) and the one-hot-encode them.\n        if threshold &gt;= input.ndim:\n            raise ValueError(\n                f\"Cannot apply argmax to Tensor of shape {input.shape}. \"\n                f\"Label dimension of {threshold} is out of range of tensor with {input.ndim} dimensions.\"\n            )\n        hard_input = input.argmax(threshold, keepdim=True)\n        input = torch.zeros_like(input)\n        input.scatter_(threshold, hard_input, 1)\n        return input\n    raise ValueError(f\"Was expecting threshold argument to be either a float or an int. Got {type(threshold)}\")\n</code></pre>"},{"location":"api/#fl4health.metrics.utils","title":"<code>utils</code>","text":""},{"location":"api/#fl4health.metrics.utils.infer_label_dim","title":"<code>infer_label_dim(tensor1, tensor2)</code>","text":"<p>Infers the label dimension given two related tensors of different shapes.</p> <p>Generally useful for inferring the label dimension when one tensor is vector-encoded and the other is not. The label dimension is inferred by looking for dimensions that either are not the same size, or are not present in tensor 2.</p> <p>Parameters:</p> Name Type Description Default <code>tensor1</code> <code>Tensor</code> <p>The reference tensor. Must have the same number of dimensions as tensor 2, or have exactly 1 more dimension (the label dimension).</p> required <code>tensor2</code> <code>Tensor</code> <p>The non-reference tensor.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the the label dimension cannot be inferred without ambiguity. For example if a dimension next to the label dimension has the same size.</p> <p>Returns:</p> Type Description <code>int</code> <p>Index of the dimension along tensor 1 that corresponds to the label dimension.</p> Source code in <code>fl4health/metrics/utils.py</code> <pre><code>def infer_label_dim(tensor1: torch.Tensor, tensor2: torch.Tensor) -&gt; int:\n    \"\"\"\n    Infers the label dimension given two related tensors of different shapes.\n\n    Generally useful for inferring the label dimension when one tensor is vector-encoded and the other is not. The\n    label dimension is inferred by looking for dimensions that either are not the same size, or are not present in\n    tensor 2.\n\n    Args:\n        tensor1 (torch.Tensor): The reference tensor. Must have the same number of dimensions as tensor 2, or have\n            exactly 1 more dimension (the label dimension).\n        tensor2 (torch.Tensor): The non-reference tensor.\n\n    Raises:\n        AssertionError: If the the label dimension cannot be inferred without ambiguity. For example if a dimension\n            next to the label dimension has the same size.\n\n    Returns:\n        (int): Index of the dimension along tensor 1 that corresponds to the label dimension.\n    \"\"\"\n    assert tensor1.shape != tensor2.shape, (\n        f\"Could not infer the label dimension of tensors with the same shape: {tensor1.shape}\"\n    )\n\n    assert 0 &lt;= (tensor1.ndim - tensor2.ndim) &lt;= 1, (\n        f\"Could not infer the label dimension of tensors with shapes: tensor1: {tensor1.shape}), tensor 2: \"\n        f\"({tensor2.shape}). Expected tensor1 to be larger than tensor2 by at most 1 dimension.\"\n    )\n\n    # Infer label dimension.\n    idx2 = 0\n    candidate_label_dims = []\n    # Iterate through dimensions of tensor1 and compare size to corresponding dimension of tensor2\n    # If tensor1 and tensor2 have the same number of dimensions, we are looking for axes in the same position with\n    # different sizes. If tensor1 has an extra dimension, then we are looking for an axes with likely unique size not\n    # present in tensor2\n    for idx1 in range(tensor1.ndim):\n        if idx2 &gt;= tensor2.ndim:\n            # If tensor1 has an extra dimension and all previous dimensions were the same shape, then the last\n            # dimension must be the additional dimension\n            candidate_label_dims.append(idx1)\n        elif tensor1.shape[idx1] == tensor2.shape[idx2]:\n            # If the dimensions have the size then they are likely not the label dimension\n            idx2 += 1\n        else:\n            candidate_label_dims.append(idx1)\n            if tensor1.ndim == tensor2.ndim:\n                # If tensor1 and tensor2 have the same number of dimensions, then we are looking for axes in the same\n                # position with different sizes. Hence we proceed to the next index of tensor 2. Otherwise, the\n                # non-matching dimension in tensor1 is an extra dimension, and so we want to act as if we skipped it\n                # and ensure the rest of the shape is the same.\n                idx2 += 1\n\n    assert len(candidate_label_dims) == 1, (\n        f\"Could not infer the label dimension of tensors with shapes: ({tensor1.shape}), ({tensor2.shape}). \"\n        \"Found multiple axes that could be the label dimension.\"\n    )\n    label_dim = candidate_label_dims[0]\n\n    # Cover edge case where dim adjacent to label dim has the same size. We will mistakenly resolve only a single\n    # candidate label dimension when technically it is ambiguous. An example is\n    #   tensor_1.shape = (5, 5, 3) and tensor_2.shape = (5, 3).\n    # The label dimension could be the first or the second.\n    if tensor1.ndim &gt; tensor2.ndim and label_dim &gt; 0:\n        assert tensor1.shape[label_dim] != tensor1.shape[label_dim - 1], (\n            f\"Could not infer the label dimension of tensors with shapes: ({tensor1.shape}), ({tensor2.shape}). \"\n            \"A dimension adjacent to the label dimension appears to have the same size.\"\n        )\n\n    # If tensors have same ndim but different shapes, then this only works if label dim was empty for one of them\n    if tensor1.ndim == tensor2.ndim:\n        assert (tensor1.shape[label_dim] == 1) or (tensor2.shape[label_dim]) == 1, (\n            f\"Could not infer the label dimension of tensors with shapes: ({tensor1.shape}), ({tensor2.shape}). \"\n            \"The inferred candidate dimension has different sizes on each tensor, was expecting one to be empty.\"\n        )\n    return label_dim\n</code></pre>"},{"location":"api/#fl4health.metrics.utils.map_label_index_tensor_to_one_hot","title":"<code>map_label_index_tensor_to_one_hot(label_index_tensor, target_shape, label_dim)</code>","text":"<p>Maps the provided <code>label_index_tensor</code>, which has label indices at the provided <code>label_dim</code>. In the tensor, this dimension should be \"empty,\" i.e. have size 1. This function uses the shape provided by <code>target_shape</code> to expand the label indices into one-hot encoded vectors in that dimension according the the size of the target dimension at <code>label_dim</code> in <code>target_shape</code>. For example, if <code>label_index_tensor</code> has shape <code>(64, 10, 10, 1)</code>, <code>label_dim = 3</code>, and <code>target_shape = (64, 10, 10, 4)</code>, then the new shape should be <code>(64, 10, 10, 4)</code> with <code>[i, j, k, :]</code> being a one-hot vector of length <code>4</code>.</p> <p>Parameters:</p> Name Type Description Default <code>label_index_tensor</code> <code>Tensor</code> <p>Tensor to have label_dim dimension one-hot encoded accounting to <code>target_shape</code> and the indices of <code>label_index_tensor</code> in the <code>label_dim</code></p> required <code>target_shape</code> <code>Size</code> <p>Shape we want to transform <code>label_index_tensor</code> to. Mainly used to establish the length of the one-hot encodings</p> required <code>label_dim</code> <code>int</code> <p>Dimension to one-hot encode.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor with one-hot encoded <code>label_dim</code>.</p> Source code in <code>fl4health/metrics/utils.py</code> <pre><code>def map_label_index_tensor_to_one_hot(\n    label_index_tensor: torch.Tensor, target_shape: torch.Size, label_dim: int\n) -&gt; torch.Tensor:\n    \"\"\"\n    Maps the provided ``label_index_tensor``, which has label indices at the provided ``label_dim``. In the tensor,\n    this dimension should be \"empty,\" i.e. have size 1. This function uses the shape provided by ``target_shape`` to\n    expand the label indices into one-hot encoded vectors in that dimension according the the size of the target\n    dimension at ``label_dim`` in ``target_shape``. For example, if ``label_index_tensor`` has shape\n    ``(64, 10, 10, 1)``, ``label_dim = 3``, and ``target_shape = (64, 10, 10, 4)``, then the new shape should be\n    ``(64, 10, 10, 4)`` with ``[i, j, k, :]`` being a one-hot vector of length ``4``.\n\n    Args:\n        label_index_tensor (torch.Tensor): Tensor to have label_dim dimension one-hot encoded accounting to\n            ``target_shape`` and the indices of ``label_index_tensor`` in the ``label_dim``\n        target_shape (torch.Size): Shape we want to transform ``label_index_tensor`` to. Mainly used to establish the\n            length of the one-hot encodings\n        label_dim (int): Dimension to one-hot encode.\n\n    Returns:\n        (torch.Tensor): Tensor with one-hot encoded ``label_dim``.\n    \"\"\"\n    label_index_tensor_shape = label_index_tensor.shape\n\n    assert label_dim &lt; len(label_index_tensor_shape), (\n        f\"Label dim: {label_dim} too large for target shape: {label_index_tensor_shape}\"\n    )\n\n    label_dim_of_tensor = label_index_tensor.shape[label_dim]\n    assert label_dim_of_tensor == 1, (\n        f\"Expected label_dim {label_dim} of label_index_tensor to be of size 1, but got {label_dim_of_tensor}\"\n    )\n\n    one_hot_encoded_tensor = torch.zeros(target_shape, device=label_index_tensor.device)\n    one_hot_encoded_tensor.scatter_(label_dim, label_index_tensor.to(torch.int64), 1)\n    return one_hot_encoded_tensor\n</code></pre>"},{"location":"api/#fl4health.metrics.utils.align_pred_and_target_shapes","title":"<code>align_pred_and_target_shapes(preds, targets, label_dim=None)</code>","text":"<p>If necessary, attempts to correct shape mismatches between the given tensors by inferring which one to one-hot-encode.</p> <p>NOTE: If both preds and targets have the same shape then nothing needs to be modified. If there is a mismatch, it is assumed that the labels in one of the predictions or the targets are vector encoded in some way (either one-hot or soft) while the other is label index encoded.</p> <p>If one is vector encoded but not the other, then both are returned as vector encoded tensors.</p> <p>NOTE: This function ASSUMES label-index encoding if the shapes are misaligned. This assumption doesn't necessarily hold in binary classification settings where continuous values might be used to indicate the positive label by default. As such, this function should not be used for those types of tensors.</p> <p>For example, consider a problem with 3 label classes with the preds vector encoded and the targets label encoded</p> <pre><code>preds = torch.Tensor([[0.1, 0.2, 0.7], [0.9, 0.1, 0.0]])\n\ntargets = torch.Tensor([[2], [1]])\n</code></pre> <p>preds has shape <code>(2, 3)</code> and targets has shape <code>(2, 1)</code>. This function will convert targets to a one-hot-encode tensor with contents</p> <pre><code>targets = torch.Tensor([0, 0, 1], [0, 1, 0])\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>The tensor with model predictions.</p> required <code>targets</code> <code>Tensor</code> <p>The tensor with model targets.</p> required <code>label_dim</code> <code>int | None</code> <p>Index of the label dimension. If left as None then this method attempts to infer the label dimension if it is needed.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>The pred and target tensors respectively now ensured to have the same shape.</p> Source code in <code>fl4health/metrics/utils.py</code> <pre><code>def align_pred_and_target_shapes(\n    preds: torch.Tensor, targets: torch.Tensor, label_dim: int | None = None\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    If necessary, attempts to correct shape mismatches between the given tensors by inferring which one to\n    one-hot-encode.\n\n    **NOTE**: If both preds and targets have the same shape then nothing needs to be modified. If there is a mismatch,\n    it is assumed that the labels in one of the predictions or the targets are vector encoded in some way\n    (either one-hot or soft) while the other is label index encoded.\n\n    If one is vector encoded but not the other, then both are returned as vector encoded tensors.\n\n    **NOTE**: This function **ASSUMES** label-index encoding if the shapes are misaligned. This assumption doesn't\n    necessarily hold in binary classification settings where continuous values might be used to indicate the positive\n    label by default. As such, this function should not be used for those types of tensors.\n\n    For example, consider a problem with 3 label classes with the preds vector encoded and the targets label encoded\n\n    ```python\n    preds = torch.Tensor([[0.1, 0.2, 0.7], [0.9, 0.1, 0.0]])\n\n    targets = torch.Tensor([[2], [1]])\n    ```\n\n    preds has shape ``(2, 3)`` and targets has shape ``(2, 1)``. This function will convert targets to a\n    one-hot-encode tensor with contents\n\n    ```python\n    targets = torch.Tensor([0, 0, 1], [0, 1, 0])\n    ```\n\n    Args:\n        preds (torch.Tensor): The tensor with model predictions.\n        targets (torch.Tensor): The tensor with model targets.\n        label_dim (int | None): Index of the label dimension. If left as None then this method attempts to infer\n            the label dimension if it is needed.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): The pred and target tensors respectively now ensured to have the same\n            shape.\n    \"\"\"\n    # Shapes are already aligned.\n    if preds.shape == targets.shape:\n        return preds, targets\n\n    # Run this assertion before in case label dim is defined.\n    assert abs(preds.ndim - targets.ndim) &lt;= 1, (\n        f\"Can not align pred and target tensors with shapes {preds.shape}, {targets.shape}\"\n    )\n\n    # If shapes are different then we assume one tensor has vector encoded labels and the other is label index encoded\n    # and will be mapped to one-hot-encoded format.\n    if preds.ndim &gt; targets.ndim:\n        # Preds must be vector encoded and targets are not\n        # Determine label dimension. Preds is the first/reference tensor because its shape is larger, if not provided\n        label_dim = infer_label_dim(preds, targets) if label_dim is None else label_dim\n        targets = targets.unsqueeze(label_dim)\n        one_hot_targets = map_label_index_tensor_to_one_hot(targets, preds.shape, label_dim)\n        return preds, one_hot_targets\n\n    if preds.ndim &lt; targets.ndim:\n        # Targets must be vector encoded and preds are not\n        # Determine label dimension. Targets is the first/reference tensor because its shape is larger, if not provided\n        label_dim = infer_label_dim(targets, preds) if label_dim is None else label_dim\n        preds = preds.unsqueeze(label_dim)\n        one_hot_preds = map_label_index_tensor_to_one_hot(preds, targets.shape, label_dim)\n        return one_hot_preds, targets\n\n    # If we're here, the shapes are the same size, but differ in at least one dimension\n    # Determine label dimension, if not provided. Because their shapes are the same length, order doesn't matter\n    label_dim = infer_label_dim(preds, targets) if label_dim is None else label_dim\n    if preds.shape[label_dim] &lt; targets.shape[label_dim]:\n        # We need to one-hot the preds tensor\n        return map_label_index_tensor_to_one_hot(preds, targets.shape, label_dim), targets\n    return preds, map_label_index_tensor_to_one_hot(targets, preds.shape, label_dim)\n</code></pre>"},{"location":"api/#fl4health.mixins","title":"<code>mixins</code>","text":""},{"location":"api/#fl4health.mixins.AdaptiveDriftConstrainedMixin","title":"<code>AdaptiveDriftConstrainedMixin</code>","text":"<p>               Bases: <code>BaseFlexibleMixin</code></p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>class AdaptiveDriftConstrainedMixin(BaseFlexibleMixin):\n    def __init__(self: AdaptiveDriftConstrainedProtocol, *args: Any, **kwargs: Any):\n        \"\"\"\n        Adaptive Drift Constrained Mixin.\n\n        To be used with ``~fl4health.BaseClient`` in order to add the ability to compute\n        losses via a constrained adaptive drift.\n\n        **NOTE**: Rather than using ``AdaptiveDriftConstraintClient``, if a client subclasses\n        ``FlexibleClient``, than this mixin could be used on that subclass to implement the\n        adaptive drift constraint.\n        \"\"\"\n        # Initialize mixin-specific attributes with default values\n        self.loss_for_adaptation = 0.1\n        self.drift_penalty_tensors = None\n        self.drift_penalty_weight = None\n\n        super().__init__(*args, **kwargs)\n\n        self.penalty_loss_function = WeightDriftLoss(self.device)\n\n    def get_parameters(self: AdaptiveDriftConstrainedProtocol, config: Config) -&gt; NDArrays:\n        \"\"\"\n        Packs the parameters and training loss into a single ``NDArrays`` to be sent to the server for aggregation. If\n        the client has not been initialized, this means the server is requesting parameters for initialization and\n        just the model parameters are sent. When using the ``FedAvgWithAdaptiveConstraint`` strategy, this should not\n        happen, as that strategy requires server-side initialization parameters. However, other strategies may handle\n        this case.\n\n        Args:\n            config (Config): Configurations to allow for customization of this functions behavior\n\n        Returns:\n            (NDArrays): Parameters and training loss packed together into a list of numpy arrays to be sent to the\n                server.\n        \"\"\"\n        if not self.initialized:\n            return self.setup_client_and_return_all_model_parameters(config)\n\n        # Make sure the proper components are there\n        assert self.model is not None and self.parameter_exchanger is not None and self.loss_for_adaptation is not None\n        model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n\n        # Weights and training loss sent to server for aggregation. Training loss is sent because server will\n        # decide to increase or decrease the penalty weight, if adaptivity is turned on.\n        return self.parameter_exchanger.pack_parameters(model_weights, self.loss_for_adaptation)\n\n    def setup_client_and_return_all_model_parameters(\n        self: AdaptiveDriftConstrainedProtocol, config: Config\n    ) -&gt; NDArrays:\n        \"\"\"\n        Function used to setup the client using the provided configuration and then exact all model parameters from\n        ``self.model`` and return them. This function is used as a helper for ``get_parameters`` when the client\n        has yet to be initialized.\n\n        Args:\n            config (Config): Configuration to be used  in setting up the client.\n\n        Returns:\n            (NDArrays): All parameters associated with the ``self.model`` property of the client.\n        \"\"\"\n        log(INFO, \"Setting up client and providing full model parameters to the server for initialization\")\n        if not config:\n            log(\n                WARNING,\n                (\n                    \"This client has not yet been initialized and the config is empty. This may cause unexpected \"\n                    \"failures, as setting up a client typically requires several configuration parameters, \"\n                    \"including batch_size and current_server_round.\"\n                ),\n            )\n\n        # If initialized is False, the server is requesting model parameters from which to initialize all other\n        # clients. As such get_parameters is being called before fit or evaluate, so we must call\n        # setup_client first.\n        self.setup_client(config)\n\n        # Need all parameters even if normally exchanging partial\n        return FullParameterExchanger().push_parameters(self.model, config=config)\n\n    def set_parameters(\n        self: AdaptiveDriftConstrainedProtocol, parameters: NDArrays, config: Config, fitting_round: bool\n    ) -&gt; None:\n        \"\"\"\n        Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n        unpacked for the clients to use in training. In the first fitting round, we assume the full model is being\n        initialized and use the ``FullParameterExchanger()`` to set all model weights.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model and also the penalty weight to be applied during training.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n                round or an evaluation round. This is used to help determine which parameter exchange should be used\n                for pulling parameters. A full parameter exchanger is always used if the current federated learning\n                round is the very first fitting round.\n        \"\"\"\n        assert self.model is not None and self.parameter_exchanger is not None\n\n        server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n        log(INFO, f\"Penalty weight received from the server: {self.drift_penalty_weight}\")\n\n        super().set_parameters(server_model_state, config, fitting_round)  # type: ignore[safe-super]\n\n    def train_step(\n        self: AdaptiveDriftConstrainedProtocol, input: TorchInputType, target: TorchTargetType\n    ) -&gt; tuple[TrainingLosses, TorchPredType]:\n        losses, preds = self._compute_preds_and_losses(self.model, self.optimizers[\"global\"], input, target)\n        loss_clone = losses.backward[\"backward\"].clone()\n\n        # apply penalty\n        penalty_loss = self.compute_penalty_loss()\n        losses.backward[\"backward\"] = losses.backward[\"backward\"] + penalty_loss\n        losses = self._apply_backwards_on_losses_and_take_step(self.model, self.optimizers[\"global\"], losses)\n\n        # prepare return values\n        additional_losses = {\n            \"penalty_loss\": penalty_loss.clone(),\n            \"local_loss\": loss_clone,\n            \"loss_for_adaptation\": loss_clone.clone(),\n        }\n        losses.additional_losses = additional_losses\n\n        return losses, preds\n\n    def get_parameter_exchanger(self: AdaptiveDriftConstrainedProtocol, config: Config) -&gt; ParameterExchanger:\n        \"\"\"\n        Setting up the parameter exchanger to include the appropriate packing functionality.\n        By default we assume that we're exchanging all parameters. Can be overridden for other behavior.\n\n        Args:\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n        Returns:\n            (ParameterExchanger): Exchanger that can handle packing/unpacking auxiliary server information.\n        \"\"\"\n        return FullParameterExchangerWithPacking(ParameterPackerAdaptiveConstraint())\n\n    def update_after_train(\n        self: AdaptiveDriftConstrainedProtocol, local_steps: int, loss_dict: dict[str, float], config: Config\n    ) -&gt; None:\n        \"\"\"\n        Called after training with the number of ``local_steps`` performed over the FL round and the corresponding loss\n        dictionary. We use this to store the training loss that we want to use to adapt the penalty weight parameter\n        on the server side.\n\n        Args:\n            local_steps (int): The number of steps so far in the round in the local training.\n            loss_dict (dict[str, float]): A dictionary of losses from local training.\n            config (Config): The config from the server\n        \"\"\"\n        assert \"loss_for_adaptation\" in loss_dict\n        # Store current loss which is the vanilla loss without the penalty term added in\n        self.loss_for_adaptation = loss_dict[\"loss_for_adaptation\"]\n        super().update_after_train(local_steps, loss_dict, config)  # type: ignore[safe-super]\n\n    def compute_penalty_loss(self: AdaptiveDriftConstrainedProtocol) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the drift loss for the client model and drift tensors.\n\n        Returns:\n            (torch.Tensor): Computed penalty loss tensor.\n        \"\"\"\n        # Penalty tensors must have been set for these clients.\n        assert self.drift_penalty_tensors is not None\n\n        return self.penalty_loss_function(self.model, self.drift_penalty_tensors, self.drift_penalty_weight)\n</code></pre>"},{"location":"api/#fl4health.mixins.AdaptiveDriftConstrainedMixin.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Adaptive Drift Constrained Mixin.</p> <p>To be used with <code>~fl4health.BaseClient</code> in order to add the ability to compute losses via a constrained adaptive drift.</p> <p>NOTE: Rather than using <code>AdaptiveDriftConstraintClient</code>, if a client subclasses <code>FlexibleClient</code>, than this mixin could be used on that subclass to implement the adaptive drift constraint.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def __init__(self: AdaptiveDriftConstrainedProtocol, *args: Any, **kwargs: Any):\n    \"\"\"\n    Adaptive Drift Constrained Mixin.\n\n    To be used with ``~fl4health.BaseClient`` in order to add the ability to compute\n    losses via a constrained adaptive drift.\n\n    **NOTE**: Rather than using ``AdaptiveDriftConstraintClient``, if a client subclasses\n    ``FlexibleClient``, than this mixin could be used on that subclass to implement the\n    adaptive drift constraint.\n    \"\"\"\n    # Initialize mixin-specific attributes with default values\n    self.loss_for_adaptation = 0.1\n    self.drift_penalty_tensors = None\n    self.drift_penalty_weight = None\n\n    super().__init__(*args, **kwargs)\n\n    self.penalty_loss_function = WeightDriftLoss(self.device)\n</code></pre>"},{"location":"api/#fl4health.mixins.AdaptiveDriftConstrainedMixin.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>Packs the parameters and training loss into a single <code>NDArrays</code> to be sent to the server for aggregation. If the client has not been initialized, this means the server is requesting parameters for initialization and just the model parameters are sent. When using the <code>FedAvgWithAdaptiveConstraint</code> strategy, this should not happen, as that strategy requires server-side initialization parameters. However, other strategies may handle this case.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configurations to allow for customization of this functions behavior</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Parameters and training loss packed together into a list of numpy arrays to be sent to the server.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def get_parameters(self: AdaptiveDriftConstrainedProtocol, config: Config) -&gt; NDArrays:\n    \"\"\"\n    Packs the parameters and training loss into a single ``NDArrays`` to be sent to the server for aggregation. If\n    the client has not been initialized, this means the server is requesting parameters for initialization and\n    just the model parameters are sent. When using the ``FedAvgWithAdaptiveConstraint`` strategy, this should not\n    happen, as that strategy requires server-side initialization parameters. However, other strategies may handle\n    this case.\n\n    Args:\n        config (Config): Configurations to allow for customization of this functions behavior\n\n    Returns:\n        (NDArrays): Parameters and training loss packed together into a list of numpy arrays to be sent to the\n            server.\n    \"\"\"\n    if not self.initialized:\n        return self.setup_client_and_return_all_model_parameters(config)\n\n    # Make sure the proper components are there\n    assert self.model is not None and self.parameter_exchanger is not None and self.loss_for_adaptation is not None\n    model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n\n    # Weights and training loss sent to server for aggregation. Training loss is sent because server will\n    # decide to increase or decrease the penalty weight, if adaptivity is turned on.\n    return self.parameter_exchanger.pack_parameters(model_weights, self.loss_for_adaptation)\n</code></pre>"},{"location":"api/#fl4health.mixins.AdaptiveDriftConstrainedMixin.setup_client_and_return_all_model_parameters","title":"<code>setup_client_and_return_all_model_parameters(config)</code>","text":"<p>Function used to setup the client using the provided configuration and then exact all model parameters from <code>self.model</code> and return them. This function is used as a helper for <code>get_parameters</code> when the client has yet to be initialized.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration to be used  in setting up the client.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>All parameters associated with the <code>self.model</code> property of the client.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def setup_client_and_return_all_model_parameters(\n    self: AdaptiveDriftConstrainedProtocol, config: Config\n) -&gt; NDArrays:\n    \"\"\"\n    Function used to setup the client using the provided configuration and then exact all model parameters from\n    ``self.model`` and return them. This function is used as a helper for ``get_parameters`` when the client\n    has yet to be initialized.\n\n    Args:\n        config (Config): Configuration to be used  in setting up the client.\n\n    Returns:\n        (NDArrays): All parameters associated with the ``self.model`` property of the client.\n    \"\"\"\n    log(INFO, \"Setting up client and providing full model parameters to the server for initialization\")\n    if not config:\n        log(\n            WARNING,\n            (\n                \"This client has not yet been initialized and the config is empty. This may cause unexpected \"\n                \"failures, as setting up a client typically requires several configuration parameters, \"\n                \"including batch_size and current_server_round.\"\n            ),\n        )\n\n    # If initialized is False, the server is requesting model parameters from which to initialize all other\n    # clients. As such get_parameters is being called before fit or evaluate, so we must call\n    # setup_client first.\n    self.setup_client(config)\n\n    # Need all parameters even if normally exchanging partial\n    return FullParameterExchanger().push_parameters(self.model, config=config)\n</code></pre>"},{"location":"api/#fl4health.mixins.AdaptiveDriftConstrainedMixin.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are unpacked for the clients to use in training. In the first fitting round, we assume the full model is being initialized and use the <code>FullParameterExchanger()</code> to set all model weights.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model and also the penalty weight to be applied during training.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. This is used to help determine which parameter exchange should be used for pulling parameters. A full parameter exchanger is always used if the current federated learning round is the very first fitting round.</p> required Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def set_parameters(\n    self: AdaptiveDriftConstrainedProtocol, parameters: NDArrays, config: Config, fitting_round: bool\n) -&gt; None:\n    \"\"\"\n    Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n    unpacked for the clients to use in training. In the first fitting round, we assume the full model is being\n    initialized and use the ``FullParameterExchanger()`` to set all model weights.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model and also the penalty weight to be applied during training.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n            round or an evaluation round. This is used to help determine which parameter exchange should be used\n            for pulling parameters. A full parameter exchanger is always used if the current federated learning\n            round is the very first fitting round.\n    \"\"\"\n    assert self.model is not None and self.parameter_exchanger is not None\n\n    server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n    log(INFO, f\"Penalty weight received from the server: {self.drift_penalty_weight}\")\n\n    super().set_parameters(server_model_state, config, fitting_round)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.AdaptiveDriftConstrainedMixin.get_parameter_exchanger","title":"<code>get_parameter_exchanger(config)</code>","text":"<p>Setting up the parameter exchanger to include the appropriate packing functionality. By default we assume that we're exchanging all parameters. Can be overridden for other behavior.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <p>Returns:</p> Type Description <code>ParameterExchanger</code> <p>Exchanger that can handle packing/unpacking auxiliary server information.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def get_parameter_exchanger(self: AdaptiveDriftConstrainedProtocol, config: Config) -&gt; ParameterExchanger:\n    \"\"\"\n    Setting up the parameter exchanger to include the appropriate packing functionality.\n    By default we assume that we're exchanging all parameters. Can be overridden for other behavior.\n\n    Args:\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n    Returns:\n        (ParameterExchanger): Exchanger that can handle packing/unpacking auxiliary server information.\n    \"\"\"\n    return FullParameterExchangerWithPacking(ParameterPackerAdaptiveConstraint())\n</code></pre>"},{"location":"api/#fl4health.mixins.AdaptiveDriftConstrainedMixin.update_after_train","title":"<code>update_after_train(local_steps, loss_dict, config)</code>","text":"<p>Called after training with the number of <code>local_steps</code> performed over the FL round and the corresponding loss dictionary. We use this to store the training loss that we want to use to adapt the penalty weight parameter on the server side.</p> <p>Parameters:</p> Name Type Description Default <code>local_steps</code> <code>int</code> <p>The number of steps so far in the round in the local training.</p> required <code>loss_dict</code> <code>dict[str, float]</code> <p>A dictionary of losses from local training.</p> required <code>config</code> <code>Config</code> <p>The config from the server</p> required Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def update_after_train(\n    self: AdaptiveDriftConstrainedProtocol, local_steps: int, loss_dict: dict[str, float], config: Config\n) -&gt; None:\n    \"\"\"\n    Called after training with the number of ``local_steps`` performed over the FL round and the corresponding loss\n    dictionary. We use this to store the training loss that we want to use to adapt the penalty weight parameter\n    on the server side.\n\n    Args:\n        local_steps (int): The number of steps so far in the round in the local training.\n        loss_dict (dict[str, float]): A dictionary of losses from local training.\n        config (Config): The config from the server\n    \"\"\"\n    assert \"loss_for_adaptation\" in loss_dict\n    # Store current loss which is the vanilla loss without the penalty term added in\n    self.loss_for_adaptation = loss_dict[\"loss_for_adaptation\"]\n    super().update_after_train(local_steps, loss_dict, config)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.AdaptiveDriftConstrainedMixin.compute_penalty_loss","title":"<code>compute_penalty_loss()</code>","text":"<p>Computes the drift loss for the client model and drift tensors.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed penalty loss tensor.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def compute_penalty_loss(self: AdaptiveDriftConstrainedProtocol) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the drift loss for the client model and drift tensors.\n\n    Returns:\n        (torch.Tensor): Computed penalty loss tensor.\n    \"\"\"\n    # Penalty tensors must have been set for these clients.\n    assert self.drift_penalty_tensors is not None\n\n    return self.penalty_loss_function(self.model, self.drift_penalty_tensors, self.drift_penalty_weight)\n</code></pre>"},{"location":"api/#fl4health.mixins.adaptive_drift_constrained","title":"<code>adaptive_drift_constrained</code>","text":"<p>AdaptiveDriftConstrainedMixin.</p>"},{"location":"api/#fl4health.mixins.adaptive_drift_constrained.AdaptiveDriftConstrainedMixin","title":"<code>AdaptiveDriftConstrainedMixin</code>","text":"<p>               Bases: <code>BaseFlexibleMixin</code></p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>class AdaptiveDriftConstrainedMixin(BaseFlexibleMixin):\n    def __init__(self: AdaptiveDriftConstrainedProtocol, *args: Any, **kwargs: Any):\n        \"\"\"\n        Adaptive Drift Constrained Mixin.\n\n        To be used with ``~fl4health.BaseClient`` in order to add the ability to compute\n        losses via a constrained adaptive drift.\n\n        **NOTE**: Rather than using ``AdaptiveDriftConstraintClient``, if a client subclasses\n        ``FlexibleClient``, than this mixin could be used on that subclass to implement the\n        adaptive drift constraint.\n        \"\"\"\n        # Initialize mixin-specific attributes with default values\n        self.loss_for_adaptation = 0.1\n        self.drift_penalty_tensors = None\n        self.drift_penalty_weight = None\n\n        super().__init__(*args, **kwargs)\n\n        self.penalty_loss_function = WeightDriftLoss(self.device)\n\n    def get_parameters(self: AdaptiveDriftConstrainedProtocol, config: Config) -&gt; NDArrays:\n        \"\"\"\n        Packs the parameters and training loss into a single ``NDArrays`` to be sent to the server for aggregation. If\n        the client has not been initialized, this means the server is requesting parameters for initialization and\n        just the model parameters are sent. When using the ``FedAvgWithAdaptiveConstraint`` strategy, this should not\n        happen, as that strategy requires server-side initialization parameters. However, other strategies may handle\n        this case.\n\n        Args:\n            config (Config): Configurations to allow for customization of this functions behavior\n\n        Returns:\n            (NDArrays): Parameters and training loss packed together into a list of numpy arrays to be sent to the\n                server.\n        \"\"\"\n        if not self.initialized:\n            return self.setup_client_and_return_all_model_parameters(config)\n\n        # Make sure the proper components are there\n        assert self.model is not None and self.parameter_exchanger is not None and self.loss_for_adaptation is not None\n        model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n\n        # Weights and training loss sent to server for aggregation. Training loss is sent because server will\n        # decide to increase or decrease the penalty weight, if adaptivity is turned on.\n        return self.parameter_exchanger.pack_parameters(model_weights, self.loss_for_adaptation)\n\n    def setup_client_and_return_all_model_parameters(\n        self: AdaptiveDriftConstrainedProtocol, config: Config\n    ) -&gt; NDArrays:\n        \"\"\"\n        Function used to setup the client using the provided configuration and then exact all model parameters from\n        ``self.model`` and return them. This function is used as a helper for ``get_parameters`` when the client\n        has yet to be initialized.\n\n        Args:\n            config (Config): Configuration to be used  in setting up the client.\n\n        Returns:\n            (NDArrays): All parameters associated with the ``self.model`` property of the client.\n        \"\"\"\n        log(INFO, \"Setting up client and providing full model parameters to the server for initialization\")\n        if not config:\n            log(\n                WARNING,\n                (\n                    \"This client has not yet been initialized and the config is empty. This may cause unexpected \"\n                    \"failures, as setting up a client typically requires several configuration parameters, \"\n                    \"including batch_size and current_server_round.\"\n                ),\n            )\n\n        # If initialized is False, the server is requesting model parameters from which to initialize all other\n        # clients. As such get_parameters is being called before fit or evaluate, so we must call\n        # setup_client first.\n        self.setup_client(config)\n\n        # Need all parameters even if normally exchanging partial\n        return FullParameterExchanger().push_parameters(self.model, config=config)\n\n    def set_parameters(\n        self: AdaptiveDriftConstrainedProtocol, parameters: NDArrays, config: Config, fitting_round: bool\n    ) -&gt; None:\n        \"\"\"\n        Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n        unpacked for the clients to use in training. In the first fitting round, we assume the full model is being\n        initialized and use the ``FullParameterExchanger()`` to set all model weights.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model and also the penalty weight to be applied during training.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n                round or an evaluation round. This is used to help determine which parameter exchange should be used\n                for pulling parameters. A full parameter exchanger is always used if the current federated learning\n                round is the very first fitting round.\n        \"\"\"\n        assert self.model is not None and self.parameter_exchanger is not None\n\n        server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n        log(INFO, f\"Penalty weight received from the server: {self.drift_penalty_weight}\")\n\n        super().set_parameters(server_model_state, config, fitting_round)  # type: ignore[safe-super]\n\n    def train_step(\n        self: AdaptiveDriftConstrainedProtocol, input: TorchInputType, target: TorchTargetType\n    ) -&gt; tuple[TrainingLosses, TorchPredType]:\n        losses, preds = self._compute_preds_and_losses(self.model, self.optimizers[\"global\"], input, target)\n        loss_clone = losses.backward[\"backward\"].clone()\n\n        # apply penalty\n        penalty_loss = self.compute_penalty_loss()\n        losses.backward[\"backward\"] = losses.backward[\"backward\"] + penalty_loss\n        losses = self._apply_backwards_on_losses_and_take_step(self.model, self.optimizers[\"global\"], losses)\n\n        # prepare return values\n        additional_losses = {\n            \"penalty_loss\": penalty_loss.clone(),\n            \"local_loss\": loss_clone,\n            \"loss_for_adaptation\": loss_clone.clone(),\n        }\n        losses.additional_losses = additional_losses\n\n        return losses, preds\n\n    def get_parameter_exchanger(self: AdaptiveDriftConstrainedProtocol, config: Config) -&gt; ParameterExchanger:\n        \"\"\"\n        Setting up the parameter exchanger to include the appropriate packing functionality.\n        By default we assume that we're exchanging all parameters. Can be overridden for other behavior.\n\n        Args:\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n        Returns:\n            (ParameterExchanger): Exchanger that can handle packing/unpacking auxiliary server information.\n        \"\"\"\n        return FullParameterExchangerWithPacking(ParameterPackerAdaptiveConstraint())\n\n    def update_after_train(\n        self: AdaptiveDriftConstrainedProtocol, local_steps: int, loss_dict: dict[str, float], config: Config\n    ) -&gt; None:\n        \"\"\"\n        Called after training with the number of ``local_steps`` performed over the FL round and the corresponding loss\n        dictionary. We use this to store the training loss that we want to use to adapt the penalty weight parameter\n        on the server side.\n\n        Args:\n            local_steps (int): The number of steps so far in the round in the local training.\n            loss_dict (dict[str, float]): A dictionary of losses from local training.\n            config (Config): The config from the server\n        \"\"\"\n        assert \"loss_for_adaptation\" in loss_dict\n        # Store current loss which is the vanilla loss without the penalty term added in\n        self.loss_for_adaptation = loss_dict[\"loss_for_adaptation\"]\n        super().update_after_train(local_steps, loss_dict, config)  # type: ignore[safe-super]\n\n    def compute_penalty_loss(self: AdaptiveDriftConstrainedProtocol) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the drift loss for the client model and drift tensors.\n\n        Returns:\n            (torch.Tensor): Computed penalty loss tensor.\n        \"\"\"\n        # Penalty tensors must have been set for these clients.\n        assert self.drift_penalty_tensors is not None\n\n        return self.penalty_loss_function(self.model, self.drift_penalty_tensors, self.drift_penalty_weight)\n</code></pre>"},{"location":"api/#fl4health.mixins.adaptive_drift_constrained.AdaptiveDriftConstrainedMixin.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Adaptive Drift Constrained Mixin.</p> <p>To be used with <code>~fl4health.BaseClient</code> in order to add the ability to compute losses via a constrained adaptive drift.</p> <p>NOTE: Rather than using <code>AdaptiveDriftConstraintClient</code>, if a client subclasses <code>FlexibleClient</code>, than this mixin could be used on that subclass to implement the adaptive drift constraint.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def __init__(self: AdaptiveDriftConstrainedProtocol, *args: Any, **kwargs: Any):\n    \"\"\"\n    Adaptive Drift Constrained Mixin.\n\n    To be used with ``~fl4health.BaseClient`` in order to add the ability to compute\n    losses via a constrained adaptive drift.\n\n    **NOTE**: Rather than using ``AdaptiveDriftConstraintClient``, if a client subclasses\n    ``FlexibleClient``, than this mixin could be used on that subclass to implement the\n    adaptive drift constraint.\n    \"\"\"\n    # Initialize mixin-specific attributes with default values\n    self.loss_for_adaptation = 0.1\n    self.drift_penalty_tensors = None\n    self.drift_penalty_weight = None\n\n    super().__init__(*args, **kwargs)\n\n    self.penalty_loss_function = WeightDriftLoss(self.device)\n</code></pre>"},{"location":"api/#fl4health.mixins.adaptive_drift_constrained.AdaptiveDriftConstrainedMixin.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>Packs the parameters and training loss into a single <code>NDArrays</code> to be sent to the server for aggregation. If the client has not been initialized, this means the server is requesting parameters for initialization and just the model parameters are sent. When using the <code>FedAvgWithAdaptiveConstraint</code> strategy, this should not happen, as that strategy requires server-side initialization parameters. However, other strategies may handle this case.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configurations to allow for customization of this functions behavior</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Parameters and training loss packed together into a list of numpy arrays to be sent to the server.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def get_parameters(self: AdaptiveDriftConstrainedProtocol, config: Config) -&gt; NDArrays:\n    \"\"\"\n    Packs the parameters and training loss into a single ``NDArrays`` to be sent to the server for aggregation. If\n    the client has not been initialized, this means the server is requesting parameters for initialization and\n    just the model parameters are sent. When using the ``FedAvgWithAdaptiveConstraint`` strategy, this should not\n    happen, as that strategy requires server-side initialization parameters. However, other strategies may handle\n    this case.\n\n    Args:\n        config (Config): Configurations to allow for customization of this functions behavior\n\n    Returns:\n        (NDArrays): Parameters and training loss packed together into a list of numpy arrays to be sent to the\n            server.\n    \"\"\"\n    if not self.initialized:\n        return self.setup_client_and_return_all_model_parameters(config)\n\n    # Make sure the proper components are there\n    assert self.model is not None and self.parameter_exchanger is not None and self.loss_for_adaptation is not None\n    model_weights = self.parameter_exchanger.push_parameters(self.model, config=config)\n\n    # Weights and training loss sent to server for aggregation. Training loss is sent because server will\n    # decide to increase or decrease the penalty weight, if adaptivity is turned on.\n    return self.parameter_exchanger.pack_parameters(model_weights, self.loss_for_adaptation)\n</code></pre>"},{"location":"api/#fl4health.mixins.adaptive_drift_constrained.AdaptiveDriftConstrainedMixin.setup_client_and_return_all_model_parameters","title":"<code>setup_client_and_return_all_model_parameters(config)</code>","text":"<p>Function used to setup the client using the provided configuration and then exact all model parameters from <code>self.model</code> and return them. This function is used as a helper for <code>get_parameters</code> when the client has yet to be initialized.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>Configuration to be used  in setting up the client.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>All parameters associated with the <code>self.model</code> property of the client.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def setup_client_and_return_all_model_parameters(\n    self: AdaptiveDriftConstrainedProtocol, config: Config\n) -&gt; NDArrays:\n    \"\"\"\n    Function used to setup the client using the provided configuration and then exact all model parameters from\n    ``self.model`` and return them. This function is used as a helper for ``get_parameters`` when the client\n    has yet to be initialized.\n\n    Args:\n        config (Config): Configuration to be used  in setting up the client.\n\n    Returns:\n        (NDArrays): All parameters associated with the ``self.model`` property of the client.\n    \"\"\"\n    log(INFO, \"Setting up client and providing full model parameters to the server for initialization\")\n    if not config:\n        log(\n            WARNING,\n            (\n                \"This client has not yet been initialized and the config is empty. This may cause unexpected \"\n                \"failures, as setting up a client typically requires several configuration parameters, \"\n                \"including batch_size and current_server_round.\"\n            ),\n        )\n\n    # If initialized is False, the server is requesting model parameters from which to initialize all other\n    # clients. As such get_parameters is being called before fit or evaluate, so we must call\n    # setup_client first.\n    self.setup_client(config)\n\n    # Need all parameters even if normally exchanging partial\n    return FullParameterExchanger().push_parameters(self.model, config=config)\n</code></pre>"},{"location":"api/#fl4health.mixins.adaptive_drift_constrained.AdaptiveDriftConstrainedMixin.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are unpacked for the clients to use in training. In the first fitting round, we assume the full model is being initialized and use the <code>FullParameterExchanger()</code> to set all model weights.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model and also the penalty weight to be applied during training.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. This is used to help determine which parameter exchange should be used for pulling parameters. A full parameter exchanger is always used if the current federated learning round is the very first fitting round.</p> required Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def set_parameters(\n    self: AdaptiveDriftConstrainedProtocol, parameters: NDArrays, config: Config, fitting_round: bool\n) -&gt; None:\n    \"\"\"\n    Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n    unpacked for the clients to use in training. In the first fitting round, we assume the full model is being\n    initialized and use the ``FullParameterExchanger()`` to set all model weights.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model and also the penalty weight to be applied during training.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n            round or an evaluation round. This is used to help determine which parameter exchange should be used\n            for pulling parameters. A full parameter exchanger is always used if the current federated learning\n            round is the very first fitting round.\n    \"\"\"\n    assert self.model is not None and self.parameter_exchanger is not None\n\n    server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n    log(INFO, f\"Penalty weight received from the server: {self.drift_penalty_weight}\")\n\n    super().set_parameters(server_model_state, config, fitting_round)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.adaptive_drift_constrained.AdaptiveDriftConstrainedMixin.get_parameter_exchanger","title":"<code>get_parameter_exchanger(config)</code>","text":"<p>Setting up the parameter exchanger to include the appropriate packing functionality. By default we assume that we're exchanging all parameters. Can be overridden for other behavior.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <p>Returns:</p> Type Description <code>ParameterExchanger</code> <p>Exchanger that can handle packing/unpacking auxiliary server information.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def get_parameter_exchanger(self: AdaptiveDriftConstrainedProtocol, config: Config) -&gt; ParameterExchanger:\n    \"\"\"\n    Setting up the parameter exchanger to include the appropriate packing functionality.\n    By default we assume that we're exchanging all parameters. Can be overridden for other behavior.\n\n    Args:\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n    Returns:\n        (ParameterExchanger): Exchanger that can handle packing/unpacking auxiliary server information.\n    \"\"\"\n    return FullParameterExchangerWithPacking(ParameterPackerAdaptiveConstraint())\n</code></pre>"},{"location":"api/#fl4health.mixins.adaptive_drift_constrained.AdaptiveDriftConstrainedMixin.update_after_train","title":"<code>update_after_train(local_steps, loss_dict, config)</code>","text":"<p>Called after training with the number of <code>local_steps</code> performed over the FL round and the corresponding loss dictionary. We use this to store the training loss that we want to use to adapt the penalty weight parameter on the server side.</p> <p>Parameters:</p> Name Type Description Default <code>local_steps</code> <code>int</code> <p>The number of steps so far in the round in the local training.</p> required <code>loss_dict</code> <code>dict[str, float]</code> <p>A dictionary of losses from local training.</p> required <code>config</code> <code>Config</code> <p>The config from the server</p> required Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def update_after_train(\n    self: AdaptiveDriftConstrainedProtocol, local_steps: int, loss_dict: dict[str, float], config: Config\n) -&gt; None:\n    \"\"\"\n    Called after training with the number of ``local_steps`` performed over the FL round and the corresponding loss\n    dictionary. We use this to store the training loss that we want to use to adapt the penalty weight parameter\n    on the server side.\n\n    Args:\n        local_steps (int): The number of steps so far in the round in the local training.\n        loss_dict (dict[str, float]): A dictionary of losses from local training.\n        config (Config): The config from the server\n    \"\"\"\n    assert \"loss_for_adaptation\" in loss_dict\n    # Store current loss which is the vanilla loss without the penalty term added in\n    self.loss_for_adaptation = loss_dict[\"loss_for_adaptation\"]\n    super().update_after_train(local_steps, loss_dict, config)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.adaptive_drift_constrained.AdaptiveDriftConstrainedMixin.compute_penalty_loss","title":"<code>compute_penalty_loss()</code>","text":"<p>Computes the drift loss for the client model and drift tensors.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Computed penalty loss tensor.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def compute_penalty_loss(self: AdaptiveDriftConstrainedProtocol) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the drift loss for the client model and drift tensors.\n\n    Returns:\n        (torch.Tensor): Computed penalty loss tensor.\n    \"\"\"\n    # Penalty tensors must have been set for these clients.\n    assert self.drift_penalty_tensors is not None\n\n    return self.penalty_loss_function(self.model, self.drift_penalty_tensors, self.drift_penalty_weight)\n</code></pre>"},{"location":"api/#fl4health.mixins.adaptive_drift_constrained.apply_adaptive_drift_to_client","title":"<code>apply_adaptive_drift_to_client(client_base_type)</code>","text":"<p>Dynamically create an adapted client class.</p> <p>Parameters:</p> Name Type Description Default <code>client_base_type</code> <code>type[FlexibleClient]</code> <p>The class to be mixed.</p> required <p>Returns:</p> Type Description <code>type[FlexibleClient]</code> <p>A basic client that has been mixed with <code>AdaptiveDriftConstrainedMixin</code>.</p> Source code in <code>fl4health/mixins/adaptive_drift_constrained.py</code> <pre><code>def apply_adaptive_drift_to_client(client_base_type: type[FlexibleClient]) -&gt; type[FlexibleClient]:\n    \"\"\"\n    Dynamically create an adapted client class.\n\n    Args:\n        client_base_type (type[FlexibleClient]): The class to be mixed.\n\n    Returns:\n        (type[FlexibleClient]): A basic client that has been mixed with `AdaptiveDriftConstrainedMixin`.\n    \"\"\"\n    return type(\n        f\"AdaptiveDrift{client_base_type.__name__}\",\n        (\n            AdaptiveDriftConstrainedMixin,\n            client_base_type,\n        ),\n        {\n            # Special flag to bypass validation\n            \"_dynamically_created\": True\n        },\n    )\n</code></pre>"},{"location":"api/#fl4health.mixins.base","title":"<code>base</code>","text":"<p>BaseFlexibleMixin.</p>"},{"location":"api/#fl4health.mixins.base.BaseFlexibleMixin","title":"<code>BaseFlexibleMixin</code>","text":"Source code in <code>fl4health/mixins/base.py</code> <pre><code>class BaseFlexibleMixin:\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize a base mixin.\"\"\"\n        super().__init__(*args, **kwargs)\n\n    def __init_subclass__(cls, **kwargs: Any) -&gt; None:\n        \"\"\"This method is called when a class inherits from BaseFlexibleMixin.\"\"\"\n        super().__init_subclass__(**kwargs)\n\n        # Skip check for other mixins\n        if cls.__name__.endswith(\"Mixin\"):\n            return\n\n        # Skip validation for dynamically created classes\n        if hasattr(cls, \"_dynamically_created\"):\n            return\n\n        # Check at class definition time if the parent class satisfies FlexibleClientProtocol\n        for base in cls.__bases__:\n            if base is not BaseFlexibleMixin and issubclass(base, FlexibleClient):\n                return\n\n        # If we get here, no compatible base was found\n        msg = (\n            f\"Class {cls.__name__} inherits from BaseFlexibleMixin but none of its other \"\n            \"base classes implement FlexibleClient.\"\n        )\n        log(ERROR, msg)\n        raise RuntimeError(msg)\n</code></pre>"},{"location":"api/#fl4health.mixins.base.BaseFlexibleMixin.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize a base mixin.</p> Source code in <code>fl4health/mixins/base.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize a base mixin.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/#fl4health.mixins.base.BaseFlexibleMixin.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>This method is called when a class inherits from BaseFlexibleMixin.</p> Source code in <code>fl4health/mixins/base.py</code> <pre><code>def __init_subclass__(cls, **kwargs: Any) -&gt; None:\n    \"\"\"This method is called when a class inherits from BaseFlexibleMixin.\"\"\"\n    super().__init_subclass__(**kwargs)\n\n    # Skip check for other mixins\n    if cls.__name__.endswith(\"Mixin\"):\n        return\n\n    # Skip validation for dynamically created classes\n    if hasattr(cls, \"_dynamically_created\"):\n        return\n\n    # Check at class definition time if the parent class satisfies FlexibleClientProtocol\n    for base in cls.__bases__:\n        if base is not BaseFlexibleMixin and issubclass(base, FlexibleClient):\n            return\n\n    # If we get here, no compatible base was found\n    msg = (\n        f\"Class {cls.__name__} inherits from BaseFlexibleMixin but none of its other \"\n        \"base classes implement FlexibleClient.\"\n    )\n    log(ERROR, msg)\n    raise RuntimeError(msg)\n</code></pre>"},{"location":"api/#fl4health.mixins.core_protocols","title":"<code>core_protocols</code>","text":""},{"location":"api/#fl4health.mixins.core_protocols.NumPyClientMinimalProtocol","title":"<code>NumPyClientMinimalProtocol</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>A minimal protocol for <code>NumPyClient</code> with just essential methods.</p> Source code in <code>fl4health/mixins/core_protocols.py</code> <pre><code>@runtime_checkable\nclass NumPyClientMinimalProtocol(Protocol):\n    \"\"\"A minimal protocol for ``NumPyClient`` with just essential methods.\"\"\"\n\n    def get_parameters(self, config: dict[str, Scalar]) -&gt; NDArrays: ...\n\n    def fit(self, parameters: NDArrays, config: dict[str, Scalar]) -&gt; tuple[NDArrays, int, dict[str, Scalar]]: ...\n\n    def evaluate(self, parameters: NDArrays, config: dict[str, Scalar]) -&gt; tuple[float, int, dict[str, Scalar]]: ...\n\n    def set_parameters(self, parameters: NDArrays, config: Config, fitting_round: bool) -&gt; None: ...\n\n    def update_after_train(self, local_steps: int, loss_dict: dict[str, float], config: Config) -&gt; None: ...\n</code></pre>"},{"location":"api/#fl4health.mixins.core_protocols.FlexibleClientProtocolPreSetup","title":"<code>FlexibleClientProtocolPreSetup</code>","text":"<p>               Bases: <code>NumPyClientMinimalProtocol</code>, <code>Protocol</code></p> <p>A minimal protocol for <code>BasicClient</code> focused on methods.</p> Source code in <code>fl4health/mixins/core_protocols.py</code> <pre><code>@runtime_checkable\nclass FlexibleClientProtocolPreSetup(NumPyClientMinimalProtocol, Protocol):\n    \"\"\"A minimal protocol for ``BasicClient`` focused on methods.\"\"\"\n\n    device: torch.device\n    initialized: bool\n\n    # Include only methods, not attributes that get initialized later\n    def setup_client(self, config: Config) -&gt; None: ...\n\n    def get_model(self, config: Config) -&gt; nn.Module: ...\n\n    def get_data_loaders(self, config: Config) -&gt; tuple[DataLoader, ...]: ...\n\n    def get_optimizer(self, config: Config) -&gt; Optimizer | dict[str, Optimizer]: ...\n\n    def get_criterion(self, config: Config) -&gt; _Loss: ...\n\n    def compute_loss_and_additional_losses(\n        self, preds: TorchPredType, features: TorchFeatureType, target: TorchTargetType\n    ) -&gt; tuple[torch.Tensor, dict[str, torch.Tensor] | None]: ...\n</code></pre>"},{"location":"api/#fl4health.mixins.core_protocols.FlexibleClientProtocol","title":"<code>FlexibleClientProtocol</code>","text":"<p>               Bases: <code>FlexibleClientProtocolPreSetup</code>, <code>Protocol</code></p> <p>A minimal protocol for <code>BasicClient</code> focused on methods.</p> Source code in <code>fl4health/mixins/core_protocols.py</code> <pre><code>@runtime_checkable\nclass FlexibleClientProtocol(FlexibleClientProtocolPreSetup, Protocol):\n    \"\"\"A minimal protocol for ``BasicClient`` focused on methods.\"\"\"\n\n    model: nn.Module\n    optimizers: dict[str, torch.optim.Optimizer]\n    train_loader: DataLoader\n    val_loader: DataLoader\n    test_loader: DataLoader | None\n    criterion: _Loss\n\n    def initialize_all_model_weights(self, parameters: NDArrays, config: Config) -&gt; None: ...\n\n    def update_before_train(self, current_server_round: int) -&gt; None: ...\n\n    def _compute_preds_and_losses(\n        self, model: nn.Module, optimizer: Optimizer, input: TorchInputType, target: TorchTargetType\n    ) -&gt; tuple[TrainingLosses, TorchPredType]: ...\n\n    def _apply_backwards_on_losses_and_take_step(\n        self, model: nn.Module, optimizer: Optimizer, losses: TrainingLosses\n    ) -&gt; TrainingLosses: ...\n\n    def _train_step_with_model_and_optimizer(\n        self, model: nn.Module, optimizer: Optimizer, input: TorchInputType, target: TorchTargetType\n    ) -&gt; tuple[TrainingLosses, TorchPredType]: ...\n\n    def _val_step_with_model(\n        self, model: nn.Module, input: TorchInputType, target: TorchTargetType\n    ) -&gt; tuple[EvaluationLosses, TorchPredType]: ...\n\n    def predict_with_model(\n        self, model: nn.Module, input: TorchInputType\n    ) -&gt; tuple[TorchPredType, TorchFeatureType]: ...\n\n    def transform_target(self, target: TorchTargetType) -&gt; TorchTargetType: ...\n\n    def _transform_gradients_with_model(self, model: torch.nn.Module, losses: TrainingLosses) -&gt; None: ...\n\n    def transform_gradients(self, losses: TrainingLosses) -&gt; None: ...\n\n    def compute_training_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses: ...\n\n    def validate(self, include_losses_in_metrics: bool = False) -&gt; tuple[float, dict[str, Scalar]]: ...\n\n    def compute_evaluation_loss(\n        self,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses: ...\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized","title":"<code>personalized</code>","text":""},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin","title":"<code>DittoPersonalizedMixin</code>","text":"<p>               Bases: <code>AdaptiveDriftConstrainedMixin</code></p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>class DittoPersonalizedMixin(AdaptiveDriftConstrainedMixin):\n    def __init__(self: DittoPersonalizedProtocol, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        This mixin implements the Ditto algorithm from Ditto: Fair and Robust Federated Learning Through\n        Personalization. This mixin inherits from the ``AdaptiveDriftConstrainedMixin``, and like that mixin,\n        this should be mixed with a ``FlexibleClient`` type in order to apply the Ditto personalization method\n        to that client.\n\n        Background Context:\n\n        The idea is that we want to train personalized versions of the global model for each client. So we\n        simultaneously train a global model that is aggregated on the server-side and use those weights to also\n        constrain the training of a local model. The constraint for this local model is identical to the FedProx loss.\n\n\n        Raises:\n            RuntimeError: If the object does not satisfy the ``FlexibleClientProtocolPreSetup`` then it will raise an\n                error. This is additional validation to ensure that the mixin was applied to an appropriate base class.\n        \"\"\"\n        # Initialize mixin-specific attributes\n        self.global_model: torch.nn.Module | None = None\n\n        super().__init__(*args, **kwargs)\n\n    def safe_global_model(self: DittoPersonalizedProtocol) -&gt; nn.Module:\n        \"\"\"\n        Convenient accessor for the global model.\n\n        Raises:\n            ValueError: If the ``global_model`` attribute has not yet been set, we will raise an error.\n\n        Returns:\n            (nn.Module): the global model if it has been set.\n        \"\"\"\n        if self.global_model:\n            return self.global_model\n        raise ValueError(\"Cannot get global model as it not yet been set.\")\n\n    @property\n    def optimizer_keys(self: DittoPersonalizedProtocol) -&gt; list[str]:\n        \"\"\"\n        Property for optimizer keys.\n\n        Returns:\n            (list[str]): list of keys for the optimizers dictionary.\n        \"\"\"\n        return [\"local\", \"global\"]\n\n    def _copy_optimizer_with_new_params(self: DittoPersonalizedProtocol, original_optimizer: Optimizer) -&gt; Optimizer:\n        \"\"\"\n        Helper method to make a copy of the original optimizer for the global model.\n\n        Args:\n            original_optimizer (Optimizer): original optimizer of the underlying `FlexibleClient`.\n\n        Returns:\n            (Optimizer): a copy of the original optimizer to be used by the global model.\n        \"\"\"\n        optim_class = original_optimizer.__class__\n        state_dict = original_optimizer.state_dict()\n\n        # Extract hyperparameters from param_groups\n        # We only take the first group's hyperparameters, excluding 'params' and 'lr'\n        param_group = state_dict[\"param_groups\"][0]\n\n        # store initial_lr to be used with schedulers\n        try:\n            initial_lr = param_group[\"initial_lr\"]\n        except KeyError:\n            if \"lr\" in original_optimizer.defaults:\n                initial_lr = original_optimizer.defaults[\"lr\"]\n            else:\n                initial_lr = 1e-3\n                log(\n                    WARN,\n                    \"Unable to get the original `lr` for the global optimizer, falling back to `1e-3`.\",\n                )\n\n        optimizer_kwargs = {k: v for k, v in param_group.items() if k not in (\"params\", \"initial_lr\")}\n        assert self.global_model is not None\n        global_optimizer = optim_class(self.global_model.parameters(), **optimizer_kwargs)\n\n        # maintain initial_lr for schedulers\n        for param_group in global_optimizer.param_groups:\n            param_group[\"initial_lr\"] = initial_lr\n\n        return global_optimizer\n\n    def get_global_model(self: DittoPersonalizedProtocol, config: Config) -&gt; nn.Module:\n        \"\"\"\n        Returns the global model to be used during Ditto training and as a constraint for the local model.\n\n        The global model should be the same architecture as the local model so we reuse the ``get_model`` call. We\n        explicitly send the model to the desired device. This is idempotent.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (nn.Module): The PyTorch model serving as the global model for Ditto\n        \"\"\"\n        model_copy = copy.deepcopy(self.get_model(config))\n        return model_copy.to(self.device)\n\n    @ensure_protocol_compliance\n    def get_optimizer(self: DittoPersonalizedProtocol, config: Config) -&gt; dict[str, Optimizer]:\n        \"\"\"\n        Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        if self.global_model is None:\n            # try set it here\n            self.global_model = self.get_global_model(config)  # is this the same config?\n            log(\n                INFO,\n                f\"global model set: {type(self.global_model).__name__} within `get_optimizer`\",\n            )\n\n        # Note that the global optimizer operates on self.global_model.parameters()\n        optimizer = super().get_optimizer(config=config)  # type: ignore[safe-super]\n        if isinstance(optimizer, dict):\n            try:\n                original_optimizer = next(el for el in optimizer.values() if isinstance(el, Optimizer))\n            except StopIteration as e:\n                log(ERROR, \"Unable to find an ~torch.optim.Optimizer object.\")\n                raise e\n        elif isinstance(optimizer, Optimizer):\n            original_optimizer = optimizer\n        else:\n            raise ValueError(\"`super().get_optimizer()` returned an invalid type.\")\n\n        global_optimizer = self._copy_optimizer_with_new_params(original_optimizer)\n        return {\"local\": original_optimizer, \"global\": global_optimizer}\n\n    def set_optimizer(self: DittoPersonalizedProtocol, config: Config) -&gt; None:\n        \"\"\"\n        Ditto requires an optimizer for the global model and one for the local model. This function simply ensures that\n        the optimizers setup by the user have the proper keys and that there are two optimizers.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        optimizers = self.get_optimizer(config)\n        assert isinstance(optimizers, dict) and set(self.optimizer_keys) == set(optimizers.keys())\n        self.optimizers = optimizers\n\n    @ensure_protocol_compliance\n    def setup_client(self: DittoPersonalizedProtocol, config: Config) -&gt; None:\n        \"\"\"\n        Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n        Then set initialized attribute to True. In this class, this function simply adds the additional step of\n        setting up the global model.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        try:\n            self.global_model = self.get_global_model(config)\n            log(INFO, f\"global model set: {type(self.global_model).__name__}\")\n        except AttributeError:\n            log(\n                INFO,\n                \"Couldn't set global model before super().setup_client(). Will try again within that setup.\",\n            )\n            pass\n        # The rest of the setup is the same\n        super().setup_client(config)  # type:ignore [safe-super]\n\n    def get_parameters(self: DittoPersonalizedProtocol, config: Config) -&gt; NDArrays:\n        \"\"\"\n        For Ditto, we transfer the **GLOBAL** model weights to the server to be aggregated. The local model weights\n        stay with the client.\n\n        Args:\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n        Returns:\n            (NDArrays): **GLOBAL** model weights to be sent to the server for aggregation.\n        \"\"\"\n        if not self.initialized:\n            return self.setup_client_and_return_all_model_parameters(config)\n\n        # NOTE: the global model weights are sent to the server here.\n        if self.global_model is None:\n            raise ValueError(\"Unable to get parameters with unset global model.\")\n        global_model_weights = self.parameter_exchanger.push_parameters(self.global_model, config=config)\n\n        # Weights and training loss sent to server for aggregation\n        # Training loss sent because server will decide to increase or decrease the penalty weight, if adaptivity\n        # is turned on\n        packed_params = self.parameter_exchanger.pack_parameters(global_model_weights, self.loss_for_adaptation)\n        log(INFO, \"Successfully packed parameters of global model\")\n        return packed_params\n\n    @ensure_protocol_compliance\n    def set_parameters(\n        self: DittoPersonalizedProtocol,\n        parameters: NDArrays,\n        config: Config,\n        fitting_round: bool,\n    ) -&gt; None:\n        \"\"\"\n        Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n        unpacked for the clients to use in training. The parameters being passed are to be routed to the global model.\n        In the first fitting round, we assume the both the global and local models are being initialized and use\n        the ``FullParameterExchanger()`` to initialize both sets of model weights to the same parameters.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model (global model for all but the first step of Ditto). These should also include a penalty weight\n                from the server that needs to be unpacked.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning\n                round is a fitting round or an evaluation round. This is used to help determine which parameter\n                exchange should be used for pulling parameters. If the current federated learning round is the very\n                first fitting round, then we initialize both the global and local Ditto models with weights sent from\n                the server.\n        \"\"\"\n        # Make sure that the proper components exist.\n        assert self.global_model is not None and self.model is not None and self.parameter_exchanger is not None\n        server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n        log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n        current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n        if current_server_round == 1 and fitting_round:\n            log(\n                INFO,\n                \"Initializing the global and local models weights for the first time\",\n            )\n            self.initialize_all_model_weights(server_model_state, config)\n        else:\n            # Route the parameters to the GLOBAL model in Ditto after the initial stage\n            log(INFO, \"Setting the global model weights\")\n            self.parameter_exchanger.pull_parameters(server_model_state, self.global_model, config)\n\n    def initialize_all_model_weights(self: DittoPersonalizedProtocol, parameters: NDArrays, config: Config) -&gt; None:\n        \"\"\"\n        If this is the first time we're initializing the model weights, we initialize both the global and the local\n        weights together.\n\n        Args:\n            parameters (NDArrays): Model parameters to be injected into the client model.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        \"\"\"\n        parameter_exchanger = cast(FullParameterExchanger, self.parameter_exchanger)\n        parameter_exchanger.pull_parameters(parameters, self.model, config)\n        parameter_exchanger.pull_parameters(parameters, self.safe_global_model(), config)\n\n    def set_initial_global_tensors(self: DittoPersonalizedProtocol) -&gt; None:\n        \"\"\"\n        Saving the initial **GLOBAL MODEL** weights and detaching them so that we don't compute gradients with\n        respect to the tensors. These are used to form the Ditto local update penalty term.\n        \"\"\"\n        self.drift_penalty_tensors = [\n            initial_layer_weights.detach().clone() for initial_layer_weights in self.safe_global_model().parameters()\n        ]\n\n    @ensure_protocol_compliance\n    def update_before_train(self: DittoPersonalizedProtocol, current_server_round: int) -&gt; None:\n        \"\"\"\n        Procedures that should occur before proceeding with the training loops for the models. In this case, we\n        save the global models parameters to be used in constraining training of the local model.\n\n        Args:\n            current_server_round (int): Indicates which server round we are currently executing.\n        \"\"\"\n        self.set_initial_global_tensors()\n\n        # Need to also set the global model to train mode before any training begins.\n        self.safe_global_model().train()\n\n        super().update_before_train(current_server_round)  # type: ignore[safe-super]\n\n    def train_step(\n        self: DittoPersonalizedProtocol, input: TorchInputType, target: TorchTargetType\n    ) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Mechanics of training loop follow from original Ditto implementation: https://github.com/litian96/ditto.\n\n        As in the implementation there, steps of the global and local models are done in tandem and for the same\n        number of steps.\n\n        Args:\n            input (TorchInputType): input tensor to be run through both the global and local models. Here,\n                ``TorchInputType`` is simply an alias for the union of ``torch.Tensor`` and\n                ``dict[str, torch.Tensor]``.\n            target (TorchTargetType): target tensor to be used to compute a loss given each models outputs.\n\n        Returns:\n            (tuple[TrainingLosses, TorchPredType]): Returns relevant loss values from both the global and local\n                model optimization steps. The prediction dictionary contains predictions indexed a \"global\" and \"local\"\n                corresponding to predictions from the global and local Ditto models for metric evaluations.\n        \"\"\"\n        # global\n        global_losses, global_preds = self._compute_preds_and_losses(\n            self.safe_global_model(), self.optimizers[\"global\"], input, target\n        )\n        # local\n        local_losses, local_preds = self._compute_preds_and_losses(self.model, self.optimizers[\"local\"], input, target)\n        local_loss_clone = local_losses.backward[\"backward\"].clone()  # need a clone for later\n\n        # take step global\n        global_losses = self._apply_backwards_on_losses_and_take_step(\n            self.safe_global_model(), self.optimizers[\"global\"], global_losses\n        )\n        # take step local\n        penalty_loss = self.compute_penalty_loss()\n        local_losses.backward[\"backward\"] = local_losses.backward[\"backward\"] + penalty_loss\n        local_losses = self._apply_backwards_on_losses_and_take_step(\n            self.model, self.optimizers[\"local\"], local_losses\n        )\n\n        # prepare return values\n        additional_losses = {\n            \"penalty_loss\": penalty_loss.clone(),\n            \"local_loss\": local_loss_clone,\n            \"global_loss\": global_losses.backward[\"backward\"],\n            \"loss_for_adaptation\": local_loss_clone.clone(),\n        }\n        local_losses.additional_losses = additional_losses\n\n        # combined preds\n        if isinstance(global_preds, torch.Tensor) and isinstance(local_preds, torch.Tensor):\n            combined_preds = {\"global\": global_preds, \"local\": local_preds}\n        elif isinstance(global_preds, dict) and isinstance(local_preds, dict):\n            combined_preds = {f\"global-{k}\": v for k, v in global_preds.items()}\n            combined_preds.update(**{f\"local-{k}\": v for k, v in local_preds.items()})\n\n        return local_losses, combined_preds\n\n    def val_step(\n        self: DittoPersonalizedProtocol, input: TorchInputType, target: TorchTargetType\n    ) -&gt; tuple[EvaluationLosses, TorchPredType]:\n        # global\n        global_losses, global_preds = self._val_step_with_model(self.safe_global_model(), input, target)\n        # local\n        local_losses, local_preds = self._val_step_with_model(self.model, input, target)\n\n        # combine\n        losses = EvaluationLosses(\n            local_losses.checkpoint,\n            additional_losses={\n                \"global_loss\": global_losses.checkpoint,\n                \"local_loss\": local_losses.checkpoint,\n            },\n        )\n        preds: TorchPredType = {}\n        preds.update(**{f\"global-{k}\": v for k, v in global_preds.items()})\n        preds.update(**{f\"local-{k}\": v for k, v in local_preds.items()})\n        return losses, preds\n\n    @ensure_protocol_compliance\n    def validate(\n        self: DittoPersonalizedProtocol, include_losses_in_metrics: bool = False\n    ) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Validate the current model on the entire validation dataset.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n        \"\"\"\n        # Set the global model to evaluate mode\n        self.safe_global_model().eval()\n        return super().validate(include_losses_in_metrics=include_losses_in_metrics)  # type: ignore[safe-super]\n\n    @ensure_protocol_compliance\n    def compute_evaluation_loss(\n        self: DittoPersonalizedProtocol,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses:\n        \"\"\"\n        Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n        For Ditto, we use the vanilla loss for the local model in checkpointing. However, during validation we also\n        compute the global model vanilla loss.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n                in preds will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n                indexed by name.\n        \"\"\"\n        # Check that both models are in eval mode\n        assert self.global_model is not None and not self.global_model.training and not self.model.training\n        return super().compute_evaluation_loss(preds, features, target)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.optimizer_keys","title":"<code>optimizer_keys</code>  <code>property</code>","text":"<p>Property for optimizer keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list of keys for the optimizers dictionary.</p>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>This mixin implements the Ditto algorithm from Ditto: Fair and Robust Federated Learning Through Personalization. This mixin inherits from the <code>AdaptiveDriftConstrainedMixin</code>, and like that mixin, this should be mixed with a <code>FlexibleClient</code> type in order to apply the Ditto personalization method to that client.</p> <p>Background Context:</p> <p>The idea is that we want to train personalized versions of the global model for each client. So we simultaneously train a global model that is aggregated on the server-side and use those weights to also constrain the training of a local model. The constraint for this local model is identical to the FedProx loss.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the object does not satisfy the <code>FlexibleClientProtocolPreSetup</code> then it will raise an error. This is additional validation to ensure that the mixin was applied to an appropriate base class.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def __init__(self: DittoPersonalizedProtocol, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    This mixin implements the Ditto algorithm from Ditto: Fair and Robust Federated Learning Through\n    Personalization. This mixin inherits from the ``AdaptiveDriftConstrainedMixin``, and like that mixin,\n    this should be mixed with a ``FlexibleClient`` type in order to apply the Ditto personalization method\n    to that client.\n\n    Background Context:\n\n    The idea is that we want to train personalized versions of the global model for each client. So we\n    simultaneously train a global model that is aggregated on the server-side and use those weights to also\n    constrain the training of a local model. The constraint for this local model is identical to the FedProx loss.\n\n\n    Raises:\n        RuntimeError: If the object does not satisfy the ``FlexibleClientProtocolPreSetup`` then it will raise an\n            error. This is additional validation to ensure that the mixin was applied to an appropriate base class.\n    \"\"\"\n    # Initialize mixin-specific attributes\n    self.global_model: torch.nn.Module | None = None\n\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.safe_global_model","title":"<code>safe_global_model()</code>","text":"<p>Convenient accessor for the global model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>global_model</code> attribute has not yet been set, we will raise an error.</p> <p>Returns:</p> Type Description <code>Module</code> <p>the global model if it has been set.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def safe_global_model(self: DittoPersonalizedProtocol) -&gt; nn.Module:\n    \"\"\"\n    Convenient accessor for the global model.\n\n    Raises:\n        ValueError: If the ``global_model`` attribute has not yet been set, we will raise an error.\n\n    Returns:\n        (nn.Module): the global model if it has been set.\n    \"\"\"\n    if self.global_model:\n        return self.global_model\n    raise ValueError(\"Cannot get global model as it not yet been set.\")\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.get_global_model","title":"<code>get_global_model(config)</code>","text":"<p>Returns the global model to be used during Ditto training and as a constraint for the local model.</p> <p>The global model should be the same architecture as the local model so we reuse the <code>get_model</code> call. We explicitly send the model to the desired device. This is idempotent.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The PyTorch model serving as the global model for Ditto</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def get_global_model(self: DittoPersonalizedProtocol, config: Config) -&gt; nn.Module:\n    \"\"\"\n    Returns the global model to be used during Ditto training and as a constraint for the local model.\n\n    The global model should be the same architecture as the local model so we reuse the ``get_model`` call. We\n    explicitly send the model to the desired device. This is idempotent.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (nn.Module): The PyTorch model serving as the global model for Ditto\n    \"\"\"\n    model_copy = copy.deepcopy(self.get_model(config))\n    return model_copy.to(self.device)\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.get_optimizer","title":"<code>get_optimizer(config)</code>","text":"<p>Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef get_optimizer(self: DittoPersonalizedProtocol, config: Config) -&gt; dict[str, Optimizer]:\n    \"\"\"\n    Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    if self.global_model is None:\n        # try set it here\n        self.global_model = self.get_global_model(config)  # is this the same config?\n        log(\n            INFO,\n            f\"global model set: {type(self.global_model).__name__} within `get_optimizer`\",\n        )\n\n    # Note that the global optimizer operates on self.global_model.parameters()\n    optimizer = super().get_optimizer(config=config)  # type: ignore[safe-super]\n    if isinstance(optimizer, dict):\n        try:\n            original_optimizer = next(el for el in optimizer.values() if isinstance(el, Optimizer))\n        except StopIteration as e:\n            log(ERROR, \"Unable to find an ~torch.optim.Optimizer object.\")\n            raise e\n    elif isinstance(optimizer, Optimizer):\n        original_optimizer = optimizer\n    else:\n        raise ValueError(\"`super().get_optimizer()` returned an invalid type.\")\n\n    global_optimizer = self._copy_optimizer_with_new_params(original_optimizer)\n    return {\"local\": original_optimizer, \"global\": global_optimizer}\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.set_optimizer","title":"<code>set_optimizer(config)</code>","text":"<p>Ditto requires an optimizer for the global model and one for the local model. This function simply ensures that the optimizers setup by the user have the proper keys and that there are two optimizers.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def set_optimizer(self: DittoPersonalizedProtocol, config: Config) -&gt; None:\n    \"\"\"\n    Ditto requires an optimizer for the global model and one for the local model. This function simply ensures that\n    the optimizers setup by the user have the proper keys and that there are two optimizers.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    optimizers = self.get_optimizer(config)\n    assert isinstance(optimizers, dict) and set(self.optimizer_keys) == set(optimizers.keys())\n    self.optimizers = optimizers\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set initialized attribute to True. In this class, this function simply adds the additional step of setting up the global model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef setup_client(self: DittoPersonalizedProtocol, config: Config) -&gt; None:\n    \"\"\"\n    Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n    Then set initialized attribute to True. In this class, this function simply adds the additional step of\n    setting up the global model.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    try:\n        self.global_model = self.get_global_model(config)\n        log(INFO, f\"global model set: {type(self.global_model).__name__}\")\n    except AttributeError:\n        log(\n            INFO,\n            \"Couldn't set global model before super().setup_client(). Will try again within that setup.\",\n        )\n        pass\n    # The rest of the setup is the same\n    super().setup_client(config)  # type:ignore [safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.get_parameters","title":"<code>get_parameters(config)</code>","text":"<p>For Ditto, we transfer the GLOBAL model weights to the server to be aggregated. The local model weights stay with the client.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>GLOBAL model weights to be sent to the server for aggregation.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def get_parameters(self: DittoPersonalizedProtocol, config: Config) -&gt; NDArrays:\n    \"\"\"\n    For Ditto, we transfer the **GLOBAL** model weights to the server to be aggregated. The local model weights\n    stay with the client.\n\n    Args:\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n    Returns:\n        (NDArrays): **GLOBAL** model weights to be sent to the server for aggregation.\n    \"\"\"\n    if not self.initialized:\n        return self.setup_client_and_return_all_model_parameters(config)\n\n    # NOTE: the global model weights are sent to the server here.\n    if self.global_model is None:\n        raise ValueError(\"Unable to get parameters with unset global model.\")\n    global_model_weights = self.parameter_exchanger.push_parameters(self.global_model, config=config)\n\n    # Weights and training loss sent to server for aggregation\n    # Training loss sent because server will decide to increase or decrease the penalty weight, if adaptivity\n    # is turned on\n    packed_params = self.parameter_exchanger.pack_parameters(global_model_weights, self.loss_for_adaptation)\n    log(INFO, \"Successfully packed parameters of global model\")\n    return packed_params\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are unpacked for the clients to use in training. The parameters being passed are to be routed to the global model. In the first fitting round, we assume the both the global and local models are being initialized and use the <code>FullParameterExchanger()</code> to initialize both sets of model weights to the same parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model (global model for all but the first step of Ditto). These should also include a penalty weight from the server that needs to be unpacked.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. This is used to help determine which parameter exchange should be used for pulling parameters. If the current federated learning round is the very first fitting round, then we initialize both the global and local Ditto models with weights sent from the server.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef set_parameters(\n    self: DittoPersonalizedProtocol,\n    parameters: NDArrays,\n    config: Config,\n    fitting_round: bool,\n) -&gt; None:\n    \"\"\"\n    Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n    unpacked for the clients to use in training. The parameters being passed are to be routed to the global model.\n    In the first fitting round, we assume the both the global and local models are being initialized and use\n    the ``FullParameterExchanger()`` to initialize both sets of model weights to the same parameters.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model (global model for all but the first step of Ditto). These should also include a penalty weight\n            from the server that needs to be unpacked.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning\n            round is a fitting round or an evaluation round. This is used to help determine which parameter\n            exchange should be used for pulling parameters. If the current federated learning round is the very\n            first fitting round, then we initialize both the global and local Ditto models with weights sent from\n            the server.\n    \"\"\"\n    # Make sure that the proper components exist.\n    assert self.global_model is not None and self.model is not None and self.parameter_exchanger is not None\n    server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n    log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n    current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n    if current_server_round == 1 and fitting_round:\n        log(\n            INFO,\n            \"Initializing the global and local models weights for the first time\",\n        )\n        self.initialize_all_model_weights(server_model_state, config)\n    else:\n        # Route the parameters to the GLOBAL model in Ditto after the initial stage\n        log(INFO, \"Setting the global model weights\")\n        self.parameter_exchanger.pull_parameters(server_model_state, self.global_model, config)\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.initialize_all_model_weights","title":"<code>initialize_all_model_weights(parameters, config)</code>","text":"<p>If this is the first time we're initializing the model weights, we initialize both the global and the local weights together.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Model parameters to be injected into the client model.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def initialize_all_model_weights(self: DittoPersonalizedProtocol, parameters: NDArrays, config: Config) -&gt; None:\n    \"\"\"\n    If this is the first time we're initializing the model weights, we initialize both the global and the local\n    weights together.\n\n    Args:\n        parameters (NDArrays): Model parameters to be injected into the client model.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n    \"\"\"\n    parameter_exchanger = cast(FullParameterExchanger, self.parameter_exchanger)\n    parameter_exchanger.pull_parameters(parameters, self.model, config)\n    parameter_exchanger.pull_parameters(parameters, self.safe_global_model(), config)\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.set_initial_global_tensors","title":"<code>set_initial_global_tensors()</code>","text":"<p>Saving the initial GLOBAL MODEL weights and detaching them so that we don't compute gradients with respect to the tensors. These are used to form the Ditto local update penalty term.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def set_initial_global_tensors(self: DittoPersonalizedProtocol) -&gt; None:\n    \"\"\"\n    Saving the initial **GLOBAL MODEL** weights and detaching them so that we don't compute gradients with\n    respect to the tensors. These are used to form the Ditto local update penalty term.\n    \"\"\"\n    self.drift_penalty_tensors = [\n        initial_layer_weights.detach().clone() for initial_layer_weights in self.safe_global_model().parameters()\n    ]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.update_before_train","title":"<code>update_before_train(current_server_round)</code>","text":"<p>Procedures that should occur before proceeding with the training loops for the models. In this case, we save the global models parameters to be used in constraining training of the local model.</p> <p>Parameters:</p> Name Type Description Default <code>current_server_round</code> <code>int</code> <p>Indicates which server round we are currently executing.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef update_before_train(self: DittoPersonalizedProtocol, current_server_round: int) -&gt; None:\n    \"\"\"\n    Procedures that should occur before proceeding with the training loops for the models. In this case, we\n    save the global models parameters to be used in constraining training of the local model.\n\n    Args:\n        current_server_round (int): Indicates which server round we are currently executing.\n    \"\"\"\n    self.set_initial_global_tensors()\n\n    # Need to also set the global model to train mode before any training begins.\n    self.safe_global_model().train()\n\n    super().update_before_train(current_server_round)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.train_step","title":"<code>train_step(input, target)</code>","text":"<p>Mechanics of training loop follow from original Ditto implementation: https://github.com/litian96/ditto.</p> <p>As in the implementation there, steps of the global and local models are done in tandem and for the same number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>input tensor to be run through both the global and local models. Here, <code>TorchInputType</code> is simply an alias for the union of <code>torch.Tensor</code> and <code>dict[str, torch.Tensor]</code>.</p> required <code>target</code> <code>TorchTargetType</code> <p>target tensor to be used to compute a loss given each models outputs.</p> required <p>Returns:</p> Type Description <code>tuple[TrainingLosses, TorchPredType]</code> <p>Returns relevant loss values from both the global and local model optimization steps. The prediction dictionary contains predictions indexed a \"global\" and \"local\" corresponding to predictions from the global and local Ditto models for metric evaluations.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def train_step(\n    self: DittoPersonalizedProtocol, input: TorchInputType, target: TorchTargetType\n) -&gt; tuple[TrainingLosses, TorchPredType]:\n    \"\"\"\n    Mechanics of training loop follow from original Ditto implementation: https://github.com/litian96/ditto.\n\n    As in the implementation there, steps of the global and local models are done in tandem and for the same\n    number of steps.\n\n    Args:\n        input (TorchInputType): input tensor to be run through both the global and local models. Here,\n            ``TorchInputType`` is simply an alias for the union of ``torch.Tensor`` and\n            ``dict[str, torch.Tensor]``.\n        target (TorchTargetType): target tensor to be used to compute a loss given each models outputs.\n\n    Returns:\n        (tuple[TrainingLosses, TorchPredType]): Returns relevant loss values from both the global and local\n            model optimization steps. The prediction dictionary contains predictions indexed a \"global\" and \"local\"\n            corresponding to predictions from the global and local Ditto models for metric evaluations.\n    \"\"\"\n    # global\n    global_losses, global_preds = self._compute_preds_and_losses(\n        self.safe_global_model(), self.optimizers[\"global\"], input, target\n    )\n    # local\n    local_losses, local_preds = self._compute_preds_and_losses(self.model, self.optimizers[\"local\"], input, target)\n    local_loss_clone = local_losses.backward[\"backward\"].clone()  # need a clone for later\n\n    # take step global\n    global_losses = self._apply_backwards_on_losses_and_take_step(\n        self.safe_global_model(), self.optimizers[\"global\"], global_losses\n    )\n    # take step local\n    penalty_loss = self.compute_penalty_loss()\n    local_losses.backward[\"backward\"] = local_losses.backward[\"backward\"] + penalty_loss\n    local_losses = self._apply_backwards_on_losses_and_take_step(\n        self.model, self.optimizers[\"local\"], local_losses\n    )\n\n    # prepare return values\n    additional_losses = {\n        \"penalty_loss\": penalty_loss.clone(),\n        \"local_loss\": local_loss_clone,\n        \"global_loss\": global_losses.backward[\"backward\"],\n        \"loss_for_adaptation\": local_loss_clone.clone(),\n    }\n    local_losses.additional_losses = additional_losses\n\n    # combined preds\n    if isinstance(global_preds, torch.Tensor) and isinstance(local_preds, torch.Tensor):\n        combined_preds = {\"global\": global_preds, \"local\": local_preds}\n    elif isinstance(global_preds, dict) and isinstance(local_preds, dict):\n        combined_preds = {f\"global-{k}\": v for k, v in global_preds.items()}\n        combined_preds.update(**{f\"local-{k}\": v for k, v in local_preds.items()})\n\n    return local_losses, combined_preds\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.validate","title":"<code>validate(include_losses_in_metrics=False)</code>","text":"<p>Validate the current model on the entire validation dataset.</p> <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]]</code> <p>The validation loss and a dictionary of metrics from validation.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef validate(\n    self: DittoPersonalizedProtocol, include_losses_in_metrics: bool = False\n) -&gt; tuple[float, dict[str, Scalar]]:\n    \"\"\"\n    Validate the current model on the entire validation dataset.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n    \"\"\"\n    # Set the global model to evaluate mode\n    self.safe_global_model().eval()\n    return super().validate(include_losses_in_metrics=include_losses_in_metrics)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.DittoPersonalizedMixin.compute_evaluation_loss","title":"<code>compute_evaluation_loss(preds, features, target)</code>","text":"<p>Computes evaluation loss given predictions (and potentially features) of the model and ground truth data. For Ditto, we use the vanilla loss for the local model in checkpointing. However, during validation we also compute the global model vanilla loss.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. Anything stored in preds will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> containing checkpoint loss and additional losses indexed by name.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef compute_evaluation_loss(\n    self: DittoPersonalizedProtocol,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; EvaluationLosses:\n    \"\"\"\n    Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n    For Ditto, we use the vanilla loss for the local model in checkpointing. However, during validation we also\n    compute the global model vanilla loss.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n            in preds will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n            indexed by name.\n    \"\"\"\n    # Check that both models are in eval mode\n    assert self.global_model is not None and not self.global_model.training and not self.model.training\n    return super().compute_evaluation_loss(preds, features, target)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.MrMtlPersonalizedMixin","title":"<code>MrMtlPersonalizedMixin</code>","text":"<p>               Bases: <code>AdaptiveDriftConstrainedMixin</code></p> Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>class MrMtlPersonalizedMixin(AdaptiveDriftConstrainedMixin):\n    def __init__(self: MrMtlPersonalizedProtocol, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        This client implements the MR-MTL algorithm from MR-MTL: On Privacy and Personalization in Cross-Silo\n        Federated Learning. The idea is that we want to train personalized versions of the global model for each\n        client. However, instead of using a separate solver for the global model, as in Ditto, we update the initial\n        global model with aggregated local models on the server-side and use those weights to also constrain the\n        training of a local model. The constraint for this local model is identical to the FedProx loss. The key\n        difference is that the local model is never replaced with aggregated weights. It is always local.\n\n        **NOTE**: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the\n        heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the\n        corresponding strategy used by the server\n        \"\"\"\n        # Initialize mixin-specific attributes\n        self.initial_global_model: torch.nn.Module | None = None\n        self.initial_global_tensors: list[torch.Tensor] = []\n\n        super().__init__(*args, **kwargs)\n\n    def get_global_model(self: MrMtlPersonalizedProtocol, config: Config) -&gt; nn.Module:\n        \"\"\"\n        Returns the global model on client setup to be used as a constraint for the local model during training.\n\n        The global model should be the same architecture as the local model so we reuse the ``get_model`` call. We\n        explicitly send the model to the desired device. This is idempotent.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (nn.Module): The PyTorch model serving as the global model for Ditto\n        \"\"\"\n        model_copy = copy.deepcopy(self.get_model(config))\n        return model_copy.to(self.device)\n\n    @ensure_protocol_compliance\n    def setup_client(self: MrMtlPersonalizedProtocol, config: Config) -&gt; None:\n        \"\"\"\n        Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n        Then set initialized attribute to True. In this class, this function simply adds the additional step of\n        setting up the global model.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        try:\n            self.initial_global_model = self.get_global_model(config)\n            log(INFO, f\"initial global model set: {type(self.initial_global_model).__name__}\")\n        except AttributeError:\n            log(\n                INFO,\n                \"Couldn't set initial global model before super().setup_client(). Will try again within that setup.\",\n            )\n            pass\n        # The rest of the setup is the same\n        super().setup_client(config)  # type:ignore [safe-super]\n\n    @ensure_protocol_compliance\n    def get_optimizer(self: MrMtlPersonalizedProtocol, config: Config) -&gt; dict[str, Optimizer]:\n        \"\"\"\n        Implementing get_optimizer as a hook to set initial global model if not already set.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        if self.initial_global_model is None:\n            # try set it here\n            self.initial_global_model = self.get_global_model(config)\n            log(\n                INFO,\n                f\"initial_global_model set: {type(self.initial_global_model).__name__} within `get_optimizer`\",\n            )\n\n        return super().get_optimizer(config=config)  # type: ignore[safe-super, return-value]\n\n    @ensure_protocol_compliance\n    def set_parameters(\n        self: MrMtlPersonalizedProtocol, parameters: NDArrays, config: Config, fitting_round: bool\n    ) -&gt; None:\n        \"\"\"\n        The parameters being passed are to be routed to the initial global model to be used in a penalty term in\n        training the local model. Despite the usual FL setup, we actually never pass the aggregated model to the\n        **LOCAL** model. Instead, we use the aggregated model to form the MR-MTL penalty term.\n\n        NOTE: In MR-MTL, unlike Ditto, the local model weights are not synced across clients to the initial global\n        model, even in the **FIRST ROUND**.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model. It will also contain a penalty weight from the server at each round (possibly adapted)\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n                round or an evaluation round. Not used here.\n        \"\"\"\n        # Make sure that the proper components exist.\n        assert self.initial_global_model is not None and self.parameter_exchanger is not None\n\n        # Route the parameters to the GLOBAL model only in MR-MTL\n        log(INFO, \"Setting the global model weights\")\n        server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n        log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n        self.parameter_exchanger.pull_parameters(server_model_state, self.initial_global_model, config)\n\n    @ensure_protocol_compliance\n    def update_before_train(self: MrMtlPersonalizedProtocol, current_server_round: int) -&gt; None:\n        assert self.initial_global_model is not None\n        # Freeze the initial weights of the INITIAL GLOBAL MODEL. These are used to form the MR-MTL\n        # update penalty term.\n        for param in self.initial_global_model.parameters():\n            param.requires_grad = False\n        self.initial_global_model.eval()\n\n        # Saving the initial GLOBAL MODEL weights and detaching them so that we don't compute gradients with\n        # respect to the tensors. These are used to form the MR-MTL local update penalty term.\n        self.drift_penalty_tensors = [\n            initial_layer_weights.detach().clone() for initial_layer_weights in self.initial_global_model.parameters()\n        ]\n\n        return super().update_before_train(current_server_round)  # type: ignore[safe-super]\n\n    @ensure_protocol_compliance\n    def compute_training_loss(\n        self: MrMtlPersonalizedProtocol,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training losses given predictions of the modes and ground truth data. We add to vanilla loss\n        function by including Mean Regularized (MR) penalty loss which is the \\\\(\\\\ell^2\\\\) inner product between the\n        initial global model weights and weights of the current model.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n                All predictions included in dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n                by name. Additional losses includes each loss component of the total loss.\n        \"\"\"\n        # Check that the initial global model isn't in training mode and that the local model is in training mode\n        assert self.initial_global_model is not None and not self.initial_global_model.training and self.model.training\n        # Use the rest of the training loss computation from the AdaptiveDriftConstraintClient parent\n        return super().compute_training_loss(preds, features, target)  # type: ignore[safe-super]\n\n    @ensure_protocol_compliance\n    def validate(\n        self: MrMtlPersonalizedProtocol, include_losses_in_metrics: bool = False\n    ) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Validate the current model on the entire validation dataset.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n        \"\"\"\n        # ensure that the initial global model is in eval mode\n        assert self.initial_global_model is not None and not self.initial_global_model.training\n        return super().validate(include_losses_in_metrics=include_losses_in_metrics)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.MrMtlPersonalizedMixin.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>This client implements the MR-MTL algorithm from MR-MTL: On Privacy and Personalization in Cross-Silo Federated Learning. The idea is that we want to train personalized versions of the global model for each client. However, instead of using a separate solver for the global model, as in Ditto, we update the initial global model with aggregated local models on the server-side and use those weights to also constrain the training of a local model. The constraint for this local model is identical to the FedProx loss. The key difference is that the local model is never replaced with aggregated weights. It is always local.</p> <p>NOTE: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the corresponding strategy used by the server</p> Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>def __init__(self: MrMtlPersonalizedProtocol, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    This client implements the MR-MTL algorithm from MR-MTL: On Privacy and Personalization in Cross-Silo\n    Federated Learning. The idea is that we want to train personalized versions of the global model for each\n    client. However, instead of using a separate solver for the global model, as in Ditto, we update the initial\n    global model with aggregated local models on the server-side and use those weights to also constrain the\n    training of a local model. The constraint for this local model is identical to the FedProx loss. The key\n    difference is that the local model is never replaced with aggregated weights. It is always local.\n\n    **NOTE**: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the\n    heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the\n    corresponding strategy used by the server\n    \"\"\"\n    # Initialize mixin-specific attributes\n    self.initial_global_model: torch.nn.Module | None = None\n    self.initial_global_tensors: list[torch.Tensor] = []\n\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.MrMtlPersonalizedMixin.get_global_model","title":"<code>get_global_model(config)</code>","text":"<p>Returns the global model on client setup to be used as a constraint for the local model during training.</p> <p>The global model should be the same architecture as the local model so we reuse the <code>get_model</code> call. We explicitly send the model to the desired device. This is idempotent.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The PyTorch model serving as the global model for Ditto</p> Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>def get_global_model(self: MrMtlPersonalizedProtocol, config: Config) -&gt; nn.Module:\n    \"\"\"\n    Returns the global model on client setup to be used as a constraint for the local model during training.\n\n    The global model should be the same architecture as the local model so we reuse the ``get_model`` call. We\n    explicitly send the model to the desired device. This is idempotent.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (nn.Module): The PyTorch model serving as the global model for Ditto\n    \"\"\"\n    model_copy = copy.deepcopy(self.get_model(config))\n    return model_copy.to(self.device)\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.MrMtlPersonalizedMixin.setup_client","title":"<code>setup_client(config)</code>","text":"<p>Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set initialized attribute to True. In this class, this function simply adds the additional step of setting up the global model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>@ensure_protocol_compliance\ndef setup_client(self: MrMtlPersonalizedProtocol, config: Config) -&gt; None:\n    \"\"\"\n    Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n    Then set initialized attribute to True. In this class, this function simply adds the additional step of\n    setting up the global model.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    try:\n        self.initial_global_model = self.get_global_model(config)\n        log(INFO, f\"initial global model set: {type(self.initial_global_model).__name__}\")\n    except AttributeError:\n        log(\n            INFO,\n            \"Couldn't set initial global model before super().setup_client(). Will try again within that setup.\",\n        )\n        pass\n    # The rest of the setup is the same\n    super().setup_client(config)  # type:ignore [safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.MrMtlPersonalizedMixin.get_optimizer","title":"<code>get_optimizer(config)</code>","text":"<p>Implementing get_optimizer as a hook to set initial global model if not already set.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>@ensure_protocol_compliance\ndef get_optimizer(self: MrMtlPersonalizedProtocol, config: Config) -&gt; dict[str, Optimizer]:\n    \"\"\"\n    Implementing get_optimizer as a hook to set initial global model if not already set.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    if self.initial_global_model is None:\n        # try set it here\n        self.initial_global_model = self.get_global_model(config)\n        log(\n            INFO,\n            f\"initial_global_model set: {type(self.initial_global_model).__name__} within `get_optimizer`\",\n        )\n\n    return super().get_optimizer(config=config)  # type: ignore[safe-super, return-value]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.MrMtlPersonalizedMixin.set_parameters","title":"<code>set_parameters(parameters, config, fitting_round)</code>","text":"<p>The parameters being passed are to be routed to the initial global model to be used in a penalty term in training the local model. Despite the usual FL setup, we actually never pass the aggregated model to the LOCAL model. Instead, we use the aggregated model to form the MR-MTL penalty term.</p> <p>NOTE: In MR-MTL, unlike Ditto, the local model weights are not synced across clients to the initial global model, even in the FIRST ROUND.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model. It will also contain a penalty weight from the server at each round (possibly adapted)</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. Not used here.</p> required Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>@ensure_protocol_compliance\ndef set_parameters(\n    self: MrMtlPersonalizedProtocol, parameters: NDArrays, config: Config, fitting_round: bool\n) -&gt; None:\n    \"\"\"\n    The parameters being passed are to be routed to the initial global model to be used in a penalty term in\n    training the local model. Despite the usual FL setup, we actually never pass the aggregated model to the\n    **LOCAL** model. Instead, we use the aggregated model to form the MR-MTL penalty term.\n\n    NOTE: In MR-MTL, unlike Ditto, the local model weights are not synced across clients to the initial global\n    model, even in the **FIRST ROUND**.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model. It will also contain a penalty weight from the server at each round (possibly adapted)\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n            round or an evaluation round. Not used here.\n    \"\"\"\n    # Make sure that the proper components exist.\n    assert self.initial_global_model is not None and self.parameter_exchanger is not None\n\n    # Route the parameters to the GLOBAL model only in MR-MTL\n    log(INFO, \"Setting the global model weights\")\n    server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n    log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n    self.parameter_exchanger.pull_parameters(server_model_state, self.initial_global_model, config)\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.MrMtlPersonalizedMixin.compute_training_loss","title":"<code>compute_training_loss(preds, features, target)</code>","text":"<p>Computes training losses given predictions of the modes and ground truth data. We add to vanilla loss function by including Mean Regularized (MR) penalty loss which is the \\(\\ell^2\\) inner product between the initial global model weights and weights of the current model.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name. Additional losses includes each loss component of the total loss.</p> Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>@ensure_protocol_compliance\ndef compute_training_loss(\n    self: MrMtlPersonalizedProtocol,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training losses given predictions of the modes and ground truth data. We add to vanilla loss\n    function by including Mean Regularized (MR) penalty loss which is the \\\\(\\\\ell^2\\\\) inner product between the\n    initial global model weights and weights of the current model.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            All predictions included in dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n            by name. Additional losses includes each loss component of the total loss.\n    \"\"\"\n    # Check that the initial global model isn't in training mode and that the local model is in training mode\n    assert self.initial_global_model is not None and not self.initial_global_model.training and self.model.training\n    # Use the rest of the training loss computation from the AdaptiveDriftConstraintClient parent\n    return super().compute_training_loss(preds, features, target)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.MrMtlPersonalizedMixin.validate","title":"<code>validate(include_losses_in_metrics=False)</code>","text":"<p>Validate the current model on the entire validation dataset.</p> <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]]</code> <p>The validation loss and a dictionary of metrics from validation.</p> Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>@ensure_protocol_compliance\ndef validate(\n    self: MrMtlPersonalizedProtocol, include_losses_in_metrics: bool = False\n) -&gt; tuple[float, dict[str, Scalar]]:\n    \"\"\"\n    Validate the current model on the entire validation dataset.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n    \"\"\"\n    # ensure that the initial global model is in eval mode\n    assert self.initial_global_model is not None and not self.initial_global_model.training\n    return super().validate(include_losses_in_metrics=include_losses_in_metrics)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.make_it_personal","title":"<code>make_it_personal(client_base_type, mode)</code>","text":"<p>A mixed class factory for converting basic clients to personalized versions.</p> Source code in <code>fl4health/mixins/personalized/__init__.py</code> <pre><code>def make_it_personal(client_base_type: type[FlexibleClient], mode: PersonalizedMode) -&gt; type[FlexibleClient]:\n    \"\"\"A mixed class factory for converting basic clients to personalized versions.\"\"\"\n    if mode in PersonalizedMixinRegistry:\n        return type(\n            f\"{mode.value}{client_base_type.__name__}\",\n            (\n                PersonalizedMixinRegistry[mode],\n                client_base_type,\n            ),\n            {\n                # Special flag to bypass validation\n                \"_dynamically_created\": True\n            },\n        )\n    raise ValueError(\"Unrecognized personalized mode.\")\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.ditto","title":"<code>ditto</code>","text":"<p>Ditto Personalized Mixin.</p>"},{"location":"api/#fl4health.mixins.personalized.ditto.DittoPersonalizedMixin","title":"<code>DittoPersonalizedMixin</code>","text":"<p>               Bases: <code>AdaptiveDriftConstrainedMixin</code></p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>class DittoPersonalizedMixin(AdaptiveDriftConstrainedMixin):\n    def __init__(self: DittoPersonalizedProtocol, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        This mixin implements the Ditto algorithm from Ditto: Fair and Robust Federated Learning Through\n        Personalization. This mixin inherits from the ``AdaptiveDriftConstrainedMixin``, and like that mixin,\n        this should be mixed with a ``FlexibleClient`` type in order to apply the Ditto personalization method\n        to that client.\n\n        Background Context:\n\n        The idea is that we want to train personalized versions of the global model for each client. So we\n        simultaneously train a global model that is aggregated on the server-side and use those weights to also\n        constrain the training of a local model. The constraint for this local model is identical to the FedProx loss.\n\n\n        Raises:\n            RuntimeError: If the object does not satisfy the ``FlexibleClientProtocolPreSetup`` then it will raise an\n                error. This is additional validation to ensure that the mixin was applied to an appropriate base class.\n        \"\"\"\n        # Initialize mixin-specific attributes\n        self.global_model: torch.nn.Module | None = None\n\n        super().__init__(*args, **kwargs)\n\n    def safe_global_model(self: DittoPersonalizedProtocol) -&gt; nn.Module:\n        \"\"\"\n        Convenient accessor for the global model.\n\n        Raises:\n            ValueError: If the ``global_model`` attribute has not yet been set, we will raise an error.\n\n        Returns:\n            (nn.Module): the global model if it has been set.\n        \"\"\"\n        if self.global_model:\n            return self.global_model\n        raise ValueError(\"Cannot get global model as it not yet been set.\")\n\n    @property\n    def optimizer_keys(self: DittoPersonalizedProtocol) -&gt; list[str]:\n        \"\"\"\n        Property for optimizer keys.\n\n        Returns:\n            (list[str]): list of keys for the optimizers dictionary.\n        \"\"\"\n        return [\"local\", \"global\"]\n\n    def _copy_optimizer_with_new_params(self: DittoPersonalizedProtocol, original_optimizer: Optimizer) -&gt; Optimizer:\n        \"\"\"\n        Helper method to make a copy of the original optimizer for the global model.\n\n        Args:\n            original_optimizer (Optimizer): original optimizer of the underlying `FlexibleClient`.\n\n        Returns:\n            (Optimizer): a copy of the original optimizer to be used by the global model.\n        \"\"\"\n        optim_class = original_optimizer.__class__\n        state_dict = original_optimizer.state_dict()\n\n        # Extract hyperparameters from param_groups\n        # We only take the first group's hyperparameters, excluding 'params' and 'lr'\n        param_group = state_dict[\"param_groups\"][0]\n\n        # store initial_lr to be used with schedulers\n        try:\n            initial_lr = param_group[\"initial_lr\"]\n        except KeyError:\n            if \"lr\" in original_optimizer.defaults:\n                initial_lr = original_optimizer.defaults[\"lr\"]\n            else:\n                initial_lr = 1e-3\n                log(\n                    WARN,\n                    \"Unable to get the original `lr` for the global optimizer, falling back to `1e-3`.\",\n                )\n\n        optimizer_kwargs = {k: v for k, v in param_group.items() if k not in (\"params\", \"initial_lr\")}\n        assert self.global_model is not None\n        global_optimizer = optim_class(self.global_model.parameters(), **optimizer_kwargs)\n\n        # maintain initial_lr for schedulers\n        for param_group in global_optimizer.param_groups:\n            param_group[\"initial_lr\"] = initial_lr\n\n        return global_optimizer\n\n    def get_global_model(self: DittoPersonalizedProtocol, config: Config) -&gt; nn.Module:\n        \"\"\"\n        Returns the global model to be used during Ditto training and as a constraint for the local model.\n\n        The global model should be the same architecture as the local model so we reuse the ``get_model`` call. We\n        explicitly send the model to the desired device. This is idempotent.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (nn.Module): The PyTorch model serving as the global model for Ditto\n        \"\"\"\n        model_copy = copy.deepcopy(self.get_model(config))\n        return model_copy.to(self.device)\n\n    @ensure_protocol_compliance\n    def get_optimizer(self: DittoPersonalizedProtocol, config: Config) -&gt; dict[str, Optimizer]:\n        \"\"\"\n        Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        if self.global_model is None:\n            # try set it here\n            self.global_model = self.get_global_model(config)  # is this the same config?\n            log(\n                INFO,\n                f\"global model set: {type(self.global_model).__name__} within `get_optimizer`\",\n            )\n\n        # Note that the global optimizer operates on self.global_model.parameters()\n        optimizer = super().get_optimizer(config=config)  # type: ignore[safe-super]\n        if isinstance(optimizer, dict):\n            try:\n                original_optimizer = next(el for el in optimizer.values() if isinstance(el, Optimizer))\n            except StopIteration as e:\n                log(ERROR, \"Unable to find an ~torch.optim.Optimizer object.\")\n                raise e\n        elif isinstance(optimizer, Optimizer):\n            original_optimizer = optimizer\n        else:\n            raise ValueError(\"`super().get_optimizer()` returned an invalid type.\")\n\n        global_optimizer = self._copy_optimizer_with_new_params(original_optimizer)\n        return {\"local\": original_optimizer, \"global\": global_optimizer}\n\n    def set_optimizer(self: DittoPersonalizedProtocol, config: Config) -&gt; None:\n        \"\"\"\n        Ditto requires an optimizer for the global model and one for the local model. This function simply ensures that\n        the optimizers setup by the user have the proper keys and that there are two optimizers.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        optimizers = self.get_optimizer(config)\n        assert isinstance(optimizers, dict) and set(self.optimizer_keys) == set(optimizers.keys())\n        self.optimizers = optimizers\n\n    @ensure_protocol_compliance\n    def setup_client(self: DittoPersonalizedProtocol, config: Config) -&gt; None:\n        \"\"\"\n        Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n        Then set initialized attribute to True. In this class, this function simply adds the additional step of\n        setting up the global model.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        try:\n            self.global_model = self.get_global_model(config)\n            log(INFO, f\"global model set: {type(self.global_model).__name__}\")\n        except AttributeError:\n            log(\n                INFO,\n                \"Couldn't set global model before super().setup_client(). Will try again within that setup.\",\n            )\n            pass\n        # The rest of the setup is the same\n        super().setup_client(config)  # type:ignore [safe-super]\n\n    def get_parameters(self: DittoPersonalizedProtocol, config: Config) -&gt; NDArrays:\n        \"\"\"\n        For Ditto, we transfer the **GLOBAL** model weights to the server to be aggregated. The local model weights\n        stay with the client.\n\n        Args:\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n        Returns:\n            (NDArrays): **GLOBAL** model weights to be sent to the server for aggregation.\n        \"\"\"\n        if not self.initialized:\n            return self.setup_client_and_return_all_model_parameters(config)\n\n        # NOTE: the global model weights are sent to the server here.\n        if self.global_model is None:\n            raise ValueError(\"Unable to get parameters with unset global model.\")\n        global_model_weights = self.parameter_exchanger.push_parameters(self.global_model, config=config)\n\n        # Weights and training loss sent to server for aggregation\n        # Training loss sent because server will decide to increase or decrease the penalty weight, if adaptivity\n        # is turned on\n        packed_params = self.parameter_exchanger.pack_parameters(global_model_weights, self.loss_for_adaptation)\n        log(INFO, \"Successfully packed parameters of global model\")\n        return packed_params\n\n    @ensure_protocol_compliance\n    def set_parameters(\n        self: DittoPersonalizedProtocol,\n        parameters: NDArrays,\n        config: Config,\n        fitting_round: bool,\n    ) -&gt; None:\n        \"\"\"\n        Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n        unpacked for the clients to use in training. The parameters being passed are to be routed to the global model.\n        In the first fitting round, we assume the both the global and local models are being initialized and use\n        the ``FullParameterExchanger()`` to initialize both sets of model weights to the same parameters.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model (global model for all but the first step of Ditto). These should also include a penalty weight\n                from the server that needs to be unpacked.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning\n                round is a fitting round or an evaluation round. This is used to help determine which parameter\n                exchange should be used for pulling parameters. If the current federated learning round is the very\n                first fitting round, then we initialize both the global and local Ditto models with weights sent from\n                the server.\n        \"\"\"\n        # Make sure that the proper components exist.\n        assert self.global_model is not None and self.model is not None and self.parameter_exchanger is not None\n        server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n        log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n        current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n        if current_server_round == 1 and fitting_round:\n            log(\n                INFO,\n                \"Initializing the global and local models weights for the first time\",\n            )\n            self.initialize_all_model_weights(server_model_state, config)\n        else:\n            # Route the parameters to the GLOBAL model in Ditto after the initial stage\n            log(INFO, \"Setting the global model weights\")\n            self.parameter_exchanger.pull_parameters(server_model_state, self.global_model, config)\n\n    def initialize_all_model_weights(self: DittoPersonalizedProtocol, parameters: NDArrays, config: Config) -&gt; None:\n        \"\"\"\n        If this is the first time we're initializing the model weights, we initialize both the global and the local\n        weights together.\n\n        Args:\n            parameters (NDArrays): Model parameters to be injected into the client model.\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        \"\"\"\n        parameter_exchanger = cast(FullParameterExchanger, self.parameter_exchanger)\n        parameter_exchanger.pull_parameters(parameters, self.model, config)\n        parameter_exchanger.pull_parameters(parameters, self.safe_global_model(), config)\n\n    def set_initial_global_tensors(self: DittoPersonalizedProtocol) -&gt; None:\n        \"\"\"\n        Saving the initial **GLOBAL MODEL** weights and detaching them so that we don't compute gradients with\n        respect to the tensors. These are used to form the Ditto local update penalty term.\n        \"\"\"\n        self.drift_penalty_tensors = [\n            initial_layer_weights.detach().clone() for initial_layer_weights in self.safe_global_model().parameters()\n        ]\n\n    @ensure_protocol_compliance\n    def update_before_train(self: DittoPersonalizedProtocol, current_server_round: int) -&gt; None:\n        \"\"\"\n        Procedures that should occur before proceeding with the training loops for the models. In this case, we\n        save the global models parameters to be used in constraining training of the local model.\n\n        Args:\n            current_server_round (int): Indicates which server round we are currently executing.\n        \"\"\"\n        self.set_initial_global_tensors()\n\n        # Need to also set the global model to train mode before any training begins.\n        self.safe_global_model().train()\n\n        super().update_before_train(current_server_round)  # type: ignore[safe-super]\n\n    def train_step(\n        self: DittoPersonalizedProtocol, input: TorchInputType, target: TorchTargetType\n    ) -&gt; tuple[TrainingLosses, TorchPredType]:\n        \"\"\"\n        Mechanics of training loop follow from original Ditto implementation: https://github.com/litian96/ditto.\n\n        As in the implementation there, steps of the global and local models are done in tandem and for the same\n        number of steps.\n\n        Args:\n            input (TorchInputType): input tensor to be run through both the global and local models. Here,\n                ``TorchInputType`` is simply an alias for the union of ``torch.Tensor`` and\n                ``dict[str, torch.Tensor]``.\n            target (TorchTargetType): target tensor to be used to compute a loss given each models outputs.\n\n        Returns:\n            (tuple[TrainingLosses, TorchPredType]): Returns relevant loss values from both the global and local\n                model optimization steps. The prediction dictionary contains predictions indexed a \"global\" and \"local\"\n                corresponding to predictions from the global and local Ditto models for metric evaluations.\n        \"\"\"\n        # global\n        global_losses, global_preds = self._compute_preds_and_losses(\n            self.safe_global_model(), self.optimizers[\"global\"], input, target\n        )\n        # local\n        local_losses, local_preds = self._compute_preds_and_losses(self.model, self.optimizers[\"local\"], input, target)\n        local_loss_clone = local_losses.backward[\"backward\"].clone()  # need a clone for later\n\n        # take step global\n        global_losses = self._apply_backwards_on_losses_and_take_step(\n            self.safe_global_model(), self.optimizers[\"global\"], global_losses\n        )\n        # take step local\n        penalty_loss = self.compute_penalty_loss()\n        local_losses.backward[\"backward\"] = local_losses.backward[\"backward\"] + penalty_loss\n        local_losses = self._apply_backwards_on_losses_and_take_step(\n            self.model, self.optimizers[\"local\"], local_losses\n        )\n\n        # prepare return values\n        additional_losses = {\n            \"penalty_loss\": penalty_loss.clone(),\n            \"local_loss\": local_loss_clone,\n            \"global_loss\": global_losses.backward[\"backward\"],\n            \"loss_for_adaptation\": local_loss_clone.clone(),\n        }\n        local_losses.additional_losses = additional_losses\n\n        # combined preds\n        if isinstance(global_preds, torch.Tensor) and isinstance(local_preds, torch.Tensor):\n            combined_preds = {\"global\": global_preds, \"local\": local_preds}\n        elif isinstance(global_preds, dict) and isinstance(local_preds, dict):\n            combined_preds = {f\"global-{k}\": v for k, v in global_preds.items()}\n            combined_preds.update(**{f\"local-{k}\": v for k, v in local_preds.items()})\n\n        return local_losses, combined_preds\n\n    def val_step(\n        self: DittoPersonalizedProtocol, input: TorchInputType, target: TorchTargetType\n    ) -&gt; tuple[EvaluationLosses, TorchPredType]:\n        # global\n        global_losses, global_preds = self._val_step_with_model(self.safe_global_model(), input, target)\n        # local\n        local_losses, local_preds = self._val_step_with_model(self.model, input, target)\n\n        # combine\n        losses = EvaluationLosses(\n            local_losses.checkpoint,\n            additional_losses={\n                \"global_loss\": global_losses.checkpoint,\n                \"local_loss\": local_losses.checkpoint,\n            },\n        )\n        preds: TorchPredType = {}\n        preds.update(**{f\"global-{k}\": v for k, v in global_preds.items()})\n        preds.update(**{f\"local-{k}\": v for k, v in local_preds.items()})\n        return losses, preds\n\n    @ensure_protocol_compliance\n    def validate(\n        self: DittoPersonalizedProtocol, include_losses_in_metrics: bool = False\n    ) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Validate the current model on the entire validation dataset.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n        \"\"\"\n        # Set the global model to evaluate mode\n        self.safe_global_model().eval()\n        return super().validate(include_losses_in_metrics=include_losses_in_metrics)  # type: ignore[safe-super]\n\n    @ensure_protocol_compliance\n    def compute_evaluation_loss(\n        self: DittoPersonalizedProtocol,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; EvaluationLosses:\n        \"\"\"\n        Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n        For Ditto, we use the vanilla loss for the local model in checkpointing. However, during validation we also\n        compute the global model vanilla loss.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n                in preds will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n                indexed by name.\n        \"\"\"\n        # Check that both models are in eval mode\n        assert self.global_model is not None and not self.global_model.training and not self.model.training\n        return super().compute_evaluation_loss(preds, features, target)  # type: ignore[safe-super]\n</code></pre> <code></code> <code>optimizer_keys</code> <code>property</code> \u00b6 <p>Property for optimizer keys.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list of keys for the optimizers dictionary.</p> <code></code> <code>__init__(*args, **kwargs)</code> \u00b6 <p>This mixin implements the Ditto algorithm from Ditto: Fair and Robust Federated Learning Through Personalization. This mixin inherits from the <code>AdaptiveDriftConstrainedMixin</code>, and like that mixin, this should be mixed with a <code>FlexibleClient</code> type in order to apply the Ditto personalization method to that client.</p> <p>Background Context:</p> <p>The idea is that we want to train personalized versions of the global model for each client. So we simultaneously train a global model that is aggregated on the server-side and use those weights to also constrain the training of a local model. The constraint for this local model is identical to the FedProx loss.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the object does not satisfy the <code>FlexibleClientProtocolPreSetup</code> then it will raise an error. This is additional validation to ensure that the mixin was applied to an appropriate base class.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def __init__(self: DittoPersonalizedProtocol, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    This mixin implements the Ditto algorithm from Ditto: Fair and Robust Federated Learning Through\n    Personalization. This mixin inherits from the ``AdaptiveDriftConstrainedMixin``, and like that mixin,\n    this should be mixed with a ``FlexibleClient`` type in order to apply the Ditto personalization method\n    to that client.\n\n    Background Context:\n\n    The idea is that we want to train personalized versions of the global model for each client. So we\n    simultaneously train a global model that is aggregated on the server-side and use those weights to also\n    constrain the training of a local model. The constraint for this local model is identical to the FedProx loss.\n\n\n    Raises:\n        RuntimeError: If the object does not satisfy the ``FlexibleClientProtocolPreSetup`` then it will raise an\n            error. This is additional validation to ensure that the mixin was applied to an appropriate base class.\n    \"\"\"\n    # Initialize mixin-specific attributes\n    self.global_model: torch.nn.Module | None = None\n\n    super().__init__(*args, **kwargs)\n</code></pre> <code></code> <code>safe_global_model()</code> \u00b6 <p>Convenient accessor for the global model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>global_model</code> attribute has not yet been set, we will raise an error.</p> <p>Returns:</p> Type Description <code>Module</code> <p>the global model if it has been set.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def safe_global_model(self: DittoPersonalizedProtocol) -&gt; nn.Module:\n    \"\"\"\n    Convenient accessor for the global model.\n\n    Raises:\n        ValueError: If the ``global_model`` attribute has not yet been set, we will raise an error.\n\n    Returns:\n        (nn.Module): the global model if it has been set.\n    \"\"\"\n    if self.global_model:\n        return self.global_model\n    raise ValueError(\"Cannot get global model as it not yet been set.\")\n</code></pre> <code></code> <code>get_global_model(config)</code> \u00b6 <p>Returns the global model to be used during Ditto training and as a constraint for the local model.</p> <p>The global model should be the same architecture as the local model so we reuse the <code>get_model</code> call. We explicitly send the model to the desired device. This is idempotent.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The PyTorch model serving as the global model for Ditto</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def get_global_model(self: DittoPersonalizedProtocol, config: Config) -&gt; nn.Module:\n    \"\"\"\n    Returns the global model to be used during Ditto training and as a constraint for the local model.\n\n    The global model should be the same architecture as the local model so we reuse the ``get_model`` call. We\n    explicitly send the model to the desired device. This is idempotent.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (nn.Module): The PyTorch model serving as the global model for Ditto\n    \"\"\"\n    model_copy = copy.deepcopy(self.get_model(config))\n    return model_copy.to(self.device)\n</code></pre> <code></code> <code>get_optimizer(config)</code> \u00b6 <p>Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef get_optimizer(self: DittoPersonalizedProtocol, config: Config) -&gt; dict[str, Optimizer]:\n    \"\"\"\n    Returns a dictionary with global and local optimizers with string keys \"global\" and \"local\" respectively.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    if self.global_model is None:\n        # try set it here\n        self.global_model = self.get_global_model(config)  # is this the same config?\n        log(\n            INFO,\n            f\"global model set: {type(self.global_model).__name__} within `get_optimizer`\",\n        )\n\n    # Note that the global optimizer operates on self.global_model.parameters()\n    optimizer = super().get_optimizer(config=config)  # type: ignore[safe-super]\n    if isinstance(optimizer, dict):\n        try:\n            original_optimizer = next(el for el in optimizer.values() if isinstance(el, Optimizer))\n        except StopIteration as e:\n            log(ERROR, \"Unable to find an ~torch.optim.Optimizer object.\")\n            raise e\n    elif isinstance(optimizer, Optimizer):\n        original_optimizer = optimizer\n    else:\n        raise ValueError(\"`super().get_optimizer()` returned an invalid type.\")\n\n    global_optimizer = self._copy_optimizer_with_new_params(original_optimizer)\n    return {\"local\": original_optimizer, \"global\": global_optimizer}\n</code></pre> <code></code> <code>set_optimizer(config)</code> \u00b6 <p>Ditto requires an optimizer for the global model and one for the local model. This function simply ensures that the optimizers setup by the user have the proper keys and that there are two optimizers.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def set_optimizer(self: DittoPersonalizedProtocol, config: Config) -&gt; None:\n    \"\"\"\n    Ditto requires an optimizer for the global model and one for the local model. This function simply ensures that\n    the optimizers setup by the user have the proper keys and that there are two optimizers.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    optimizers = self.get_optimizer(config)\n    assert isinstance(optimizers, dict) and set(self.optimizer_keys) == set(optimizers.keys())\n    self.optimizers = optimizers\n</code></pre> <code></code> <code>setup_client(config)</code> \u00b6 <p>Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set initialized attribute to True. In this class, this function simply adds the additional step of setting up the global model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef setup_client(self: DittoPersonalizedProtocol, config: Config) -&gt; None:\n    \"\"\"\n    Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n    Then set initialized attribute to True. In this class, this function simply adds the additional step of\n    setting up the global model.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    try:\n        self.global_model = self.get_global_model(config)\n        log(INFO, f\"global model set: {type(self.global_model).__name__}\")\n    except AttributeError:\n        log(\n            INFO,\n            \"Couldn't set global model before super().setup_client(). Will try again within that setup.\",\n        )\n        pass\n    # The rest of the setup is the same\n    super().setup_client(config)  # type:ignore [safe-super]\n</code></pre> <code></code> <code>get_parameters(config)</code> \u00b6 <p>For Ditto, we transfer the GLOBAL model weights to the server to be aggregated. The local model weights stay with the client.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>GLOBAL model weights to be sent to the server for aggregation.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def get_parameters(self: DittoPersonalizedProtocol, config: Config) -&gt; NDArrays:\n    \"\"\"\n    For Ditto, we transfer the **GLOBAL** model weights to the server to be aggregated. The local model weights\n    stay with the client.\n\n    Args:\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n\n    Returns:\n        (NDArrays): **GLOBAL** model weights to be sent to the server for aggregation.\n    \"\"\"\n    if not self.initialized:\n        return self.setup_client_and_return_all_model_parameters(config)\n\n    # NOTE: the global model weights are sent to the server here.\n    if self.global_model is None:\n        raise ValueError(\"Unable to get parameters with unset global model.\")\n    global_model_weights = self.parameter_exchanger.push_parameters(self.global_model, config=config)\n\n    # Weights and training loss sent to server for aggregation\n    # Training loss sent because server will decide to increase or decrease the penalty weight, if adaptivity\n    # is turned on\n    packed_params = self.parameter_exchanger.pack_parameters(global_model_weights, self.loss_for_adaptation)\n    log(INFO, \"Successfully packed parameters of global model\")\n    return packed_params\n</code></pre> <code></code> <code>set_parameters(parameters, config, fitting_round)</code> \u00b6 <p>Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are unpacked for the clients to use in training. The parameters being passed are to be routed to the global model. In the first fitting round, we assume the both the global and local models are being initialized and use the <code>FullParameterExchanger()</code> to initialize both sets of model weights to the same parameters.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model (global model for all but the first step of Ditto). These should also include a penalty weight from the server that needs to be unpacked.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. This is used to help determine which parameter exchange should be used for pulling parameters. If the current federated learning round is the very first fitting round, then we initialize both the global and local Ditto models with weights sent from the server.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef set_parameters(\n    self: DittoPersonalizedProtocol,\n    parameters: NDArrays,\n    config: Config,\n    fitting_round: bool,\n) -&gt; None:\n    \"\"\"\n    Assumes that the parameters being passed contain model parameters concatenated with a penalty weight. They are\n    unpacked for the clients to use in training. The parameters being passed are to be routed to the global model.\n    In the first fitting round, we assume the both the global and local models are being initialized and use\n    the ``FullParameterExchanger()`` to initialize both sets of model weights to the same parameters.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model (global model for all but the first step of Ditto). These should also include a penalty weight\n            from the server that needs to be unpacked.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning\n            round is a fitting round or an evaluation round. This is used to help determine which parameter\n            exchange should be used for pulling parameters. If the current federated learning round is the very\n            first fitting round, then we initialize both the global and local Ditto models with weights sent from\n            the server.\n    \"\"\"\n    # Make sure that the proper components exist.\n    assert self.global_model is not None and self.model is not None and self.parameter_exchanger is not None\n    server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n    log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n    current_server_round = narrow_dict_type(config, \"current_server_round\", int)\n    if current_server_round == 1 and fitting_round:\n        log(\n            INFO,\n            \"Initializing the global and local models weights for the first time\",\n        )\n        self.initialize_all_model_weights(server_model_state, config)\n    else:\n        # Route the parameters to the GLOBAL model in Ditto after the initial stage\n        log(INFO, \"Setting the global model weights\")\n        self.parameter_exchanger.pull_parameters(server_model_state, self.global_model, config)\n</code></pre> <code></code> <code>initialize_all_model_weights(parameters, config)</code> \u00b6 <p>If this is the first time we're initializing the model weights, we initialize both the global and the local weights together.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Model parameters to be injected into the client model.</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def initialize_all_model_weights(self: DittoPersonalizedProtocol, parameters: NDArrays, config: Config) -&gt; None:\n    \"\"\"\n    If this is the first time we're initializing the model weights, we initialize both the global and the local\n    weights together.\n\n    Args:\n        parameters (NDArrays): Model parameters to be injected into the client model.\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n    \"\"\"\n    parameter_exchanger = cast(FullParameterExchanger, self.parameter_exchanger)\n    parameter_exchanger.pull_parameters(parameters, self.model, config)\n    parameter_exchanger.pull_parameters(parameters, self.safe_global_model(), config)\n</code></pre> <code></code> <code>set_initial_global_tensors()</code> \u00b6 <p>Saving the initial GLOBAL MODEL weights and detaching them so that we don't compute gradients with respect to the tensors. These are used to form the Ditto local update penalty term.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def set_initial_global_tensors(self: DittoPersonalizedProtocol) -&gt; None:\n    \"\"\"\n    Saving the initial **GLOBAL MODEL** weights and detaching them so that we don't compute gradients with\n    respect to the tensors. These are used to form the Ditto local update penalty term.\n    \"\"\"\n    self.drift_penalty_tensors = [\n        initial_layer_weights.detach().clone() for initial_layer_weights in self.safe_global_model().parameters()\n    ]\n</code></pre> <code></code> <code>update_before_train(current_server_round)</code> \u00b6 <p>Procedures that should occur before proceeding with the training loops for the models. In this case, we save the global models parameters to be used in constraining training of the local model.</p> <p>Parameters:</p> Name Type Description Default <code>current_server_round</code> <code>int</code> <p>Indicates which server round we are currently executing.</p> required Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef update_before_train(self: DittoPersonalizedProtocol, current_server_round: int) -&gt; None:\n    \"\"\"\n    Procedures that should occur before proceeding with the training loops for the models. In this case, we\n    save the global models parameters to be used in constraining training of the local model.\n\n    Args:\n        current_server_round (int): Indicates which server round we are currently executing.\n    \"\"\"\n    self.set_initial_global_tensors()\n\n    # Need to also set the global model to train mode before any training begins.\n    self.safe_global_model().train()\n\n    super().update_before_train(current_server_round)  # type: ignore[safe-super]\n</code></pre> <code></code> <code>train_step(input, target)</code> \u00b6 <p>Mechanics of training loop follow from original Ditto implementation: https://github.com/litian96/ditto.</p> <p>As in the implementation there, steps of the global and local models are done in tandem and for the same number of steps.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>input tensor to be run through both the global and local models. Here, <code>TorchInputType</code> is simply an alias for the union of <code>torch.Tensor</code> and <code>dict[str, torch.Tensor]</code>.</p> required <code>target</code> <code>TorchTargetType</code> <p>target tensor to be used to compute a loss given each models outputs.</p> required <p>Returns:</p> Type Description <code>tuple[TrainingLosses, TorchPredType]</code> <p>Returns relevant loss values from both the global and local model optimization steps. The prediction dictionary contains predictions indexed a \"global\" and \"local\" corresponding to predictions from the global and local Ditto models for metric evaluations.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>def train_step(\n    self: DittoPersonalizedProtocol, input: TorchInputType, target: TorchTargetType\n) -&gt; tuple[TrainingLosses, TorchPredType]:\n    \"\"\"\n    Mechanics of training loop follow from original Ditto implementation: https://github.com/litian96/ditto.\n\n    As in the implementation there, steps of the global and local models are done in tandem and for the same\n    number of steps.\n\n    Args:\n        input (TorchInputType): input tensor to be run through both the global and local models. Here,\n            ``TorchInputType`` is simply an alias for the union of ``torch.Tensor`` and\n            ``dict[str, torch.Tensor]``.\n        target (TorchTargetType): target tensor to be used to compute a loss given each models outputs.\n\n    Returns:\n        (tuple[TrainingLosses, TorchPredType]): Returns relevant loss values from both the global and local\n            model optimization steps. The prediction dictionary contains predictions indexed a \"global\" and \"local\"\n            corresponding to predictions from the global and local Ditto models for metric evaluations.\n    \"\"\"\n    # global\n    global_losses, global_preds = self._compute_preds_and_losses(\n        self.safe_global_model(), self.optimizers[\"global\"], input, target\n    )\n    # local\n    local_losses, local_preds = self._compute_preds_and_losses(self.model, self.optimizers[\"local\"], input, target)\n    local_loss_clone = local_losses.backward[\"backward\"].clone()  # need a clone for later\n\n    # take step global\n    global_losses = self._apply_backwards_on_losses_and_take_step(\n        self.safe_global_model(), self.optimizers[\"global\"], global_losses\n    )\n    # take step local\n    penalty_loss = self.compute_penalty_loss()\n    local_losses.backward[\"backward\"] = local_losses.backward[\"backward\"] + penalty_loss\n    local_losses = self._apply_backwards_on_losses_and_take_step(\n        self.model, self.optimizers[\"local\"], local_losses\n    )\n\n    # prepare return values\n    additional_losses = {\n        \"penalty_loss\": penalty_loss.clone(),\n        \"local_loss\": local_loss_clone,\n        \"global_loss\": global_losses.backward[\"backward\"],\n        \"loss_for_adaptation\": local_loss_clone.clone(),\n    }\n    local_losses.additional_losses = additional_losses\n\n    # combined preds\n    if isinstance(global_preds, torch.Tensor) and isinstance(local_preds, torch.Tensor):\n        combined_preds = {\"global\": global_preds, \"local\": local_preds}\n    elif isinstance(global_preds, dict) and isinstance(local_preds, dict):\n        combined_preds = {f\"global-{k}\": v for k, v in global_preds.items()}\n        combined_preds.update(**{f\"local-{k}\": v for k, v in local_preds.items()})\n\n    return local_losses, combined_preds\n</code></pre> <code></code> <code>validate(include_losses_in_metrics=False)</code> \u00b6 <p>Validate the current model on the entire validation dataset.</p> <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]]</code> <p>The validation loss and a dictionary of metrics from validation.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef validate(\n    self: DittoPersonalizedProtocol, include_losses_in_metrics: bool = False\n) -&gt; tuple[float, dict[str, Scalar]]:\n    \"\"\"\n    Validate the current model on the entire validation dataset.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n    \"\"\"\n    # Set the global model to evaluate mode\n    self.safe_global_model().eval()\n    return super().validate(include_losses_in_metrics=include_losses_in_metrics)  # type: ignore[safe-super]\n</code></pre> <code></code> <code>compute_evaluation_loss(preds, features, target)</code> \u00b6 <p>Computes evaluation loss given predictions (and potentially features) of the model and ground truth data. For Ditto, we use the vanilla loss for the local model in checkpointing. However, during validation we also compute the global model vanilla loss.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. Anything stored in preds will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> containing checkpoint loss and additional losses indexed by name.</p> Source code in <code>fl4health/mixins/personalized/ditto.py</code> <pre><code>@ensure_protocol_compliance\ndef compute_evaluation_loss(\n    self: DittoPersonalizedProtocol,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; EvaluationLosses:\n    \"\"\"\n    Computes evaluation loss given predictions (and potentially features) of the model and ground truth data.\n    For Ditto, we use the vanilla loss for the local model in checkpointing. However, during validation we also\n    compute the global model vanilla loss.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name. Anything stored\n            in preds will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` containing checkpoint loss and additional losses\n            indexed by name.\n    \"\"\"\n    # Check that both models are in eval mode\n    assert self.global_model is not None and not self.global_model.training and not self.model.training\n    return super().compute_evaluation_loss(preds, features, target)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.mr_mtl","title":"<code>mr_mtl</code>","text":"<p>MR MTL Personalized Mixin.</p>"},{"location":"api/#fl4health.mixins.personalized.mr_mtl.MrMtlPersonalizedMixin","title":"<code>MrMtlPersonalizedMixin</code>","text":"<p>               Bases: <code>AdaptiveDriftConstrainedMixin</code></p> Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>class MrMtlPersonalizedMixin(AdaptiveDriftConstrainedMixin):\n    def __init__(self: MrMtlPersonalizedProtocol, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"\n        This client implements the MR-MTL algorithm from MR-MTL: On Privacy and Personalization in Cross-Silo\n        Federated Learning. The idea is that we want to train personalized versions of the global model for each\n        client. However, instead of using a separate solver for the global model, as in Ditto, we update the initial\n        global model with aggregated local models on the server-side and use those weights to also constrain the\n        training of a local model. The constraint for this local model is identical to the FedProx loss. The key\n        difference is that the local model is never replaced with aggregated weights. It is always local.\n\n        **NOTE**: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the\n        heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the\n        corresponding strategy used by the server\n        \"\"\"\n        # Initialize mixin-specific attributes\n        self.initial_global_model: torch.nn.Module | None = None\n        self.initial_global_tensors: list[torch.Tensor] = []\n\n        super().__init__(*args, **kwargs)\n\n    def get_global_model(self: MrMtlPersonalizedProtocol, config: Config) -&gt; nn.Module:\n        \"\"\"\n        Returns the global model on client setup to be used as a constraint for the local model during training.\n\n        The global model should be the same architecture as the local model so we reuse the ``get_model`` call. We\n        explicitly send the model to the desired device. This is idempotent.\n\n        Args:\n            config (Config): The config from the server.\n\n        Returns:\n            (nn.Module): The PyTorch model serving as the global model for Ditto\n        \"\"\"\n        model_copy = copy.deepcopy(self.get_model(config))\n        return model_copy.to(self.device)\n\n    @ensure_protocol_compliance\n    def setup_client(self: MrMtlPersonalizedProtocol, config: Config) -&gt; None:\n        \"\"\"\n        Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n        Then set initialized attribute to True. In this class, this function simply adds the additional step of\n        setting up the global model.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        try:\n            self.initial_global_model = self.get_global_model(config)\n            log(INFO, f\"initial global model set: {type(self.initial_global_model).__name__}\")\n        except AttributeError:\n            log(\n                INFO,\n                \"Couldn't set initial global model before super().setup_client(). Will try again within that setup.\",\n            )\n            pass\n        # The rest of the setup is the same\n        super().setup_client(config)  # type:ignore [safe-super]\n\n    @ensure_protocol_compliance\n    def get_optimizer(self: MrMtlPersonalizedProtocol, config: Config) -&gt; dict[str, Optimizer]:\n        \"\"\"\n        Implementing get_optimizer as a hook to set initial global model if not already set.\n\n        Args:\n            config (Config): The config from the server.\n        \"\"\"\n        if self.initial_global_model is None:\n            # try set it here\n            self.initial_global_model = self.get_global_model(config)\n            log(\n                INFO,\n                f\"initial_global_model set: {type(self.initial_global_model).__name__} within `get_optimizer`\",\n            )\n\n        return super().get_optimizer(config=config)  # type: ignore[safe-super, return-value]\n\n    @ensure_protocol_compliance\n    def set_parameters(\n        self: MrMtlPersonalizedProtocol, parameters: NDArrays, config: Config, fitting_round: bool\n    ) -&gt; None:\n        \"\"\"\n        The parameters being passed are to be routed to the initial global model to be used in a penalty term in\n        training the local model. Despite the usual FL setup, we actually never pass the aggregated model to the\n        **LOCAL** model. Instead, we use the aggregated model to form the MR-MTL penalty term.\n\n        NOTE: In MR-MTL, unlike Ditto, the local model weights are not synced across clients to the initial global\n        model, even in the **FIRST ROUND**.\n\n        Args:\n            parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n                model. It will also contain a penalty weight from the server at each round (possibly adapted)\n            config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n            fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n                round or an evaluation round. Not used here.\n        \"\"\"\n        # Make sure that the proper components exist.\n        assert self.initial_global_model is not None and self.parameter_exchanger is not None\n\n        # Route the parameters to the GLOBAL model only in MR-MTL\n        log(INFO, \"Setting the global model weights\")\n        server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n        log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n        self.parameter_exchanger.pull_parameters(server_model_state, self.initial_global_model, config)\n\n    @ensure_protocol_compliance\n    def update_before_train(self: MrMtlPersonalizedProtocol, current_server_round: int) -&gt; None:\n        assert self.initial_global_model is not None\n        # Freeze the initial weights of the INITIAL GLOBAL MODEL. These are used to form the MR-MTL\n        # update penalty term.\n        for param in self.initial_global_model.parameters():\n            param.requires_grad = False\n        self.initial_global_model.eval()\n\n        # Saving the initial GLOBAL MODEL weights and detaching them so that we don't compute gradients with\n        # respect to the tensors. These are used to form the MR-MTL local update penalty term.\n        self.drift_penalty_tensors = [\n            initial_layer_weights.detach().clone() for initial_layer_weights in self.initial_global_model.parameters()\n        ]\n\n        return super().update_before_train(current_server_round)  # type: ignore[safe-super]\n\n    @ensure_protocol_compliance\n    def compute_training_loss(\n        self: MrMtlPersonalizedProtocol,\n        preds: TorchPredType,\n        features: TorchFeatureType,\n        target: TorchTargetType,\n    ) -&gt; TrainingLosses:\n        \"\"\"\n        Computes training losses given predictions of the modes and ground truth data. We add to vanilla loss\n        function by including Mean Regularized (MR) penalty loss which is the \\\\(\\\\ell^2\\\\) inner product between the\n        initial global model weights and weights of the current model.\n\n        Args:\n            preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n                All predictions included in dictionary will be used to compute metrics.\n            features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n            target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n                by name. Additional losses includes each loss component of the total loss.\n        \"\"\"\n        # Check that the initial global model isn't in training mode and that the local model is in training mode\n        assert self.initial_global_model is not None and not self.initial_global_model.training and self.model.training\n        # Use the rest of the training loss computation from the AdaptiveDriftConstraintClient parent\n        return super().compute_training_loss(preds, features, target)  # type: ignore[safe-super]\n\n    @ensure_protocol_compliance\n    def validate(\n        self: MrMtlPersonalizedProtocol, include_losses_in_metrics: bool = False\n    ) -&gt; tuple[float, dict[str, Scalar]]:\n        \"\"\"\n        Validate the current model on the entire validation dataset.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n        \"\"\"\n        # ensure that the initial global model is in eval mode\n        assert self.initial_global_model is not None and not self.initial_global_model.training\n        return super().validate(include_losses_in_metrics=include_losses_in_metrics)  # type: ignore[safe-super]\n</code></pre> <code></code> <code>__init__(*args, **kwargs)</code> \u00b6 <p>This client implements the MR-MTL algorithm from MR-MTL: On Privacy and Personalization in Cross-Silo Federated Learning. The idea is that we want to train personalized versions of the global model for each client. However, instead of using a separate solver for the global model, as in Ditto, we update the initial global model with aggregated local models on the server-side and use those weights to also constrain the training of a local model. The constraint for this local model is identical to the FedProx loss. The key difference is that the local model is never replaced with aggregated weights. It is always local.</p> <p>NOTE: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the corresponding strategy used by the server</p> Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>def __init__(self: MrMtlPersonalizedProtocol, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"\n    This client implements the MR-MTL algorithm from MR-MTL: On Privacy and Personalization in Cross-Silo\n    Federated Learning. The idea is that we want to train personalized versions of the global model for each\n    client. However, instead of using a separate solver for the global model, as in Ditto, we update the initial\n    global model with aggregated local models on the server-side and use those weights to also constrain the\n    training of a local model. The constraint for this local model is identical to the FedProx loss. The key\n    difference is that the local model is never replaced with aggregated weights. It is always local.\n\n    **NOTE**: lambda, the drift loss weight, is initially set and potentially adapted by the server akin to the\n    heuristic suggested in the original FedProx paper. Adaptation is optional and can be disabled in the\n    corresponding strategy used by the server\n    \"\"\"\n    # Initialize mixin-specific attributes\n    self.initial_global_model: torch.nn.Module | None = None\n    self.initial_global_tensors: list[torch.Tensor] = []\n\n    super().__init__(*args, **kwargs)\n</code></pre> <code></code> <code>get_global_model(config)</code> \u00b6 <p>Returns the global model on client setup to be used as a constraint for the local model during training.</p> <p>The global model should be the same architecture as the local model so we reuse the <code>get_model</code> call. We explicitly send the model to the desired device. This is idempotent.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The PyTorch model serving as the global model for Ditto</p> Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>def get_global_model(self: MrMtlPersonalizedProtocol, config: Config) -&gt; nn.Module:\n    \"\"\"\n    Returns the global model on client setup to be used as a constraint for the local model during training.\n\n    The global model should be the same architecture as the local model so we reuse the ``get_model`` call. We\n    explicitly send the model to the desired device. This is idempotent.\n\n    Args:\n        config (Config): The config from the server.\n\n    Returns:\n        (nn.Module): The PyTorch model serving as the global model for Ditto\n    \"\"\"\n    model_copy = copy.deepcopy(self.get_model(config))\n    return model_copy.to(self.device)\n</code></pre> <code></code> <code>setup_client(config)</code> \u00b6 <p>Set dataloaders, optimizers, parameter exchangers and other attributes derived from these. Then set initialized attribute to True. In this class, this function simply adds the additional step of setting up the global model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>@ensure_protocol_compliance\ndef setup_client(self: MrMtlPersonalizedProtocol, config: Config) -&gt; None:\n    \"\"\"\n    Set dataloaders, optimizers, parameter exchangers and other attributes derived from these.\n    Then set initialized attribute to True. In this class, this function simply adds the additional step of\n    setting up the global model.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    try:\n        self.initial_global_model = self.get_global_model(config)\n        log(INFO, f\"initial global model set: {type(self.initial_global_model).__name__}\")\n    except AttributeError:\n        log(\n            INFO,\n            \"Couldn't set initial global model before super().setup_client(). Will try again within that setup.\",\n        )\n        pass\n    # The rest of the setup is the same\n    super().setup_client(config)  # type:ignore [safe-super]\n</code></pre> <code></code> <code>get_optimizer(config)</code> \u00b6 <p>Implementing get_optimizer as a hook to set initial global model if not already set.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config from the server.</p> required Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>@ensure_protocol_compliance\ndef get_optimizer(self: MrMtlPersonalizedProtocol, config: Config) -&gt; dict[str, Optimizer]:\n    \"\"\"\n    Implementing get_optimizer as a hook to set initial global model if not already set.\n\n    Args:\n        config (Config): The config from the server.\n    \"\"\"\n    if self.initial_global_model is None:\n        # try set it here\n        self.initial_global_model = self.get_global_model(config)\n        log(\n            INFO,\n            f\"initial_global_model set: {type(self.initial_global_model).__name__} within `get_optimizer`\",\n        )\n\n    return super().get_optimizer(config=config)  # type: ignore[safe-super, return-value]\n</code></pre> <code></code> <code>set_parameters(parameters, config, fitting_round)</code> \u00b6 <p>The parameters being passed are to be routed to the initial global model to be used in a penalty term in training the local model. Despite the usual FL setup, we actually never pass the aggregated model to the LOCAL model. Instead, we use the aggregated model to form the MR-MTL penalty term.</p> <p>NOTE: In MR-MTL, unlike Ditto, the local model weights are not synced across clients to the initial global model, even in the FIRST ROUND.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameters have information about model state to be added to the relevant client model. It will also contain a penalty weight from the server at each round (possibly adapted)</p> required <code>config</code> <code>Config</code> <p>The config is sent by the FL server to allow for customization in the function if desired.</p> required <code>fitting_round</code> <code>bool</code> <p>Boolean that indicates whether the current federated learning round is a fitting round or an evaluation round. Not used here.</p> required Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>@ensure_protocol_compliance\ndef set_parameters(\n    self: MrMtlPersonalizedProtocol, parameters: NDArrays, config: Config, fitting_round: bool\n) -&gt; None:\n    \"\"\"\n    The parameters being passed are to be routed to the initial global model to be used in a penalty term in\n    training the local model. Despite the usual FL setup, we actually never pass the aggregated model to the\n    **LOCAL** model. Instead, we use the aggregated model to form the MR-MTL penalty term.\n\n    NOTE: In MR-MTL, unlike Ditto, the local model weights are not synced across clients to the initial global\n    model, even in the **FIRST ROUND**.\n\n    Args:\n        parameters (NDArrays): Parameters have information about model state to be added to the relevant client\n            model. It will also contain a penalty weight from the server at each round (possibly adapted)\n        config (Config): The config is sent by the FL server to allow for customization in the function if desired.\n        fitting_round (bool): Boolean that indicates whether the current federated learning round is a fitting\n            round or an evaluation round. Not used here.\n    \"\"\"\n    # Make sure that the proper components exist.\n    assert self.initial_global_model is not None and self.parameter_exchanger is not None\n\n    # Route the parameters to the GLOBAL model only in MR-MTL\n    log(INFO, \"Setting the global model weights\")\n    server_model_state, self.drift_penalty_weight = self.parameter_exchanger.unpack_parameters(parameters)\n    log(INFO, f\"Lambda weight received from the server: {self.drift_penalty_weight}\")\n\n    self.parameter_exchanger.pull_parameters(server_model_state, self.initial_global_model, config)\n</code></pre> <code></code> <code>compute_training_loss(preds, features, target)</code> \u00b6 <p>Computes training losses given predictions of the modes and ground truth data. We add to vanilla loss function by including Mean Regularized (MR) penalty loss which is the \\(\\ell^2\\) inner product between the initial global model weights and weights of the current model.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>TorchPredType</code> <p>Prediction(s) of the model(s) indexed by name. All predictions included in dictionary will be used to compute metrics.</p> required <code>features</code> <code>TorchFeatureType</code> <p>Feature(s) of the model(s) indexed by name.</p> required <code>target</code> <code>TorchTargetType</code> <p>Ground truth data to evaluate predictions against.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> containing backward loss and additional losses indexed by name. Additional losses includes each loss component of the total loss.</p> Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>@ensure_protocol_compliance\ndef compute_training_loss(\n    self: MrMtlPersonalizedProtocol,\n    preds: TorchPredType,\n    features: TorchFeatureType,\n    target: TorchTargetType,\n) -&gt; TrainingLosses:\n    \"\"\"\n    Computes training losses given predictions of the modes and ground truth data. We add to vanilla loss\n    function by including Mean Regularized (MR) penalty loss which is the \\\\(\\\\ell^2\\\\) inner product between the\n    initial global model weights and weights of the current model.\n\n    Args:\n        preds (TorchPredType): Prediction(s) of the model(s) indexed by name.\n            All predictions included in dictionary will be used to compute metrics.\n        features (TorchFeatureType): Feature(s) of the model(s) indexed by name.\n        target (TorchTargetType): Ground truth data to evaluate predictions against.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` containing backward loss and additional losses indexed\n            by name. Additional losses includes each loss component of the total loss.\n    \"\"\"\n    # Check that the initial global model isn't in training mode and that the local model is in training mode\n    assert self.initial_global_model is not None and not self.initial_global_model.training and self.model.training\n    # Use the rest of the training loss computation from the AdaptiveDriftConstraintClient parent\n    return super().compute_training_loss(preds, features, target)  # type: ignore[safe-super]\n</code></pre> <code></code> <code>validate(include_losses_in_metrics=False)</code> \u00b6 <p>Validate the current model on the entire validation dataset.</p> <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]]</code> <p>The validation loss and a dictionary of metrics from validation.</p> Source code in <code>fl4health/mixins/personalized/mr_mtl.py</code> <pre><code>@ensure_protocol_compliance\ndef validate(\n    self: MrMtlPersonalizedProtocol, include_losses_in_metrics: bool = False\n) -&gt; tuple[float, dict[str, Scalar]]:\n    \"\"\"\n    Validate the current model on the entire validation dataset.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]]): The validation loss and a dictionary of metrics from validation.\n    \"\"\"\n    # ensure that the initial global model is in eval mode\n    assert self.initial_global_model is not None and not self.initial_global_model.training\n    return super().validate(include_losses_in_metrics=include_losses_in_metrics)  # type: ignore[safe-super]\n</code></pre>"},{"location":"api/#fl4health.mixins.personalized.utils","title":"<code>utils</code>","text":""},{"location":"api/#fl4health.mixins.personalized.utils.ensure_protocol_compliance","title":"<code>ensure_protocol_compliance(func, instance, args, kwargs)</code>","text":"<p>Wrapper to ensure that the instance is of <code>FlexibleClient</code> type.</p> <p>NOTE: This should only be used within a <code>FlexibleClient</code>.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>The function to be wrapped.</p> required <code>instance</code> <code>Any | None</code> <p>The associated instance if it is a method belonging to a class or a standalone</p> required <code>args</code> <code>Any</code> <p>args passed to func.</p> required <code>kwargs</code> <code>Any</code> <p>kwargs passed to func.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>We raise this error if the instance is not a <code>FlexibleClient</code>.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Application of the function to the args and kwargs.</p> Source code in <code>fl4health/mixins/personalized/utils.py</code> <pre><code>@wrapt.decorator\ndef ensure_protocol_compliance(func: Callable, instance: Any | None, args: Any, kwargs: Any) -&gt; Any:\n    \"\"\"\n    Wrapper to ensure that the instance is of ``FlexibleClient`` type.\n\n    **NOTE**: This should only be used within a ``FlexibleClient``.\n\n    Args:\n        func (Callable): The function to be wrapped.\n        instance (Any | None): The associated instance if it is a method belonging to a class or a standalone\n        args (Any): args passed to func.\n        kwargs (Any): kwargs passed to func.\n\n    Raises:\n        TypeError: We raise this error if the instance is not a ``FlexibleClient``.\n\n    Returns:\n        (Any): Application of the function to the args and kwargs.\n    \"\"\"\n    # validate self is a FlexibleClient\n    if not isinstance(instance, FlexibleClient):\n        raise TypeError(\"Protocol requirements not met.\")\n\n    return func(*args, **kwargs)\n</code></pre>"},{"location":"api/#fl4health.model_bases","title":"<code>model_bases</code>","text":""},{"location":"api/#fl4health.model_bases.apfl_base","title":"<code>apfl_base</code>","text":""},{"location":"api/#fl4health.model_bases.apfl_base.ApflModule","title":"<code>ApflModule</code>","text":"<p>               Bases: <code>PartialLayerExchangeModel</code></p> Source code in <code>fl4health/model_bases/apfl_base.py</code> <pre><code>class ApflModule(PartialLayerExchangeModel):\n    def __init__(\n        self,\n        model: nn.Module,\n        adaptive_alpha: bool = True,\n        alpha: float = 0.5,\n        alpha_lr: float = 0.01,\n    ) -&gt; None:\n        \"\"\"\n        Defines a model compatible with the APFL approach.\n\n        Args:\n            model (nn.Module): The underlying model architecture to be optimized. A twin of this model will be created\n                to initialize a local and global version of this architecture.\n            adaptive_alpha (bool, optional): Whether or not the mixing parameter \\\\(\\\\alpha\\\\) will be adapted\n                during training. Predictions of the local and global models are combined using \\\\(\\\\alpha\\\\) to\n                provide a final prediction. Defaults to True.\n            alpha (float, optional): The initial value for the mixing parameter \\\\(\\\\alpha\\\\). Defaults to 0.5.\n            alpha_lr (float, optional): The learning rate to be applied when adaptive \\\\(\\\\alpha\\\\) during training.\n                If ``adaptive_alpha`` is False, then this parameter does nothing. Defaults to 0.01.\n        \"\"\"\n        super().__init__()\n        self.local_model: nn.Module = model\n        self.global_model: nn.Module = copy.deepcopy(model)\n\n        self.adaptive_alpha = adaptive_alpha\n        self.alpha = alpha\n        self.alpha_lr = alpha_lr\n\n    def global_forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward function that runs the input tensor through the **GLOBAL** model only.\n\n        Args:\n            input (torch.Tensor): tensor to be run through the global model\n\n        Returns:\n            (torch.Tensor): output from the global model only.\n        \"\"\"\n        return self.global_model(input)\n\n    def local_forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward function that runs the input tensor through the **LOCAL** model only.\n\n        Args:\n            input (torch.Tensor): tensor to be run through the local model.\n\n        Returns:\n            (torch.Tensor): output from the local model only.\n        \"\"\"\n        return self.local_model(input)\n\n    def forward(self, input: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n        \"\"\"\n        Forward function for the full APFL model. This includes mixing of the global and local model predictions using\n        \\\\(\\\\alpha\\\\). The predictions are combined as follows.\n\n        \\\\[\\\\alpha \\\\cdot \\\\text{local_logits} + (1.0 - \\\\alpha) \\\\cdot \\\\text{global_logits}\\\\]\n\n        Args:\n            input (torch.Tensor): Input tensor to be run through both the local and global models\n\n        Returns:\n            (dict[str, torch.Tensor]): Final prediction after mixing predictions produced by the local and global\n                models. This dictionary stores these predictions under the key \"personal\" while the local and global\n                model predictions are stored under the keys \"global\" and \"local.\"\n        \"\"\"\n        # Forward return dictionary because APFL has multiple different prediction types\n        global_logits = self.global_forward(input)\n        local_logits = self.local_forward(input)\n        personal_logits = self.alpha * local_logits + (1.0 - self.alpha) * global_logits\n        return {\"personal\": personal_logits, \"global\": global_logits, \"local\": local_logits}\n\n    def update_alpha(self) -&gt; None:\n        \"\"\"\n        Updates to mixture parameter follow original implementation:\n\n        https://github.com/MLOPTPSU/FedTorch/blob/ab8068dbc96804a5c1a8b898fd115175cfebfe75/fedtorch/comms/utils/flow_utils.py#L240\n        \"\"\"  # noqa\n\n        # Need to filter out frozen parameters, as they have no grad object\n        local_parameters = [\n            local_params for local_params in self.local_model.parameters() if local_params.requires_grad\n        ]\n        global_parameters = [\n            global_params for global_params in self.global_model.parameters() if global_params.requires_grad\n        ]\n\n        # Accumulate gradient of alpha across layers\n        grad_alpha: float = 0.0\n        for local_p, global_p in zip(local_parameters, global_parameters):\n            local_grad = local_p.grad\n            global_grad = global_p.grad\n            assert local_grad is not None and global_grad is not None\n            dif = local_p - global_p\n            grad = torch.tensor(self.alpha) * local_grad + torch.tensor(1.0 - self.alpha) * global_grad\n            grad_alpha += torch.mul(dif, grad).sum().detach().cpu().numpy().item()\n\n        # This update constant of 0.02 is not referenced in the paper\n        # but is present in the official implementation and other ones I have seen\n        # Not sure its function, just adding a number proportional to alpha to the grad\n        # Leaving in for consistency with official implementation\n        grad_alpha += 0.02 * self.alpha\n        alpha = self.alpha - self.alpha_lr * grad_alpha\n        # Clip alpha to be between [0, 1]\n        alpha = max(min(alpha, 1), 0)\n        self.alpha = alpha\n\n    def layers_to_exchange(self) -&gt; list[str]:\n        \"\"\"\n        Specifies the model layers to be exchanged with the server. These are a fixed set of layers exchanged every\n        round. For APFL, these are any layers associated with the ``global_model``. That is, none of the parameters\n        of the local model are aggregated on the server side, nor is \\\\(\\\\alpha\\\\).\n\n        Returns:\n            (list[str]): Names of layers associated with the global model. These correspond to the layer names in the\n                state dictionary of this entire module.\n        \"\"\"\n        layers_to_exchange: list[str] = [layer for layer in self.state_dict() if layer.startswith(\"global_model.\")]\n        return layers_to_exchange\n</code></pre>"},{"location":"api/#fl4health.model_bases.apfl_base.ApflModule.__init__","title":"<code>__init__(model, adaptive_alpha=True, alpha=0.5, alpha_lr=0.01)</code>","text":"<p>Defines a model compatible with the APFL approach.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The underlying model architecture to be optimized. A twin of this model will be created to initialize a local and global version of this architecture.</p> required <code>adaptive_alpha</code> <code>bool</code> <p>Whether or not the mixing parameter \\(\\alpha\\) will be adapted during training. Predictions of the local and global models are combined using \\(\\alpha\\) to provide a final prediction. Defaults to True.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>The initial value for the mixing parameter \\(\\alpha\\). Defaults to 0.5.</p> <code>0.5</code> <code>alpha_lr</code> <code>float</code> <p>The learning rate to be applied when adaptive \\(\\alpha\\) during training. If <code>adaptive_alpha</code> is False, then this parameter does nothing. Defaults to 0.01.</p> <code>0.01</code> Source code in <code>fl4health/model_bases/apfl_base.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    adaptive_alpha: bool = True,\n    alpha: float = 0.5,\n    alpha_lr: float = 0.01,\n) -&gt; None:\n    \"\"\"\n    Defines a model compatible with the APFL approach.\n\n    Args:\n        model (nn.Module): The underlying model architecture to be optimized. A twin of this model will be created\n            to initialize a local and global version of this architecture.\n        adaptive_alpha (bool, optional): Whether or not the mixing parameter \\\\(\\\\alpha\\\\) will be adapted\n            during training. Predictions of the local and global models are combined using \\\\(\\\\alpha\\\\) to\n            provide a final prediction. Defaults to True.\n        alpha (float, optional): The initial value for the mixing parameter \\\\(\\\\alpha\\\\). Defaults to 0.5.\n        alpha_lr (float, optional): The learning rate to be applied when adaptive \\\\(\\\\alpha\\\\) during training.\n            If ``adaptive_alpha`` is False, then this parameter does nothing. Defaults to 0.01.\n    \"\"\"\n    super().__init__()\n    self.local_model: nn.Module = model\n    self.global_model: nn.Module = copy.deepcopy(model)\n\n    self.adaptive_alpha = adaptive_alpha\n    self.alpha = alpha\n    self.alpha_lr = alpha_lr\n</code></pre>"},{"location":"api/#fl4health.model_bases.apfl_base.ApflModule.global_forward","title":"<code>global_forward(input)</code>","text":"<p>Forward function that runs the input tensor through the GLOBAL model only.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>tensor to be run through the global model</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output from the global model only.</p> Source code in <code>fl4health/model_bases/apfl_base.py</code> <pre><code>def global_forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward function that runs the input tensor through the **GLOBAL** model only.\n\n    Args:\n        input (torch.Tensor): tensor to be run through the global model\n\n    Returns:\n        (torch.Tensor): output from the global model only.\n    \"\"\"\n    return self.global_model(input)\n</code></pre>"},{"location":"api/#fl4health.model_bases.apfl_base.ApflModule.local_forward","title":"<code>local_forward(input)</code>","text":"<p>Forward function that runs the input tensor through the LOCAL model only.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>tensor to be run through the local model.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output from the local model only.</p> Source code in <code>fl4health/model_bases/apfl_base.py</code> <pre><code>def local_forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward function that runs the input tensor through the **LOCAL** model only.\n\n    Args:\n        input (torch.Tensor): tensor to be run through the local model.\n\n    Returns:\n        (torch.Tensor): output from the local model only.\n    \"\"\"\n    return self.local_model(input)\n</code></pre>"},{"location":"api/#fl4health.model_bases.apfl_base.ApflModule.forward","title":"<code>forward(input)</code>","text":"<p>Forward function for the full APFL model. This includes mixing of the global and local model predictions using \\(\\alpha\\). The predictions are combined as follows.</p> \\[\\alpha \\cdot \\text{local_logits} + (1.0 - \\alpha) \\cdot \\text{global_logits}\\] <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor to be run through both the local and global models</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>Final prediction after mixing predictions produced by the local and global models. This dictionary stores these predictions under the key \"personal\" while the local and global model predictions are stored under the keys \"global\" and \"local.\"</p> Source code in <code>fl4health/model_bases/apfl_base.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Forward function for the full APFL model. This includes mixing of the global and local model predictions using\n    \\\\(\\\\alpha\\\\). The predictions are combined as follows.\n\n    \\\\[\\\\alpha \\\\cdot \\\\text{local_logits} + (1.0 - \\\\alpha) \\\\cdot \\\\text{global_logits}\\\\]\n\n    Args:\n        input (torch.Tensor): Input tensor to be run through both the local and global models\n\n    Returns:\n        (dict[str, torch.Tensor]): Final prediction after mixing predictions produced by the local and global\n            models. This dictionary stores these predictions under the key \"personal\" while the local and global\n            model predictions are stored under the keys \"global\" and \"local.\"\n    \"\"\"\n    # Forward return dictionary because APFL has multiple different prediction types\n    global_logits = self.global_forward(input)\n    local_logits = self.local_forward(input)\n    personal_logits = self.alpha * local_logits + (1.0 - self.alpha) * global_logits\n    return {\"personal\": personal_logits, \"global\": global_logits, \"local\": local_logits}\n</code></pre>"},{"location":"api/#fl4health.model_bases.apfl_base.ApflModule.update_alpha","title":"<code>update_alpha()</code>","text":"<p>Updates to mixture parameter follow original implementation:</p> <p>https://github.com/MLOPTPSU/FedTorch/blob/ab8068dbc96804a5c1a8b898fd115175cfebfe75/fedtorch/comms/utils/flow_utils.py#L240</p> Source code in <code>fl4health/model_bases/apfl_base.py</code> <pre><code>def update_alpha(self) -&gt; None:\n    \"\"\"\n    Updates to mixture parameter follow original implementation:\n\n    https://github.com/MLOPTPSU/FedTorch/blob/ab8068dbc96804a5c1a8b898fd115175cfebfe75/fedtorch/comms/utils/flow_utils.py#L240\n    \"\"\"  # noqa\n\n    # Need to filter out frozen parameters, as they have no grad object\n    local_parameters = [\n        local_params for local_params in self.local_model.parameters() if local_params.requires_grad\n    ]\n    global_parameters = [\n        global_params for global_params in self.global_model.parameters() if global_params.requires_grad\n    ]\n\n    # Accumulate gradient of alpha across layers\n    grad_alpha: float = 0.0\n    for local_p, global_p in zip(local_parameters, global_parameters):\n        local_grad = local_p.grad\n        global_grad = global_p.grad\n        assert local_grad is not None and global_grad is not None\n        dif = local_p - global_p\n        grad = torch.tensor(self.alpha) * local_grad + torch.tensor(1.0 - self.alpha) * global_grad\n        grad_alpha += torch.mul(dif, grad).sum().detach().cpu().numpy().item()\n\n    # This update constant of 0.02 is not referenced in the paper\n    # but is present in the official implementation and other ones I have seen\n    # Not sure its function, just adding a number proportional to alpha to the grad\n    # Leaving in for consistency with official implementation\n    grad_alpha += 0.02 * self.alpha\n    alpha = self.alpha - self.alpha_lr * grad_alpha\n    # Clip alpha to be between [0, 1]\n    alpha = max(min(alpha, 1), 0)\n    self.alpha = alpha\n</code></pre>"},{"location":"api/#fl4health.model_bases.apfl_base.ApflModule.layers_to_exchange","title":"<code>layers_to_exchange()</code>","text":"<p>Specifies the model layers to be exchanged with the server. These are a fixed set of layers exchanged every round. For APFL, these are any layers associated with the <code>global_model</code>. That is, none of the parameters of the local model are aggregated on the server side, nor is \\(\\alpha\\).</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Names of layers associated with the global model. These correspond to the layer names in the state dictionary of this entire module.</p> Source code in <code>fl4health/model_bases/apfl_base.py</code> <pre><code>def layers_to_exchange(self) -&gt; list[str]:\n    \"\"\"\n    Specifies the model layers to be exchanged with the server. These are a fixed set of layers exchanged every\n    round. For APFL, these are any layers associated with the ``global_model``. That is, none of the parameters\n    of the local model are aggregated on the server side, nor is \\\\(\\\\alpha\\\\).\n\n    Returns:\n        (list[str]): Names of layers associated with the global model. These correspond to the layer names in the\n            state dictionary of this entire module.\n    \"\"\"\n    layers_to_exchange: list[str] = [layer for layer in self.state_dict() if layer.startswith(\"global_model.\")]\n    return layers_to_exchange\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base","title":"<code>autoencoders_base</code>","text":""},{"location":"api/#fl4health.model_bases.autoencoders_base.AbstractAe","title":"<code>AbstractAe</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>class AbstractAe(nn.Module, ABC):\n    def __init__(\n        self,\n        encoder: nn.Module,\n        decoder: nn.Module,\n    ) -&gt; None:\n        \"\"\"\n        The base class for all autoencoder based models. To define this model, we need to define the structure of\n        the encoder and the decoder modules. This type of model should have the capability to encode data using the\n        encoder module and decode the output of the encoder using the decoder module.\n\n        Args:\n            encoder (nn.Module): Model for encoding the input.\n            decoder (nn.Module): Model for encoding the output. This module should be compatible with the output\n                structure of the encoder module.\n        \"\"\"\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    @abstractmethod\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward is called in client classes with a single input tensor.\n\n        Args:\n            input (torch.Tensor): Input tensor.\n\n        Raises:\n            NotImplementedError: Should be implemented in inheriting classes.\n\n        Returns:\n            (torch.Tensor): Tensor after passing through module.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.AbstractAe.__init__","title":"<code>__init__(encoder, decoder)</code>","text":"<p>The base class for all autoencoder based models. To define this model, we need to define the structure of the encoder and the decoder modules. This type of model should have the capability to encode data using the encoder module and decode the output of the encoder using the decoder module.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Module</code> <p>Model for encoding the input.</p> required <code>decoder</code> <code>Module</code> <p>Model for encoding the output. This module should be compatible with the output structure of the encoder module.</p> required Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def __init__(\n    self,\n    encoder: nn.Module,\n    decoder: nn.Module,\n) -&gt; None:\n    \"\"\"\n    The base class for all autoencoder based models. To define this model, we need to define the structure of\n    the encoder and the decoder modules. This type of model should have the capability to encode data using the\n    encoder module and decode the output of the encoder using the decoder module.\n\n    Args:\n        encoder (nn.Module): Model for encoding the input.\n        decoder (nn.Module): Model for encoding the output. This module should be compatible with the output\n            structure of the encoder module.\n    \"\"\"\n    super().__init__()\n    self.encoder = encoder\n    self.decoder = decoder\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.AbstractAe.forward","title":"<code>forward(input)</code>  <code>abstractmethod</code>","text":"<p>Forward is called in client classes with a single input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Should be implemented in inheriting classes.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor after passing through module.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>@abstractmethod\ndef forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward is called in client classes with a single input tensor.\n\n    Args:\n        input (torch.Tensor): Input tensor.\n\n    Raises:\n        NotImplementedError: Should be implemented in inheriting classes.\n\n    Returns:\n        (torch.Tensor): Tensor after passing through module.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.BasicAe","title":"<code>BasicAe</code>","text":"<p>               Bases: <code>AbstractAe</code></p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>class BasicAe(AbstractAe):\n    def __init__(self, encoder: nn.Module, decoder: nn.Module) -&gt; None:\n        \"\"\"\n        Standard auto-encoder structure. To define this model, we need to define the structure of the encoder and\n        the decoder modules. This type of model should have the capability to encode data using the  encoder module\n        and decode the output of the encoder using the decoder module.\n\n        Args:\n            encoder (nn.Module): Model for encoding the input.\n            decoder (nn.Module): Model for encoding the output. This module should be compatible with the output\n                structure of the encoder module.\n        \"\"\"\n        super().__init__(encoder, decoder)\n\n    def encode(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Defines the forward associated with encoding the provided input tensor. We reuse the forward for the encoder\n        module.\n\n        Args:\n            input (torch.Tensor): Input tensor to be encoded.\n\n        Returns:\n            (torch.Tensor): Encoding associated with the input tensor.\n        \"\"\"\n        return self.encoder(input)\n\n    def decode(self, latent_vector: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Defines the forward associated with decoding a latent vector encoded by the encoder from some input.\n\n        Args:\n            latent_vector (torch.Tensor): Latent vector to be decoded.\n\n        Returns:\n            (torch.Tensor): Decoded tensor.\n        \"\"\"\n        return self.decoder(latent_vector)\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward function for the ``BasicAe`` model. It simply pieces the encoding and decoding forwards together\n        to reconstruct the input through the encoder-decoder pipeline.\n\n        Args:\n            input (torch.Tensor): Input to pass through the encoder.\n\n        Returns:\n            (torch.Tensor): Reconstructed input after encoding and decoding with the model.\n        \"\"\"\n        z = self.encode(input)\n        return self.decode(z)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.BasicAe.__init__","title":"<code>__init__(encoder, decoder)</code>","text":"<p>Standard auto-encoder structure. To define this model, we need to define the structure of the encoder and the decoder modules. This type of model should have the capability to encode data using the  encoder module and decode the output of the encoder using the decoder module.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Module</code> <p>Model for encoding the input.</p> required <code>decoder</code> <code>Module</code> <p>Model for encoding the output. This module should be compatible with the output structure of the encoder module.</p> required Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def __init__(self, encoder: nn.Module, decoder: nn.Module) -&gt; None:\n    \"\"\"\n    Standard auto-encoder structure. To define this model, we need to define the structure of the encoder and\n    the decoder modules. This type of model should have the capability to encode data using the  encoder module\n    and decode the output of the encoder using the decoder module.\n\n    Args:\n        encoder (nn.Module): Model for encoding the input.\n        decoder (nn.Module): Model for encoding the output. This module should be compatible with the output\n            structure of the encoder module.\n    \"\"\"\n    super().__init__(encoder, decoder)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.BasicAe.encode","title":"<code>encode(input)</code>","text":"<p>Defines the forward associated with encoding the provided input tensor. We reuse the forward for the encoder module.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor to be encoded.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoding associated with the input tensor.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def encode(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Defines the forward associated with encoding the provided input tensor. We reuse the forward for the encoder\n    module.\n\n    Args:\n        input (torch.Tensor): Input tensor to be encoded.\n\n    Returns:\n        (torch.Tensor): Encoding associated with the input tensor.\n    \"\"\"\n    return self.encoder(input)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.BasicAe.decode","title":"<code>decode(latent_vector)</code>","text":"<p>Defines the forward associated with decoding a latent vector encoded by the encoder from some input.</p> <p>Parameters:</p> Name Type Description Default <code>latent_vector</code> <code>Tensor</code> <p>Latent vector to be decoded.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded tensor.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def decode(self, latent_vector: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Defines the forward associated with decoding a latent vector encoded by the encoder from some input.\n\n    Args:\n        latent_vector (torch.Tensor): Latent vector to be decoded.\n\n    Returns:\n        (torch.Tensor): Decoded tensor.\n    \"\"\"\n    return self.decoder(latent_vector)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.BasicAe.forward","title":"<code>forward(input)</code>","text":"<p>Forward function for the <code>BasicAe</code> model. It simply pieces the encoding and decoding forwards together to reconstruct the input through the encoder-decoder pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input to pass through the encoder.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed input after encoding and decoding with the model.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward function for the ``BasicAe`` model. It simply pieces the encoding and decoding forwards together\n    to reconstruct the input through the encoder-decoder pipeline.\n\n    Args:\n        input (torch.Tensor): Input to pass through the encoder.\n\n    Returns:\n        (torch.Tensor): Reconstructed input after encoding and decoding with the model.\n    \"\"\"\n    z = self.encode(input)\n    return self.decode(z)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.VariationalAe","title":"<code>VariationalAe</code>","text":"<p>               Bases: <code>AbstractAe</code></p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>class VariationalAe(AbstractAe):\n    def __init__(\n        self,\n        encoder: nn.Module,\n        decoder: nn.Module,\n    ) -&gt; None:\n        \"\"\"\n        Variational Auto-Encoder model base class.\n\n        Args:\n            encoder (nn.Module): Encoder module defined by the user.\n            decoder (nn.Module): Decoder module defined by the user.\n        \"\"\"\n        super().__init__(encoder, decoder)\n\n    def encode(self, input: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Encodes the provided input using the provided encoder. That assumption is that the encoder produces a mean\n        and log variance value each of the same dimension from the input in the standard flow used by variational\n        autoencoders.\n\n        Args:\n            input (torch.Tensor): Input to be encoded.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Mean and log variance values of the same dimensionality representing\n                the latent vector information to be used in VAE reconstruction.\n        \"\"\"\n        mu, logvar = self.encoder(input)\n        return mu, logvar\n\n    def decode(self, latent_vector: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        From a latent vector, this function aims to reconstruct the input that was used to generate the provided\n        latent representation.\n\n        Args:\n            latent_vector (torch.Tensor): Latent vector. For VAEs this is a vector of some fixed dimension, given\n                logvar and \\\\(\\\\mu\\\\) generated by the encoder as\n\n                \\\\[\\\\mu + \\\\epsilon \\\\cdot \\\\exp \\\\left(0.5 \\\\cdot \\\\text{logvar} \\\\right),\\\\]\n\n                where \\\\(\\\\epsilon \\\\sim \\\\mathcal{N}(\\\\mathbf{0}, I)\\\\).\n\n        Returns:\n            (torch.Tensor): Decoding from the latent vector.\n        \"\"\"\n        return self.decoder(latent_vector)\n\n    def sampling(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Sampling using the reparameterization trick to make the VAE differentiable. This sampling produces a vector\n        as if it were sampled from \\\\(\\\\mathcal{N}\\\\left(\\\\mu, \\\\exp(0.5 \\\\cdot \\\\text{logvar}) I \\\\right)\\\\).\n\n        Args:\n            mu (torch.Tensor): Mean of the normal distribution from which to sample.\n            logvar (torch.Tensor): Log of the variance of the normal distribution from which to sample.\n\n        Returns:\n            (torch.Tensor): Latent vector sampled from the appropriate normal distribution.\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward through the full encoder and decoder.\n\n        Args:\n            input (torch.Tensor): input tensor.\n\n        Returns:\n            (torch.Tensor): reconstruction of the input tensor.\n        \"\"\"\n        mu, logvar = self.encode(input)\n        z = self.sampling(mu, logvar)\n        output = self.decode(z)\n        # Output (reconstruction) is flattened to be concatenated with mu and logvar vectors.\n        # The shape of the flattened_output can be later restored by having the training data shape,\n        # or the decoder structure.\n        # This assumes output is \"batch first\".\n        flattened_output = output.view(output.shape[0], -1)\n        return torch.cat((logvar, mu, flattened_output), dim=1)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.VariationalAe.__init__","title":"<code>__init__(encoder, decoder)</code>","text":"<p>Variational Auto-Encoder model base class.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Module</code> <p>Encoder module defined by the user.</p> required <code>decoder</code> <code>Module</code> <p>Decoder module defined by the user.</p> required Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def __init__(\n    self,\n    encoder: nn.Module,\n    decoder: nn.Module,\n) -&gt; None:\n    \"\"\"\n    Variational Auto-Encoder model base class.\n\n    Args:\n        encoder (nn.Module): Encoder module defined by the user.\n        decoder (nn.Module): Decoder module defined by the user.\n    \"\"\"\n    super().__init__(encoder, decoder)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.VariationalAe.encode","title":"<code>encode(input)</code>","text":"<p>Encodes the provided input using the provided encoder. That assumption is that the encoder produces a mean and log variance value each of the same dimension from the input in the standard flow used by variational autoencoders.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input to be encoded.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Mean and log variance values of the same dimensionality representing the latent vector information to be used in VAE reconstruction.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def encode(self, input: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Encodes the provided input using the provided encoder. That assumption is that the encoder produces a mean\n    and log variance value each of the same dimension from the input in the standard flow used by variational\n    autoencoders.\n\n    Args:\n        input (torch.Tensor): Input to be encoded.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Mean and log variance values of the same dimensionality representing\n            the latent vector information to be used in VAE reconstruction.\n    \"\"\"\n    mu, logvar = self.encoder(input)\n    return mu, logvar\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.VariationalAe.decode","title":"<code>decode(latent_vector)</code>","text":"<p>From a latent vector, this function aims to reconstruct the input that was used to generate the provided latent representation.</p> <p>Parameters:</p> Name Type Description Default <code>latent_vector</code> <code>Tensor</code> <p>Latent vector. For VAEs this is a vector of some fixed dimension, given logvar and \\(\\mu\\) generated by the encoder as</p> \\[\\mu + \\epsilon \\cdot \\exp \\left(0.5 \\cdot \\text{logvar} \\right),\\] <p>where \\(\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, I)\\).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoding from the latent vector.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def decode(self, latent_vector: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    From a latent vector, this function aims to reconstruct the input that was used to generate the provided\n    latent representation.\n\n    Args:\n        latent_vector (torch.Tensor): Latent vector. For VAEs this is a vector of some fixed dimension, given\n            logvar and \\\\(\\\\mu\\\\) generated by the encoder as\n\n            \\\\[\\\\mu + \\\\epsilon \\\\cdot \\\\exp \\\\left(0.5 \\\\cdot \\\\text{logvar} \\\\right),\\\\]\n\n            where \\\\(\\\\epsilon \\\\sim \\\\mathcal{N}(\\\\mathbf{0}, I)\\\\).\n\n    Returns:\n        (torch.Tensor): Decoding from the latent vector.\n    \"\"\"\n    return self.decoder(latent_vector)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.VariationalAe.sampling","title":"<code>sampling(mu, logvar)</code>","text":"<p>Sampling using the reparameterization trick to make the VAE differentiable. This sampling produces a vector as if it were sampled from \\(\\mathcal{N}\\left(\\mu, \\exp(0.5 \\cdot \\text{logvar}) I \\right)\\).</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>Mean of the normal distribution from which to sample.</p> required <code>logvar</code> <code>Tensor</code> <p>Log of the variance of the normal distribution from which to sample.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Latent vector sampled from the appropriate normal distribution.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def sampling(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Sampling using the reparameterization trick to make the VAE differentiable. This sampling produces a vector\n    as if it were sampled from \\\\(\\\\mathcal{N}\\\\left(\\\\mu, \\\\exp(0.5 \\\\cdot \\\\text{logvar}) I \\\\right)\\\\).\n\n    Args:\n        mu (torch.Tensor): Mean of the normal distribution from which to sample.\n        logvar (torch.Tensor): Log of the variance of the normal distribution from which to sample.\n\n    Returns:\n        (torch.Tensor): Latent vector sampled from the appropriate normal distribution.\n    \"\"\"\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.VariationalAe.forward","title":"<code>forward(input)</code>","text":"<p>Forward through the full encoder and decoder.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>reconstruction of the input tensor.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward through the full encoder and decoder.\n\n    Args:\n        input (torch.Tensor): input tensor.\n\n    Returns:\n        (torch.Tensor): reconstruction of the input tensor.\n    \"\"\"\n    mu, logvar = self.encode(input)\n    z = self.sampling(mu, logvar)\n    output = self.decode(z)\n    # Output (reconstruction) is flattened to be concatenated with mu and logvar vectors.\n    # The shape of the flattened_output can be later restored by having the training data shape,\n    # or the decoder structure.\n    # This assumes output is \"batch first\".\n    flattened_output = output.view(output.shape[0], -1)\n    return torch.cat((logvar, mu, flattened_output), dim=1)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.ConditionalVae","title":"<code>ConditionalVae</code>","text":"<p>               Bases: <code>AbstractAe</code></p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>class ConditionalVae(AbstractAe):\n    def __init__(\n        self,\n        encoder: nn.Module,\n        decoder: nn.Module,\n        unpack_input_condition: Callable[[torch.Tensor], tuple[torch.Tensor, torch.Tensor]] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Conditional Variational Auto-Encoder model.\n\n        Args:\n            encoder (nn.Module): The encoder used to map input to latent space.\n            decoder (nn.Module): The decoder used to reconstruct the input using a vector in latent space.\n            unpack_input_condition (Callable | None, optional): For unpacking the input and condition tensors.\n        \"\"\"\n        super().__init__(encoder, decoder)\n        self.unpack_input_condition = unpack_input_condition\n\n    def encode(self, input: torch.Tensor, condition: torch.Tensor | None = None) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        User can decide how to use the condition in the encoder, by defining the architecture and forward function\n        accordingly. Ex: Using the condition in the middle layers of encoder.\n\n        Args:\n            input (torch.Tensor): Input tensor.\n            condition (torch.Tensor | None, optional): Conditional information to be used by the encoder. Defaults to\n                None.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): ``mu`` and ``logvar``, similar to the variational auto-encoder.\n        \"\"\"\n        mu, logvar = self.encoder(input, condition)\n        return mu, logvar\n\n    def decode(self, latent_vector: torch.Tensor, condition: torch.Tensor | None = None) -&gt; torch.Tensor:\n        \"\"\"\n        User can decide how to use the condition in the decoder, by defining the architecture and forward function to\n        inject the conditioning. Ex: Using the condition in the middle layers of decoder, or not using it at all.\n\n        Args:\n            latent_vector (torch.Tensor): The latent vector sampled from the distribution specified by the encoder.\n                For CVAEs this is a vector of some fixed dimension, given logvar and \\\\(\\\\mu\\\\) generated by the\n                encoder, perhaps using conditional information.\n\n                \\\\[\\\\mu + \\\\epsilon \\\\cdot \\\\exp \\\\left(0.5 \\\\cdot \\\\text{logvar} \\\\right),\\\\]\n\n                where \\\\(\\\\epsilon \\\\sim \\\\mathcal{N}(\\\\mathbf{0}, I)\\\\).\n            condition (torch.Tensor | None, optional): Conditioning information to be used by the decoder during the\n                mapping of the ``latent_vector`` to an output. Defaults to None.\n\n        Returns:\n            (torch.Tensor): Decoded tensor from the latent vector and (potentially) the conditioning vector.\n        \"\"\"\n        return self.decoder(latent_vector, condition)\n\n    def sampling(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Sampling using the reparameterization trick to make the CVAE differentiable. This sampling produces a vector\n        as if it were sampled from \\\\(\\\\mathcal{N}\\\\left(\\\\mu, \\\\exp(0.5 \\\\cdot \\\\text{logvar}) I \\\\right)\\\\).\n\n        Args:\n            mu (torch.Tensor): Mean of the normal distribution from which to sample.\n            logvar (torch.Tensor): Log of the variance of the normal distribution from which to sample.\n\n        Returns:\n            (torch.Tensor): Latent vector sampled from the appropriate normal distribution.\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass for the CVAE that orchestrates the (conditional) encoding and (conditional) decoding from the\n        provided input. This input may have conditional information packed into it. If the function\n        ``unpack_input_condition`` is defined, it is used to un-pack the conditional information from the input tensor\n        This conditioning information is then provided to both the encoder and decoder modules. They may or may not\n        make use of this information, depending on how their architectures and forwards are defined.\n\n        **NOTE**: Output (reconstruction) is flattened to be concatenated with mu and logvar vectors. The shape of the\n        ``flattened_output`` can be later restored by having the training data shape, or the decoder structure.\n\n        **NOTE**: This assumes output is \"batch first\".\n\n        Args:\n            input (torch.Tensor): Input vector.\n\n        Returns:\n            (torch.Tensor): Reconstructed input vector, which has been flattened and concatenated with the mu and\n                logvar tensors.\n        \"\"\"\n        assert self.unpack_input_condition is not None\n        input, condition = self.unpack_input_condition(input)\n        mu, logvar = self.encode(input, condition)\n        z = self.sampling(mu, logvar)\n        output = self.decode(z, condition)\n        flattened_output = output.view(output.shape[0], -1)\n        return torch.cat((logvar, mu, flattened_output), dim=1)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.ConditionalVae.__init__","title":"<code>__init__(encoder, decoder, unpack_input_condition=None)</code>","text":"<p>Conditional Variational Auto-Encoder model.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Module</code> <p>The encoder used to map input to latent space.</p> required <code>decoder</code> <code>Module</code> <p>The decoder used to reconstruct the input using a vector in latent space.</p> required <code>unpack_input_condition</code> <code>Callable | None</code> <p>For unpacking the input and condition tensors.</p> <code>None</code> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def __init__(\n    self,\n    encoder: nn.Module,\n    decoder: nn.Module,\n    unpack_input_condition: Callable[[torch.Tensor], tuple[torch.Tensor, torch.Tensor]] | None = None,\n) -&gt; None:\n    \"\"\"\n    Conditional Variational Auto-Encoder model.\n\n    Args:\n        encoder (nn.Module): The encoder used to map input to latent space.\n        decoder (nn.Module): The decoder used to reconstruct the input using a vector in latent space.\n        unpack_input_condition (Callable | None, optional): For unpacking the input and condition tensors.\n    \"\"\"\n    super().__init__(encoder, decoder)\n    self.unpack_input_condition = unpack_input_condition\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.ConditionalVae.encode","title":"<code>encode(input, condition=None)</code>","text":"<p>User can decide how to use the condition in the encoder, by defining the architecture and forward function accordingly. Ex: Using the condition in the middle layers of encoder.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor.</p> required <code>condition</code> <code>Tensor | None</code> <p>Conditional information to be used by the encoder. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p><code>mu</code> and <code>logvar</code>, similar to the variational auto-encoder.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def encode(self, input: torch.Tensor, condition: torch.Tensor | None = None) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    User can decide how to use the condition in the encoder, by defining the architecture and forward function\n    accordingly. Ex: Using the condition in the middle layers of encoder.\n\n    Args:\n        input (torch.Tensor): Input tensor.\n        condition (torch.Tensor | None, optional): Conditional information to be used by the encoder. Defaults to\n            None.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): ``mu`` and ``logvar``, similar to the variational auto-encoder.\n    \"\"\"\n    mu, logvar = self.encoder(input, condition)\n    return mu, logvar\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.ConditionalVae.decode","title":"<code>decode(latent_vector, condition=None)</code>","text":"<p>User can decide how to use the condition in the decoder, by defining the architecture and forward function to inject the conditioning. Ex: Using the condition in the middle layers of decoder, or not using it at all.</p> <p>Parameters:</p> Name Type Description Default <code>latent_vector</code> <code>Tensor</code> <p>The latent vector sampled from the distribution specified by the encoder. For CVAEs this is a vector of some fixed dimension, given logvar and \\(\\mu\\) generated by the encoder, perhaps using conditional information.</p> \\[\\mu + \\epsilon \\cdot \\exp \\left(0.5 \\cdot \\text{logvar} \\right),\\] <p>where \\(\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, I)\\).</p> required <code>condition</code> <code>Tensor | None</code> <p>Conditioning information to be used by the decoder during the mapping of the <code>latent_vector</code> to an output. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded tensor from the latent vector and (potentially) the conditioning vector.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def decode(self, latent_vector: torch.Tensor, condition: torch.Tensor | None = None) -&gt; torch.Tensor:\n    \"\"\"\n    User can decide how to use the condition in the decoder, by defining the architecture and forward function to\n    inject the conditioning. Ex: Using the condition in the middle layers of decoder, or not using it at all.\n\n    Args:\n        latent_vector (torch.Tensor): The latent vector sampled from the distribution specified by the encoder.\n            For CVAEs this is a vector of some fixed dimension, given logvar and \\\\(\\\\mu\\\\) generated by the\n            encoder, perhaps using conditional information.\n\n            \\\\[\\\\mu + \\\\epsilon \\\\cdot \\\\exp \\\\left(0.5 \\\\cdot \\\\text{logvar} \\\\right),\\\\]\n\n            where \\\\(\\\\epsilon \\\\sim \\\\mathcal{N}(\\\\mathbf{0}, I)\\\\).\n        condition (torch.Tensor | None, optional): Conditioning information to be used by the decoder during the\n            mapping of the ``latent_vector`` to an output. Defaults to None.\n\n    Returns:\n        (torch.Tensor): Decoded tensor from the latent vector and (potentially) the conditioning vector.\n    \"\"\"\n    return self.decoder(latent_vector, condition)\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.ConditionalVae.sampling","title":"<code>sampling(mu, logvar)</code>","text":"<p>Sampling using the reparameterization trick to make the CVAE differentiable. This sampling produces a vector as if it were sampled from \\(\\mathcal{N}\\left(\\mu, \\exp(0.5 \\cdot \\text{logvar}) I \\right)\\).</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>Mean of the normal distribution from which to sample.</p> required <code>logvar</code> <code>Tensor</code> <p>Log of the variance of the normal distribution from which to sample.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Latent vector sampled from the appropriate normal distribution.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def sampling(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Sampling using the reparameterization trick to make the CVAE differentiable. This sampling produces a vector\n    as if it were sampled from \\\\(\\\\mathcal{N}\\\\left(\\\\mu, \\\\exp(0.5 \\\\cdot \\\\text{logvar}) I \\\\right)\\\\).\n\n    Args:\n        mu (torch.Tensor): Mean of the normal distribution from which to sample.\n        logvar (torch.Tensor): Log of the variance of the normal distribution from which to sample.\n\n    Returns:\n        (torch.Tensor): Latent vector sampled from the appropriate normal distribution.\n    \"\"\"\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n</code></pre>"},{"location":"api/#fl4health.model_bases.autoencoders_base.ConditionalVae.forward","title":"<code>forward(input)</code>","text":"<p>Forward pass for the CVAE that orchestrates the (conditional) encoding and (conditional) decoding from the provided input. This input may have conditional information packed into it. If the function <code>unpack_input_condition</code> is defined, it is used to un-pack the conditional information from the input tensor This conditioning information is then provided to both the encoder and decoder modules. They may or may not make use of this information, depending on how their architectures and forwards are defined.</p> <p>NOTE: Output (reconstruction) is flattened to be concatenated with mu and logvar vectors. The shape of the <code>flattened_output</code> can be later restored by having the training data shape, or the decoder structure.</p> <p>NOTE: This assumes output is \"batch first\".</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input vector.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstructed input vector, which has been flattened and concatenated with the mu and logvar tensors.</p> Source code in <code>fl4health/model_bases/autoencoders_base.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass for the CVAE that orchestrates the (conditional) encoding and (conditional) decoding from the\n    provided input. This input may have conditional information packed into it. If the function\n    ``unpack_input_condition`` is defined, it is used to un-pack the conditional information from the input tensor\n    This conditioning information is then provided to both the encoder and decoder modules. They may or may not\n    make use of this information, depending on how their architectures and forwards are defined.\n\n    **NOTE**: Output (reconstruction) is flattened to be concatenated with mu and logvar vectors. The shape of the\n    ``flattened_output`` can be later restored by having the training data shape, or the decoder structure.\n\n    **NOTE**: This assumes output is \"batch first\".\n\n    Args:\n        input (torch.Tensor): Input vector.\n\n    Returns:\n        (torch.Tensor): Reconstructed input vector, which has been flattened and concatenated with the mu and\n            logvar tensors.\n    \"\"\"\n    assert self.unpack_input_condition is not None\n    input, condition = self.unpack_input_condition(input)\n    mu, logvar = self.encode(input, condition)\n    z = self.sampling(mu, logvar)\n    output = self.decode(z, condition)\n    flattened_output = output.view(output.shape[0], -1)\n    return torch.cat((logvar, mu, flattened_output), dim=1)\n</code></pre>"},{"location":"api/#fl4health.model_bases.ensemble_base","title":"<code>ensemble_base</code>","text":""},{"location":"api/#fl4health.model_bases.ensemble_base.EnsembleModel","title":"<code>EnsembleModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/model_bases/ensemble_base.py</code> <pre><code>class EnsembleModel(nn.Module):\n    def __init__(\n        self,\n        ensemble_models: dict[str, nn.Module],\n        aggregation_mode: EnsembleAggregationMode | None = EnsembleAggregationMode.AVERAGE,\n    ) -&gt; None:\n        \"\"\"\n        Class that acts a wrapper to an ensemble of models to be trained in federated manner with support\n        for both voting and averaging prediction of individual models.\n\n        Args:\n            ensemble_models (dict[str, nn.Module]): A dictionary of models that make up the ensemble.\n            aggregation_mode (EnsembleAggregationMode | None): The mode in which to aggregate the predictions of\n                individual models.\n        \"\"\"\n        super().__init__()\n\n        self.ensemble_models = nn.ModuleDict(ensemble_models)\n        self.aggregation_mode = aggregation_mode\n\n    def forward(self, input: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n        \"\"\"\n        Produce the predictions of the ensemble models given input data.\n\n        Args:\n            input (torch.Tensor): A batch of input data.\n\n        Returns:\n            (dict[str, torch.Tensor]): A dictionary of predictions of the individual ensemble models as well as\n                prediction of the ensemble as a whole.\n        \"\"\"\n        preds = {}\n        for key, model in self.ensemble_models.items():\n            preds[key] = model(input)\n\n        # Don't store gradients when computing ensemble predictions\n        with torch.no_grad():\n            if self.aggregation_mode == EnsembleAggregationMode.AVERAGE:\n                ensemble_pred = self.ensemble_average(list(preds.values()))\n            else:\n                ensemble_pred = self.ensemble_vote(list(preds.values()))\n\n        preds[\"ensemble-pred\"] = ensemble_pred\n\n        return preds\n\n    def ensemble_vote(self, preds_list: list[torch.Tensor]) -&gt; torch.Tensor:\n        \"\"\"\n        Produces the aggregated prediction of the ensemble via voting. Expects predictions to be in a format where\n        the 0 axis represents the sample index and the -1 axis represents the class dimension.\n\n        Args:\n            preds_list (list[torch.Tensor]): A list of predictions of the models in the ensemble.\n\n        Returns:\n            (torch.Tensor): The vote prediction of the ensemble.\n        \"\"\"\n        assert all(preds.shape == preds_list[0].shape for preds in preds_list)\n        preds_dimension = list(preds_list[0].shape)\n\n        # If larger than two dimensions, we map to 2D to perform voting operation (and reshape later)\n        if len(preds_dimension) &gt; EXPECTED_MAX_PRED_N_DIMS:\n            preds_list = [preds.reshape(-1, preds_dimension[-1]) for preds in preds_list]\n\n        # For each model prediction, compute the argmax of the model over the classes and stack column-wise into matrix\n        # Each row of matrix represents the argmax of each model for a given sample\n        argmax_per_model = torch.hstack([torch.argmax(preds, dim=1, keepdim=True) for preds in preds_list])\n        # For each row (sample), compute the unique class predictions and their respective counts\n        index_count_list = map(lambda x: torch.unique(x, return_counts=True), argmax_per_model.unbind())  # noqa: C417\n        # For each element of list (class index, class count) pairing\n        # extract index with the highest count and create tensor\n        indices_with_highest_counts = torch.tensor([index[torch.argmax(count)] for index, count in index_count_list])\n        # One hot encode ensemble prediction for each sample\n        vote_preds = nn.functional.one_hot(indices_with_highest_counts, num_classes=preds_dimension[-1])\n\n        # If larger than two dimensions, map back to original dimensions\n        if len(preds_dimension) &gt; EXPECTED_MAX_PRED_N_DIMS:\n            vote_preds = vote_preds.reshape(*preds_dimension)\n\n        return vote_preds\n\n    def ensemble_average(self, preds_list: list[torch.Tensor]) -&gt; torch.Tensor:\n        \"\"\"\n        Produces the aggregated prediction of the ensemble via averaging.\n\n        Args:\n            preds_list (list[torch.Tensor]): A list of predictions of the models in the ensemble.\n\n        Returns:\n            (torch.Tensor): The average prediction of the ensemble.\n        \"\"\"\n        stacked_model_preds = torch.stack(preds_list)\n        return torch.mean(stacked_model_preds, dim=0)\n</code></pre>"},{"location":"api/#fl4health.model_bases.ensemble_base.EnsembleModel.__init__","title":"<code>__init__(ensemble_models, aggregation_mode=EnsembleAggregationMode.AVERAGE)</code>","text":"<p>Class that acts a wrapper to an ensemble of models to be trained in federated manner with support for both voting and averaging prediction of individual models.</p> <p>Parameters:</p> Name Type Description Default <code>ensemble_models</code> <code>dict[str, Module]</code> <p>A dictionary of models that make up the ensemble.</p> required <code>aggregation_mode</code> <code>EnsembleAggregationMode | None</code> <p>The mode in which to aggregate the predictions of individual models.</p> <code>AVERAGE</code> Source code in <code>fl4health/model_bases/ensemble_base.py</code> <pre><code>def __init__(\n    self,\n    ensemble_models: dict[str, nn.Module],\n    aggregation_mode: EnsembleAggregationMode | None = EnsembleAggregationMode.AVERAGE,\n) -&gt; None:\n    \"\"\"\n    Class that acts a wrapper to an ensemble of models to be trained in federated manner with support\n    for both voting and averaging prediction of individual models.\n\n    Args:\n        ensemble_models (dict[str, nn.Module]): A dictionary of models that make up the ensemble.\n        aggregation_mode (EnsembleAggregationMode | None): The mode in which to aggregate the predictions of\n            individual models.\n    \"\"\"\n    super().__init__()\n\n    self.ensemble_models = nn.ModuleDict(ensemble_models)\n    self.aggregation_mode = aggregation_mode\n</code></pre>"},{"location":"api/#fl4health.model_bases.ensemble_base.EnsembleModel.forward","title":"<code>forward(input)</code>","text":"<p>Produce the predictions of the ensemble models given input data.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>A batch of input data.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>A dictionary of predictions of the individual ensemble models as well as prediction of the ensemble as a whole.</p> Source code in <code>fl4health/model_bases/ensemble_base.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Produce the predictions of the ensemble models given input data.\n\n    Args:\n        input (torch.Tensor): A batch of input data.\n\n    Returns:\n        (dict[str, torch.Tensor]): A dictionary of predictions of the individual ensemble models as well as\n            prediction of the ensemble as a whole.\n    \"\"\"\n    preds = {}\n    for key, model in self.ensemble_models.items():\n        preds[key] = model(input)\n\n    # Don't store gradients when computing ensemble predictions\n    with torch.no_grad():\n        if self.aggregation_mode == EnsembleAggregationMode.AVERAGE:\n            ensemble_pred = self.ensemble_average(list(preds.values()))\n        else:\n            ensemble_pred = self.ensemble_vote(list(preds.values()))\n\n    preds[\"ensemble-pred\"] = ensemble_pred\n\n    return preds\n</code></pre>"},{"location":"api/#fl4health.model_bases.ensemble_base.EnsembleModel.ensemble_vote","title":"<code>ensemble_vote(preds_list)</code>","text":"<p>Produces the aggregated prediction of the ensemble via voting. Expects predictions to be in a format where the 0 axis represents the sample index and the -1 axis represents the class dimension.</p> <p>Parameters:</p> Name Type Description Default <code>preds_list</code> <code>list[Tensor]</code> <p>A list of predictions of the models in the ensemble.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The vote prediction of the ensemble.</p> Source code in <code>fl4health/model_bases/ensemble_base.py</code> <pre><code>def ensemble_vote(self, preds_list: list[torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"\n    Produces the aggregated prediction of the ensemble via voting. Expects predictions to be in a format where\n    the 0 axis represents the sample index and the -1 axis represents the class dimension.\n\n    Args:\n        preds_list (list[torch.Tensor]): A list of predictions of the models in the ensemble.\n\n    Returns:\n        (torch.Tensor): The vote prediction of the ensemble.\n    \"\"\"\n    assert all(preds.shape == preds_list[0].shape for preds in preds_list)\n    preds_dimension = list(preds_list[0].shape)\n\n    # If larger than two dimensions, we map to 2D to perform voting operation (and reshape later)\n    if len(preds_dimension) &gt; EXPECTED_MAX_PRED_N_DIMS:\n        preds_list = [preds.reshape(-1, preds_dimension[-1]) for preds in preds_list]\n\n    # For each model prediction, compute the argmax of the model over the classes and stack column-wise into matrix\n    # Each row of matrix represents the argmax of each model for a given sample\n    argmax_per_model = torch.hstack([torch.argmax(preds, dim=1, keepdim=True) for preds in preds_list])\n    # For each row (sample), compute the unique class predictions and their respective counts\n    index_count_list = map(lambda x: torch.unique(x, return_counts=True), argmax_per_model.unbind())  # noqa: C417\n    # For each element of list (class index, class count) pairing\n    # extract index with the highest count and create tensor\n    indices_with_highest_counts = torch.tensor([index[torch.argmax(count)] for index, count in index_count_list])\n    # One hot encode ensemble prediction for each sample\n    vote_preds = nn.functional.one_hot(indices_with_highest_counts, num_classes=preds_dimension[-1])\n\n    # If larger than two dimensions, map back to original dimensions\n    if len(preds_dimension) &gt; EXPECTED_MAX_PRED_N_DIMS:\n        vote_preds = vote_preds.reshape(*preds_dimension)\n\n    return vote_preds\n</code></pre>"},{"location":"api/#fl4health.model_bases.ensemble_base.EnsembleModel.ensemble_average","title":"<code>ensemble_average(preds_list)</code>","text":"<p>Produces the aggregated prediction of the ensemble via averaging.</p> <p>Parameters:</p> Name Type Description Default <code>preds_list</code> <code>list[Tensor]</code> <p>A list of predictions of the models in the ensemble.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The average prediction of the ensemble.</p> Source code in <code>fl4health/model_bases/ensemble_base.py</code> <pre><code>def ensemble_average(self, preds_list: list[torch.Tensor]) -&gt; torch.Tensor:\n    \"\"\"\n    Produces the aggregated prediction of the ensemble via averaging.\n\n    Args:\n        preds_list (list[torch.Tensor]): A list of predictions of the models in the ensemble.\n\n    Returns:\n        (torch.Tensor): The average prediction of the ensemble.\n    \"\"\"\n    stacked_model_preds = torch.stack(preds_list)\n    return torch.mean(stacked_model_preds, dim=0)\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer","title":"<code>feature_extractor_buffer</code>","text":""},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer","title":"<code>FeatureExtractorBuffer</code>","text":"Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>class FeatureExtractorBuffer:\n    def __init__(self, model: nn.Module, flatten_feature_extraction_layers: dict[str, bool]) -&gt; None:\n        \"\"\"\n        This class is used to extract features from the intermediate layers of a neural network model and store them in\n        a buffer. The features are extracted using additional hooks that are registered to the model. The extracted\n        features are stored in a dictionary where the keys are the layer names and the values are the extracted\n        features as torch Tensors.\n\n        Attributes:\n        - ``model`` (nn.Module): The neural network model.\n        - ``flatten_feature_extraction_layers`` (dict[str, bool]): A dictionary specifying whether to flatten the\n          feature extraction layers.\n        - ``fhooks`` (list[RemovableHandle]): A list to store the handles for removing hooks.\n        - ``accumulate_features`` (bool): A flag indicating whether to accumulate features.\n        - ``extracted_features_buffers`` (dict[str, list[torch.Tensor]]): A dictionary to store the extracted features\n          for each layer.\n\n        Args:\n            model (nn.Module): The neural network model.\n            flatten_feature_extraction_layers (dict[str, bool]): Dictionary of layers to extract features from them\n                and whether to flatten them. Keys are the layer names that are extracted from the ``named_modules`` and\n                values are boolean.\n        \"\"\"\n        self.model = model\n        self.flatten_feature_extraction_layers = flatten_feature_extraction_layers\n        self.fhooks: list[RemovableHandle] = []\n\n        self.accumulate_features: bool = False\n        self.extracted_features_buffers: dict[str, list[torch.Tensor]] = {\n            layer: [] for layer in flatten_feature_extraction_layers\n        }\n\n    def enable_accumulating_features(self) -&gt; None:\n        \"\"\"\n        Enables the accumulation of features in the buffers for multiple forward passes.\n\n        This method sets the ``accumulate_features`` flag to True, allowing the model to accumulate features in the\n        buffers for multiple forward passes. This can be useful in scenarios where you want to extract features from\n        intermediate layers of the model during inference.\n        \"\"\"\n        self.accumulate_features = True\n\n    def disable_accumulating_features(self) -&gt; None:\n        \"\"\"\n        Disables the accumulation of features in the buffers.\n\n        This method sets the ``accumulate_features`` attribute to False, which prevents the buffers from accumulating\n        features and overwrites them for each forward pass.\n        \"\"\"\n        self.accumulate_features = False\n\n    def clear_buffers(self) -&gt; None:\n        \"\"\"Clears the extracted features buffers for all layers.\"\"\"\n        self.extracted_features_buffers = {layer: [] for layer in self.flatten_feature_extraction_layers}\n\n    def get_hierarchical_attr(self, module: nn.Module, layer_hierarchy: list[str]) -&gt; nn.Module:\n        \"\"\"\n        Traverse the hierarchical attributes of the module to get the desired attribute. Hooks should be\n        registered to specific layers of the model, not to ``nn.Sequential`` or ``nn.ModuleList``.\n\n        Args:\n            module (nn.Module): The ``nn.Module`` object to traverse.\n            layer_hierarchy (list[str]): The hierarchical list of name of desired layer.\n\n        Returns:\n            (nn.Module): The desired layer of the model.\n        \"\"\"\n        if len(layer_hierarchy) == 1:\n            return getattr(module, layer_hierarchy[0])\n        return self.get_hierarchical_attr(getattr(module, layer_hierarchy[0]), layer_hierarchy[1:])\n\n    def find_last_common_prefix(self, prefix: str, layers_name: list[str]) -&gt; str:\n        \"\"\"\n        Check the model's list of named modules to filter any layer that starts with the given prefix and\n        return the last one.\n\n        Args:\n            prefix (str): The prefix of the layer name for registering the hook.\n            layers_name (list[str]): The list of named modules of the model. The assumption is that list of\n                named modules is sorted in the order of the model's forward pass with depth-first traversal. This\n                will allow the user to specify the generic name of the layer instead of the full hierarchical name.\n\n        Returns:\n            (str): The complete name of last named layer that matches the prefix.\n        \"\"\"\n        filtered_layers = [layer for layer in layers_name if layer.startswith(prefix)]\n\n        # Return the last element that matches the criteria\n        return filtered_layers[-1]\n\n    def _maybe_register_hooks(self) -&gt; None:\n        \"\"\"\n        Checks if hooks are already registered and registers them if not.\n        Hooks extract the intermediate feature as output of the selected layers in the model.\n        \"\"\"\n        if len(self.fhooks) == 0:\n            log(INFO, \"Starting to register hooks:\")\n            named_layers = list(dict(self.model.named_modules()).keys())\n            for layer in self.flatten_feature_extraction_layers:\n                log(INFO, f\"Registering hook for layer: {layer}\")\n                # Find the last specific layer under a given generic name\n                specific_layer = self.find_last_common_prefix(layer, named_layers)\n                # Split the specific layer name by '.' to get the hierarchical attribute\n                layer_hierarchy_list = specific_layer.split(\".\")\n                self.fhooks.append(\n                    self.get_hierarchical_attr(self.model, layer_hierarchy_list).register_forward_hook(\n                        self.forward_hook(layer)\n                    )\n                )\n        else:\n            log(INFO, \"Hooks already registered.\")\n\n    def remove_hooks(self) -&gt; None:\n        \"\"\"\n        Removes the hooks from the model for checkpointing and clears the hook list. This method is used to remove\n        any hooks that have been added to the feature extractor buffer. It is typically called prior to checkpointing\n        the model.\n        \"\"\"\n        log(INFO, \"Removing hooks.\")\n        for hook in self.fhooks:\n            hook.remove()\n        self.fhooks.clear()\n\n    def forward_hook(self, layer_name: str) -&gt; Callable:\n        \"\"\"\n        Returns a hook function that is called during the forward pass of a module.\n\n        Args:\n            layer_name (str): The name of the layer.\n\n        Returns:\n            (Callable): The hook function that takes in a module, input, and output tensors.\n        \"\"\"\n\n        def hook(module: nn.Module, input: torch.Tensor, output: torch.Tensor) -&gt; None:\n            if not self.accumulate_features:\n                self.extracted_features_buffers[layer_name] = [output]\n            else:\n                self.extracted_features_buffers[layer_name].append(output)\n\n        return hook\n\n    def flatten(self, features: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Flattens the input tensor along the batch dimension. The features are of shape (``batch_size``, \\\\*).\n        We flatten them across the batch dimension to get a 2D tensor of shape (``batch_size``, ``feature_size``).\n\n        Args:\n            features (torch.Tensor): The input tensor of shape (``batch_size``, \\\\*).\n\n        Returns:\n            (torch.Tensor): The flattened tensor of shape (``batch_size``, ``feature_size``).\n        \"\"\"\n        return features.reshape(len(features), -1)\n\n    def get_extracted_features(self) -&gt; dict[str, torch.Tensor]:\n        \"\"\"\n        Returns a dictionary of extracted features.\n\n        Returns:\n            (dict[str, torch.Tensor]): A dictionary where the keys are the layer names and the values are\n                the extracted features as torch Tensors.\n        \"\"\"\n        features = {}\n\n        for layer in self.extracted_features_buffers:\n            features[layer] = (\n                self.flatten(torch.cat(self.extracted_features_buffers[layer], dim=0))\n                if self.flatten_feature_extraction_layers[layer]\n                else torch.cat(self.extracted_features_buffers[layer], dim=0)\n            )\n\n        return features\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer.__init__","title":"<code>__init__(model, flatten_feature_extraction_layers)</code>","text":"<p>This class is used to extract features from the intermediate layers of a neural network model and store them in a buffer. The features are extracted using additional hooks that are registered to the model. The extracted features are stored in a dictionary where the keys are the layer names and the values are the extracted features as torch Tensors.</p> <p>Attributes: - <code>model</code> (nn.Module): The neural network model. - <code>flatten_feature_extraction_layers</code> (dict[str, bool]): A dictionary specifying whether to flatten the   feature extraction layers. - <code>fhooks</code> (list[RemovableHandle]): A list to store the handles for removing hooks. - <code>accumulate_features</code> (bool): A flag indicating whether to accumulate features. - <code>extracted_features_buffers</code> (dict[str, list[torch.Tensor]]): A dictionary to store the extracted features   for each layer.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The neural network model.</p> required <code>flatten_feature_extraction_layers</code> <code>dict[str, bool]</code> <p>Dictionary of layers to extract features from them and whether to flatten them. Keys are the layer names that are extracted from the <code>named_modules</code> and values are boolean.</p> required Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>def __init__(self, model: nn.Module, flatten_feature_extraction_layers: dict[str, bool]) -&gt; None:\n    \"\"\"\n    This class is used to extract features from the intermediate layers of a neural network model and store them in\n    a buffer. The features are extracted using additional hooks that are registered to the model. The extracted\n    features are stored in a dictionary where the keys are the layer names and the values are the extracted\n    features as torch Tensors.\n\n    Attributes:\n    - ``model`` (nn.Module): The neural network model.\n    - ``flatten_feature_extraction_layers`` (dict[str, bool]): A dictionary specifying whether to flatten the\n      feature extraction layers.\n    - ``fhooks`` (list[RemovableHandle]): A list to store the handles for removing hooks.\n    - ``accumulate_features`` (bool): A flag indicating whether to accumulate features.\n    - ``extracted_features_buffers`` (dict[str, list[torch.Tensor]]): A dictionary to store the extracted features\n      for each layer.\n\n    Args:\n        model (nn.Module): The neural network model.\n        flatten_feature_extraction_layers (dict[str, bool]): Dictionary of layers to extract features from them\n            and whether to flatten them. Keys are the layer names that are extracted from the ``named_modules`` and\n            values are boolean.\n    \"\"\"\n    self.model = model\n    self.flatten_feature_extraction_layers = flatten_feature_extraction_layers\n    self.fhooks: list[RemovableHandle] = []\n\n    self.accumulate_features: bool = False\n    self.extracted_features_buffers: dict[str, list[torch.Tensor]] = {\n        layer: [] for layer in flatten_feature_extraction_layers\n    }\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer.enable_accumulating_features","title":"<code>enable_accumulating_features()</code>","text":"<p>Enables the accumulation of features in the buffers for multiple forward passes.</p> <p>This method sets the <code>accumulate_features</code> flag to True, allowing the model to accumulate features in the buffers for multiple forward passes. This can be useful in scenarios where you want to extract features from intermediate layers of the model during inference.</p> Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>def enable_accumulating_features(self) -&gt; None:\n    \"\"\"\n    Enables the accumulation of features in the buffers for multiple forward passes.\n\n    This method sets the ``accumulate_features`` flag to True, allowing the model to accumulate features in the\n    buffers for multiple forward passes. This can be useful in scenarios where you want to extract features from\n    intermediate layers of the model during inference.\n    \"\"\"\n    self.accumulate_features = True\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer.disable_accumulating_features","title":"<code>disable_accumulating_features()</code>","text":"<p>Disables the accumulation of features in the buffers.</p> <p>This method sets the <code>accumulate_features</code> attribute to False, which prevents the buffers from accumulating features and overwrites them for each forward pass.</p> Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>def disable_accumulating_features(self) -&gt; None:\n    \"\"\"\n    Disables the accumulation of features in the buffers.\n\n    This method sets the ``accumulate_features`` attribute to False, which prevents the buffers from accumulating\n    features and overwrites them for each forward pass.\n    \"\"\"\n    self.accumulate_features = False\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer.clear_buffers","title":"<code>clear_buffers()</code>","text":"<p>Clears the extracted features buffers for all layers.</p> Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>def clear_buffers(self) -&gt; None:\n    \"\"\"Clears the extracted features buffers for all layers.\"\"\"\n    self.extracted_features_buffers = {layer: [] for layer in self.flatten_feature_extraction_layers}\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer.get_hierarchical_attr","title":"<code>get_hierarchical_attr(module, layer_hierarchy)</code>","text":"<p>Traverse the hierarchical attributes of the module to get the desired attribute. Hooks should be registered to specific layers of the model, not to <code>nn.Sequential</code> or <code>nn.ModuleList</code>.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>The <code>nn.Module</code> object to traverse.</p> required <code>layer_hierarchy</code> <code>list[str]</code> <p>The hierarchical list of name of desired layer.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>The desired layer of the model.</p> Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>def get_hierarchical_attr(self, module: nn.Module, layer_hierarchy: list[str]) -&gt; nn.Module:\n    \"\"\"\n    Traverse the hierarchical attributes of the module to get the desired attribute. Hooks should be\n    registered to specific layers of the model, not to ``nn.Sequential`` or ``nn.ModuleList``.\n\n    Args:\n        module (nn.Module): The ``nn.Module`` object to traverse.\n        layer_hierarchy (list[str]): The hierarchical list of name of desired layer.\n\n    Returns:\n        (nn.Module): The desired layer of the model.\n    \"\"\"\n    if len(layer_hierarchy) == 1:\n        return getattr(module, layer_hierarchy[0])\n    return self.get_hierarchical_attr(getattr(module, layer_hierarchy[0]), layer_hierarchy[1:])\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer.find_last_common_prefix","title":"<code>find_last_common_prefix(prefix, layers_name)</code>","text":"<p>Check the model's list of named modules to filter any layer that starts with the given prefix and return the last one.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The prefix of the layer name for registering the hook.</p> required <code>layers_name</code> <code>list[str]</code> <p>The list of named modules of the model. The assumption is that list of named modules is sorted in the order of the model's forward pass with depth-first traversal. This will allow the user to specify the generic name of the layer instead of the full hierarchical name.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The complete name of last named layer that matches the prefix.</p> Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>def find_last_common_prefix(self, prefix: str, layers_name: list[str]) -&gt; str:\n    \"\"\"\n    Check the model's list of named modules to filter any layer that starts with the given prefix and\n    return the last one.\n\n    Args:\n        prefix (str): The prefix of the layer name for registering the hook.\n        layers_name (list[str]): The list of named modules of the model. The assumption is that list of\n            named modules is sorted in the order of the model's forward pass with depth-first traversal. This\n            will allow the user to specify the generic name of the layer instead of the full hierarchical name.\n\n    Returns:\n        (str): The complete name of last named layer that matches the prefix.\n    \"\"\"\n    filtered_layers = [layer for layer in layers_name if layer.startswith(prefix)]\n\n    # Return the last element that matches the criteria\n    return filtered_layers[-1]\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer.remove_hooks","title":"<code>remove_hooks()</code>","text":"<p>Removes the hooks from the model for checkpointing and clears the hook list. This method is used to remove any hooks that have been added to the feature extractor buffer. It is typically called prior to checkpointing the model.</p> Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>def remove_hooks(self) -&gt; None:\n    \"\"\"\n    Removes the hooks from the model for checkpointing and clears the hook list. This method is used to remove\n    any hooks that have been added to the feature extractor buffer. It is typically called prior to checkpointing\n    the model.\n    \"\"\"\n    log(INFO, \"Removing hooks.\")\n    for hook in self.fhooks:\n        hook.remove()\n    self.fhooks.clear()\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer.forward_hook","title":"<code>forward_hook(layer_name)</code>","text":"<p>Returns a hook function that is called during the forward pass of a module.</p> <p>Parameters:</p> Name Type Description Default <code>layer_name</code> <code>str</code> <p>The name of the layer.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The hook function that takes in a module, input, and output tensors.</p> Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>def forward_hook(self, layer_name: str) -&gt; Callable:\n    \"\"\"\n    Returns a hook function that is called during the forward pass of a module.\n\n    Args:\n        layer_name (str): The name of the layer.\n\n    Returns:\n        (Callable): The hook function that takes in a module, input, and output tensors.\n    \"\"\"\n\n    def hook(module: nn.Module, input: torch.Tensor, output: torch.Tensor) -&gt; None:\n        if not self.accumulate_features:\n            self.extracted_features_buffers[layer_name] = [output]\n        else:\n            self.extracted_features_buffers[layer_name].append(output)\n\n    return hook\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer.flatten","title":"<code>flatten(features)</code>","text":"<p>Flattens the input tensor along the batch dimension. The features are of shape (<code>batch_size</code>, *). We flatten them across the batch dimension to get a 2D tensor of shape (<code>batch_size</code>, <code>feature_size</code>).</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Tensor</code> <p>The input tensor of shape (<code>batch_size</code>, *).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The flattened tensor of shape (<code>batch_size</code>, <code>feature_size</code>).</p> Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>def flatten(self, features: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Flattens the input tensor along the batch dimension. The features are of shape (``batch_size``, \\\\*).\n    We flatten them across the batch dimension to get a 2D tensor of shape (``batch_size``, ``feature_size``).\n\n    Args:\n        features (torch.Tensor): The input tensor of shape (``batch_size``, \\\\*).\n\n    Returns:\n        (torch.Tensor): The flattened tensor of shape (``batch_size``, ``feature_size``).\n    \"\"\"\n    return features.reshape(len(features), -1)\n</code></pre>"},{"location":"api/#fl4health.model_bases.feature_extractor_buffer.FeatureExtractorBuffer.get_extracted_features","title":"<code>get_extracted_features()</code>","text":"<p>Returns a dictionary of extracted features.</p> <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>A dictionary where the keys are the layer names and the values are the extracted features as torch Tensors.</p> Source code in <code>fl4health/model_bases/feature_extractor_buffer.py</code> <pre><code>def get_extracted_features(self) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Returns a dictionary of extracted features.\n\n    Returns:\n        (dict[str, torch.Tensor]): A dictionary where the keys are the layer names and the values are\n            the extracted features as torch Tensors.\n    \"\"\"\n    features = {}\n\n    for layer in self.extracted_features_buffers:\n        features[layer] = (\n            self.flatten(torch.cat(self.extracted_features_buffers[layer], dim=0))\n            if self.flatten_feature_extraction_layers[layer]\n            else torch.cat(self.extracted_features_buffers[layer], dim=0)\n        )\n\n    return features\n</code></pre>"},{"location":"api/#fl4health.model_bases.fedrep_base","title":"<code>fedrep_base</code>","text":""},{"location":"api/#fl4health.model_bases.fedrep_base.FedRepModel","title":"<code>FedRepModel</code>","text":"<p>               Bases: <code>SequentiallySplitExchangeBaseModel</code></p> <p>Implementation of the FedRep model structure: https://arxiv.org/pdf/2102.07078.pdf.</p> <p>The architecture is fairly straightforward. The global module represents the first set of layers. These are learned with FedAvg. The <code>local_prediction_head</code> are the last layers, these are not exchanged with the server. The approach resembles FENDA, but vertical rather than parallel models. It also resembles MOON, but with partial weight exchange for weight aggregation.</p> Source code in <code>fl4health/model_bases/fedrep_base.py</code> <pre><code>class FedRepModel(SequentiallySplitExchangeBaseModel):\n    \"\"\"\n    Implementation of the FedRep model structure: https://arxiv.org/pdf/2102.07078.pdf.\n\n    The architecture is fairly straightforward. The global module represents the first set of layers. These are\n    learned with FedAvg. The ``local_prediction_head`` are the last layers, these are not exchanged with the server.\n    The approach resembles FENDA, but vertical rather than parallel models. It also resembles MOON, but with\n    partial weight exchange for weight aggregation.\n    \"\"\"\n\n    def freeze_base_module(self) -&gt; None:\n        \"\"\"Any parameters in the ``base_module`` are fixed by setting ``requires_grad`` to False.\"\"\"\n        for parameters in self.base_module.parameters():\n            parameters.requires_grad = False\n\n    def unfreeze_base_module(self) -&gt; None:\n        \"\"\"Any parameters in the ``base_module`` are unfrozen by setting ``requires_grad`` to True.\"\"\"\n        for parameters in self.base_module.parameters():\n            parameters.requires_grad = True\n\n    def freeze_head_module(self) -&gt; None:\n        \"\"\"Any parameters in the ``head_module`` are fixed by setting ``requires_grad`` to False.\"\"\"\n        for parameters in self.head_module.parameters():\n            parameters.requires_grad = False\n\n    def unfreeze_head_module(self) -&gt; None:\n        \"\"\"Any parameters in the ``head_module`` are unfrozen by setting ``requires_grad`` to True.\"\"\"\n        for parameters in self.head_module.parameters():\n            parameters.requires_grad = True\n</code></pre>"},{"location":"api/#fl4health.model_bases.fedrep_base.FedRepModel.freeze_base_module","title":"<code>freeze_base_module()</code>","text":"<p>Any parameters in the <code>base_module</code> are fixed by setting <code>requires_grad</code> to False.</p> Source code in <code>fl4health/model_bases/fedrep_base.py</code> <pre><code>def freeze_base_module(self) -&gt; None:\n    \"\"\"Any parameters in the ``base_module`` are fixed by setting ``requires_grad`` to False.\"\"\"\n    for parameters in self.base_module.parameters():\n        parameters.requires_grad = False\n</code></pre>"},{"location":"api/#fl4health.model_bases.fedrep_base.FedRepModel.unfreeze_base_module","title":"<code>unfreeze_base_module()</code>","text":"<p>Any parameters in the <code>base_module</code> are unfrozen by setting <code>requires_grad</code> to True.</p> Source code in <code>fl4health/model_bases/fedrep_base.py</code> <pre><code>def unfreeze_base_module(self) -&gt; None:\n    \"\"\"Any parameters in the ``base_module`` are unfrozen by setting ``requires_grad`` to True.\"\"\"\n    for parameters in self.base_module.parameters():\n        parameters.requires_grad = True\n</code></pre>"},{"location":"api/#fl4health.model_bases.fedrep_base.FedRepModel.freeze_head_module","title":"<code>freeze_head_module()</code>","text":"<p>Any parameters in the <code>head_module</code> are fixed by setting <code>requires_grad</code> to False.</p> Source code in <code>fl4health/model_bases/fedrep_base.py</code> <pre><code>def freeze_head_module(self) -&gt; None:\n    \"\"\"Any parameters in the ``head_module`` are fixed by setting ``requires_grad`` to False.\"\"\"\n    for parameters in self.head_module.parameters():\n        parameters.requires_grad = False\n</code></pre>"},{"location":"api/#fl4health.model_bases.fedrep_base.FedRepModel.unfreeze_head_module","title":"<code>unfreeze_head_module()</code>","text":"<p>Any parameters in the <code>head_module</code> are unfrozen by setting <code>requires_grad</code> to True.</p> Source code in <code>fl4health/model_bases/fedrep_base.py</code> <pre><code>def unfreeze_head_module(self) -&gt; None:\n    \"\"\"Any parameters in the ``head_module`` are unfrozen by setting ``requires_grad`` to True.\"\"\"\n    for parameters in self.head_module.parameters():\n        parameters.requires_grad = True\n</code></pre>"},{"location":"api/#fl4health.model_bases.fedsimclr_base","title":"<code>fedsimclr_base</code>","text":""},{"location":"api/#fl4health.model_bases.fedsimclr_base.FedSimClrModel","title":"<code>FedSimClrModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/model_bases/fedsimclr_base.py</code> <pre><code>class FedSimClrModel(nn.Module):\n    def __init__(\n        self,\n        encoder: nn.Module,\n        projection_head: nn.Module = DEFAULT_PROJECTION_HEAD,\n        prediction_head: nn.Module | None = None,\n        pretrain: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Model base to train SimCLR (https://arxiv.org/pdf/2002.05709)\n        in a federated manner presented in (https://arxiv.org/pdf/2207.09158).\n        Can be used in pretraining and optionally finetuning.\n\n        Args:\n            encoder (nn.Module): Encoder that extracts a feature vector. given an input sample.\n            projection_head (nn.Module): Projection Head that maps output of encoder to final representation used in\n                contrastive loss for pretraining stage. Defaults to identity transformation.\n            prediction_head (nn.Module | None): Prediction head that maps output of encoder to prediction in the\n                finetuning stage. Defaults to None.\n            pretrain (bool): Determines whether or not to use the ``projection_head`` (True) or the\n                ``prediction_head`` (False). Defaults to True.\n        \"\"\"\n        super().__init__()\n\n        assert not (prediction_head is None and not pretrain), (\n            \"Model with pretrain==False must have prediction head (ie not None)\"\n        )\n\n        self.encoder = encoder\n        self.projection_head = projection_head\n        self.prediction_head = prediction_head\n        self.pretrain = pretrain\n\n    def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Passes the input tensor through the encoder module. If we're in the pretraining phase, the output of the\n        encoder is flattened/projected for similarity computations. If we're fine-tuning the model, these latent\n        features are passes through the provided prediction head.\n\n        Args:\n            input (torch.Tensor): Input to be mapped to either latent features or a final prediction depending on the\n                training phase.\n\n        Returns:\n            (torch.Tensor): The output from either the ``projection_head`` module if pre-training or the\n                ``prediction_head`` if fine-tuning.\n        \"\"\"\n        features = self.encoder(input)\n        if self.pretrain:\n            return self.projection_head(features)\n        assert self.prediction_head is not None, \"Model with pretrain==False must have prediction_head (ie not None)\"\n        return self.prediction_head(features)\n\n    @staticmethod\n    def load_pretrained_model(model_path: Path) -&gt; FedSimClrModel:\n        \"\"\"\n        Given a path, this function loads a model from the path, assuming was of type ``FedSimClrModel``. The proper\n        components are then routed to form a new model with the pre-existing weights.\n\n        **NOTE**: Loaded models automatically set ``pretrain`` to False\n\n        Args:\n            model_path (Path): Path to a ``FedSimClrModel`` object saved using ``torch.save``\n\n        Returns:\n            (FedSimClrModel): A model with pre-existing weights loaded and ``pretrain`` set to False\n        \"\"\"\n        prev_model = torch.load(model_path, weights_only=False)\n        return FedSimClrModel(\n            encoder=prev_model.encoder,\n            projection_head=prev_model.projection_head,\n            prediction_head=prev_model.prediction_head,\n            pretrain=False,\n        )\n</code></pre>"},{"location":"api/#fl4health.model_bases.fedsimclr_base.FedSimClrModel.__init__","title":"<code>__init__(encoder, projection_head=DEFAULT_PROJECTION_HEAD, prediction_head=None, pretrain=True)</code>","text":"<p>Model base to train SimCLR (https://arxiv.org/pdf/2002.05709) in a federated manner presented in (https://arxiv.org/pdf/2207.09158). Can be used in pretraining and optionally finetuning.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Module</code> <p>Encoder that extracts a feature vector. given an input sample.</p> required <code>projection_head</code> <code>Module</code> <p>Projection Head that maps output of encoder to final representation used in contrastive loss for pretraining stage. Defaults to identity transformation.</p> <code>DEFAULT_PROJECTION_HEAD</code> <code>prediction_head</code> <code>Module | None</code> <p>Prediction head that maps output of encoder to prediction in the finetuning stage. Defaults to None.</p> <code>None</code> <code>pretrain</code> <code>bool</code> <p>Determines whether or not to use the <code>projection_head</code> (True) or the <code>prediction_head</code> (False). Defaults to True.</p> <code>True</code> Source code in <code>fl4health/model_bases/fedsimclr_base.py</code> <pre><code>def __init__(\n    self,\n    encoder: nn.Module,\n    projection_head: nn.Module = DEFAULT_PROJECTION_HEAD,\n    prediction_head: nn.Module | None = None,\n    pretrain: bool = True,\n) -&gt; None:\n    \"\"\"\n    Model base to train SimCLR (https://arxiv.org/pdf/2002.05709)\n    in a federated manner presented in (https://arxiv.org/pdf/2207.09158).\n    Can be used in pretraining and optionally finetuning.\n\n    Args:\n        encoder (nn.Module): Encoder that extracts a feature vector. given an input sample.\n        projection_head (nn.Module): Projection Head that maps output of encoder to final representation used in\n            contrastive loss for pretraining stage. Defaults to identity transformation.\n        prediction_head (nn.Module | None): Prediction head that maps output of encoder to prediction in the\n            finetuning stage. Defaults to None.\n        pretrain (bool): Determines whether or not to use the ``projection_head`` (True) or the\n            ``prediction_head`` (False). Defaults to True.\n    \"\"\"\n    super().__init__()\n\n    assert not (prediction_head is None and not pretrain), (\n        \"Model with pretrain==False must have prediction head (ie not None)\"\n    )\n\n    self.encoder = encoder\n    self.projection_head = projection_head\n    self.prediction_head = prediction_head\n    self.pretrain = pretrain\n</code></pre>"},{"location":"api/#fl4health.model_bases.fedsimclr_base.FedSimClrModel.forward","title":"<code>forward(input)</code>","text":"<p>Passes the input tensor through the encoder module. If we're in the pretraining phase, the output of the encoder is flattened/projected for similarity computations. If we're fine-tuning the model, these latent features are passes through the provided prediction head.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input to be mapped to either latent features or a final prediction depending on the training phase.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The output from either the <code>projection_head</code> module if pre-training or the <code>prediction_head</code> if fine-tuning.</p> Source code in <code>fl4health/model_bases/fedsimclr_base.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Passes the input tensor through the encoder module. If we're in the pretraining phase, the output of the\n    encoder is flattened/projected for similarity computations. If we're fine-tuning the model, these latent\n    features are passes through the provided prediction head.\n\n    Args:\n        input (torch.Tensor): Input to be mapped to either latent features or a final prediction depending on the\n            training phase.\n\n    Returns:\n        (torch.Tensor): The output from either the ``projection_head`` module if pre-training or the\n            ``prediction_head`` if fine-tuning.\n    \"\"\"\n    features = self.encoder(input)\n    if self.pretrain:\n        return self.projection_head(features)\n    assert self.prediction_head is not None, \"Model with pretrain==False must have prediction_head (ie not None)\"\n    return self.prediction_head(features)\n</code></pre>"},{"location":"api/#fl4health.model_bases.fedsimclr_base.FedSimClrModel.load_pretrained_model","title":"<code>load_pretrained_model(model_path)</code>  <code>staticmethod</code>","text":"<p>Given a path, this function loads a model from the path, assuming was of type <code>FedSimClrModel</code>. The proper components are then routed to form a new model with the pre-existing weights.</p> <p>NOTE: Loaded models automatically set <code>pretrain</code> to False</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Path</code> <p>Path to a <code>FedSimClrModel</code> object saved using <code>torch.save</code></p> required <p>Returns:</p> Type Description <code>FedSimClrModel</code> <p>A model with pre-existing weights loaded and <code>pretrain</code> set to False</p> Source code in <code>fl4health/model_bases/fedsimclr_base.py</code> <pre><code>@staticmethod\ndef load_pretrained_model(model_path: Path) -&gt; FedSimClrModel:\n    \"\"\"\n    Given a path, this function loads a model from the path, assuming was of type ``FedSimClrModel``. The proper\n    components are then routed to form a new model with the pre-existing weights.\n\n    **NOTE**: Loaded models automatically set ``pretrain`` to False\n\n    Args:\n        model_path (Path): Path to a ``FedSimClrModel`` object saved using ``torch.save``\n\n    Returns:\n        (FedSimClrModel): A model with pre-existing weights loaded and ``pretrain`` set to False\n    \"\"\"\n    prev_model = torch.load(model_path, weights_only=False)\n    return FedSimClrModel(\n        encoder=prev_model.encoder,\n        projection_head=prev_model.projection_head,\n        prediction_head=prev_model.prediction_head,\n        pretrain=False,\n    )\n</code></pre>"},{"location":"api/#fl4health.model_bases.fenda_base","title":"<code>fenda_base</code>","text":""},{"location":"api/#fl4health.model_bases.fenda_base.FendaModel","title":"<code>FendaModel</code>","text":"<p>               Bases: <code>PartialLayerExchangeModel</code>, <code>ParallelSplitModel</code></p> Source code in <code>fl4health/model_bases/fenda_base.py</code> <pre><code>class FendaModel(PartialLayerExchangeModel, ParallelSplitModel):\n    def __init__(self, local_module: nn.Module, global_module: nn.Module, model_head: ParallelSplitHeadModule) -&gt; None:\n        \"\"\"\n        This is the base model to be used when implementing FENDA-FL models and training. A FENDA model is essentially\n        a parallel split model (i.e. it has two parallel feature extractors), where only one feature extractor is\n        exchanged with the server (the ``global_module``) while the other remains local to the client itself.\n\n        Args:\n            local_module (nn.Module): Feature extraction module that is NOT exchanged with the server.\n            global_module (nn.Module): Feature extraction module that is exchanged with the server and aggregated with\n                other client modules.\n            model_head (ParallelSplitHeadModule): The model head that takes the output features from both the local\n                and global modules to produce a prediction.\n        \"\"\"\n        ParallelSplitModel.__init__(\n            self, first_feature_extractor=local_module, second_feature_extractor=global_module, model_head=model_head\n        )\n\n    def layers_to_exchange(self) -&gt; list[str]:\n        return [layer_name for layer_name in self.state_dict() if layer_name.startswith(\"second_feature_extractor.\")]\n</code></pre>"},{"location":"api/#fl4health.model_bases.fenda_base.FendaModel.__init__","title":"<code>__init__(local_module, global_module, model_head)</code>","text":"<p>This is the base model to be used when implementing FENDA-FL models and training. A FENDA model is essentially a parallel split model (i.e. it has two parallel feature extractors), where only one feature extractor is exchanged with the server (the <code>global_module</code>) while the other remains local to the client itself.</p> <p>Parameters:</p> Name Type Description Default <code>local_module</code> <code>Module</code> <p>Feature extraction module that is NOT exchanged with the server.</p> required <code>global_module</code> <code>Module</code> <p>Feature extraction module that is exchanged with the server and aggregated with other client modules.</p> required <code>model_head</code> <code>ParallelSplitHeadModule</code> <p>The model head that takes the output features from both the local and global modules to produce a prediction.</p> required Source code in <code>fl4health/model_bases/fenda_base.py</code> <pre><code>def __init__(self, local_module: nn.Module, global_module: nn.Module, model_head: ParallelSplitHeadModule) -&gt; None:\n    \"\"\"\n    This is the base model to be used when implementing FENDA-FL models and training. A FENDA model is essentially\n    a parallel split model (i.e. it has two parallel feature extractors), where only one feature extractor is\n    exchanged with the server (the ``global_module``) while the other remains local to the client itself.\n\n    Args:\n        local_module (nn.Module): Feature extraction module that is NOT exchanged with the server.\n        global_module (nn.Module): Feature extraction module that is exchanged with the server and aggregated with\n            other client modules.\n        model_head (ParallelSplitHeadModule): The model head that takes the output features from both the local\n            and global modules to produce a prediction.\n    \"\"\"\n    ParallelSplitModel.__init__(\n        self, first_feature_extractor=local_module, second_feature_extractor=global_module, model_head=model_head\n    )\n</code></pre>"},{"location":"api/#fl4health.model_bases.fenda_base.FendaModelWithFeatureState","title":"<code>FendaModelWithFeatureState</code>","text":"<p>               Bases: <code>FendaModel</code></p> Source code in <code>fl4health/model_bases/fenda_base.py</code> <pre><code>class FendaModelWithFeatureState(FendaModel):\n    def __init__(\n        self,\n        local_module: nn.Module,\n        global_module: nn.Module,\n        model_head: ParallelSplitHeadModule,\n        flatten_features: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        This is the base model to be used when implementing FENDA-FL models and training when extraction and\n        storage of the latent features produced by each of the parallel feature extractors is required/desired. This\n        is a FENDA model, but the feature space outputs are guaranteed to be stored with the keys \"local_features\"\n        and \"global_features\" along with the predictions. The user also has the option to \"flatten\" these features\n        to be of shape ``(batch_size, number of features)``.\n\n        Args:\n            local_module (nn.Module): Feature extraction module that is **NOT** exchanged with the server.\n            global_module (nn.Module): Feature extraction module that is exchanged with the server and aggregated with\n                other client modules.\n            model_head (ParallelSplitHeadModule): The model head that takes the output features from both the local\n                and global modules to produce a prediction.\n            flatten_features (bool, optional): Whether the output features should be flattened to have shape\n                ``(batch_size, number of features)``. Defaults to False.\n        \"\"\"\n        super().__init__(local_module=local_module, global_module=global_module, model_head=model_head)\n        self.flatten_features = flatten_features\n\n    def forward(self, input: torch.Tensor) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n        \"\"\"\n        Mapping input through the FENDA model local and global feature extractors and the classification head.\n\n        Args:\n            input (torch.Tensor): input is expected to be of shape (``batch_size``, \\\\*)\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): Tuple of predictions and feature maps. FENDA\n                predictions are simply stored under the key \"prediction.\" The features for the local and global feature\n                extraction modules are stored under keys \"local_features\" and \"global_features,\" respectively.\n        \"\"\"\n        local_output = self.first_feature_extractor.forward(input)\n        global_output = self.second_feature_extractor.forward(input)\n        preds = {\"prediction\": self.model_head.forward(local_output, global_output)}\n\n        if self.flatten_features:\n            features = {\"local_features\": local_output, \"global_features\": global_output}\n        else:\n            features = {\n                \"local_features\": local_output.reshape(len(local_output), -1),\n                \"global_features\": global_output.reshape(len(global_output), -1),\n            }\n        return preds, features\n</code></pre>"},{"location":"api/#fl4health.model_bases.fenda_base.FendaModelWithFeatureState.__init__","title":"<code>__init__(local_module, global_module, model_head, flatten_features=False)</code>","text":"<p>This is the base model to be used when implementing FENDA-FL models and training when extraction and storage of the latent features produced by each of the parallel feature extractors is required/desired. This is a FENDA model, but the feature space outputs are guaranteed to be stored with the keys \"local_features\" and \"global_features\" along with the predictions. The user also has the option to \"flatten\" these features to be of shape <code>(batch_size, number of features)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>local_module</code> <code>Module</code> <p>Feature extraction module that is NOT exchanged with the server.</p> required <code>global_module</code> <code>Module</code> <p>Feature extraction module that is exchanged with the server and aggregated with other client modules.</p> required <code>model_head</code> <code>ParallelSplitHeadModule</code> <p>The model head that takes the output features from both the local and global modules to produce a prediction.</p> required <code>flatten_features</code> <code>bool</code> <p>Whether the output features should be flattened to have shape <code>(batch_size, number of features)</code>. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/model_bases/fenda_base.py</code> <pre><code>def __init__(\n    self,\n    local_module: nn.Module,\n    global_module: nn.Module,\n    model_head: ParallelSplitHeadModule,\n    flatten_features: bool = False,\n) -&gt; None:\n    \"\"\"\n    This is the base model to be used when implementing FENDA-FL models and training when extraction and\n    storage of the latent features produced by each of the parallel feature extractors is required/desired. This\n    is a FENDA model, but the feature space outputs are guaranteed to be stored with the keys \"local_features\"\n    and \"global_features\" along with the predictions. The user also has the option to \"flatten\" these features\n    to be of shape ``(batch_size, number of features)``.\n\n    Args:\n        local_module (nn.Module): Feature extraction module that is **NOT** exchanged with the server.\n        global_module (nn.Module): Feature extraction module that is exchanged with the server and aggregated with\n            other client modules.\n        model_head (ParallelSplitHeadModule): The model head that takes the output features from both the local\n            and global modules to produce a prediction.\n        flatten_features (bool, optional): Whether the output features should be flattened to have shape\n            ``(batch_size, number of features)``. Defaults to False.\n    \"\"\"\n    super().__init__(local_module=local_module, global_module=global_module, model_head=model_head)\n    self.flatten_features = flatten_features\n</code></pre>"},{"location":"api/#fl4health.model_bases.fenda_base.FendaModelWithFeatureState.forward","title":"<code>forward(input)</code>","text":"<p>Mapping input through the FENDA model local and global feature extractors and the classification head.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>input is expected to be of shape (<code>batch_size</code>, *)</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>Tuple of predictions and feature maps. FENDA predictions are simply stored under the key \"prediction.\" The features for the local and global feature extraction modules are stored under keys \"local_features\" and \"global_features,\" respectively.</p> Source code in <code>fl4health/model_bases/fenda_base.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n    \"\"\"\n    Mapping input through the FENDA model local and global feature extractors and the classification head.\n\n    Args:\n        input (torch.Tensor): input is expected to be of shape (``batch_size``, \\\\*)\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): Tuple of predictions and feature maps. FENDA\n            predictions are simply stored under the key \"prediction.\" The features for the local and global feature\n            extraction modules are stored under keys \"local_features\" and \"global_features,\" respectively.\n    \"\"\"\n    local_output = self.first_feature_extractor.forward(input)\n    global_output = self.second_feature_extractor.forward(input)\n    preds = {\"prediction\": self.model_head.forward(local_output, global_output)}\n\n    if self.flatten_features:\n        features = {\"local_features\": local_output, \"global_features\": global_output}\n    else:\n        features = {\n            \"local_features\": local_output.reshape(len(local_output), -1),\n            \"global_features\": global_output.reshape(len(global_output), -1),\n        }\n    return preds, features\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base","title":"<code>gpfl_base</code>","text":""},{"location":"api/#fl4health.model_bases.gpfl_base.Gce","title":"<code>Gce</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>class Gce(nn.Module):\n    def __init__(self, feature_dim: int, num_classes: int) -&gt; None:\n        \"\"\"\n        Taken from the official implementation at : https://github.com/TsingZ0/GPFL/blob/main/system/flcore/servers/servergp.py\n        GCE module as described in the GPFL paper. This module is used as a lookup table of global class embeddings.\n        The size of the embedding matrix (the lookup table) is (num_classes, feature_dim). The goal is to learn\n        and store representative class embeddings.\n\n        Args:\n            feature_dim (int): The dimension of the feature tensor.\n            num_classes (int): The number of classes represented in the embedding table.\n        \"\"\"\n        super(Gce, self).__init__()\n        self.feature_dim = feature_dim\n        self.num_classes = num_classes\n        self.embedding = nn.Embedding(num_classes, feature_dim)\n\n    def forward(self, feature_tensor: torch.Tensor, label: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs a forward pass through the GCE module. It computes the cosine similarity between the feature tensors\n        and the class embeddings, and then computes the log softmax loss based on the provided labels.\n\n        Args:\n            feature_tensor (torch.Tensor): The global features computed by the CoV module.\n            label (torch.Tensor): The true label for the input data, which is used to compute the loss.\n\n        Returns:\n            (torch.Tensor): Log softmax loss.\n        \"\"\"\n        # Invoke the forward of the embedding layer to make sure the computation graph is connected\n        # and embedding parameters are updated during the backward pass.\n        embeddings = self.embedding(torch.tensor(range(self.num_classes)))\n        # We are computing the dot product using F.Linear.\n        cosine = F.linear(F.normalize(feature_tensor), F.normalize(embeddings))\n        if label.dim() == 1:\n            one_hot = torch.zeros(cosine.size())\n            one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        else:\n            assert label.shape[1] == self.num_classes, (\n                \"Shape of the one-hot encoded labels should be (batch_size, num_classes).\"\n            )\n            # Label is already one-hot encoded with the shape of (batch_size, num_classes)\n            one_hot = label\n\n        softmax_value = F.log_softmax(cosine, dim=1)\n        softmax_loss = one_hot * softmax_value\n        return -torch.mean(torch.sum(softmax_loss, dim=1))\n\n    def lookup(self, target: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Extracts the class embeddings for the given target vectors.\n\n        Args:\n            target (torch.Tensor): A tensor containing the indices or one-hot embeddings\n                of the classes to look up.\n\n        Returns:\n            (torch.Tensor): The class embeddings corresponding to the provided targets.\n        \"\"\"\n        if self.training:\n            log(\n                WARNING,\n                \"Lookup method should not be used for training. \"\n                \"This method is intended for the purpose of embedding lookup, and \"\n                \"does not invoke the forward pass.\",\n            )\n        one_hot_n_dim = 2  # To avoid having magic numbers\n        if target.dim() == one_hot_n_dim:\n            assert target.shape[1] == self.num_classes, (\n                \"Shape of the one-hot encoded labels should be (batch_size, num_classes).\"\n            )\n            # If the target is one-hot encoded, convert it to indices.\n            target = torch.argmax(target, dim=1)\n\n        assert target.shape == (target.shape[0],), \"lookup requires 1D tensor of class indices.\"\n        return self.embedding.weight.data[target.int()]\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.Gce.__init__","title":"<code>__init__(feature_dim, num_classes)</code>","text":"<p>Taken from the official implementation at : https://github.com/TsingZ0/GPFL/blob/main/system/flcore/servers/servergp.py GCE module as described in the GPFL paper. This module is used as a lookup table of global class embeddings. The size of the embedding matrix (the lookup table) is (num_classes, feature_dim). The goal is to learn and store representative class embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int</code> <p>The dimension of the feature tensor.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes represented in the embedding table.</p> required Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>def __init__(self, feature_dim: int, num_classes: int) -&gt; None:\n    \"\"\"\n    Taken from the official implementation at : https://github.com/TsingZ0/GPFL/blob/main/system/flcore/servers/servergp.py\n    GCE module as described in the GPFL paper. This module is used as a lookup table of global class embeddings.\n    The size of the embedding matrix (the lookup table) is (num_classes, feature_dim). The goal is to learn\n    and store representative class embeddings.\n\n    Args:\n        feature_dim (int): The dimension of the feature tensor.\n        num_classes (int): The number of classes represented in the embedding table.\n    \"\"\"\n    super(Gce, self).__init__()\n    self.feature_dim = feature_dim\n    self.num_classes = num_classes\n    self.embedding = nn.Embedding(num_classes, feature_dim)\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.Gce.forward","title":"<code>forward(feature_tensor, label)</code>","text":"<p>Performs a forward pass through the GCE module. It computes the cosine similarity between the feature tensors and the class embeddings, and then computes the log softmax loss based on the provided labels.</p> <p>Parameters:</p> Name Type Description Default <code>feature_tensor</code> <code>Tensor</code> <p>The global features computed by the CoV module.</p> required <code>label</code> <code>Tensor</code> <p>The true label for the input data, which is used to compute the loss.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Log softmax loss.</p> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>def forward(self, feature_tensor: torch.Tensor, label: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs a forward pass through the GCE module. It computes the cosine similarity between the feature tensors\n    and the class embeddings, and then computes the log softmax loss based on the provided labels.\n\n    Args:\n        feature_tensor (torch.Tensor): The global features computed by the CoV module.\n        label (torch.Tensor): The true label for the input data, which is used to compute the loss.\n\n    Returns:\n        (torch.Tensor): Log softmax loss.\n    \"\"\"\n    # Invoke the forward of the embedding layer to make sure the computation graph is connected\n    # and embedding parameters are updated during the backward pass.\n    embeddings = self.embedding(torch.tensor(range(self.num_classes)))\n    # We are computing the dot product using F.Linear.\n    cosine = F.linear(F.normalize(feature_tensor), F.normalize(embeddings))\n    if label.dim() == 1:\n        one_hot = torch.zeros(cosine.size())\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n    else:\n        assert label.shape[1] == self.num_classes, (\n            \"Shape of the one-hot encoded labels should be (batch_size, num_classes).\"\n        )\n        # Label is already one-hot encoded with the shape of (batch_size, num_classes)\n        one_hot = label\n\n    softmax_value = F.log_softmax(cosine, dim=1)\n    softmax_loss = one_hot * softmax_value\n    return -torch.mean(torch.sum(softmax_loss, dim=1))\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.Gce.lookup","title":"<code>lookup(target)</code>","text":"<p>Extracts the class embeddings for the given target vectors.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>A tensor containing the indices or one-hot embeddings of the classes to look up.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The class embeddings corresponding to the provided targets.</p> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>def lookup(self, target: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Extracts the class embeddings for the given target vectors.\n\n    Args:\n        target (torch.Tensor): A tensor containing the indices or one-hot embeddings\n            of the classes to look up.\n\n    Returns:\n        (torch.Tensor): The class embeddings corresponding to the provided targets.\n    \"\"\"\n    if self.training:\n        log(\n            WARNING,\n            \"Lookup method should not be used for training. \"\n            \"This method is intended for the purpose of embedding lookup, and \"\n            \"does not invoke the forward pass.\",\n        )\n    one_hot_n_dim = 2  # To avoid having magic numbers\n    if target.dim() == one_hot_n_dim:\n        assert target.shape[1] == self.num_classes, (\n            \"Shape of the one-hot encoded labels should be (batch_size, num_classes).\"\n        )\n        # If the target is one-hot encoded, convert it to indices.\n        target = torch.argmax(target, dim=1)\n\n    assert target.shape == (target.shape[0],), \"lookup requires 1D tensor of class indices.\"\n    return self.embedding.weight.data[target.int()]\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.CoV","title":"<code>CoV</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>class CoV(nn.Module):\n    def __init__(self, feature_dim: int) -&gt; None:\n        \"\"\"\n        Taken from the official implementation at : https://github.com/TsingZ0/GPFL/blob/main/system/flcore/servers/servergp.py\n        CoV (Conditional Value) module as described in the GPFL paper. This module consists of two parts.\n        1) First, uses the provided context tensor to compute two vectors, \\\\(\\\\gamma\\\\) and \\\\(\\\\beta\\\\) using\n        ``conditional_gamma`` and ``conditional_beta`` sub-modules, respectively.\n        In the paper: \\\\([\\\\mathbf{\\\\gamma_i}, \\\\mathbf{\\\\beta_i} = \\\\text{CoV}(\\\\mathbf{f}_i, \\\\cdot, V)]\\\\)\n        2) Then, applies an affine transformation followed by a ReLU activation to the feature tensors based on\n        the computed \\\\(\\\\gamma\\\\) and \\\\(\\\\beta\\\\) vectors.\n        Affine transformation in the paper:\n        \\\\([(\\\\mathbf{\\\\gamma} + \\\\mathbf{1})\\\\odot \\\\mathbf{f}_i + \\\\mathbf{\\\\beta}]\\\\)\n        Parameters of the sub-modules (``conditional_gamma`` and ``conditional_beta`` modules) are the main\n        components of this module, and are optimized during the training process.\n\n        Args:\n            feature_dim (int): The dimension of the feature tensor.\n        \"\"\"\n        super(CoV, self).__init__()\n        self.conditional_gamma = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim),\n            nn.ReLU(),\n            nn.LayerNorm([feature_dim]),\n        )\n        self.conditional_beta = nn.Sequential(\n            nn.Linear(feature_dim, feature_dim),\n            nn.ReLU(),\n            nn.LayerNorm([feature_dim]),\n        )\n        self.activation = nn.ReLU()\n\n    def forward(self, feature_tensor: torch.Tensor, context: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Uses the context tensor to compute gamma and beta vectors. Then, applies a conditional\n        affine transformation to the feature tensor based on the computed gamma and beta vectors.\n\n        Args:\n            feature_tensor (torch.Tensor): Output of the base feature extractor.\n            context (torch.Tensor): The conditional tensor that could be global or personalized.\n\n        Returns:\n            (torch.Tensor): The transformed feature tensor after applying the conditional affine transformation.\n        \"\"\"\n        # Call submodules to compute gamma and beta vectors.\n        gamma = self.conditional_gamma(context)\n        beta = self.conditional_beta(context)\n\n        # Now do the affine transformation with gamma and beta vectors.\n        out = torch.multiply(feature_tensor, gamma + 1)\n        out = torch.add(out, beta)\n        return self.activation(out)\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.CoV.__init__","title":"<code>__init__(feature_dim)</code>","text":"<p>Taken from the official implementation at : https://github.com/TsingZ0/GPFL/blob/main/system/flcore/servers/servergp.py CoV (Conditional Value) module as described in the GPFL paper. This module consists of two parts. 1) First, uses the provided context tensor to compute two vectors, \\(\\gamma\\) and \\(\\beta\\) using <code>conditional_gamma</code> and <code>conditional_beta</code> sub-modules, respectively. In the paper: \\([\\mathbf{\\gamma_i}, \\mathbf{\\beta_i} = \\text{CoV}(\\mathbf{f}_i, \\cdot, V)]\\) 2) Then, applies an affine transformation followed by a ReLU activation to the feature tensors based on the computed \\(\\gamma\\) and \\(\\beta\\) vectors. Affine transformation in the paper: \\([(\\mathbf{\\gamma} + \\mathbf{1})\\odot \\mathbf{f}_i + \\mathbf{\\beta}]\\) Parameters of the sub-modules (<code>conditional_gamma</code> and <code>conditional_beta</code> modules) are the main components of this module, and are optimized during the training process.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dim</code> <code>int</code> <p>The dimension of the feature tensor.</p> required Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>def __init__(self, feature_dim: int) -&gt; None:\n    \"\"\"\n    Taken from the official implementation at : https://github.com/TsingZ0/GPFL/blob/main/system/flcore/servers/servergp.py\n    CoV (Conditional Value) module as described in the GPFL paper. This module consists of two parts.\n    1) First, uses the provided context tensor to compute two vectors, \\\\(\\\\gamma\\\\) and \\\\(\\\\beta\\\\) using\n    ``conditional_gamma`` and ``conditional_beta`` sub-modules, respectively.\n    In the paper: \\\\([\\\\mathbf{\\\\gamma_i}, \\\\mathbf{\\\\beta_i} = \\\\text{CoV}(\\\\mathbf{f}_i, \\\\cdot, V)]\\\\)\n    2) Then, applies an affine transformation followed by a ReLU activation to the feature tensors based on\n    the computed \\\\(\\\\gamma\\\\) and \\\\(\\\\beta\\\\) vectors.\n    Affine transformation in the paper:\n    \\\\([(\\\\mathbf{\\\\gamma} + \\\\mathbf{1})\\\\odot \\\\mathbf{f}_i + \\\\mathbf{\\\\beta}]\\\\)\n    Parameters of the sub-modules (``conditional_gamma`` and ``conditional_beta`` modules) are the main\n    components of this module, and are optimized during the training process.\n\n    Args:\n        feature_dim (int): The dimension of the feature tensor.\n    \"\"\"\n    super(CoV, self).__init__()\n    self.conditional_gamma = nn.Sequential(\n        nn.Linear(feature_dim, feature_dim),\n        nn.ReLU(),\n        nn.LayerNorm([feature_dim]),\n    )\n    self.conditional_beta = nn.Sequential(\n        nn.Linear(feature_dim, feature_dim),\n        nn.ReLU(),\n        nn.LayerNorm([feature_dim]),\n    )\n    self.activation = nn.ReLU()\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.CoV.forward","title":"<code>forward(feature_tensor, context)</code>","text":"<p>Uses the context tensor to compute gamma and beta vectors. Then, applies a conditional affine transformation to the feature tensor based on the computed gamma and beta vectors.</p> <p>Parameters:</p> Name Type Description Default <code>feature_tensor</code> <code>Tensor</code> <p>Output of the base feature extractor.</p> required <code>context</code> <code>Tensor</code> <p>The conditional tensor that could be global or personalized.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The transformed feature tensor after applying the conditional affine transformation.</p> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>def forward(self, feature_tensor: torch.Tensor, context: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Uses the context tensor to compute gamma and beta vectors. Then, applies a conditional\n    affine transformation to the feature tensor based on the computed gamma and beta vectors.\n\n    Args:\n        feature_tensor (torch.Tensor): Output of the base feature extractor.\n        context (torch.Tensor): The conditional tensor that could be global or personalized.\n\n    Returns:\n        (torch.Tensor): The transformed feature tensor after applying the conditional affine transformation.\n    \"\"\"\n    # Call submodules to compute gamma and beta vectors.\n    gamma = self.conditional_gamma(context)\n    beta = self.conditional_beta(context)\n\n    # Now do the affine transformation with gamma and beta vectors.\n    out = torch.multiply(feature_tensor, gamma + 1)\n    out = torch.add(out, beta)\n    return self.activation(out)\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.GpflBaseAndHeadModules","title":"<code>GpflBaseAndHeadModules</code>","text":"<p>               Bases: <code>SequentiallySplitExchangeBaseModel</code></p> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>class GpflBaseAndHeadModules(SequentiallySplitExchangeBaseModel):\n    def __init__(self, base_module: nn.Module, head_module: nn.Module, flatten_features: bool) -&gt; None:\n        \"\"\"\n        This module class holds the main components for prediction in the GPFL model.\n        This is mainly used to enable defining one optimizer for the base and head modules.\n\n        Args:\n            base_module (nn.Module): Base feature extractor module that generates a feature tensor from the input.\n            head_module (nn.Module): Head module that takes a personalized feature tensor and produces the\n                final predictions.\n            flatten_features (bool): Whether the ``base_module``'s output features should be flattened or not.\n        \"\"\"\n        super().__init__(base_module=base_module, head_module=head_module, flatten_features=flatten_features)\n\n    def forward(self, input: torch.Tensor) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n        \"\"\"\n        A wrapper around the default sequential forward pass of the GPFL model base to restrict its usage.\n\n        Args:\n            input (torch.Tensor): Input to the model forward pass.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Return the prediction dictionary and a features dictionaries.\n        \"\"\"\n        # Throw an error because this function should not directly be called with this class.\n        raise NotImplementedError(\"Forward pass should not be used for the GpflBaseAndHeadModules class. \")\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.GpflBaseAndHeadModules.__init__","title":"<code>__init__(base_module, head_module, flatten_features)</code>","text":"<p>This module class holds the main components for prediction in the GPFL model. This is mainly used to enable defining one optimizer for the base and head modules.</p> <p>Parameters:</p> Name Type Description Default <code>base_module</code> <code>Module</code> <p>Base feature extractor module that generates a feature tensor from the input.</p> required <code>head_module</code> <code>Module</code> <p>Head module that takes a personalized feature tensor and produces the final predictions.</p> required <code>flatten_features</code> <code>bool</code> <p>Whether the <code>base_module</code>'s output features should be flattened or not.</p> required Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>def __init__(self, base_module: nn.Module, head_module: nn.Module, flatten_features: bool) -&gt; None:\n    \"\"\"\n    This module class holds the main components for prediction in the GPFL model.\n    This is mainly used to enable defining one optimizer for the base and head modules.\n\n    Args:\n        base_module (nn.Module): Base feature extractor module that generates a feature tensor from the input.\n        head_module (nn.Module): Head module that takes a personalized feature tensor and produces the\n            final predictions.\n        flatten_features (bool): Whether the ``base_module``'s output features should be flattened or not.\n    \"\"\"\n    super().__init__(base_module=base_module, head_module=head_module, flatten_features=flatten_features)\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.GpflBaseAndHeadModules.forward","title":"<code>forward(input)</code>","text":"<p>A wrapper around the default sequential forward pass of the GPFL model base to restrict its usage.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input to the model forward pass.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Return the prediction dictionary and a features dictionaries.</p> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n    \"\"\"\n    A wrapper around the default sequential forward pass of the GPFL model base to restrict its usage.\n\n    Args:\n        input (torch.Tensor): Input to the model forward pass.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Return the prediction dictionary and a features dictionaries.\n    \"\"\"\n    # Throw an error because this function should not directly be called with this class.\n    raise NotImplementedError(\"Forward pass should not be used for the GpflBaseAndHeadModules class. \")\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.GpflModel","title":"<code>GpflModel</code>","text":"<p>               Bases: <code>PartialLayerExchangeModel</code></p> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>class GpflModel(PartialLayerExchangeModel):\n    def __init__(\n        self,\n        base_module: nn.Module,\n        head_module: nn.Module,\n        feature_dim: int,\n        num_classes: int,\n        flatten_features: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        GPFL model base as described in the paper \"GPFL: Simultaneously Learning Global and Personalized\n        Feature Information for Personalized Federated Learning.\" https://arxiv.org/abs/2308.10279\n        This base module consists of three main sub-modules: the main_module, which consists of\n        a feature extractor and a head module; the GCE (Global Conditional Embedding) module; and\n        the CoV (Conditional Value) module.\n\n        Args:\n            base_module (nn.Module): Base feature extractor module that generates a feature tensor from the input.\n            head_module (nn.Module): Head module that takes a personalized feature tensor and produces the\n                final predictions.\n            feature_dim (int): The output dimension of the base feature extractor. This is also the input dimension\n                of the head and CoV modules.\n            num_classes (int): This is used to construct the GCE module.\n            flatten_features (bool, optional): Whether the ``base_module``'s output features should be\n                flattened or not. Defaults to False.\n        \"\"\"\n        super().__init__()\n\n        self.feature_dim = feature_dim\n        self.num_classes = num_classes\n        self.gpfl_main_module = GpflBaseAndHeadModules(base_module, head_module, flatten_features)\n        self.cov = CoV(feature_dim)\n        self.gce = Gce(feature_dim, num_classes)\n\n    def forward(\n        self,\n        input: torch.Tensor,\n        global_conditional_input: torch.Tensor,\n        personalized_conditional_input: torch.Tensor,\n    ) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n        \"\"\"\n        There are two types of forward passes in this model base. The first is the forward pass preformed\n        during training.\n        During training:\n        1) Input is passed through the base feature extractor.\n        2) Then the CoV module maps the extracted features into two feature tensors corresponding to local and global\n           features. The CoV module requires ``global_conditional_input`` and ``personalized_conditional_input``\n           tensors, which are used to condition the output of the CoV module. These tensors are computed in clients at\n           the beginning of each round.\n        3) The ``local_features`` are fed into the ``head_module`` to produce class predictions.\n        4) The ``global_conditional_input`` is used to compute the global features, and these ``global_features`` to\n           be used in loss calculations and are returned only during training.\n\n        The second type of forward pass happens during evaluation. For evaluation:\n\n        1) Input is passed through the base feature extractor.\n        2) ``local_features`` are generated by the CoV module.\n        3) These local features are passed through the head module to produce the final predictions.\n\n        Args:\n            input (torch.Tensor): Input tensor to be fed into the feature extractor.\n            global_conditional_input (torch.Tensor): The conditional input tensor used by the CoV module\n                to generate the global features.\n            personalized_conditional_input (torch.Tensor): The conditional input tensor used by the CoV module\n                to generate the local features.\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple in which the first element\n                contains a dictionary of predictions and the second element contains intermediate features\n                indexed by name.\n        \"\"\"\n        # Pass the input through the base feature extractor and potentially flatten the features.\n        features = self.gpfl_main_module.features_forward(input)\n        assert features.shape[1] == self.feature_dim, (\n            \"Feature dimension mismatch between output of the base module and the expected feature_dim by CoV.\"\n        )\n        local_features = self.cov(features, personalized_conditional_input)\n        assert local_features.shape[1] == self.feature_dim, (\n            \"Local feature dimension mismatch between output of the CoV module \"\n            \"and the expected feature_dim by the head module.\"\n        )\n        predictions = self.gpfl_main_module.head_module.forward(local_features)\n        if not self.training:\n            return {\"prediction\": predictions}, {}\n\n        assert len(global_conditional_input) == self.feature_dim, (\n            \"global_conditional_input must match the expected feature dimension by the CoV module.\"\n        )\n        global_features = self.cov(features, global_conditional_input)\n        assert global_features.shape[1] == self.feature_dim, \"global_features dimension should match the feature_dim.\"\n        return {\"prediction\": predictions}, {\"local_features\": local_features, \"global_features\": global_features}\n\n    def layers_to_exchange(self) -&gt; list[str]:\n        \"\"\"\n        Returns a list of layer names that should be exchanged between the server and clients.\n\n        Returns:\n            (list[str]): A list of layer names that should be exchanged. This is used by the\n                ``FixedLayerExchanger`` class to determine which layers to exchange during the FL process.\n        \"\"\"\n        base_layers = self.gpfl_main_module.layers_to_exchange()\n        # gpfl_main_module's layers_to_exchange returns base module layers starting with \"base_module.\"\n        # We need to prepend \"gpfl_main_module.\" to these layer names to match the state_dict keys.\n        complete_base_layer_names = [f\"gpfl_main_module.{layer_name}\" for layer_name in base_layers]\n        gpfl_module_layers = [\n            layer_name for layer_name in self.state_dict() if layer_name.startswith((\"cov.\", \"gce.\"))\n        ]\n        return complete_base_layer_names + gpfl_module_layers\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.GpflModel.__init__","title":"<code>__init__(base_module, head_module, feature_dim, num_classes, flatten_features=False)</code>","text":"<p>GPFL model base as described in the paper \"GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning.\" https://arxiv.org/abs/2308.10279 This base module consists of three main sub-modules: the main_module, which consists of a feature extractor and a head module; the GCE (Global Conditional Embedding) module; and the CoV (Conditional Value) module.</p> <p>Parameters:</p> Name Type Description Default <code>base_module</code> <code>Module</code> <p>Base feature extractor module that generates a feature tensor from the input.</p> required <code>head_module</code> <code>Module</code> <p>Head module that takes a personalized feature tensor and produces the final predictions.</p> required <code>feature_dim</code> <code>int</code> <p>The output dimension of the base feature extractor. This is also the input dimension of the head and CoV modules.</p> required <code>num_classes</code> <code>int</code> <p>This is used to construct the GCE module.</p> required <code>flatten_features</code> <code>bool</code> <p>Whether the <code>base_module</code>'s output features should be flattened or not. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>def __init__(\n    self,\n    base_module: nn.Module,\n    head_module: nn.Module,\n    feature_dim: int,\n    num_classes: int,\n    flatten_features: bool = False,\n) -&gt; None:\n    \"\"\"\n    GPFL model base as described in the paper \"GPFL: Simultaneously Learning Global and Personalized\n    Feature Information for Personalized Federated Learning.\" https://arxiv.org/abs/2308.10279\n    This base module consists of three main sub-modules: the main_module, which consists of\n    a feature extractor and a head module; the GCE (Global Conditional Embedding) module; and\n    the CoV (Conditional Value) module.\n\n    Args:\n        base_module (nn.Module): Base feature extractor module that generates a feature tensor from the input.\n        head_module (nn.Module): Head module that takes a personalized feature tensor and produces the\n            final predictions.\n        feature_dim (int): The output dimension of the base feature extractor. This is also the input dimension\n            of the head and CoV modules.\n        num_classes (int): This is used to construct the GCE module.\n        flatten_features (bool, optional): Whether the ``base_module``'s output features should be\n            flattened or not. Defaults to False.\n    \"\"\"\n    super().__init__()\n\n    self.feature_dim = feature_dim\n    self.num_classes = num_classes\n    self.gpfl_main_module = GpflBaseAndHeadModules(base_module, head_module, flatten_features)\n    self.cov = CoV(feature_dim)\n    self.gce = Gce(feature_dim, num_classes)\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.GpflModel.forward","title":"<code>forward(input, global_conditional_input, personalized_conditional_input)</code>","text":"<p>There are two types of forward passes in this model base. The first is the forward pass preformed during training. During training: 1) Input is passed through the base feature extractor. 2) Then the CoV module maps the extracted features into two feature tensors corresponding to local and global    features. The CoV module requires <code>global_conditional_input</code> and <code>personalized_conditional_input</code>    tensors, which are used to condition the output of the CoV module. These tensors are computed in clients at    the beginning of each round. 3) The <code>local_features</code> are fed into the <code>head_module</code> to produce class predictions. 4) The <code>global_conditional_input</code> is used to compute the global features, and these <code>global_features</code> to    be used in loss calculations and are returned only during training.</p> <p>The second type of forward pass happens during evaluation. For evaluation:</p> <p>1) Input is passed through the base feature extractor. 2) <code>local_features</code> are generated by the CoV module. 3) These local features are passed through the head module to produce the final predictions.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor to be fed into the feature extractor.</p> required <code>global_conditional_input</code> <code>Tensor</code> <p>The conditional input tensor used by the CoV module to generate the global features.</p> required <code>personalized_conditional_input</code> <code>Tensor</code> <p>The conditional input tensor used by the CoV module to generate the local features.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>A tuple in which the first element contains a dictionary of predictions and the second element contains intermediate features indexed by name.</p> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>def forward(\n    self,\n    input: torch.Tensor,\n    global_conditional_input: torch.Tensor,\n    personalized_conditional_input: torch.Tensor,\n) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n    \"\"\"\n    There are two types of forward passes in this model base. The first is the forward pass preformed\n    during training.\n    During training:\n    1) Input is passed through the base feature extractor.\n    2) Then the CoV module maps the extracted features into two feature tensors corresponding to local and global\n       features. The CoV module requires ``global_conditional_input`` and ``personalized_conditional_input``\n       tensors, which are used to condition the output of the CoV module. These tensors are computed in clients at\n       the beginning of each round.\n    3) The ``local_features`` are fed into the ``head_module`` to produce class predictions.\n    4) The ``global_conditional_input`` is used to compute the global features, and these ``global_features`` to\n       be used in loss calculations and are returned only during training.\n\n    The second type of forward pass happens during evaluation. For evaluation:\n\n    1) Input is passed through the base feature extractor.\n    2) ``local_features`` are generated by the CoV module.\n    3) These local features are passed through the head module to produce the final predictions.\n\n    Args:\n        input (torch.Tensor): Input tensor to be fed into the feature extractor.\n        global_conditional_input (torch.Tensor): The conditional input tensor used by the CoV module\n            to generate the global features.\n        personalized_conditional_input (torch.Tensor): The conditional input tensor used by the CoV module\n            to generate the local features.\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): A tuple in which the first element\n            contains a dictionary of predictions and the second element contains intermediate features\n            indexed by name.\n    \"\"\"\n    # Pass the input through the base feature extractor and potentially flatten the features.\n    features = self.gpfl_main_module.features_forward(input)\n    assert features.shape[1] == self.feature_dim, (\n        \"Feature dimension mismatch between output of the base module and the expected feature_dim by CoV.\"\n    )\n    local_features = self.cov(features, personalized_conditional_input)\n    assert local_features.shape[1] == self.feature_dim, (\n        \"Local feature dimension mismatch between output of the CoV module \"\n        \"and the expected feature_dim by the head module.\"\n    )\n    predictions = self.gpfl_main_module.head_module.forward(local_features)\n    if not self.training:\n        return {\"prediction\": predictions}, {}\n\n    assert len(global_conditional_input) == self.feature_dim, (\n        \"global_conditional_input must match the expected feature dimension by the CoV module.\"\n    )\n    global_features = self.cov(features, global_conditional_input)\n    assert global_features.shape[1] == self.feature_dim, \"global_features dimension should match the feature_dim.\"\n    return {\"prediction\": predictions}, {\"local_features\": local_features, \"global_features\": global_features}\n</code></pre>"},{"location":"api/#fl4health.model_bases.gpfl_base.GpflModel.layers_to_exchange","title":"<code>layers_to_exchange()</code>","text":"<p>Returns a list of layer names that should be exchanged between the server and clients.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>A list of layer names that should be exchanged. This is used by the <code>FixedLayerExchanger</code> class to determine which layers to exchange during the FL process.</p> Source code in <code>fl4health/model_bases/gpfl_base.py</code> <pre><code>def layers_to_exchange(self) -&gt; list[str]:\n    \"\"\"\n    Returns a list of layer names that should be exchanged between the server and clients.\n\n    Returns:\n        (list[str]): A list of layer names that should be exchanged. This is used by the\n            ``FixedLayerExchanger`` class to determine which layers to exchange during the FL process.\n    \"\"\"\n    base_layers = self.gpfl_main_module.layers_to_exchange()\n    # gpfl_main_module's layers_to_exchange returns base module layers starting with \"base_module.\"\n    # We need to prepend \"gpfl_main_module.\" to these layer names to match the state_dict keys.\n    complete_base_layer_names = [f\"gpfl_main_module.{layer_name}\" for layer_name in base_layers]\n    gpfl_module_layers = [\n        layer_name for layer_name in self.state_dict() if layer_name.startswith((\"cov.\", \"gce.\"))\n    ]\n    return complete_base_layer_names + gpfl_module_layers\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers","title":"<code>masked_layers</code>","text":""},{"location":"api/#fl4health.model_bases.masked_layers.masked_conv","title":"<code>masked_conv</code>","text":""},{"location":"api/#fl4health.model_bases.masked_layers.masked_conv.MaskedConv1d","title":"<code>MaskedConv1d</code>","text":"<p>               Bases: <code>Conv1d</code></p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>class MaskedConv1d(nn.Conv1d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_1_t,\n        stride: _size_1_t = 1,\n        padding: str | _size_1_t = 0,\n        dilation: _size_1_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = \"zeros\",\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Implementation of masked ``Conv1d`` layers.\n\n        Like regular ``Conv1d`` layers (i.e., ``nn.Conv1d`` module), a masked convolutional layer has a weight (i.e.,\n        convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in\n        back propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained.\n\n        In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores,\n        which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to\n        the weight and the bias. During training, gradients with respect to the score tensors are computed and used to\n        update the score tensors.\n\n        **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n        Args:\n            in_channels (int): Number of channels in the input image.\n            out_channels (int): Number of channels produced by the convolution.\n            kernel_size (int or tuple): Size of the convolving kernel.\n            stride (int or tuple, optional): Stride of the convolution. Default: 1.\n            padding (int, tuple or str, optional): Padding added to both sides of the input. Default: 0.\n            padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n                Default: ``'zeros'``.\n            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n            groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n            bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n            device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n            dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n        \"\"\"\n        # Attributes:\n        # weight: weights of the module.\n        # bias:  bias of the module.\n        # weight_score: learnable scores for the weights. Has the same shape as weight.\n        # bias_score: learnable scores for the bias. Has the same shape as bias.\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            padding_mode,\n            device,\n            dtype,\n        )\n        self.weight.requires_grad = False\n        self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n        if bias:\n            assert self.bias is not None\n            self.bias.requires_grad = False\n            self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n        else:\n            self.register_parameter(\"bias_scores\", None)\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        \"\"\"\n        Forward for the mask 1D Convolution.\n\n        Args:\n            input (Tensor): input tensor for the layer\n\n        Returns:\n            (Tensor): output tensor for the convolution\n        \"\"\"\n        weight_prob_scores = torch.sigmoid(self.weight_scores)\n        weight_mask = bernoulli_sample(weight_prob_scores)\n        masked_weight = weight_mask * self.weight\n        if self.bias is not None:\n            bias_prob_scores = torch.sigmoid(self.bias_scores)\n            bias_mask = bernoulli_sample(bias_prob_scores)\n            masked_bias = bias_mask * self.bias\n        else:\n            masked_bias = None\n        return self._conv_forward(input, weight=masked_weight, bias=masked_bias)\n\n    @classmethod\n    def from_pretrained(cls, conv_module: nn.Conv1d) -&gt; MaskedConv1d:\n        \"\"\"\n        Return an instance of ``MaskedConv1d`` whose weight and bias have the same values as those of ``conv_module``.\n\n        Args:\n            conv_module (nn.Conv1d): Module to be converted.\n\n        Returns:\n            (MaskedConv1d): Module with masked layers added to enable FedPM training.\n        \"\"\"\n        has_bias = conv_module.bias is not None\n        # we create new variables below to make mypy happy since kernel_size has\n        # type int | tuple[int] and kernel_size_ has type tuple[int]\n        kernel_size_ = _single(conv_module.kernel_size)\n        stride_ = _single(conv_module.stride)\n        padding_ = conv_module.padding if isinstance(conv_module.padding, str) else _single(conv_module.padding)\n        dilation_ = _single(conv_module.dilation)\n        masked_conv_module = cls(\n            in_channels=conv_module.in_channels,\n            out_channels=conv_module.out_channels,\n            kernel_size=kernel_size_,\n            stride=stride_,\n            padding=padding_,\n            dilation=dilation_,\n            groups=conv_module.groups,\n            bias=has_bias,\n            padding_mode=conv_module.padding_mode,\n        )\n        masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n        masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n        if has_bias:\n            assert conv_module.bias is not None\n            masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n            masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n        return masked_conv_module\n</code></pre> <code></code> <code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)</code> \u00b6 <p>Implementation of masked <code>Conv1d</code> layers.</p> <p>Like regular <code>Conv1d</code> layers (i.e., <code>nn.Conv1d</code> module), a masked convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained.</p> <p>In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to the score tensors are computed and used to update the score tensors.</p> <p>NOTE: The scores are not assumed to be bounded between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>int or tuple</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>int or tuple</code> <p>Stride of the convolution. Default: 1.</p> <code>1</code> <code>padding</code> <code>(int, tuple or str)</code> <p>Padding added to both sides of the input. Default: 0.</p> <code>0</code> <code>padding_mode</code> <code>str</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code>.</p> <code>'zeros'</code> <code>dilation</code> <code>int or tuple</code> <p>Spacing between kernel elements. Default: 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>.</p> <code>True</code> <code>device</code> <code>device | None</code> <p>Device to which this module should be sent. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Type of the tensors. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_1_t,\n    stride: _size_1_t = 1,\n    padding: str | _size_1_t = 0,\n    dilation: _size_1_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"zeros\",\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; None:\n    \"\"\"\n    Implementation of masked ``Conv1d`` layers.\n\n    Like regular ``Conv1d`` layers (i.e., ``nn.Conv1d`` module), a masked convolutional layer has a weight (i.e.,\n    convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in\n    back propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained.\n\n    In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores,\n    which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to\n    the weight and the bias. During training, gradients with respect to the score tensors are computed and used to\n    update the score tensors.\n\n    **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n    Args:\n        in_channels (int): Number of channels in the input image.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int or tuple): Size of the convolving kernel.\n        stride (int or tuple, optional): Stride of the convolution. Default: 1.\n        padding (int, tuple or str, optional): Padding added to both sides of the input. Default: 0.\n        padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n            Default: ``'zeros'``.\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n        device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n        dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n    \"\"\"\n    # Attributes:\n    # weight: weights of the module.\n    # bias:  bias of the module.\n    # weight_score: learnable scores for the weights. Has the same shape as weight.\n    # bias_score: learnable scores for the bias. Has the same shape as bias.\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        groups,\n        bias,\n        padding_mode,\n        device,\n        dtype,\n    )\n    self.weight.requires_grad = False\n    self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n    if bias:\n        assert self.bias is not None\n        self.bias.requires_grad = False\n        self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n    else:\n        self.register_parameter(\"bias_scores\", None)\n</code></pre> <code></code> <code>forward(input)</code> \u00b6 <p>Forward for the mask 1D Convolution.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>input tensor for the layer</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor for the convolution</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def forward(self, input: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward for the mask 1D Convolution.\n\n    Args:\n        input (Tensor): input tensor for the layer\n\n    Returns:\n        (Tensor): output tensor for the convolution\n    \"\"\"\n    weight_prob_scores = torch.sigmoid(self.weight_scores)\n    weight_mask = bernoulli_sample(weight_prob_scores)\n    masked_weight = weight_mask * self.weight\n    if self.bias is not None:\n        bias_prob_scores = torch.sigmoid(self.bias_scores)\n        bias_mask = bernoulli_sample(bias_prob_scores)\n        masked_bias = bias_mask * self.bias\n    else:\n        masked_bias = None\n    return self._conv_forward(input, weight=masked_weight, bias=masked_bias)\n</code></pre> <code></code> <code>from_pretrained(conv_module)</code> <code>classmethod</code> \u00b6 <p>Return an instance of <code>MaskedConv1d</code> whose weight and bias have the same values as those of <code>conv_module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>conv_module</code> <code>Conv1d</code> <p>Module to be converted.</p> required <p>Returns:</p> Type Description <code>MaskedConv1d</code> <p>Module with masked layers added to enable FedPM training.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, conv_module: nn.Conv1d) -&gt; MaskedConv1d:\n    \"\"\"\n    Return an instance of ``MaskedConv1d`` whose weight and bias have the same values as those of ``conv_module``.\n\n    Args:\n        conv_module (nn.Conv1d): Module to be converted.\n\n    Returns:\n        (MaskedConv1d): Module with masked layers added to enable FedPM training.\n    \"\"\"\n    has_bias = conv_module.bias is not None\n    # we create new variables below to make mypy happy since kernel_size has\n    # type int | tuple[int] and kernel_size_ has type tuple[int]\n    kernel_size_ = _single(conv_module.kernel_size)\n    stride_ = _single(conv_module.stride)\n    padding_ = conv_module.padding if isinstance(conv_module.padding, str) else _single(conv_module.padding)\n    dilation_ = _single(conv_module.dilation)\n    masked_conv_module = cls(\n        in_channels=conv_module.in_channels,\n        out_channels=conv_module.out_channels,\n        kernel_size=kernel_size_,\n        stride=stride_,\n        padding=padding_,\n        dilation=dilation_,\n        groups=conv_module.groups,\n        bias=has_bias,\n        padding_mode=conv_module.padding_mode,\n    )\n    masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n    masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n    if has_bias:\n        assert conv_module.bias is not None\n        masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n        masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n    return masked_conv_module\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_conv.MaskedConv2d","title":"<code>MaskedConv2d</code>","text":"<p>               Bases: <code>Conv2d</code></p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>class MaskedConv2d(nn.Conv2d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: str | _size_2_t = 0,\n        dilation: _size_2_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = \"zeros\",\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Implementation of masked ``Conv2d`` layers.\n\n        Like regular ``Conv2d`` layers (i.e., ``nn.Conv2d`` module), a masked convolutional layer has a weight (i.e.,\n        convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in back\n        propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained.\n        In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which\n        are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the\n        weight and the bias. During training, gradients with respect to the score tensors are computed and used to\n        update the score tensors.\n\n        **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n        Args:\n            in_channels (int): Number of channels in the input image.\n            out_channels (int): Number of channels produced by the convolution.\n            kernel_size (int or tuple): Size of the convolving kernel.\n            stride (int or tuple, optional): Stride of the convolution. Default: 1.\n            padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: 0.\n            padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n                Default: ``'zeros'``.\n            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n            groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n            bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n            device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n            dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n        \"\"\"\n        # Attributes:\n        # weight: weights of the module.\n        # bias:  bias of the module.\n        # weight_score: learnable scores for the weights. Has the same shape as weight.\n        # bias_score: learnable scores for the bias. Has the same shape as bias.\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            padding_mode,\n            device,\n            dtype,\n        )\n        self.weight.requires_grad = False\n        self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n        if bias:\n            assert self.bias is not None\n            self.bias.requires_grad = False\n            self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n        else:\n            self.register_parameter(\"bias_scores\", None)\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        \"\"\"\n        Forward for the Masked 2D Convolution.\n\n        Args:\n            input (Tensor): input tensor for the layer.\n\n        Returns:\n            (Tensor): output tensor for the convolution\n        \"\"\"\n        weight_prob_scores = torch.sigmoid(self.weight_scores)\n        weight_mask = bernoulli_sample(weight_prob_scores)\n        masked_weight = weight_mask * self.weight\n        if self.bias is not None:\n            bias_prob_scores = torch.sigmoid(self.bias_scores)\n            bias_mask = bernoulli_sample(bias_prob_scores)\n            masked_bias = bias_mask * self.bias\n        else:\n            masked_bias = None\n        return self._conv_forward(input, weight=masked_weight, bias=masked_bias)\n\n    @classmethod\n    def from_pretrained(cls, conv_module: nn.Conv2d) -&gt; MaskedConv2d:\n        \"\"\"\n        Return an instance of ``MaskedConv2d`` whose weight and bias have the same values as those of ``conv_module``.\n\n        Args:\n            conv_module (nn.Conv2d): Module to be converted.\n\n        Returns:\n            (MaskedConv2d): Module with masked layers to enable FedPM.\n        \"\"\"\n        has_bias = conv_module.bias is not None\n        kernel_size_ = _pair(conv_module.kernel_size)\n        stride_ = _pair(conv_module.stride)\n        padding_ = conv_module.padding if isinstance(conv_module.padding, str) else _pair(conv_module.padding)\n        dilation_ = _pair(conv_module.dilation)\n        masked_conv_module = cls(\n            in_channels=conv_module.in_channels,\n            out_channels=conv_module.out_channels,\n            kernel_size=kernel_size_,\n            stride=stride_,\n            padding=padding_,\n            dilation=dilation_,\n            groups=conv_module.groups,\n            bias=has_bias,\n            padding_mode=conv_module.padding_mode,\n        )\n        masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n        masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n        if has_bias:\n            assert conv_module.bias is not None\n            masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n            masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n        return masked_conv_module\n</code></pre> <code></code> <code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)</code> \u00b6 <p>Implementation of masked <code>Conv2d</code> layers.</p> <p>Like regular <code>Conv2d</code> layers (i.e., <code>nn.Conv2d</code> module), a masked convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to the score tensors are computed and used to update the score tensors.</p> <p>NOTE: The scores are not assumed to be bounded between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>int or tuple</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>int or tuple</code> <p>Stride of the convolution. Default: 1.</p> <code>1</code> <code>padding</code> <code>(int, tuple or str)</code> <p>Padding added to all four sides of the input. Default: 0.</p> <code>0</code> <code>padding_mode</code> <code>str</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code>.</p> <code>'zeros'</code> <code>dilation</code> <code>int or tuple</code> <p>Spacing between kernel elements. Default: 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>.</p> <code>True</code> <code>device</code> <code>device | None</code> <p>Device to which this module should be sent. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Type of the tensors. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: str | _size_2_t = 0,\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"zeros\",\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; None:\n    \"\"\"\n    Implementation of masked ``Conv2d`` layers.\n\n    Like regular ``Conv2d`` layers (i.e., ``nn.Conv2d`` module), a masked convolutional layer has a weight (i.e.,\n    convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in back\n    propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained.\n    In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which\n    are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the\n    weight and the bias. During training, gradients with respect to the score tensors are computed and used to\n    update the score tensors.\n\n    **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n    Args:\n        in_channels (int): Number of channels in the input image.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int or tuple): Size of the convolving kernel.\n        stride (int or tuple, optional): Stride of the convolution. Default: 1.\n        padding (int, tuple or str, optional): Padding added to all four sides of the input. Default: 0.\n        padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n            Default: ``'zeros'``.\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n        device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n        dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n    \"\"\"\n    # Attributes:\n    # weight: weights of the module.\n    # bias:  bias of the module.\n    # weight_score: learnable scores for the weights. Has the same shape as weight.\n    # bias_score: learnable scores for the bias. Has the same shape as bias.\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        groups,\n        bias,\n        padding_mode,\n        device,\n        dtype,\n    )\n    self.weight.requires_grad = False\n    self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n    if bias:\n        assert self.bias is not None\n        self.bias.requires_grad = False\n        self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n    else:\n        self.register_parameter(\"bias_scores\", None)\n</code></pre> <code></code> <code>forward(input)</code> \u00b6 <p>Forward for the Masked 2D Convolution.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>input tensor for the layer.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor for the convolution</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def forward(self, input: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward for the Masked 2D Convolution.\n\n    Args:\n        input (Tensor): input tensor for the layer.\n\n    Returns:\n        (Tensor): output tensor for the convolution\n    \"\"\"\n    weight_prob_scores = torch.sigmoid(self.weight_scores)\n    weight_mask = bernoulli_sample(weight_prob_scores)\n    masked_weight = weight_mask * self.weight\n    if self.bias is not None:\n        bias_prob_scores = torch.sigmoid(self.bias_scores)\n        bias_mask = bernoulli_sample(bias_prob_scores)\n        masked_bias = bias_mask * self.bias\n    else:\n        masked_bias = None\n    return self._conv_forward(input, weight=masked_weight, bias=masked_bias)\n</code></pre> <code></code> <code>from_pretrained(conv_module)</code> <code>classmethod</code> \u00b6 <p>Return an instance of <code>MaskedConv2d</code> whose weight and bias have the same values as those of <code>conv_module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>conv_module</code> <code>Conv2d</code> <p>Module to be converted.</p> required <p>Returns:</p> Type Description <code>MaskedConv2d</code> <p>Module with masked layers to enable FedPM.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, conv_module: nn.Conv2d) -&gt; MaskedConv2d:\n    \"\"\"\n    Return an instance of ``MaskedConv2d`` whose weight and bias have the same values as those of ``conv_module``.\n\n    Args:\n        conv_module (nn.Conv2d): Module to be converted.\n\n    Returns:\n        (MaskedConv2d): Module with masked layers to enable FedPM.\n    \"\"\"\n    has_bias = conv_module.bias is not None\n    kernel_size_ = _pair(conv_module.kernel_size)\n    stride_ = _pair(conv_module.stride)\n    padding_ = conv_module.padding if isinstance(conv_module.padding, str) else _pair(conv_module.padding)\n    dilation_ = _pair(conv_module.dilation)\n    masked_conv_module = cls(\n        in_channels=conv_module.in_channels,\n        out_channels=conv_module.out_channels,\n        kernel_size=kernel_size_,\n        stride=stride_,\n        padding=padding_,\n        dilation=dilation_,\n        groups=conv_module.groups,\n        bias=has_bias,\n        padding_mode=conv_module.padding_mode,\n    )\n    masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n    masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n    if has_bias:\n        assert conv_module.bias is not None\n        masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n        masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n    return masked_conv_module\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_conv.MaskedConv3d","title":"<code>MaskedConv3d</code>","text":"<p>               Bases: <code>Conv3d</code></p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>class MaskedConv3d(nn.Conv3d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_3_t,\n        stride: _size_3_t = 1,\n        padding: str | _size_3_t = 0,\n        dilation: _size_3_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = \"zeros\",\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Implementation of masked ``Conv3d`` layers.\n\n        Like regular ``Conv3d`` layers (i.e., ``nn.Conv3d`` module), a masked convolutional layer has a weight (i.e.,\n        convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in back\n        propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained. In the\n        forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which are\n        then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the weight\n        and the bias. During training, gradients with respect to the score tensors are computed and used to update the\n        score tensors.\n\n        **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n        Args:\n            in_channels (int): Number of channels in the input image.\n            out_channels (int): Number of channels produced by the convolution.\n            kernel_size (int or tuple): Size of the convolving kernel.\n            stride (int or tuple, optional): Stride of the convolution. Default: 1.\n            padding (int, tuple or str, optional): Padding added to all six sides of the input. Default: 0.\n            padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n                Default: ``'zeros'``.\n            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n            groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n            bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n            device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n            dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n        \"\"\"\n        # Attributes:\n        # weight: weights of the module.\n        # bias:  bias of the module.\n        # weight_score: learnable scores for the weights. Has the same shape as weight.\n        # bias_score: learnable scores for the bias. Has the same shape as bias.\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            padding_mode,\n            device,\n            dtype,\n        )\n        self.weight.requires_grad = False\n        self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n        if bias:\n            assert self.bias is not None\n            self.bias.requires_grad = False\n            self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n        else:\n            self.register_parameter(\"bias_scores\", None)\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        \"\"\"\n        Forward for the Masked 3D Convolution.\n\n        Args:\n            input (Tensor): input tensor for the layer.\n\n        Returns:\n            (Tensor): output tensor for the convolution.\n        \"\"\"\n        weight_prob_scores = torch.sigmoid(self.weight_scores)\n        weight_mask = bernoulli_sample(weight_prob_scores)\n        masked_weight = weight_mask * self.weight\n        if self.bias is not None:\n            bias_prob_scores = torch.sigmoid(self.bias_scores)\n            bias_mask = bernoulli_sample(bias_prob_scores)\n            masked_bias = bias_mask * self.bias\n        else:\n            masked_bias = None\n        return self._conv_forward(input, weight=masked_weight, bias=masked_bias)\n\n    @classmethod\n    def from_pretrained(cls, conv_module: nn.Conv3d) -&gt; MaskedConv3d:\n        \"\"\"\n        Return an instance of ``MaskedConv3d`` whose weight and bias have the same values as those of ``conv_module``.\n\n        Args:\n            conv_module (nn.Conv3d): Module to convert.\n\n        Returns:\n            (MaskedConv3d): Module with mask layers added to enable FedPM.\n        \"\"\"\n        has_bias = conv_module.bias is not None\n        kernel_size_ = _triple(conv_module.kernel_size)\n        stride_ = _triple(conv_module.stride)\n        padding_ = conv_module.padding if isinstance(conv_module.padding, str) else _triple(conv_module.padding)\n        dilation_ = _triple(conv_module.dilation)\n        masked_conv_module = cls(\n            in_channels=conv_module.in_channels,\n            out_channels=conv_module.out_channels,\n            kernel_size=kernel_size_,\n            stride=stride_,\n            padding=padding_,\n            dilation=dilation_,\n            groups=conv_module.groups,\n            bias=has_bias,\n            padding_mode=conv_module.padding_mode,\n        )\n        masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n        masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n        if has_bias:\n            assert conv_module.bias is not None\n            masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n            masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n        return masked_conv_module\n</code></pre> <code></code> <code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)</code> \u00b6 <p>Implementation of masked <code>Conv3d</code> layers.</p> <p>Like regular <code>Conv3d</code> layers (i.e., <code>nn.Conv3d</code> module), a masked convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to the score tensors are computed and used to update the score tensors.</p> <p>NOTE: The scores are not assumed to be bounded between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the convolution.</p> required <code>kernel_size</code> <code>int or tuple</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>int or tuple</code> <p>Stride of the convolution. Default: 1.</p> <code>1</code> <code>padding</code> <code>(int, tuple or str)</code> <p>Padding added to all six sides of the input. Default: 0.</p> <code>0</code> <code>padding_mode</code> <code>str</code> <p><code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code>.</p> <code>'zeros'</code> <code>dilation</code> <code>int or tuple</code> <p>Spacing between kernel elements. Default: 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>.</p> <code>True</code> <code>device</code> <code>device | None</code> <p>Device to which this module should be sent. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Type of the tensors. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_3_t,\n    stride: _size_3_t = 1,\n    padding: str | _size_3_t = 0,\n    dilation: _size_3_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"zeros\",\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; None:\n    \"\"\"\n    Implementation of masked ``Conv3d`` layers.\n\n    Like regular ``Conv3d`` layers (i.e., ``nn.Conv3d`` module), a masked convolutional layer has a weight (i.e.,\n    convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in back\n    propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained. In the\n    forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which are\n    then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the weight\n    and the bias. During training, gradients with respect to the score tensors are computed and used to update the\n    score tensors.\n\n    **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n    Args:\n        in_channels (int): Number of channels in the input image.\n        out_channels (int): Number of channels produced by the convolution.\n        kernel_size (int or tuple): Size of the convolving kernel.\n        stride (int or tuple, optional): Stride of the convolution. Default: 1.\n        padding (int, tuple or str, optional): Padding added to all six sides of the input. Default: 0.\n        padding_mode (str, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.\n            Default: ``'zeros'``.\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n        device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n        dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n    \"\"\"\n    # Attributes:\n    # weight: weights of the module.\n    # bias:  bias of the module.\n    # weight_score: learnable scores for the weights. Has the same shape as weight.\n    # bias_score: learnable scores for the bias. Has the same shape as bias.\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        groups,\n        bias,\n        padding_mode,\n        device,\n        dtype,\n    )\n    self.weight.requires_grad = False\n    self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n    if bias:\n        assert self.bias is not None\n        self.bias.requires_grad = False\n        self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n    else:\n        self.register_parameter(\"bias_scores\", None)\n</code></pre> <code></code> <code>forward(input)</code> \u00b6 <p>Forward for the Masked 3D Convolution.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>input tensor for the layer.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor for the convolution.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def forward(self, input: Tensor) -&gt; Tensor:\n    \"\"\"\n    Forward for the Masked 3D Convolution.\n\n    Args:\n        input (Tensor): input tensor for the layer.\n\n    Returns:\n        (Tensor): output tensor for the convolution.\n    \"\"\"\n    weight_prob_scores = torch.sigmoid(self.weight_scores)\n    weight_mask = bernoulli_sample(weight_prob_scores)\n    masked_weight = weight_mask * self.weight\n    if self.bias is not None:\n        bias_prob_scores = torch.sigmoid(self.bias_scores)\n        bias_mask = bernoulli_sample(bias_prob_scores)\n        masked_bias = bias_mask * self.bias\n    else:\n        masked_bias = None\n    return self._conv_forward(input, weight=masked_weight, bias=masked_bias)\n</code></pre> <code></code> <code>from_pretrained(conv_module)</code> <code>classmethod</code> \u00b6 <p>Return an instance of <code>MaskedConv3d</code> whose weight and bias have the same values as those of <code>conv_module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>conv_module</code> <code>Conv3d</code> <p>Module to convert.</p> required <p>Returns:</p> Type Description <code>MaskedConv3d</code> <p>Module with mask layers added to enable FedPM.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, conv_module: nn.Conv3d) -&gt; MaskedConv3d:\n    \"\"\"\n    Return an instance of ``MaskedConv3d`` whose weight and bias have the same values as those of ``conv_module``.\n\n    Args:\n        conv_module (nn.Conv3d): Module to convert.\n\n    Returns:\n        (MaskedConv3d): Module with mask layers added to enable FedPM.\n    \"\"\"\n    has_bias = conv_module.bias is not None\n    kernel_size_ = _triple(conv_module.kernel_size)\n    stride_ = _triple(conv_module.stride)\n    padding_ = conv_module.padding if isinstance(conv_module.padding, str) else _triple(conv_module.padding)\n    dilation_ = _triple(conv_module.dilation)\n    masked_conv_module = cls(\n        in_channels=conv_module.in_channels,\n        out_channels=conv_module.out_channels,\n        kernel_size=kernel_size_,\n        stride=stride_,\n        padding=padding_,\n        dilation=dilation_,\n        groups=conv_module.groups,\n        bias=has_bias,\n        padding_mode=conv_module.padding_mode,\n    )\n    masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n    masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n    if has_bias:\n        assert conv_module.bias is not None\n        masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n        masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n    return masked_conv_module\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_conv.MaskedConvTranspose1d","title":"<code>MaskedConvTranspose1d</code>","text":"<p>               Bases: <code>ConvTranspose1d</code></p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>class MaskedConvTranspose1d(nn.ConvTranspose1d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_1_t,\n        stride: _size_1_t = 1,\n        padding: _size_1_t = 0,\n        output_padding: _size_1_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_1_t = 1,\n        padding_mode: str = \"zeros\",\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Implementation of masked ``ConvTranspose1d`` layers. For more information on transposed convolution,\n        please see the PyTorch implementation of ``nn.Conv1d``.\n\n        (https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose1d)\n\n        Like regular ``ConvTranspose1d`` layers (i.e., ``nn.ConvTranspose1d`` module), a masked transpose\n        convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and\n        the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and\n        another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid\n        function into probability scores, which are then used to produce binary masks via Bernoulli sampling.\n        Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to\n        the score tensors are computed and used to update the score tensors.\n\n        **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n        Args:\n            in_channels (int): Number of channels in the input image.\n            out_channels (int): Number of channels produced by the transposed convolution.\n            kernel_size (int or tuple): Size of the convolving kernel.\n            stride (int or tuple, optional): Stride of the convolution. Default: 1.\n            padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to\n                both sides of the input. Default: 0.\n            output_padding (int or tuple, optional): Additional size added to one side of the output shape. Default: 0.\n            groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n            bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n            padding_mode (str, optional): Mode to be used in padding the input image for processing. Defaults to\n                \"zeros\".\n            device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n            dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n        \"\"\"\n        # Attributes:\n        # weight (Tensor): weights of the module.\n        # bias (Tensor):   bias of the module.\n        # weight_score: learnable scores for the weights. Has the same shape as weight.\n        # bias_score: learnable scores for the bias. Has the same shape as bias.\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            output_padding,\n            groups,\n            bias,\n            dilation,\n            padding_mode,\n            device,\n            dtype,\n        )\n        self.weight.requires_grad = False\n        self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n        if bias:\n            assert self.bias is not None\n            self.bias.requires_grad = False\n            self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n        else:\n            self.register_parameter(\"bias_scores\", None)\n\n    def forward(self, input: Tensor, output_size: list[int] | None = None) -&gt; Tensor:\n        \"\"\"\n        Forward for the ``MaskedConvTranspose1D``.\n\n        Args:\n            input (Tensor): input to be mapped with the module.\n            output_size (list[int] | None, optional): Desired output from the transpose. Defaults to None.\n\n        Raises:\n            ValueError: If something other than \"zeros\" padding has been requested.\n\n        Returns:\n            (Tensor): Output tensors.\n        \"\"\"\n        # Note: the same check is already present in super().__init__\n        if self.padding_mode != \"zeros\":\n            raise ValueError(\"Only `zeros` padding mode is supported for ConvTranspose1d\")\n        assert isinstance(self.padding, tuple)\n\n        # (The type ignore below is just used to resolve some small typing issue.)\n        # One cannot replace List by Tuple or Sequence in \"_output_padding\"\n        # because TorchScript does not support `Sequence[T]` or `tuple[T, ...]`.\n        output_padding = self._output_padding(\n            input,\n            output_size,\n            self.stride,  # type: ignore[arg-type]\n            self.padding,  # type: ignore[arg-type]\n            self.kernel_size,  # type: ignore[arg-type]\n            num_spatial_dims=1,\n            dilation=self.dilation,  # type: ignore[arg-type]\n        )\n\n        weight_prob_scores = torch.sigmoid(self.weight_scores)\n        weight_mask = bernoulli_sample(weight_prob_scores)\n        masked_weight = weight_mask * self.weight\n        if self.bias is not None:\n            bias_prob_scores = torch.sigmoid(self.bias_scores)\n            bias_mask = bernoulli_sample(bias_prob_scores)\n            masked_bias = bias_mask * self.bias\n        else:\n            masked_bias = None\n\n        return F.conv_transpose1d(\n            input, masked_weight, masked_bias, self.stride, self.padding, output_padding, self.groups, self.dilation\n        )\n\n    @classmethod\n    def from_pretrained(cls, conv_module: nn.ConvTranspose1d) -&gt; MaskedConvTranspose1d:\n        \"\"\"\n        Return an instance of ``MaskedConvTranspose1d`` whose weight and bias have the same values as those of\n        ``conv_module``.\n\n        Args:\n            conv_module (nn.ConvTranspose1d): Target module to be converted.\n\n        Returns:\n            (MaskedConvTranspose1d): Module with masked layers to enable FedPM.\n        \"\"\"\n        has_bias = conv_module.bias is not None\n        # we create new variables below to make mypy happy since kernel_size has\n        # type int | tuple[int] and kernel_size_ has type tuple[int]\n        kernel_size_ = _single(conv_module.kernel_size)\n        stride_ = _single(conv_module.stride)\n        padding_ = _single(conv_module.padding)\n        dilation_ = _single(conv_module.dilation)\n        output_padding_ = _single(conv_module.output_padding)\n        masked_conv_module = cls(\n            in_channels=conv_module.in_channels,\n            out_channels=conv_module.out_channels,\n            kernel_size=kernel_size_,\n            stride=stride_,\n            padding=padding_,\n            output_padding=output_padding_,\n            groups=conv_module.groups,\n            bias=has_bias,\n            dilation=dilation_,\n            padding_mode=conv_module.padding_mode,\n        )\n        masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n        masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n        if has_bias:\n            assert conv_module.bias is not None\n            masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n            masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n        return masked_conv_module\n</code></pre> <code></code> <code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)</code> \u00b6 <p>Implementation of masked <code>ConvTranspose1d</code> layers. For more information on transposed convolution, please see the PyTorch implementation of <code>nn.Conv1d</code>.</p> <p>(https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose1d)</p> <p>Like regular <code>ConvTranspose1d</code> layers (i.e., <code>nn.ConvTranspose1d</code> module), a masked transpose convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to the score tensors are computed and used to update the score tensors.</p> <p>NOTE: The scores are not assumed to be bounded between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the transposed convolution.</p> required <code>kernel_size</code> <code>int or tuple</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>int or tuple</code> <p>Stride of the convolution. Default: 1.</p> <code>1</code> <code>padding</code> <code>int or tuple</code> <p><code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both sides of the input. Default: 0.</p> <code>0</code> <code>output_padding</code> <code>int or tuple</code> <p>Additional size added to one side of the output shape. Default: 0.</p> <code>0</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>.</p> <code>True</code> <code>dilation</code> <code>int or tuple</code> <p>Spacing between kernel elements. Default: 1.</p> <code>1</code> <code>padding_mode</code> <code>str</code> <p>Mode to be used in padding the input image for processing. Defaults to \"zeros\".</p> <code>'zeros'</code> <code>device</code> <code>device | None</code> <p>Device to which this module should be sent. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Type of the tensors. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_1_t,\n    stride: _size_1_t = 1,\n    padding: _size_1_t = 0,\n    output_padding: _size_1_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_1_t = 1,\n    padding_mode: str = \"zeros\",\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; None:\n    \"\"\"\n    Implementation of masked ``ConvTranspose1d`` layers. For more information on transposed convolution,\n    please see the PyTorch implementation of ``nn.Conv1d``.\n\n    (https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose1d)\n\n    Like regular ``ConvTranspose1d`` layers (i.e., ``nn.ConvTranspose1d`` module), a masked transpose\n    convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and\n    the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and\n    another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid\n    function into probability scores, which are then used to produce binary masks via Bernoulli sampling.\n    Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to\n    the score tensors are computed and used to update the score tensors.\n\n    **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n    Args:\n        in_channels (int): Number of channels in the input image.\n        out_channels (int): Number of channels produced by the transposed convolution.\n        kernel_size (int or tuple): Size of the convolving kernel.\n        stride (int or tuple, optional): Stride of the convolution. Default: 1.\n        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to\n            both sides of the input. Default: 0.\n        output_padding (int or tuple, optional): Additional size added to one side of the output shape. Default: 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n        padding_mode (str, optional): Mode to be used in padding the input image for processing. Defaults to\n            \"zeros\".\n        device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n        dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n    \"\"\"\n    # Attributes:\n    # weight (Tensor): weights of the module.\n    # bias (Tensor):   bias of the module.\n    # weight_score: learnable scores for the weights. Has the same shape as weight.\n    # bias_score: learnable scores for the bias. Has the same shape as bias.\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        bias,\n        dilation,\n        padding_mode,\n        device,\n        dtype,\n    )\n    self.weight.requires_grad = False\n    self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n    if bias:\n        assert self.bias is not None\n        self.bias.requires_grad = False\n        self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n    else:\n        self.register_parameter(\"bias_scores\", None)\n</code></pre> <code></code> <code>forward(input, output_size=None)</code> \u00b6 <p>Forward for the <code>MaskedConvTranspose1D</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>input to be mapped with the module.</p> required <code>output_size</code> <code>list[int] | None</code> <p>Desired output from the transpose. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If something other than \"zeros\" padding has been requested.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensors.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def forward(self, input: Tensor, output_size: list[int] | None = None) -&gt; Tensor:\n    \"\"\"\n    Forward for the ``MaskedConvTranspose1D``.\n\n    Args:\n        input (Tensor): input to be mapped with the module.\n        output_size (list[int] | None, optional): Desired output from the transpose. Defaults to None.\n\n    Raises:\n        ValueError: If something other than \"zeros\" padding has been requested.\n\n    Returns:\n        (Tensor): Output tensors.\n    \"\"\"\n    # Note: the same check is already present in super().__init__\n    if self.padding_mode != \"zeros\":\n        raise ValueError(\"Only `zeros` padding mode is supported for ConvTranspose1d\")\n    assert isinstance(self.padding, tuple)\n\n    # (The type ignore below is just used to resolve some small typing issue.)\n    # One cannot replace List by Tuple or Sequence in \"_output_padding\"\n    # because TorchScript does not support `Sequence[T]` or `tuple[T, ...]`.\n    output_padding = self._output_padding(\n        input,\n        output_size,\n        self.stride,  # type: ignore[arg-type]\n        self.padding,  # type: ignore[arg-type]\n        self.kernel_size,  # type: ignore[arg-type]\n        num_spatial_dims=1,\n        dilation=self.dilation,  # type: ignore[arg-type]\n    )\n\n    weight_prob_scores = torch.sigmoid(self.weight_scores)\n    weight_mask = bernoulli_sample(weight_prob_scores)\n    masked_weight = weight_mask * self.weight\n    if self.bias is not None:\n        bias_prob_scores = torch.sigmoid(self.bias_scores)\n        bias_mask = bernoulli_sample(bias_prob_scores)\n        masked_bias = bias_mask * self.bias\n    else:\n        masked_bias = None\n\n    return F.conv_transpose1d(\n        input, masked_weight, masked_bias, self.stride, self.padding, output_padding, self.groups, self.dilation\n    )\n</code></pre> <code></code> <code>from_pretrained(conv_module)</code> <code>classmethod</code> \u00b6 <p>Return an instance of <code>MaskedConvTranspose1d</code> whose weight and bias have the same values as those of <code>conv_module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>conv_module</code> <code>ConvTranspose1d</code> <p>Target module to be converted.</p> required <p>Returns:</p> Type Description <code>MaskedConvTranspose1d</code> <p>Module with masked layers to enable FedPM.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, conv_module: nn.ConvTranspose1d) -&gt; MaskedConvTranspose1d:\n    \"\"\"\n    Return an instance of ``MaskedConvTranspose1d`` whose weight and bias have the same values as those of\n    ``conv_module``.\n\n    Args:\n        conv_module (nn.ConvTranspose1d): Target module to be converted.\n\n    Returns:\n        (MaskedConvTranspose1d): Module with masked layers to enable FedPM.\n    \"\"\"\n    has_bias = conv_module.bias is not None\n    # we create new variables below to make mypy happy since kernel_size has\n    # type int | tuple[int] and kernel_size_ has type tuple[int]\n    kernel_size_ = _single(conv_module.kernel_size)\n    stride_ = _single(conv_module.stride)\n    padding_ = _single(conv_module.padding)\n    dilation_ = _single(conv_module.dilation)\n    output_padding_ = _single(conv_module.output_padding)\n    masked_conv_module = cls(\n        in_channels=conv_module.in_channels,\n        out_channels=conv_module.out_channels,\n        kernel_size=kernel_size_,\n        stride=stride_,\n        padding=padding_,\n        output_padding=output_padding_,\n        groups=conv_module.groups,\n        bias=has_bias,\n        dilation=dilation_,\n        padding_mode=conv_module.padding_mode,\n    )\n    masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n    masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n    if has_bias:\n        assert conv_module.bias is not None\n        masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n        masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n    return masked_conv_module\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_conv.MaskedConvTranspose2d","title":"<code>MaskedConvTranspose2d</code>","text":"<p>               Bases: <code>ConvTranspose2d</code></p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>class MaskedConvTranspose2d(nn.ConvTranspose2d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        output_padding: _size_2_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_2_t = 1,\n        padding_mode: str = \"zeros\",\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Implementation of masked ``ConvTranspose2d`` layers. For more information on transposed convolution,\n        please see the PyTorch implementation of ``nn.Conv2d``.\n\n        (https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose2d)\n\n        Like regular ``ConvTranspose2d`` layers (i.e., ``nn.ConvTranspose2d`` module), a masked transpose\n        convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and\n        the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and\n        another for the bias - are maintained. In the forward pass, the score tensors are transformed by the\n        Sigmoid function into probability scores, which are then used to produce binary masks via Bernoulli sampling.\n        Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to\n        the score tensors are computed and used to update the score tensors.\n\n        **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n        Args:\n            in_channels (int): Number of channels in the input image.\n            out_channels (int): Number of channels produced by the transposed convolution.\n            kernel_size (int or tuple): Size of the convolving kernel.\n            stride (int or tuple, optional): Stride of the convolution. Default: 1.\n            padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding will be added\n                to both sides of each dimension in the input. Default: 0.\n            output_padding (int or tuple, optional): Additional size added to one side of each dimension in the\n                output shape. Default: 0.\n            groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n            bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n            padding_mode (str, optional): Mode to be used in padding the input image for processing. Defaults to\n                \"zeros\".\n            device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n            dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n        \"\"\"\n        # Attributes:\n        # weight (Tensor): weights of the module.\n        # bias (Tensor):   bias of the module.\n        # weight_score: learnable scores for the weights. Has the same shape as weight.\n        # bias_score: learnable scores for the bias. Has the same shape as bias.\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            output_padding,\n            groups,\n            bias,\n            dilation,\n            padding_mode,\n            device,\n            dtype,\n        )\n        self.weight.requires_grad = False\n        self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n        if bias:\n            assert self.bias is not None\n            self.bias.requires_grad = False\n            self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n        else:\n            self.register_parameter(\"bias_scores\", None)\n\n    def forward(self, input: Tensor, output_size: list[int] | None = None) -&gt; Tensor:\n        \"\"\"\n        Maps input tensor through the ``MaskedConvTranspose2D`` module.\n\n        Args:\n            input (Tensor): tensor to be mapped.\n            output_size (list[int] | None, optional): Desired output size from the module. Defaults to None.\n\n        Raises:\n            ValueError: Thrown if anything except \"zeros\" padding is requested.\n\n        Returns:\n            (Tensor): Mapped tensor.\n        \"\"\"\n        # Note: the same check is already present in super().__init__\n        if self.padding_mode != \"zeros\":\n            raise ValueError(\"Only `zeros` padding mode is supported for ConvTranspose1d\")\n        assert isinstance(self.padding, tuple)\n\n        output_padding = self._output_padding(\n            input,\n            output_size,\n            self.stride,  # type: ignore[arg-type]\n            self.padding,  # type: ignore[arg-type]\n            self.kernel_size,  # type: ignore[arg-type]\n            num_spatial_dims=2,\n            dilation=self.dilation,  # type: ignore[arg-type]\n        )\n\n        weight_prob_scores = torch.sigmoid(self.weight_scores)\n        weight_mask = bernoulli_sample(weight_prob_scores)\n        masked_weight = weight_mask * self.weight\n        if self.bias is not None:\n            bias_prob_scores = torch.sigmoid(self.bias_scores)\n            bias_mask = bernoulli_sample(bias_prob_scores)\n            masked_bias = bias_mask * self.bias\n        else:\n            masked_bias = None\n\n        return F.conv_transpose2d(\n            input, masked_weight, masked_bias, self.stride, self.padding, output_padding, self.groups, self.dilation\n        )\n\n    @classmethod\n    def from_pretrained(cls, conv_module: nn.ConvTranspose2d) -&gt; MaskedConvTranspose2d:\n        \"\"\"\n        Return an instance of ``MaskedConvTranspose2d`` whose weight and bias have the same values as those of\n        ``conv_module``.\n\n        Args:\n            conv_module (nn.ConvTranspose2d): Target module to be converted.\n\n        Returns:\n            (MaskedConvTranspose2d): Module with mask layers added to enable FedPM.\n        \"\"\"\n        has_bias = conv_module.bias is not None\n        # we create new variables below to make mypy happy since kernel_size has\n        # type int | tuple[int] and kernel_size_ has type tuple[int]\n        kernel_size_ = _pair(conv_module.kernel_size)\n        stride_ = _pair(conv_module.stride)\n        padding_ = _pair(conv_module.padding)\n        dilation_ = _pair(conv_module.dilation)\n        output_padding_ = _pair(conv_module.output_padding)\n        masked_conv_module = cls(\n            in_channels=conv_module.in_channels,\n            out_channels=conv_module.out_channels,\n            kernel_size=kernel_size_,\n            stride=stride_,\n            padding=padding_,\n            output_padding=output_padding_,\n            groups=conv_module.groups,\n            bias=has_bias,\n            dilation=dilation_,\n            padding_mode=conv_module.padding_mode,\n        )\n        masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n        masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n        if has_bias:\n            assert conv_module.bias is not None\n            masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n            masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n        return masked_conv_module\n</code></pre> <code></code> <code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)</code> \u00b6 <p>Implementation of masked <code>ConvTranspose2d</code> layers. For more information on transposed convolution, please see the PyTorch implementation of <code>nn.Conv2d</code>.</p> <p>(https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose2d)</p> <p>Like regular <code>ConvTranspose2d</code> layers (i.e., <code>nn.ConvTranspose2d</code> module), a masked transpose convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to the score tensors are computed and used to update the score tensors.</p> <p>NOTE: The scores are not assumed to be bounded between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the transposed convolution.</p> required <code>kernel_size</code> <code>int or tuple</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>int or tuple</code> <p>Stride of the convolution. Default: 1.</p> <code>1</code> <code>padding</code> <code>int or tuple</code> <p><code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both sides of each dimension in the input. Default: 0.</p> <code>0</code> <code>output_padding</code> <code>int or tuple</code> <p>Additional size added to one side of each dimension in the output shape. Default: 0.</p> <code>0</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>.</p> <code>True</code> <code>dilation</code> <code>int or tuple</code> <p>Spacing between kernel elements. Default: 1.</p> <code>1</code> <code>padding_mode</code> <code>str</code> <p>Mode to be used in padding the input image for processing. Defaults to \"zeros\".</p> <code>'zeros'</code> <code>device</code> <code>device | None</code> <p>Device to which this module should be sent. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Type of the tensors. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: _size_2_t = 0,\n    output_padding: _size_2_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_2_t = 1,\n    padding_mode: str = \"zeros\",\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; None:\n    \"\"\"\n    Implementation of masked ``ConvTranspose2d`` layers. For more information on transposed convolution,\n    please see the PyTorch implementation of ``nn.Conv2d``.\n\n    (https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose2d)\n\n    Like regular ``ConvTranspose2d`` layers (i.e., ``nn.ConvTranspose2d`` module), a masked transpose\n    convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and\n    the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and\n    another for the bias - are maintained. In the forward pass, the score tensors are transformed by the\n    Sigmoid function into probability scores, which are then used to produce binary masks via Bernoulli sampling.\n    Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to\n    the score tensors are computed and used to update the score tensors.\n\n    **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n    Args:\n        in_channels (int): Number of channels in the input image.\n        out_channels (int): Number of channels produced by the transposed convolution.\n        kernel_size (int or tuple): Size of the convolving kernel.\n        stride (int or tuple, optional): Stride of the convolution. Default: 1.\n        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding will be added\n            to both sides of each dimension in the input. Default: 0.\n        output_padding (int or tuple, optional): Additional size added to one side of each dimension in the\n            output shape. Default: 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n        padding_mode (str, optional): Mode to be used in padding the input image for processing. Defaults to\n            \"zeros\".\n        device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n        dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n    \"\"\"\n    # Attributes:\n    # weight (Tensor): weights of the module.\n    # bias (Tensor):   bias of the module.\n    # weight_score: learnable scores for the weights. Has the same shape as weight.\n    # bias_score: learnable scores for the bias. Has the same shape as bias.\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        bias,\n        dilation,\n        padding_mode,\n        device,\n        dtype,\n    )\n    self.weight.requires_grad = False\n    self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n    if bias:\n        assert self.bias is not None\n        self.bias.requires_grad = False\n        self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n    else:\n        self.register_parameter(\"bias_scores\", None)\n</code></pre> <code></code> <code>forward(input, output_size=None)</code> \u00b6 <p>Maps input tensor through the <code>MaskedConvTranspose2D</code> module.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>tensor to be mapped.</p> required <code>output_size</code> <code>list[int] | None</code> <p>Desired output size from the module. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Thrown if anything except \"zeros\" padding is requested.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Mapped tensor.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def forward(self, input: Tensor, output_size: list[int] | None = None) -&gt; Tensor:\n    \"\"\"\n    Maps input tensor through the ``MaskedConvTranspose2D`` module.\n\n    Args:\n        input (Tensor): tensor to be mapped.\n        output_size (list[int] | None, optional): Desired output size from the module. Defaults to None.\n\n    Raises:\n        ValueError: Thrown if anything except \"zeros\" padding is requested.\n\n    Returns:\n        (Tensor): Mapped tensor.\n    \"\"\"\n    # Note: the same check is already present in super().__init__\n    if self.padding_mode != \"zeros\":\n        raise ValueError(\"Only `zeros` padding mode is supported for ConvTranspose1d\")\n    assert isinstance(self.padding, tuple)\n\n    output_padding = self._output_padding(\n        input,\n        output_size,\n        self.stride,  # type: ignore[arg-type]\n        self.padding,  # type: ignore[arg-type]\n        self.kernel_size,  # type: ignore[arg-type]\n        num_spatial_dims=2,\n        dilation=self.dilation,  # type: ignore[arg-type]\n    )\n\n    weight_prob_scores = torch.sigmoid(self.weight_scores)\n    weight_mask = bernoulli_sample(weight_prob_scores)\n    masked_weight = weight_mask * self.weight\n    if self.bias is not None:\n        bias_prob_scores = torch.sigmoid(self.bias_scores)\n        bias_mask = bernoulli_sample(bias_prob_scores)\n        masked_bias = bias_mask * self.bias\n    else:\n        masked_bias = None\n\n    return F.conv_transpose2d(\n        input, masked_weight, masked_bias, self.stride, self.padding, output_padding, self.groups, self.dilation\n    )\n</code></pre> <code></code> <code>from_pretrained(conv_module)</code> <code>classmethod</code> \u00b6 <p>Return an instance of <code>MaskedConvTranspose2d</code> whose weight and bias have the same values as those of <code>conv_module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>conv_module</code> <code>ConvTranspose2d</code> <p>Target module to be converted.</p> required <p>Returns:</p> Type Description <code>MaskedConvTranspose2d</code> <p>Module with mask layers added to enable FedPM.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, conv_module: nn.ConvTranspose2d) -&gt; MaskedConvTranspose2d:\n    \"\"\"\n    Return an instance of ``MaskedConvTranspose2d`` whose weight and bias have the same values as those of\n    ``conv_module``.\n\n    Args:\n        conv_module (nn.ConvTranspose2d): Target module to be converted.\n\n    Returns:\n        (MaskedConvTranspose2d): Module with mask layers added to enable FedPM.\n    \"\"\"\n    has_bias = conv_module.bias is not None\n    # we create new variables below to make mypy happy since kernel_size has\n    # type int | tuple[int] and kernel_size_ has type tuple[int]\n    kernel_size_ = _pair(conv_module.kernel_size)\n    stride_ = _pair(conv_module.stride)\n    padding_ = _pair(conv_module.padding)\n    dilation_ = _pair(conv_module.dilation)\n    output_padding_ = _pair(conv_module.output_padding)\n    masked_conv_module = cls(\n        in_channels=conv_module.in_channels,\n        out_channels=conv_module.out_channels,\n        kernel_size=kernel_size_,\n        stride=stride_,\n        padding=padding_,\n        output_padding=output_padding_,\n        groups=conv_module.groups,\n        bias=has_bias,\n        dilation=dilation_,\n        padding_mode=conv_module.padding_mode,\n    )\n    masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n    masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n    if has_bias:\n        assert conv_module.bias is not None\n        masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n        masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n    return masked_conv_module\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_conv.MaskedConvTranspose3d","title":"<code>MaskedConvTranspose3d</code>","text":"<p>               Bases: <code>ConvTranspose3d</code></p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>class MaskedConvTranspose3d(nn.ConvTranspose3d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_3_t,\n        stride: _size_3_t = 1,\n        padding: _size_3_t = 0,\n        output_padding: _size_3_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_3_t = 1,\n        padding_mode: str = \"zeros\",\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Implementation of masked ``ConvTranspose3d`` layers. For more information on transposed convolution,\n        please see the PyTorch implementation of ``nn.Conv3d``.\n\n        (https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose3d)\n\n        Like regular ``ConvTranspose3d`` layers (i.e., ``nn.ConvTranspose3d`` module), a masked transpose\n        convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and\n        the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and\n        another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid\n        function into probability scores, which are then used to produce binary masks via Bernoulli sampling.\n        Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to\n        the score tensors are computed and used to update the score tensors.\n\n        **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n        Args:\n            in_channels (int): Number of channels in the input image.\n            out_channels (int): Number of channels produced by the transposed convolution.\n            kernel_size (int or tuple): Size of the convolving kernel.\n            stride (int or tuple, optional): Stride of the convolution. Default: 1.\n            padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to\n                both sides of each dimension in the input. Default: 0.\n            output_padding (int or tuple, optional): Additional size added to one side of each dimension in the\n                output shape. Default: 0.\n            groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n            bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n            padding_mode (str, optional): Mode to be used in padding the input image for processing. Defaults to\n                \"zeros\".\n            device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n            dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n        \"\"\"\n        # Attributes:\n        # weight (Tensor): weights of the module.\n        # bias (Tensor):   bias of the module.\n        # weight_score: learnable scores for the weights. Has the same shape as weight.\n        # bias_score: learnable scores for the bias. Has the same shape as bias.\n        super().__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            output_padding,\n            groups,\n            bias,\n            dilation,\n            padding_mode,\n            device,\n            dtype,\n        )\n        self.weight.requires_grad = False\n        self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n        if bias:\n            assert self.bias is not None\n            self.bias.requires_grad = False\n            self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n        else:\n            self.register_parameter(\"bias_scores\", None)\n\n    def forward(self, input: Tensor, output_size: list[int] | None = None) -&gt; Tensor:\n        \"\"\"\n        Maps the input tensor with ``MaskedConvTranspose3D``.\n\n        Args:\n            input (Tensor): Tensor to be mapped.\n            output_size (list[int] | None, optional): Desired output size from the transpose. Defaults to None.\n\n        Raises:\n            ValueError: Throws if anything except \"zeros\" padding is requested.\n\n        Returns:\n            (Tensor): Mapped tensor.\n        \"\"\"\n        # Note: the same check is already present in super().__init__\n        if self.padding_mode != \"zeros\":\n            raise ValueError(\"Only `zeros` padding mode is supported for ConvTranspose1d\")\n        assert isinstance(self.padding, tuple)\n\n        output_padding = self._output_padding(\n            input,\n            output_size,\n            self.stride,  # type: ignore[arg-type]\n            self.padding,  # type: ignore[arg-type]\n            self.kernel_size,  # type: ignore[arg-type]\n            num_spatial_dims=3,\n            dilation=self.dilation,  # type: ignore[arg-type]\n        )\n\n        weight_prob_scores = torch.sigmoid(self.weight_scores)\n        weight_mask = bernoulli_sample(weight_prob_scores)\n        masked_weight = weight_mask * self.weight\n        if self.bias is not None:\n            bias_prob_scores = torch.sigmoid(self.bias_scores)\n            bias_mask = bernoulli_sample(bias_prob_scores)\n            masked_bias = bias_mask * self.bias\n        else:\n            masked_bias = None\n\n        return F.conv_transpose3d(\n            input, masked_weight, masked_bias, self.stride, self.padding, output_padding, self.groups, self.dilation\n        )\n\n    @classmethod\n    def from_pretrained(cls, conv_module: nn.ConvTranspose3d) -&gt; MaskedConvTranspose3d:\n        \"\"\"\n        Return an instance of ``MaskedConvTranspose3d`` whose weight and bias have the same values as those of\n        ``conv_module``.\n\n        Args:\n            conv_module (nn.ConvTranspose3d): Target module to be converted.\n\n        Returns:\n            (MaskedConvTranspose3d): Module with masked layers added to enable FedPM.\n        \"\"\"\n        has_bias = conv_module.bias is not None\n        # we create new variables below to make mypy happy since kernel_size has\n        # type int | tuple[int] and kernel_size_ has type tuple[int]\n        kernel_size_ = _triple(conv_module.kernel_size)\n        stride_ = _triple(conv_module.stride)\n        padding_ = _triple(conv_module.padding)\n        dilation_ = _triple(conv_module.dilation)\n        output_padding_ = _triple(conv_module.output_padding)\n        masked_conv_module = cls(\n            in_channels=conv_module.in_channels,\n            out_channels=conv_module.out_channels,\n            kernel_size=kernel_size_,\n            stride=stride_,\n            padding=padding_,\n            output_padding=output_padding_,\n            groups=conv_module.groups,\n            bias=has_bias,\n            dilation=dilation_,\n            padding_mode=conv_module.padding_mode,\n        )\n        masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n        masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n        if has_bias:\n            assert conv_module.bias is not None\n            masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n            masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n        return masked_conv_module\n</code></pre> <code></code> <code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)</code> \u00b6 <p>Implementation of masked <code>ConvTranspose3d</code> layers. For more information on transposed convolution, please see the PyTorch implementation of <code>nn.Conv3d</code>.</p> <p>(https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose3d)</p> <p>Like regular <code>ConvTranspose3d</code> layers (i.e., <code>nn.ConvTranspose3d</code> module), a masked transpose convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to the score tensors are computed and used to update the score tensors.</p> <p>NOTE: The scores are not assumed to be bounded between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of channels in the input image.</p> required <code>out_channels</code> <code>int</code> <p>Number of channels produced by the transposed convolution.</p> required <code>kernel_size</code> <code>int or tuple</code> <p>Size of the convolving kernel.</p> required <code>stride</code> <code>int or tuple</code> <p>Stride of the convolution. Default: 1.</p> <code>1</code> <code>padding</code> <code>int or tuple</code> <p><code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both sides of each dimension in the input. Default: 0.</p> <code>0</code> <code>output_padding</code> <code>int or tuple</code> <p>Additional size added to one side of each dimension in the output shape. Default: 0.</p> <code>0</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input channels to output channels. Default: 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>.</p> <code>True</code> <code>dilation</code> <code>int or tuple</code> <p>Spacing between kernel elements. Default: 1.</p> <code>1</code> <code>padding_mode</code> <code>str</code> <p>Mode to be used in padding the input image for processing. Defaults to \"zeros\".</p> <code>'zeros'</code> <code>device</code> <code>device | None</code> <p>Device to which this module should be sent. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Type of the tensors. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_3_t,\n    stride: _size_3_t = 1,\n    padding: _size_3_t = 0,\n    output_padding: _size_3_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_3_t = 1,\n    padding_mode: str = \"zeros\",\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; None:\n    \"\"\"\n    Implementation of masked ``ConvTranspose3d`` layers. For more information on transposed convolution,\n    please see the PyTorch implementation of ``nn.Conv3d``.\n\n    (https://pytorch.org/docs/stable/_modules/torch/nn/modules/conv.html#ConvTranspose3d)\n\n    Like regular ``ConvTranspose3d`` layers (i.e., ``nn.ConvTranspose3d`` module), a masked transpose\n    convolutional layer has a weight (i.e., convolutional filter) and a (optional) bias. However, the weight and\n    the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and\n    another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid\n    function into probability scores, which are then used to produce binary masks via Bernoulli sampling.\n    Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to\n    the score tensors are computed and used to update the score tensors.\n\n    **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n    Args:\n        in_channels (int): Number of channels in the input image.\n        out_channels (int): Number of channels produced by the transposed convolution.\n        kernel_size (int or tuple): Size of the convolving kernel.\n        stride (int or tuple, optional): Stride of the convolution. Default: 1.\n        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to\n            both sides of each dimension in the input. Default: 0.\n        output_padding (int or tuple, optional): Additional size added to one side of each dimension in the\n            output shape. Default: 0.\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1.\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``.\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1.\n        padding_mode (str, optional): Mode to be used in padding the input image for processing. Defaults to\n            \"zeros\".\n        device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n        dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n    \"\"\"\n    # Attributes:\n    # weight (Tensor): weights of the module.\n    # bias (Tensor):   bias of the module.\n    # weight_score: learnable scores for the weights. Has the same shape as weight.\n    # bias_score: learnable scores for the bias. Has the same shape as bias.\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        bias,\n        dilation,\n        padding_mode,\n        device,\n        dtype,\n    )\n    self.weight.requires_grad = False\n    self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n    if bias:\n        assert self.bias is not None\n        self.bias.requires_grad = False\n        self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n    else:\n        self.register_parameter(\"bias_scores\", None)\n</code></pre> <code></code> <code>forward(input, output_size=None)</code> \u00b6 <p>Maps the input tensor with <code>MaskedConvTranspose3D</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Tensor to be mapped.</p> required <code>output_size</code> <code>list[int] | None</code> <p>Desired output size from the transpose. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Throws if anything except \"zeros\" padding is requested.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Mapped tensor.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>def forward(self, input: Tensor, output_size: list[int] | None = None) -&gt; Tensor:\n    \"\"\"\n    Maps the input tensor with ``MaskedConvTranspose3D``.\n\n    Args:\n        input (Tensor): Tensor to be mapped.\n        output_size (list[int] | None, optional): Desired output size from the transpose. Defaults to None.\n\n    Raises:\n        ValueError: Throws if anything except \"zeros\" padding is requested.\n\n    Returns:\n        (Tensor): Mapped tensor.\n    \"\"\"\n    # Note: the same check is already present in super().__init__\n    if self.padding_mode != \"zeros\":\n        raise ValueError(\"Only `zeros` padding mode is supported for ConvTranspose1d\")\n    assert isinstance(self.padding, tuple)\n\n    output_padding = self._output_padding(\n        input,\n        output_size,\n        self.stride,  # type: ignore[arg-type]\n        self.padding,  # type: ignore[arg-type]\n        self.kernel_size,  # type: ignore[arg-type]\n        num_spatial_dims=3,\n        dilation=self.dilation,  # type: ignore[arg-type]\n    )\n\n    weight_prob_scores = torch.sigmoid(self.weight_scores)\n    weight_mask = bernoulli_sample(weight_prob_scores)\n    masked_weight = weight_mask * self.weight\n    if self.bias is not None:\n        bias_prob_scores = torch.sigmoid(self.bias_scores)\n        bias_mask = bernoulli_sample(bias_prob_scores)\n        masked_bias = bias_mask * self.bias\n    else:\n        masked_bias = None\n\n    return F.conv_transpose3d(\n        input, masked_weight, masked_bias, self.stride, self.padding, output_padding, self.groups, self.dilation\n    )\n</code></pre> <code></code> <code>from_pretrained(conv_module)</code> <code>classmethod</code> \u00b6 <p>Return an instance of <code>MaskedConvTranspose3d</code> whose weight and bias have the same values as those of <code>conv_module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>conv_module</code> <code>ConvTranspose3d</code> <p>Target module to be converted.</p> required <p>Returns:</p> Type Description <code>MaskedConvTranspose3d</code> <p>Module with masked layers added to enable FedPM.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_conv.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, conv_module: nn.ConvTranspose3d) -&gt; MaskedConvTranspose3d:\n    \"\"\"\n    Return an instance of ``MaskedConvTranspose3d`` whose weight and bias have the same values as those of\n    ``conv_module``.\n\n    Args:\n        conv_module (nn.ConvTranspose3d): Target module to be converted.\n\n    Returns:\n        (MaskedConvTranspose3d): Module with masked layers added to enable FedPM.\n    \"\"\"\n    has_bias = conv_module.bias is not None\n    # we create new variables below to make mypy happy since kernel_size has\n    # type int | tuple[int] and kernel_size_ has type tuple[int]\n    kernel_size_ = _triple(conv_module.kernel_size)\n    stride_ = _triple(conv_module.stride)\n    padding_ = _triple(conv_module.padding)\n    dilation_ = _triple(conv_module.dilation)\n    output_padding_ = _triple(conv_module.output_padding)\n    masked_conv_module = cls(\n        in_channels=conv_module.in_channels,\n        out_channels=conv_module.out_channels,\n        kernel_size=kernel_size_,\n        stride=stride_,\n        padding=padding_,\n        output_padding=output_padding_,\n        groups=conv_module.groups,\n        bias=has_bias,\n        dilation=dilation_,\n        padding_mode=conv_module.padding_mode,\n    )\n    masked_conv_module.weight = Parameter(conv_module.weight.clone().detach(), requires_grad=False)\n    masked_conv_module.weight_scores = Parameter(torch.randn_like(masked_conv_module.weight), requires_grad=True)\n    if has_bias:\n        assert conv_module.bias is not None\n        masked_conv_module.bias = Parameter(conv_module.bias.clone().detach(), requires_grad=False)\n        masked_conv_module.bias_scores = Parameter(torch.randn_like(masked_conv_module.bias), requires_grad=True)\n    return masked_conv_module\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_layers_utils","title":"<code>masked_layers_utils</code>","text":""},{"location":"api/#fl4health.model_bases.masked_layers.masked_layers_utils.convert_to_masked_model","title":"<code>convert_to_masked_model(original_model)</code>","text":"<p>Given a model, convert every one of its layers to a masked layer of the same kind, if applicable.</p> <p>Parameters:</p> Name Type Description Default <code>original_model</code> <code>Module</code> <p>Module to be converted to a masked module.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>New copy of the original model but with masked layers injected to enable FedPM.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_layers_utils.py</code> <pre><code>def convert_to_masked_model(original_model: nn.Module) -&gt; nn.Module:\n    \"\"\"\n    Given a model, convert every one of its layers to a masked layer of the same kind, if applicable.\n\n    Args:\n        original_model (nn.Module): Module to be converted to a masked module.\n\n    Returns:\n        (nn.Module): New copy of the original model but with masked layers injected to enable FedPM.\n    \"\"\"\n\n    def replace_with_masked(module: nn.Module) -&gt; None:\n        # Replace layers with their masked versions.\n        for name, child in module.named_children():\n            # Linear layers\n            if isinstance(child, nn.Linear) and not isinstance(child, MaskedLinear):\n                setattr(module, name, MaskedLinear.from_pretrained(child))\n            # 1d, 2d, 3d convolutional layers and transposed convolutional layers\n            elif isinstance(child, nn.Conv1d) and not isinstance(child, MaskedConv1d):\n                setattr(module, name, MaskedConv1d.from_pretrained(child))\n            elif isinstance(child, nn.Conv2d) and not isinstance(child, MaskedConv2d):\n                setattr(module, name, MaskedConv2d.from_pretrained(child))\n            elif isinstance(child, nn.Conv3d) and not isinstance(child, MaskedConv3d):\n                setattr(module, name, MaskedConv3d.from_pretrained(child))\n            elif isinstance(child, nn.ConvTranspose1d) and not isinstance(child, MaskedConvTranspose1d):\n                setattr(module, name, MaskedConvTranspose1d.from_pretrained(child))\n            elif isinstance(child, nn.ConvTranspose2d) and not isinstance(child, MaskedConvTranspose2d):\n                setattr(module, name, MaskedConvTranspose2d.from_pretrained(child))\n            elif isinstance(child, nn.ConvTranspose3d) and not isinstance(child, MaskedConvTranspose3d):\n                setattr(module, name, MaskedConvTranspose3d.from_pretrained(child))\n            # LayerNorm\n            elif isinstance(child, nn.LayerNorm) and not isinstance(child, MaskedLayerNorm):\n                setattr(module, name, MaskedLayerNorm.from_pretrained(child))\n            # 1d, 2d, and 3d BatchNorm\n            elif isinstance(child, nn.BatchNorm1d):\n                setattr(module, name, MaskedBatchNorm1d.from_pretrained(child))\n            elif isinstance(child, nn.BatchNorm2d):\n                setattr(module, name, MaskedBatchNorm2d.from_pretrained(child))\n            elif isinstance(child, nn.BatchNorm3d):\n                setattr(module, name, MaskedBatchNorm3d.from_pretrained(child))\n            # Recursively process the submodules of child\n            else:\n                replace_with_masked(child)\n\n    # Deepcopy the model to avoid modifying the original\n    masked_model = copy.deepcopy(original_model)\n    replace_with_masked(masked_model)\n    return masked_model\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_layers_utils.is_masked_module","title":"<code>is_masked_module(module)</code>","text":"<p>Checks whether the provided module is a masked module of the kind supported.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>Module to be checked</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the module is a masked type and False otherwise.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_layers_utils.py</code> <pre><code>def is_masked_module(module: nn.Module) -&gt; bool:\n    \"\"\"\n    Checks whether the provided module is a masked module of the kind supported.\n\n    Args:\n        module (nn.Module): Module to be checked\n\n    Returns:\n        (bool): True if the module is a masked type and False otherwise.\n    \"\"\"\n    return isinstance(\n        module,\n        (\n            MaskedLinear,\n            MaskedConv1d,\n            MaskedConv2d,\n            MaskedConv3d,\n            MaskedConvTranspose1d,\n            MaskedConvTranspose2d,\n            MaskedConvTranspose3d,\n            MaskedLayerNorm,\n            _MaskedBatchNorm,\n        ),\n    )\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_linear","title":"<code>masked_linear</code>","text":""},{"location":"api/#fl4health.model_bases.masked_layers.masked_linear.MaskedLinear","title":"<code>MaskedLinear</code>","text":"<p>               Bases: <code>Linear</code></p> Source code in <code>fl4health/model_bases/masked_layers/masked_linear.py</code> <pre><code>class MaskedLinear(nn.Linear):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = True,\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Implementation of masked linear layers.\n\n        Like regular linear layers (i.e., ``nn.Linear module``), a masked linear layer has a weight and a bias.\n        However, the weight and the bias do not receive gradient in back propagation. Instead, two score tensors - one\n        for the weight and another for the bias - are maintained. In the forward pass, the score tensors are\n        transformed by the Sigmoid function into probability scores, which are then used to produce binary masks via\n        Bernoulli sampling. Finally, the binary masks are applied to the weight and the bias. During training,\n        gradients with respect to the score tensors are computed and used to update the score tensors.\n\n        **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n        Args:\n            in_features: size of each input sample.\n            out_features: size of each output sample.\n            bias: If set to ``False``, the layer will not learn an additive bias. Default: ``True``.\n            device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n            dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n        \"\"\"\n        # Attributes:\n        # weight: weights of the module.\n        # bias:  bias of the module.\n        # weight_score: learnable scores for the weights. Has the same shape as weight.\n        # bias_score: learnable scores for the bias. Has the same shape as bias.\n        super().__init__(in_features, out_features, bias, device, dtype)\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight.requires_grad = False\n        self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n        if bias:\n            assert self.bias is not None\n            self.bias.requires_grad = False\n            self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n        else:\n            self.register_parameter(\"bias_scores\", None)\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        \"\"\"\n        Mapping function for the ``MaskedLinear`` layer.\n\n        Args:\n            input (Tensor): input tensor to be transformed.\n\n        Returns:\n            (Tensor): output tensor from the layer.\n        \"\"\"\n        # Produce probability scores and perform Bernoulli sampling\n        weight_prob_scores = torch.sigmoid(self.weight_scores)\n        weight_mask = bernoulli_sample(weight_prob_scores)\n        masked_weight = weight_mask * self.weight\n\n        if self.bias is not None:\n            bias_prob_scores = torch.sigmoid(self.bias_scores)\n            bias_mask = bernoulli_sample(bias_prob_scores)\n            masked_bias = bias_mask * self.bias\n        else:\n            masked_bias = None\n\n        # Apply the masks to weight and bias\n        return F.linear(input, masked_weight, masked_bias)\n\n    @classmethod\n    def from_pretrained(cls, linear_module: nn.Linear) -&gt; MaskedLinear:\n        \"\"\"\n        Return an instance of ``MaskedLinear`` whose weight and bias have the same values as those of\n        ``linear_module``.\n\n        Args:\n            linear_module (nn.Linear): Target layer to be transformed.\n\n        Returns:\n            (MaskedLinear): New copy of the provided module with masked layers inserted to enable FedPM.\n        \"\"\"\n        has_bias = linear_module.bias is not None\n        masked_linear_module = cls(\n            in_features=linear_module.in_features,\n            out_features=linear_module.out_features,\n            bias=has_bias,\n        )\n        masked_linear_module.weight = Parameter(linear_module.weight.clone().detach(), requires_grad=False)\n        masked_linear_module.weight_scores = Parameter(torch.randn_like(linear_module.weight), requires_grad=True)\n        if has_bias:\n            masked_linear_module.bias = Parameter(linear_module.bias.clone().detach(), requires_grad=False)\n            masked_linear_module.bias_scores = Parameter(torch.randn_like(linear_module.bias), requires_grad=True)\n        return masked_linear_module\n</code></pre> <code></code> <code>__init__(in_features, out_features, bias=True, device=None, dtype=None)</code> \u00b6 <p>Implementation of masked linear layers.</p> <p>Like regular linear layers (i.e., <code>nn.Linear module</code>), a masked linear layer has a weight and a bias. However, the weight and the bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to the score tensors are computed and used to update the score tensors.</p> <p>NOTE: The scores are not assumed to be bounded between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>size of each input sample.</p> required <code>out_features</code> <code>int</code> <p>size of each output sample.</p> required <code>bias</code> <code>bool</code> <p>If set to <code>False</code>, the layer will not learn an additive bias. Default: <code>True</code>.</p> <code>True</code> <code>device</code> <code>device | None</code> <p>Device to which this module should be sent. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Type of the tensors. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/model_bases/masked_layers/masked_linear.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; None:\n    \"\"\"\n    Implementation of masked linear layers.\n\n    Like regular linear layers (i.e., ``nn.Linear module``), a masked linear layer has a weight and a bias.\n    However, the weight and the bias do not receive gradient in back propagation. Instead, two score tensors - one\n    for the weight and another for the bias - are maintained. In the forward pass, the score tensors are\n    transformed by the Sigmoid function into probability scores, which are then used to produce binary masks via\n    Bernoulli sampling. Finally, the binary masks are applied to the weight and the bias. During training,\n    gradients with respect to the score tensors are computed and used to update the score tensors.\n\n    **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n    Args:\n        in_features: size of each input sample.\n        out_features: size of each output sample.\n        bias: If set to ``False``, the layer will not learn an additive bias. Default: ``True``.\n        device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n        dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n    \"\"\"\n    # Attributes:\n    # weight: weights of the module.\n    # bias:  bias of the module.\n    # weight_score: learnable scores for the weights. Has the same shape as weight.\n    # bias_score: learnable scores for the bias. Has the same shape as bias.\n    super().__init__(in_features, out_features, bias, device, dtype)\n    self.in_features = in_features\n    self.out_features = out_features\n    self.weight.requires_grad = False\n    self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n    if bias:\n        assert self.bias is not None\n        self.bias.requires_grad = False\n        self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n    else:\n        self.register_parameter(\"bias_scores\", None)\n</code></pre> <code></code> <code>forward(input)</code> \u00b6 <p>Mapping function for the <code>MaskedLinear</code> layer.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>input tensor to be transformed.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>output tensor from the layer.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_linear.py</code> <pre><code>def forward(self, input: Tensor) -&gt; Tensor:\n    \"\"\"\n    Mapping function for the ``MaskedLinear`` layer.\n\n    Args:\n        input (Tensor): input tensor to be transformed.\n\n    Returns:\n        (Tensor): output tensor from the layer.\n    \"\"\"\n    # Produce probability scores and perform Bernoulli sampling\n    weight_prob_scores = torch.sigmoid(self.weight_scores)\n    weight_mask = bernoulli_sample(weight_prob_scores)\n    masked_weight = weight_mask * self.weight\n\n    if self.bias is not None:\n        bias_prob_scores = torch.sigmoid(self.bias_scores)\n        bias_mask = bernoulli_sample(bias_prob_scores)\n        masked_bias = bias_mask * self.bias\n    else:\n        masked_bias = None\n\n    # Apply the masks to weight and bias\n    return F.linear(input, masked_weight, masked_bias)\n</code></pre> <code></code> <code>from_pretrained(linear_module)</code> <code>classmethod</code> \u00b6 <p>Return an instance of <code>MaskedLinear</code> whose weight and bias have the same values as those of <code>linear_module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>linear_module</code> <code>Linear</code> <p>Target layer to be transformed.</p> required <p>Returns:</p> Type Description <code>MaskedLinear</code> <p>New copy of the provided module with masked layers inserted to enable FedPM.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_linear.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, linear_module: nn.Linear) -&gt; MaskedLinear:\n    \"\"\"\n    Return an instance of ``MaskedLinear`` whose weight and bias have the same values as those of\n    ``linear_module``.\n\n    Args:\n        linear_module (nn.Linear): Target layer to be transformed.\n\n    Returns:\n        (MaskedLinear): New copy of the provided module with masked layers inserted to enable FedPM.\n    \"\"\"\n    has_bias = linear_module.bias is not None\n    masked_linear_module = cls(\n        in_features=linear_module.in_features,\n        out_features=linear_module.out_features,\n        bias=has_bias,\n    )\n    masked_linear_module.weight = Parameter(linear_module.weight.clone().detach(), requires_grad=False)\n    masked_linear_module.weight_scores = Parameter(torch.randn_like(linear_module.weight), requires_grad=True)\n    if has_bias:\n        masked_linear_module.bias = Parameter(linear_module.bias.clone().detach(), requires_grad=False)\n        masked_linear_module.bias_scores = Parameter(torch.randn_like(linear_module.bias), requires_grad=True)\n    return masked_linear_module\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_normalization_layers","title":"<code>masked_normalization_layers</code>","text":""},{"location":"api/#fl4health.model_bases.masked_layers.masked_normalization_layers.MaskedLayerNorm","title":"<code>MaskedLayerNorm</code>","text":"<p>               Bases: <code>LayerNorm</code></p> Source code in <code>fl4health/model_bases/masked_layers/masked_normalization_layers.py</code> <pre><code>class MaskedLayerNorm(nn.LayerNorm):\n    def __init__(\n        self,\n        normalized_shape: TorchShape,\n        eps: float = 1e-5,\n        elementwise_affine: bool = True,\n        bias: bool = True,\n        device: torch.device | None = None,\n        dtype: torch.dtype | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Implementation of the masked Layer Normalization module. When ``elementwise_affine`` is True, ``nn.LayerNorm``\n        has a learnable weight and (optional) bias. For ``MaskedLayerNorm``, the weight and bias do not receive\n        gradient in back propagation. Instead, two score tensors - one for the weight and another for the bias - are\n        maintained. In the forward pass, the score tensors are transformed by the Sigmoid function into probability\n        scores, which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are\n        applied to the weight and the bias. During training, gradients with respect to the score tensors are computed\n        and used to update the score tensors.\n\n        When ``elementwise_affine`` is False, ``nn.LayerNorm`` does not have weight or bias. Under this condition, both\n        score tensors are None and ``MaskedLayerNorm`` acts in the same way as ``nn.LayerNorm``.\n\n        **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n        Args:\n            normalized_shape (TorchShape): Input shape from an expected input. If a single integer is used, it is\n                treated as a singleton list, and this module will normalize over the last dimension which is expected\n                to be of that specific size.\n            eps: A value added to the denominator for numerical stability. Default: 1e-5\n            elementwise_affine: A boolean value that when set to ``True``, this module has learnable per-element\n                affine parameters initialized to ones (for weights) and zeros (for biases). Default: ``True``.\n            bias: If set to ``False``, the layer will not learn an additive bias (only relevant if\n                ``elementwise_affine`` is ``True``). Default: ``True``.\n            device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n            dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n        \"\"\"\n        # Attributes:\n        # weight: the weights of the module. The values are initialized to 1.\n        # bias:   the bias of the module. The values are initialized to 0.\n        # weight_score: learnable scores for the weights. Has the same shape as weight. When applied\n        # to the default initial values of self.weight (i.e., all ones), this is equivalent to\n        # randomly dropping out certain features.\n        # bias_score: learnable scores for the bias. Has the same shape as bias. When applied to\n        # the default initial values of self.bias (i.e., all zeros), it does not have any actual\n        # effect. Thus, bias_score only influences training when MaskedLayerNorm is created\n        # from some pretrained nn.LayerNorm module whose bias is not all zeros.\n        super().__init__(\n            normalized_shape=normalized_shape,\n            eps=eps,\n            elementwise_affine=elementwise_affine,\n            bias=bias,\n            device=device,\n            dtype=dtype,\n        )\n        if self.elementwise_affine:\n            assert self.weight is not None\n            self.weight.requires_grad = False\n            self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n            if self.bias is not None:\n                self.bias.requires_grad = False\n                self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n            else:\n                self.register_parameter(\"bias_scores\", None)\n        else:\n            self.register_parameter(\"weight_scores\", None)\n            self.register_parameter(\"bias_scores\", None)\n\n    def forward(self, input: Tensor) -&gt; Tensor:\n        \"\"\"\n        Mapping function for the ``MaskedLayerNorm``.\n\n        Args:\n            input (Tensor): Tensor to be mapped by the layer.\n\n        Returns:\n            (Tensor): Output tensor after mapping of the input tensor.\n        \"\"\"\n        if not self.elementwise_affine:\n            return F.layer_norm(input, self.normalized_shape, self.weight, self.bias, self.eps)\n        assert self.weight is not None\n        weight_prob_scores = torch.sigmoid(self.weight_scores)\n        weight_mask = bernoulli_sample(weight_prob_scores)\n        masked_weight = weight_mask * self.weight\n        if self.bias is not None:\n            bias_prob_scores = torch.sigmoid(self.bias_scores)\n            bias_mask = bernoulli_sample(bias_prob_scores)\n            masked_bias = bias_mask * self.bias\n        else:\n            masked_bias = None\n        return F.layer_norm(input, self.normalized_shape, masked_weight, masked_bias, self.eps)\n\n    @classmethod\n    def from_pretrained(cls, layer_norm_module: nn.LayerNorm) -&gt; MaskedLayerNorm:\n        \"\"\"\n        Return an instance of ``MaskedLayerNorm`` whose weight and bias have the same values as those of\n        ``layer_norm_module``.\n\n        Args:\n            layer_norm_module (nn.LayerNorm): Target module to be converted\n\n        Returns:\n            (MaskedLayerNorm): New copy of the provided module with mask layers added to enable FedPM\n        \"\"\"\n        masked_layer_norm_module = cls(\n            # layer_norm_module.normalized_shape is a tuple so we\n            # simply transform it into torch.Size so it is compatible with\n            # the constructor's type signature.\n            normalized_shape=torch.Size(layer_norm_module.normalized_shape),\n            eps=layer_norm_module.eps,\n            elementwise_affine=layer_norm_module.elementwise_affine,\n            bias=(layer_norm_module.bias is not None),\n        )\n\n        if layer_norm_module.elementwise_affine:\n            assert layer_norm_module.weight is not None\n            masked_layer_norm_module.weight = Parameter(layer_norm_module.weight.clone().detach(), requires_grad=False)\n            masked_layer_norm_module.weight_scores = Parameter(\n                torch.randn_like(layer_norm_module.weight), requires_grad=True\n            )\n            if layer_norm_module.bias is not None:\n                masked_layer_norm_module.bias = Parameter(layer_norm_module.bias.clone().detach(), requires_grad=False)\n                masked_layer_norm_module.bias_scores = Parameter(\n                    torch.randn_like(layer_norm_module.bias), requires_grad=True\n                )\n\n        return masked_layer_norm_module\n</code></pre> <code></code> <code>__init__(normalized_shape, eps=1e-05, elementwise_affine=True, bias=True, device=None, dtype=None)</code> \u00b6 <p>Implementation of the masked Layer Normalization module. When <code>elementwise_affine</code> is True, <code>nn.LayerNorm</code> has a learnable weight and (optional) bias. For <code>MaskedLayerNorm</code>, the weight and bias do not receive gradient in back propagation. Instead, two score tensors - one for the weight and another for the bias - are maintained. In the forward pass, the score tensors are transformed by the Sigmoid function into probability scores, which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are applied to the weight and the bias. During training, gradients with respect to the score tensors are computed and used to update the score tensors.</p> <p>When <code>elementwise_affine</code> is False, <code>nn.LayerNorm</code> does not have weight or bias. Under this condition, both score tensors are None and <code>MaskedLayerNorm</code> acts in the same way as <code>nn.LayerNorm</code>.</p> <p>NOTE: The scores are not assumed to be bounded between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>normalized_shape</code> <code>TorchShape</code> <p>Input shape from an expected input. If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.</p> required <code>eps</code> <code>float</code> <p>A value added to the denominator for numerical stability. Default: 1e-5</p> <code>1e-05</code> <code>elementwise_affine</code> <code>bool</code> <p>A boolean value that when set to <code>True</code>, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: <code>True</code>.</p> <code>True</code> <code>bias</code> <code>bool</code> <p>If set to <code>False</code>, the layer will not learn an additive bias (only relevant if <code>elementwise_affine</code> is <code>True</code>). Default: <code>True</code>.</p> <code>True</code> <code>device</code> <code>device | None</code> <p>Device to which this module should be sent. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype | None</code> <p>Type of the tensors. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/model_bases/masked_layers/masked_normalization_layers.py</code> <pre><code>def __init__(\n    self,\n    normalized_shape: TorchShape,\n    eps: float = 1e-5,\n    elementwise_affine: bool = True,\n    bias: bool = True,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n) -&gt; None:\n    \"\"\"\n    Implementation of the masked Layer Normalization module. When ``elementwise_affine`` is True, ``nn.LayerNorm``\n    has a learnable weight and (optional) bias. For ``MaskedLayerNorm``, the weight and bias do not receive\n    gradient in back propagation. Instead, two score tensors - one for the weight and another for the bias - are\n    maintained. In the forward pass, the score tensors are transformed by the Sigmoid function into probability\n    scores, which are then used to produce binary masks via Bernoulli sampling. Finally, the binary masks are\n    applied to the weight and the bias. During training, gradients with respect to the score tensors are computed\n    and used to update the score tensors.\n\n    When ``elementwise_affine`` is False, ``nn.LayerNorm`` does not have weight or bias. Under this condition, both\n    score tensors are None and ``MaskedLayerNorm`` acts in the same way as ``nn.LayerNorm``.\n\n    **NOTE**: The scores are not assumed to be bounded between 0 and 1.\n\n    Args:\n        normalized_shape (TorchShape): Input shape from an expected input. If a single integer is used, it is\n            treated as a singleton list, and this module will normalize over the last dimension which is expected\n            to be of that specific size.\n        eps: A value added to the denominator for numerical stability. Default: 1e-5\n        elementwise_affine: A boolean value that when set to ``True``, this module has learnable per-element\n            affine parameters initialized to ones (for weights) and zeros (for biases). Default: ``True``.\n        bias: If set to ``False``, the layer will not learn an additive bias (only relevant if\n            ``elementwise_affine`` is ``True``). Default: ``True``.\n        device (torch.device | None, optional): Device to which this module should be sent. Defaults to None.\n        dtype (torch.dtype | None, optional): Type of the tensors. Defaults to None.\n    \"\"\"\n    # Attributes:\n    # weight: the weights of the module. The values are initialized to 1.\n    # bias:   the bias of the module. The values are initialized to 0.\n    # weight_score: learnable scores for the weights. Has the same shape as weight. When applied\n    # to the default initial values of self.weight (i.e., all ones), this is equivalent to\n    # randomly dropping out certain features.\n    # bias_score: learnable scores for the bias. Has the same shape as bias. When applied to\n    # the default initial values of self.bias (i.e., all zeros), it does not have any actual\n    # effect. Thus, bias_score only influences training when MaskedLayerNorm is created\n    # from some pretrained nn.LayerNorm module whose bias is not all zeros.\n    super().__init__(\n        normalized_shape=normalized_shape,\n        eps=eps,\n        elementwise_affine=elementwise_affine,\n        bias=bias,\n        device=device,\n        dtype=dtype,\n    )\n    if self.elementwise_affine:\n        assert self.weight is not None\n        self.weight.requires_grad = False\n        self.weight_scores = Parameter(torch.randn_like(self.weight), requires_grad=True)\n        if self.bias is not None:\n            self.bias.requires_grad = False\n            self.bias_scores = Parameter(torch.randn_like(self.bias), requires_grad=True)\n        else:\n            self.register_parameter(\"bias_scores\", None)\n    else:\n        self.register_parameter(\"weight_scores\", None)\n        self.register_parameter(\"bias_scores\", None)\n</code></pre> <code></code> <code>forward(input)</code> \u00b6 <p>Mapping function for the <code>MaskedLayerNorm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Tensor to be mapped by the layer.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output tensor after mapping of the input tensor.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_normalization_layers.py</code> <pre><code>def forward(self, input: Tensor) -&gt; Tensor:\n    \"\"\"\n    Mapping function for the ``MaskedLayerNorm``.\n\n    Args:\n        input (Tensor): Tensor to be mapped by the layer.\n\n    Returns:\n        (Tensor): Output tensor after mapping of the input tensor.\n    \"\"\"\n    if not self.elementwise_affine:\n        return F.layer_norm(input, self.normalized_shape, self.weight, self.bias, self.eps)\n    assert self.weight is not None\n    weight_prob_scores = torch.sigmoid(self.weight_scores)\n    weight_mask = bernoulli_sample(weight_prob_scores)\n    masked_weight = weight_mask * self.weight\n    if self.bias is not None:\n        bias_prob_scores = torch.sigmoid(self.bias_scores)\n        bias_mask = bernoulli_sample(bias_prob_scores)\n        masked_bias = bias_mask * self.bias\n    else:\n        masked_bias = None\n    return F.layer_norm(input, self.normalized_shape, masked_weight, masked_bias, self.eps)\n</code></pre> <code></code> <code>from_pretrained(layer_norm_module)</code> <code>classmethod</code> \u00b6 <p>Return an instance of <code>MaskedLayerNorm</code> whose weight and bias have the same values as those of <code>layer_norm_module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>layer_norm_module</code> <code>LayerNorm</code> <p>Target module to be converted</p> required <p>Returns:</p> Type Description <code>MaskedLayerNorm</code> <p>New copy of the provided module with mask layers added to enable FedPM</p> Source code in <code>fl4health/model_bases/masked_layers/masked_normalization_layers.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, layer_norm_module: nn.LayerNorm) -&gt; MaskedLayerNorm:\n    \"\"\"\n    Return an instance of ``MaskedLayerNorm`` whose weight and bias have the same values as those of\n    ``layer_norm_module``.\n\n    Args:\n        layer_norm_module (nn.LayerNorm): Target module to be converted\n\n    Returns:\n        (MaskedLayerNorm): New copy of the provided module with mask layers added to enable FedPM\n    \"\"\"\n    masked_layer_norm_module = cls(\n        # layer_norm_module.normalized_shape is a tuple so we\n        # simply transform it into torch.Size so it is compatible with\n        # the constructor's type signature.\n        normalized_shape=torch.Size(layer_norm_module.normalized_shape),\n        eps=layer_norm_module.eps,\n        elementwise_affine=layer_norm_module.elementwise_affine,\n        bias=(layer_norm_module.bias is not None),\n    )\n\n    if layer_norm_module.elementwise_affine:\n        assert layer_norm_module.weight is not None\n        masked_layer_norm_module.weight = Parameter(layer_norm_module.weight.clone().detach(), requires_grad=False)\n        masked_layer_norm_module.weight_scores = Parameter(\n            torch.randn_like(layer_norm_module.weight), requires_grad=True\n        )\n        if layer_norm_module.bias is not None:\n            masked_layer_norm_module.bias = Parameter(layer_norm_module.bias.clone().detach(), requires_grad=False)\n            masked_layer_norm_module.bias_scores = Parameter(\n                torch.randn_like(layer_norm_module.bias), requires_grad=True\n            )\n\n    return masked_layer_norm_module\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_normalization_layers.MaskedBatchNorm1d","title":"<code>MaskedBatchNorm1d</code>","text":"<p>               Bases: <code>_MaskedBatchNorm</code></p> <p>Applies (masked) Batch Normalization over a 2D or 3D input. Input shape should be <code>(N, C)</code> or <code>(N, C, L)</code>, where <code>N</code> is the batch size, <code>C</code> is the number of features/channels, and <code>L</code> is the sequence length.</p> Source code in <code>fl4health/model_bases/masked_layers/masked_normalization_layers.py</code> <pre><code>class MaskedBatchNorm1d(_MaskedBatchNorm):\n    \"\"\"\n    Applies (masked) Batch Normalization over a 2D or 3D input. Input shape should be ``(N, C)`` or ``(N, C, L)``,\n    where ``N`` is the batch size, ``C`` is the number of features/channels, and ``L`` is the sequence length.\n    \"\"\"\n\n    def _check_input_dim(self, input: Tensor) -&gt; None:\n        if input.dim() not in BATCH_NORM_1D_INPUT_LENGTHS:\n            raise ValueError(f\"expected 2D or 3D input (got {input.dim()}D input)\")\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_normalization_layers.MaskedBatchNorm2d","title":"<code>MaskedBatchNorm2d</code>","text":"<p>               Bases: <code>_MaskedBatchNorm</code></p> <p>Applies (masked) Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension).</p> Source code in <code>fl4health/model_bases/masked_layers/masked_normalization_layers.py</code> <pre><code>class MaskedBatchNorm2d(_MaskedBatchNorm):\n    \"\"\"\n    Applies (masked) Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel\n    dimension).\n    \"\"\"\n\n    def _check_input_dim(self, input: Tensor) -&gt; None:\n        if input.dim() != BATCH_NORM_2D_INPUT_LENGTH:\n            raise ValueError(f\"expected 4D input (got {input.dim()}D input)\")\n</code></pre>"},{"location":"api/#fl4health.model_bases.masked_layers.masked_normalization_layers.MaskedBatchNorm3d","title":"<code>MaskedBatchNorm3d</code>","text":"<p>               Bases: <code>_MaskedBatchNorm</code></p> <p>Applies (masked) Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension).</p> Source code in <code>fl4health/model_bases/masked_layers/masked_normalization_layers.py</code> <pre><code>class MaskedBatchNorm3d(_MaskedBatchNorm):\n    \"\"\"\n    Applies (masked) Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel\n    dimension).\n    \"\"\"\n\n    def _check_input_dim(self, input: Tensor) -&gt; None:\n        if input.dim() != BATCH_NORM_3D_INPUT_LENGTH:\n            raise ValueError(f\"expected 5D input (got {input.dim()}D input)\")\n</code></pre>"},{"location":"api/#fl4health.model_bases.moon_base","title":"<code>moon_base</code>","text":""},{"location":"api/#fl4health.model_bases.moon_base.MoonModel","title":"<code>MoonModel</code>","text":"<p>               Bases: <code>SequentiallySplitModel</code></p> Source code in <code>fl4health/model_bases/moon_base.py</code> <pre><code>class MoonModel(SequentiallySplitModel):\n    def __init__(\n        self, base_module: nn.Module, head_module: nn.Module, projection_module: nn.Module | None = None\n    ) -&gt; None:\n        \"\"\"\n        A MOON Model is a specific type of sequentially split model, where one may specify an optional projection\n        module to be used for feature manipulation. The model always stores the features produced by the base module\n        as they will be used in contrastive loss function calculations. These features are, also, always flattened to\n        be compatible with such losses.\n\n        Args:\n            base_module (nn.Module): Feature extractor component of the model\n            head_module (nn.Module): Classification (or other type) of head used by the model\n            projection_module (nn.Module | None, optional): An optional module for manipulating the features before\n                they are passed to the ``head_module``. Defaults to None.\n        \"\"\"\n        # Features are forced to be stored and flattened in this model, as it is expected to always be used with the\n        # contrastive loss function.\n        super().__init__(base_module, head_module, flatten_features=True)\n        self.projection_module = projection_module\n\n    def sequential_forward(self, input: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Overriding the sequential forward of the ``SequentiallySplitModel`` parent to allow for the injection of a\n        projection module into the forward pass. The remainder of the functionality stays the same. That is,\n        We run a forward pass using the sequentially split modules ``base_module`` -&gt; ``head_module``.\n\n        Args:\n            input (torch.Tensor): Input to the model forward pass. Expected to be of shape (``batch_size``, \\\\*)\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Returns the predictions and features tensor from the sequential\n                forward.\n        \"\"\"\n        x = self.base_module.forward(input)\n        # A projection module is optionally specified for MOON models. If no module is provided, it is simply skipped\n        features = self.projection_module.forward(x) if self.projection_module else x\n        predictions = self.head_module.forward(features)\n        return predictions, features\n</code></pre>"},{"location":"api/#fl4health.model_bases.moon_base.MoonModel.__init__","title":"<code>__init__(base_module, head_module, projection_module=None)</code>","text":"<p>A MOON Model is a specific type of sequentially split model, where one may specify an optional projection module to be used for feature manipulation. The model always stores the features produced by the base module as they will be used in contrastive loss function calculations. These features are, also, always flattened to be compatible with such losses.</p> <p>Parameters:</p> Name Type Description Default <code>base_module</code> <code>Module</code> <p>Feature extractor component of the model</p> required <code>head_module</code> <code>Module</code> <p>Classification (or other type) of head used by the model</p> required <code>projection_module</code> <code>Module | None</code> <p>An optional module for manipulating the features before they are passed to the <code>head_module</code>. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/model_bases/moon_base.py</code> <pre><code>def __init__(\n    self, base_module: nn.Module, head_module: nn.Module, projection_module: nn.Module | None = None\n) -&gt; None:\n    \"\"\"\n    A MOON Model is a specific type of sequentially split model, where one may specify an optional projection\n    module to be used for feature manipulation. The model always stores the features produced by the base module\n    as they will be used in contrastive loss function calculations. These features are, also, always flattened to\n    be compatible with such losses.\n\n    Args:\n        base_module (nn.Module): Feature extractor component of the model\n        head_module (nn.Module): Classification (or other type) of head used by the model\n        projection_module (nn.Module | None, optional): An optional module for manipulating the features before\n            they are passed to the ``head_module``. Defaults to None.\n    \"\"\"\n    # Features are forced to be stored and flattened in this model, as it is expected to always be used with the\n    # contrastive loss function.\n    super().__init__(base_module, head_module, flatten_features=True)\n    self.projection_module = projection_module\n</code></pre>"},{"location":"api/#fl4health.model_bases.moon_base.MoonModel.sequential_forward","title":"<code>sequential_forward(input)</code>","text":"<p>Overriding the sequential forward of the <code>SequentiallySplitModel</code> parent to allow for the injection of a projection module into the forward pass. The remainder of the functionality stays the same. That is, We run a forward pass using the sequentially split modules <code>base_module</code> -&gt; <code>head_module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input to the model forward pass. Expected to be of shape (<code>batch_size</code>, *)</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Returns the predictions and features tensor from the sequential forward.</p> Source code in <code>fl4health/model_bases/moon_base.py</code> <pre><code>def sequential_forward(self, input: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Overriding the sequential forward of the ``SequentiallySplitModel`` parent to allow for the injection of a\n    projection module into the forward pass. The remainder of the functionality stays the same. That is,\n    We run a forward pass using the sequentially split modules ``base_module`` -&gt; ``head_module``.\n\n    Args:\n        input (torch.Tensor): Input to the model forward pass. Expected to be of shape (``batch_size``, \\\\*)\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Returns the predictions and features tensor from the sequential\n            forward.\n    \"\"\"\n    x = self.base_module.forward(input)\n    # A projection module is optionally specified for MOON models. If no module is provided, it is simply skipped\n    features = self.projection_module.forward(x) if self.projection_module else x\n    predictions = self.head_module.forward(features)\n    return predictions, features\n</code></pre>"},{"location":"api/#fl4health.model_bases.parallel_split_models","title":"<code>parallel_split_models</code>","text":""},{"location":"api/#fl4health.model_bases.parallel_split_models.ParallelSplitHeadModule","title":"<code>ParallelSplitHeadModule</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> Source code in <code>fl4health/model_bases/parallel_split_models.py</code> <pre><code>class ParallelSplitHeadModule(nn.Module, ABC):\n    def __init__(self, mode: ParallelFeatureJoinMode) -&gt; None:\n        \"\"\"\n        This is a head module to be used as part of ``ParallelSplitModel`` type models. This module is responsible for\n        merging inputs from two parallel feature extractors and acting on those inputs to produce a prediction.\n\n        Args:\n            mode (ParallelFeatureJoinMode): This determines **HOW** the head module is meant to combine the features\n                produced by the extraction modules. Currently, there are two modes, concatenation or summation of the\n                inputs before producing a prediction.\n        \"\"\"\n        super().__init__()\n        self.mode = mode\n\n    @abstractmethod\n    def parallel_output_join(self, local_tensor: torch.Tensor, global_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Defines how the local and global feature tensors that are output by the preceding parallel feature extractors\n        are meant to be joined together when the ``self.mode`` is set to ``ParallelFeatureJoinMode.CONCATENATE``.\n\n        Args:\n            local_tensor (torch.Tensor): First tensor to be joined.\n            global_tensor (torch.Tensor): Second tensor to be joined.\n\n        Raises:\n            NotImplementedError: Any implementing child class must produce this method if it is to be used.\n\n        Returns:\n            (torch.Tensor): A single tensor with the two tensors joined together in some way.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def head_forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward function for the head module.\n\n        Args:\n            input_tensor (torch.Tensor): Input tensor to be mapped.\n\n        Raises:\n            NotImplementedError: Must be implemented by any child class.\n\n        Returns:\n            (torch.Tensor): Output of the head module from the given input.\n        \"\"\"\n        raise NotImplementedError\n\n    def forward(self, first_tensor: torch.Tensor, second_tensor: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward function for the head module of ``ParallelSplitModels``. The inputs (``first_tensor``,\n        ``second_tensor``) are concatenated or added together depending on the mode specified in ``self.mode``\n        The concatenation procedure is defined by ``parallel_output_join``. This concatenated or added together\n        tensor is then passed through the forward function of the head module.\n\n        Args:\n            first_tensor (torch.Tensor): Output from one parallel module.\n            second_tensor (torch.Tensor): Output from one parallel module.\n\n        Returns:\n            (torch.Tensor): Output from the head module.\n        \"\"\"\n        head_input = (\n            self.parallel_output_join(first_tensor, second_tensor)\n            if self.mode == ParallelFeatureJoinMode.CONCATENATE\n            else torch.add(first_tensor, second_tensor)\n        )\n        return self.head_forward(head_input)\n</code></pre>"},{"location":"api/#fl4health.model_bases.parallel_split_models.ParallelSplitHeadModule.__init__","title":"<code>__init__(mode)</code>","text":"<p>This is a head module to be used as part of <code>ParallelSplitModel</code> type models. This module is responsible for merging inputs from two parallel feature extractors and acting on those inputs to produce a prediction.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>ParallelFeatureJoinMode</code> <p>This determines HOW the head module is meant to combine the features produced by the extraction modules. Currently, there are two modes, concatenation or summation of the inputs before producing a prediction.</p> required Source code in <code>fl4health/model_bases/parallel_split_models.py</code> <pre><code>def __init__(self, mode: ParallelFeatureJoinMode) -&gt; None:\n    \"\"\"\n    This is a head module to be used as part of ``ParallelSplitModel`` type models. This module is responsible for\n    merging inputs from two parallel feature extractors and acting on those inputs to produce a prediction.\n\n    Args:\n        mode (ParallelFeatureJoinMode): This determines **HOW** the head module is meant to combine the features\n            produced by the extraction modules. Currently, there are two modes, concatenation or summation of the\n            inputs before producing a prediction.\n    \"\"\"\n    super().__init__()\n    self.mode = mode\n</code></pre>"},{"location":"api/#fl4health.model_bases.parallel_split_models.ParallelSplitHeadModule.parallel_output_join","title":"<code>parallel_output_join(local_tensor, global_tensor)</code>  <code>abstractmethod</code>","text":"<p>Defines how the local and global feature tensors that are output by the preceding parallel feature extractors are meant to be joined together when the <code>self.mode</code> is set to <code>ParallelFeatureJoinMode.CONCATENATE</code>.</p> <p>Parameters:</p> Name Type Description Default <code>local_tensor</code> <code>Tensor</code> <p>First tensor to be joined.</p> required <code>global_tensor</code> <code>Tensor</code> <p>Second tensor to be joined.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Any implementing child class must produce this method if it is to be used.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>A single tensor with the two tensors joined together in some way.</p> Source code in <code>fl4health/model_bases/parallel_split_models.py</code> <pre><code>@abstractmethod\ndef parallel_output_join(self, local_tensor: torch.Tensor, global_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Defines how the local and global feature tensors that are output by the preceding parallel feature extractors\n    are meant to be joined together when the ``self.mode`` is set to ``ParallelFeatureJoinMode.CONCATENATE``.\n\n    Args:\n        local_tensor (torch.Tensor): First tensor to be joined.\n        global_tensor (torch.Tensor): Second tensor to be joined.\n\n    Raises:\n        NotImplementedError: Any implementing child class must produce this method if it is to be used.\n\n    Returns:\n        (torch.Tensor): A single tensor with the two tensors joined together in some way.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.model_bases.parallel_split_models.ParallelSplitHeadModule.head_forward","title":"<code>head_forward(input_tensor)</code>  <code>abstractmethod</code>","text":"<p>Forward function for the head module.</p> <p>Parameters:</p> Name Type Description Default <code>input_tensor</code> <code>Tensor</code> <p>Input tensor to be mapped.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Must be implemented by any child class.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Output of the head module from the given input.</p> Source code in <code>fl4health/model_bases/parallel_split_models.py</code> <pre><code>@abstractmethod\ndef head_forward(self, input_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward function for the head module.\n\n    Args:\n        input_tensor (torch.Tensor): Input tensor to be mapped.\n\n    Raises:\n        NotImplementedError: Must be implemented by any child class.\n\n    Returns:\n        (torch.Tensor): Output of the head module from the given input.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.model_bases.parallel_split_models.ParallelSplitHeadModule.forward","title":"<code>forward(first_tensor, second_tensor)</code>","text":"<p>Forward function for the head module of <code>ParallelSplitModels</code>. The inputs (<code>first_tensor</code>, <code>second_tensor</code>) are concatenated or added together depending on the mode specified in <code>self.mode</code> The concatenation procedure is defined by <code>parallel_output_join</code>. This concatenated or added together tensor is then passed through the forward function of the head module.</p> <p>Parameters:</p> Name Type Description Default <code>first_tensor</code> <code>Tensor</code> <p>Output from one parallel module.</p> required <code>second_tensor</code> <code>Tensor</code> <p>Output from one parallel module.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Output from the head module.</p> Source code in <code>fl4health/model_bases/parallel_split_models.py</code> <pre><code>def forward(self, first_tensor: torch.Tensor, second_tensor: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward function for the head module of ``ParallelSplitModels``. The inputs (``first_tensor``,\n    ``second_tensor``) are concatenated or added together depending on the mode specified in ``self.mode``\n    The concatenation procedure is defined by ``parallel_output_join``. This concatenated or added together\n    tensor is then passed through the forward function of the head module.\n\n    Args:\n        first_tensor (torch.Tensor): Output from one parallel module.\n        second_tensor (torch.Tensor): Output from one parallel module.\n\n    Returns:\n        (torch.Tensor): Output from the head module.\n    \"\"\"\n    head_input = (\n        self.parallel_output_join(first_tensor, second_tensor)\n        if self.mode == ParallelFeatureJoinMode.CONCATENATE\n        else torch.add(first_tensor, second_tensor)\n    )\n    return self.head_forward(head_input)\n</code></pre>"},{"location":"api/#fl4health.model_bases.parallel_split_models.ParallelSplitModel","title":"<code>ParallelSplitModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/model_bases/parallel_split_models.py</code> <pre><code>class ParallelSplitModel(nn.Module):\n    def __init__(\n        self,\n        first_feature_extractor: nn.Module,\n        second_feature_extractor: nn.Module,\n        model_head: ParallelSplitHeadModule,\n    ) -&gt; None:\n        \"\"\"\n        This defines a model that has been split into two parallel feature extractors. The outputs of these feature\n        extractors are merged together and mapped to a prediction by a ``ParallelSplitHeadModule``. By default, no\n        feature tensors are stored. Only a prediction tensor is produced.\n\n        Args:\n            first_feature_extractor (nn.Module): First parallel feature extractor.\n            second_feature_extractor (nn.Module): Second parallel feature extractor.\n            model_head (ParallelSplitHeadModule): Module responsible for taking the outputs of the two feature\n                extractors and using them to produce a prediction.\n        \"\"\"\n        super().__init__()\n        self.first_feature_extractor = first_feature_extractor\n        self.second_feature_extractor = second_feature_extractor\n        self.model_head = model_head\n\n    def forward(self, input: torch.Tensor) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n        \"\"\"\n        Composite forward function. The input tensor is first passed through the two parallel feature extractors and\n        then finally through the head model. The outputs and joining mechanism defined in the head model need to\n        be compatible with the head model input itself. This is left to the user to handle.\n\n        Args:\n            input (torch.Tensor): Input tensor to be passed through the set of forwards.\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): Prediction tensor from the head model. These\n                predictions are stored under the \"prediction\" key of the dictionary. The second feature dictionary is\n                empty by default.\n        \"\"\"\n        first_output = self.first_feature_extractor.forward(input)\n        second_output = self.second_feature_extractor.forward(input)\n        preds = {\"prediction\": self.model_head.forward(first_output, second_output)}\n        # No features are returned in the vanilla ParallelSplitModel implementation\n        return preds, {}\n</code></pre>"},{"location":"api/#fl4health.model_bases.parallel_split_models.ParallelSplitModel.__init__","title":"<code>__init__(first_feature_extractor, second_feature_extractor, model_head)</code>","text":"<p>This defines a model that has been split into two parallel feature extractors. The outputs of these feature extractors are merged together and mapped to a prediction by a <code>ParallelSplitHeadModule</code>. By default, no feature tensors are stored. Only a prediction tensor is produced.</p> <p>Parameters:</p> Name Type Description Default <code>first_feature_extractor</code> <code>Module</code> <p>First parallel feature extractor.</p> required <code>second_feature_extractor</code> <code>Module</code> <p>Second parallel feature extractor.</p> required <code>model_head</code> <code>ParallelSplitHeadModule</code> <p>Module responsible for taking the outputs of the two feature extractors and using them to produce a prediction.</p> required Source code in <code>fl4health/model_bases/parallel_split_models.py</code> <pre><code>def __init__(\n    self,\n    first_feature_extractor: nn.Module,\n    second_feature_extractor: nn.Module,\n    model_head: ParallelSplitHeadModule,\n) -&gt; None:\n    \"\"\"\n    This defines a model that has been split into two parallel feature extractors. The outputs of these feature\n    extractors are merged together and mapped to a prediction by a ``ParallelSplitHeadModule``. By default, no\n    feature tensors are stored. Only a prediction tensor is produced.\n\n    Args:\n        first_feature_extractor (nn.Module): First parallel feature extractor.\n        second_feature_extractor (nn.Module): Second parallel feature extractor.\n        model_head (ParallelSplitHeadModule): Module responsible for taking the outputs of the two feature\n            extractors and using them to produce a prediction.\n    \"\"\"\n    super().__init__()\n    self.first_feature_extractor = first_feature_extractor\n    self.second_feature_extractor = second_feature_extractor\n    self.model_head = model_head\n</code></pre>"},{"location":"api/#fl4health.model_bases.parallel_split_models.ParallelSplitModel.forward","title":"<code>forward(input)</code>","text":"<p>Composite forward function. The input tensor is first passed through the two parallel feature extractors and then finally through the head model. The outputs and joining mechanism defined in the head model need to be compatible with the head model input itself. This is left to the user to handle.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input tensor to be passed through the set of forwards.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>Prediction tensor from the head model. These predictions are stored under the \"prediction\" key of the dictionary. The second feature dictionary is empty by default.</p> Source code in <code>fl4health/model_bases/parallel_split_models.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n    \"\"\"\n    Composite forward function. The input tensor is first passed through the two parallel feature extractors and\n    then finally through the head model. The outputs and joining mechanism defined in the head model need to\n    be compatible with the head model input itself. This is left to the user to handle.\n\n    Args:\n        input (torch.Tensor): Input tensor to be passed through the set of forwards.\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): Prediction tensor from the head model. These\n            predictions are stored under the \"prediction\" key of the dictionary. The second feature dictionary is\n            empty by default.\n    \"\"\"\n    first_output = self.first_feature_extractor.forward(input)\n    second_output = self.second_feature_extractor.forward(input)\n    preds = {\"prediction\": self.model_head.forward(first_output, second_output)}\n    # No features are returned in the vanilla ParallelSplitModel implementation\n    return preds, {}\n</code></pre>"},{"location":"api/#fl4health.model_bases.pca","title":"<code>pca</code>","text":""},{"location":"api/#fl4health.model_bases.pca.PcaModule","title":"<code>PcaModule</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/model_bases/pca.py</code> <pre><code>class PcaModule(nn.Module):\n    def __init__(self, low_rank: bool = False, full_svd: bool = False, rank_estimation: int = 6) -&gt; None:\n        \"\"\"\n        PyTorch module for performing Principal Component Analysis.\n\n        Notes:\n        - If ``low_rank`` is set to True, then a value \\\\(q\\\\) for ``rank_estimation`` is required (either specified\n          by the user or via its default value). If \\\\(q\\\\) is too far away from the actual rank \\\\(k\\\\) of the\n          data matrix, then the resulting rank-q svd approximation is not guaranteed to be a good approximation of the\n          data matrix.\n        - If ``low_rank`` is set to True, then a value \\\\(q\\\\) for ``rank_estimation`` can be chosen according to the\n          following criteria:\n\n          - In general, \\\\(k \\\\leq q \\\\leq \\\\min(2\\\\cdot k, m, n)\\\\). For large low-rank matrices, take\n            \\\\(q = k + l\\\\), where \\\\(5 \\\\leq l \\\\leq 10\\\\).\n            If \\\\(k\\\\) is relatively small compared to \\\\(\\\\min(m, n)\\\\), choosing \\\\(l = 0, 1, 2\\\\) may be\n            sufficient.\n        - If ``low_rank`` is set to True and ``rank_estimation`` is set to \\\\(q\\\\), then the module will utilize a\n          randomized algorithm to compute a rank-q approximation of the data matrix via SVD.\n\n        For more details on this, see:\n\n        https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html\n\n        and\n\n        https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html\n\n        As per the official documentation of PyTorch, in general, the user should set ``low_rank`` to False. Setting\n        it to True would be useful for huge sparse matrices.\n\n        Args:\n            low_rank (bool, optional): Indicates whether the data matrix can be well-approximated by a low-rank\n                singular value decomposition. If the user has good reasons to believe so, then this parameter can be\n                set to True to allow for more efficient computations. Defaults to False.\n            full_svd (bool, optional): Indicates whether full SVD or reduced SVD is performed. If ``low_rank`` is set\n                to True, then an alternative implementation of SVD will be used and this argument is ignored.\n                Defaults to False.\n            rank_estimation (int, optional): A slight overestimation of the rank of the data matrix. Only used if\n                ``self.low_rank`` is True. Defaults to 6.\n        \"\"\"\n        super().__init__()\n        self.low_rank = low_rank\n        self.full_svd = full_svd\n        self.rank_estimation = rank_estimation\n        self.principal_components: Parameter\n        self.singular_values: Parameter\n        self.data_mean: Tensor\n\n    def forward(self, x: Tensor, center_data: bool) -&gt; tuple[Tensor, Tensor]:\n        \"\"\"\n        Perform PCA on the data matrix x by computing its SVD.\n\n        **NOTE**: the algorithm assumes that the rows of X are the data points (after reshaping as needed).\n        Consequently, the principal components, which are the eigenvectors of X.T @ X, are the right singular vectors\n        in the SVD of X.\n\n        Args:\n            x (Tensor): Data matrix.\n            center_data (bool): If true, then the data mean will be subtracted from all data points prior to\n                performing PCA. If ``center_data`` is false, it is expected that the data has already been centered\n                and an exception will be thrown if it is not.\n\n        Returns:\n            (tuple[Tensor, Tensor]): The principal components (i.e., right singular vectors) and their corresponding\n                singular values.\n        \"\"\"\n        x_prime = self.prepare_data_forward(x, center_data=center_data)\n        if self.low_rank:\n            log(INFO, \"Assuming data matrix is low rank, using low-rank PCA implementation.\")\n            m, n = x_prime.size(0), x_prime.size(1)\n            if self.rank_estimation &gt; m or self.rank_estimation &gt; n:\n                log(WARNING, \"Estimate of data rank given by user is larger than the actual rank.\")\n            q = min(self.rank_estimation, m, n)\n            _, singular_values, principal_components = torch.pca_lowrank(x_prime, q=q, center=False)\n        else:\n            if self.full_svd:\n                log(INFO, \"Performing full SVD on data matrix.\")\n            else:\n                log(INFO, \"Performing reduced SVD on data matrix.\")\n            _, singular_values, vh = torch.linalg.svd(x_prime, full_matrices=self.full_svd)\n            principal_components = vh.T\n        return principal_components, singular_values\n\n    def maybe_reshape(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Reshape input tensor ``X`` as needed so SVD can be computed. Reshaping is required when each data point is an\n        N-dimensional tensor because PCA requires ``X`` to be a 2D data matrix.\n\n        Args:\n            x (Tensor): Data matrix.\n\n        Returns:\n            (Tensor): Tensor flattened to be 2D.\n        \"\"\"\n        if len(x.size()) == TWO_D_TENSOR_SHAPE_LENGTH:\n            return torch.squeeze(x.float())\n        dim0 = x.size(0)\n        return torch.squeeze(x.view(dim0, -1).float())\n\n    def set_data_mean(self, x: Tensor) -&gt; None:\n        \"\"\"\n        The primary purpose of this method is to store the mean of the training data so it can be used to center\n        validation/test data later, if needed.\n\n        Args:\n            x (Tensor): Data matrix.\n        \"\"\"\n        self.data_mean = torch.mean(x, dim=0)\n\n    def center_data(self, x: Tensor) -&gt; Tensor:\n        assert self.data_mean is not None\n        return x - self.data_mean\n\n    def prepare_data_forward(self, x: Tensor, center_data: bool) -&gt; Tensor:\n        \"\"\"\n        Prepare input data ``X`` for PCA by reshaping and centering it as needed.\n\n        Args:\n            x (Tensor): Data matrix.\n            center_data (bool): If true, then the data mean will be subtracted from all data points prior to\n                performing PCA. If ``center_data`` is false, it is expected that the data has already been centered and\n                an exception will be thrown if it is not.\n\n        Returns:\n            (Tensor): Prepared data matrix.\n        \"\"\"\n        x = self.maybe_reshape(x)\n        if center_data:\n            self.set_data_mean(x)\n            return self.center_data(x)\n        # Check if the mean of X is already (nearly) zero.\n        # Throw an exception if it is not.\n        data_mean = torch.mean(x, dim=0)\n        assert torch.allclose(torch.zeros(data_mean.size()), data_mean, atol=1e-6)\n        return x\n\n    def project_lower_dim(self, x: Tensor, k: int | None = None, center_data: bool = False) -&gt; Tensor:\n        \"\"\"\n        Project input data ``X`` onto the top k principal components.\n\n        *NOTE**: The result of projection (after centering) is ``X @ U`` because this method assumes that the rows of\n        ``X`` are the data points while the columns of ``U`` are the principal components.\n\n        Args:\n            x (Tensor): Input data matrix whose rows are the data points.\n            k (int | None, optional): The number of principal components onto which projection is done. If ``k`` is\n                None, then all principal components will be used in the projection. Defaults to None.\n            center_data (bool, optional): If true, then the *training* data mean (learned in the forward pass)\n                will be subtracted from all data points prior to projection. If ``center_data`` is false, it is\n                expected that the data has already been centered in this manner by the user. Defaults to False.\n\n        Returns:\n            (Tensor): Projection result.\n        \"\"\"\n        x_prime = self.maybe_reshape(x)\n        if center_data:\n            x_prime = self.center_data(x)\n        if k:\n            return torch.matmul(x_prime, self.principal_components[:, :k])\n        return torch.matmul(x_prime, self.principal_components)\n\n    def project_back(self, x_lower_dim: Tensor, add_mean: bool = False) -&gt; Tensor:\n        \"\"\"\n        Project low-dimensional principal representations back into the original space to recover the reconstruction\n        of data points.\n\n        Args:\n            x_lower_dim (Tensor): Matrix whose rows are low-dimensional principal representations of the original data.\n            add_mean (bool, optional): Indicates whether the training data mean should be added to the projection\n                result. This can be set to True if the user centered the data prior to dimensionality reduction and\n                now wish to add back the data mean. Defaults to False.\n\n        Returns:\n            (Tensor): Reconstruction of data points.\n        \"\"\"\n        x_lower_dim_prime = self.maybe_reshape(x_lower_dim)\n        k = x_lower_dim.size(1)\n        if add_mean:\n            assert self.data_mean is not None\n            return torch.matmul(x_lower_dim_prime, self.principal_components[:, :k].T) + self.data_mean\n        return torch.matmul(x_lower_dim_prime, self.principal_components[:, :k].T)\n\n    def compute_reconstruction_error(self, x: Tensor, k: int | None, center_data: bool = False) -&gt; float:\n        \"\"\"\n        Compute the reconstruction error of X under PCA reconstruction.\n\n        More precisely, if ``X`` is an ``N`` by ``d`` data matrix whose *rows* are the data points,\n        and U is the matrix whose *columns* are the principal components of X, then the reconstruction\n        loss is defined as ``1 / N * | X @ U @ U.T - X| ** 2``.\n\n        **NOTE**: The reconstruction (after centering) is ``X @ U @ U.T`` because this method assumes that the rows\n        of ``X`` are the data points while the columns of U are the principal components.\n\n        Args:\n            x (Tensor): Input data tensor whose rows represent data points.\n            k (int | None): The number of principal components onto which projection is applied.\n            center_data (bool, optional): Indicates whether to subtract data mean prior to projecting the data into a\n                lower-dimensional subspace, and whether to add the data mean after projecting back. Defaults to False.\n\n        Returns:\n            (float): Reconstruction loss as defined above.\n        \"\"\"\n        n = x.size(0)\n        x_lower_dim = self.project_lower_dim(x, k, center_data=center_data)\n        reconstruction = self.project_back(x_lower_dim, add_mean=center_data)\n        return (torch.linalg.norm(reconstruction - x) ** 2).item() / n\n\n    def compute_projection_variance(self, x: Tensor, k: int | None, center_data: bool = False) -&gt; float:\n        \"\"\"\n        Compute the variance of the data matrix X after projection via PCA.\n\n        The variance is defined as ``| X @ U |_F ** 2``\n\n        Args:\n            x (Tensor): input data tensor whose rows represent data points.\n            k (int | None): the number of principal components onto which projection is applied.\n            center_data (bool, optional): Indicates whether to subtract data mean prior to projecting the data into a\n                lower-dimensional subspace, and whether to add the data mean after projecting back. Defaults to False.\n\n        Returns:\n            (float): Variance after projection as defined above.\n        \"\"\"\n        return (torch.linalg.norm(self.project_lower_dim(x, k, center_data)) ** 2).item()\n\n    def compute_cumulative_explained_variance(self) -&gt; float:\n        return torch.sum(self.singular_values**2).item()\n\n    def compute_explained_variance_ratios(self) -&gt; Tensor:\n        return (self.singular_values**2) / self.compute_cumulative_explained_variance()\n\n    def set_principal_components(self, principal_components: Tensor, singular_values: Tensor) -&gt; None:\n        self.principal_components = Parameter(data=principal_components, requires_grad=False)\n        self.singular_values = Parameter(data=singular_values, requires_grad=False)\n</code></pre>"},{"location":"api/#fl4health.model_bases.pca.PcaModule.__init__","title":"<code>__init__(low_rank=False, full_svd=False, rank_estimation=6)</code>","text":"<p>PyTorch module for performing Principal Component Analysis.</p> <p>Notes: - If <code>low_rank</code> is set to True, then a value \\(q\\) for <code>rank_estimation</code> is required (either specified   by the user or via its default value). If \\(q\\) is too far away from the actual rank \\(k\\) of the   data matrix, then the resulting rank-q svd approximation is not guaranteed to be a good approximation of the   data matrix. - If <code>low_rank</code> is set to True, then a value \\(q\\) for <code>rank_estimation</code> can be chosen according to the   following criteria:</p> <ul> <li>In general, \\(k \\leq q \\leq \\min(2\\cdot k, m, n)\\). For large low-rank matrices, take     \\(q = k + l\\), where \\(5 \\leq l \\leq 10\\).     If \\(k\\) is relatively small compared to \\(\\min(m, n)\\), choosing \\(l = 0, 1, 2\\) may be     sufficient.</li> <li>If <code>low_rank</code> is set to True and <code>rank_estimation</code> is set to \\(q\\), then the module will utilize a   randomized algorithm to compute a rank-q approximation of the data matrix via SVD.</li> </ul> <p>For more details on this, see:</p> <p>https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html</p> <p>and</p> <p>https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html</p> <p>As per the official documentation of PyTorch, in general, the user should set <code>low_rank</code> to False. Setting it to True would be useful for huge sparse matrices.</p> <p>Parameters:</p> Name Type Description Default <code>low_rank</code> <code>bool</code> <p>Indicates whether the data matrix can be well-approximated by a low-rank singular value decomposition. If the user has good reasons to believe so, then this parameter can be set to True to allow for more efficient computations. Defaults to False.</p> <code>False</code> <code>full_svd</code> <code>bool</code> <p>Indicates whether full SVD or reduced SVD is performed. If <code>low_rank</code> is set to True, then an alternative implementation of SVD will be used and this argument is ignored. Defaults to False.</p> <code>False</code> <code>rank_estimation</code> <code>int</code> <p>A slight overestimation of the rank of the data matrix. Only used if <code>self.low_rank</code> is True. Defaults to 6.</p> <code>6</code> Source code in <code>fl4health/model_bases/pca.py</code> <pre><code>def __init__(self, low_rank: bool = False, full_svd: bool = False, rank_estimation: int = 6) -&gt; None:\n    \"\"\"\n    PyTorch module for performing Principal Component Analysis.\n\n    Notes:\n    - If ``low_rank`` is set to True, then a value \\\\(q\\\\) for ``rank_estimation`` is required (either specified\n      by the user or via its default value). If \\\\(q\\\\) is too far away from the actual rank \\\\(k\\\\) of the\n      data matrix, then the resulting rank-q svd approximation is not guaranteed to be a good approximation of the\n      data matrix.\n    - If ``low_rank`` is set to True, then a value \\\\(q\\\\) for ``rank_estimation`` can be chosen according to the\n      following criteria:\n\n      - In general, \\\\(k \\\\leq q \\\\leq \\\\min(2\\\\cdot k, m, n)\\\\). For large low-rank matrices, take\n        \\\\(q = k + l\\\\), where \\\\(5 \\\\leq l \\\\leq 10\\\\).\n        If \\\\(k\\\\) is relatively small compared to \\\\(\\\\min(m, n)\\\\), choosing \\\\(l = 0, 1, 2\\\\) may be\n        sufficient.\n    - If ``low_rank`` is set to True and ``rank_estimation`` is set to \\\\(q\\\\), then the module will utilize a\n      randomized algorithm to compute a rank-q approximation of the data matrix via SVD.\n\n    For more details on this, see:\n\n    https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html\n\n    and\n\n    https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html\n\n    As per the official documentation of PyTorch, in general, the user should set ``low_rank`` to False. Setting\n    it to True would be useful for huge sparse matrices.\n\n    Args:\n        low_rank (bool, optional): Indicates whether the data matrix can be well-approximated by a low-rank\n            singular value decomposition. If the user has good reasons to believe so, then this parameter can be\n            set to True to allow for more efficient computations. Defaults to False.\n        full_svd (bool, optional): Indicates whether full SVD or reduced SVD is performed. If ``low_rank`` is set\n            to True, then an alternative implementation of SVD will be used and this argument is ignored.\n            Defaults to False.\n        rank_estimation (int, optional): A slight overestimation of the rank of the data matrix. Only used if\n            ``self.low_rank`` is True. Defaults to 6.\n    \"\"\"\n    super().__init__()\n    self.low_rank = low_rank\n    self.full_svd = full_svd\n    self.rank_estimation = rank_estimation\n    self.principal_components: Parameter\n    self.singular_values: Parameter\n    self.data_mean: Tensor\n</code></pre>"},{"location":"api/#fl4health.model_bases.pca.PcaModule.forward","title":"<code>forward(x, center_data)</code>","text":"<p>Perform PCA on the data matrix x by computing its SVD.</p> <p>NOTE: the algorithm assumes that the rows of X are the data points (after reshaping as needed). Consequently, the principal components, which are the eigenvectors of X.T @ X, are the right singular vectors in the SVD of X.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Data matrix.</p> required <code>center_data</code> <code>bool</code> <p>If true, then the data mean will be subtracted from all data points prior to performing PCA. If <code>center_data</code> is false, it is expected that the data has already been centered and an exception will be thrown if it is not.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>The principal components (i.e., right singular vectors) and their corresponding singular values.</p> Source code in <code>fl4health/model_bases/pca.py</code> <pre><code>def forward(self, x: Tensor, center_data: bool) -&gt; tuple[Tensor, Tensor]:\n    \"\"\"\n    Perform PCA on the data matrix x by computing its SVD.\n\n    **NOTE**: the algorithm assumes that the rows of X are the data points (after reshaping as needed).\n    Consequently, the principal components, which are the eigenvectors of X.T @ X, are the right singular vectors\n    in the SVD of X.\n\n    Args:\n        x (Tensor): Data matrix.\n        center_data (bool): If true, then the data mean will be subtracted from all data points prior to\n            performing PCA. If ``center_data`` is false, it is expected that the data has already been centered\n            and an exception will be thrown if it is not.\n\n    Returns:\n        (tuple[Tensor, Tensor]): The principal components (i.e., right singular vectors) and their corresponding\n            singular values.\n    \"\"\"\n    x_prime = self.prepare_data_forward(x, center_data=center_data)\n    if self.low_rank:\n        log(INFO, \"Assuming data matrix is low rank, using low-rank PCA implementation.\")\n        m, n = x_prime.size(0), x_prime.size(1)\n        if self.rank_estimation &gt; m or self.rank_estimation &gt; n:\n            log(WARNING, \"Estimate of data rank given by user is larger than the actual rank.\")\n        q = min(self.rank_estimation, m, n)\n        _, singular_values, principal_components = torch.pca_lowrank(x_prime, q=q, center=False)\n    else:\n        if self.full_svd:\n            log(INFO, \"Performing full SVD on data matrix.\")\n        else:\n            log(INFO, \"Performing reduced SVD on data matrix.\")\n        _, singular_values, vh = torch.linalg.svd(x_prime, full_matrices=self.full_svd)\n        principal_components = vh.T\n    return principal_components, singular_values\n</code></pre>"},{"location":"api/#fl4health.model_bases.pca.PcaModule.maybe_reshape","title":"<code>maybe_reshape(x)</code>","text":"<p>Reshape input tensor <code>X</code> as needed so SVD can be computed. Reshaping is required when each data point is an N-dimensional tensor because PCA requires <code>X</code> to be a 2D data matrix.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Data matrix.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor flattened to be 2D.</p> Source code in <code>fl4health/model_bases/pca.py</code> <pre><code>def maybe_reshape(self, x: Tensor) -&gt; Tensor:\n    \"\"\"\n    Reshape input tensor ``X`` as needed so SVD can be computed. Reshaping is required when each data point is an\n    N-dimensional tensor because PCA requires ``X`` to be a 2D data matrix.\n\n    Args:\n        x (Tensor): Data matrix.\n\n    Returns:\n        (Tensor): Tensor flattened to be 2D.\n    \"\"\"\n    if len(x.size()) == TWO_D_TENSOR_SHAPE_LENGTH:\n        return torch.squeeze(x.float())\n    dim0 = x.size(0)\n    return torch.squeeze(x.view(dim0, -1).float())\n</code></pre>"},{"location":"api/#fl4health.model_bases.pca.PcaModule.set_data_mean","title":"<code>set_data_mean(x)</code>","text":"<p>The primary purpose of this method is to store the mean of the training data so it can be used to center validation/test data later, if needed.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Data matrix.</p> required Source code in <code>fl4health/model_bases/pca.py</code> <pre><code>def set_data_mean(self, x: Tensor) -&gt; None:\n    \"\"\"\n    The primary purpose of this method is to store the mean of the training data so it can be used to center\n    validation/test data later, if needed.\n\n    Args:\n        x (Tensor): Data matrix.\n    \"\"\"\n    self.data_mean = torch.mean(x, dim=0)\n</code></pre>"},{"location":"api/#fl4health.model_bases.pca.PcaModule.prepare_data_forward","title":"<code>prepare_data_forward(x, center_data)</code>","text":"<p>Prepare input data <code>X</code> for PCA by reshaping and centering it as needed.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Data matrix.</p> required <code>center_data</code> <code>bool</code> <p>If true, then the data mean will be subtracted from all data points prior to performing PCA. If <code>center_data</code> is false, it is expected that the data has already been centered and an exception will be thrown if it is not.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Prepared data matrix.</p> Source code in <code>fl4health/model_bases/pca.py</code> <pre><code>def prepare_data_forward(self, x: Tensor, center_data: bool) -&gt; Tensor:\n    \"\"\"\n    Prepare input data ``X`` for PCA by reshaping and centering it as needed.\n\n    Args:\n        x (Tensor): Data matrix.\n        center_data (bool): If true, then the data mean will be subtracted from all data points prior to\n            performing PCA. If ``center_data`` is false, it is expected that the data has already been centered and\n            an exception will be thrown if it is not.\n\n    Returns:\n        (Tensor): Prepared data matrix.\n    \"\"\"\n    x = self.maybe_reshape(x)\n    if center_data:\n        self.set_data_mean(x)\n        return self.center_data(x)\n    # Check if the mean of X is already (nearly) zero.\n    # Throw an exception if it is not.\n    data_mean = torch.mean(x, dim=0)\n    assert torch.allclose(torch.zeros(data_mean.size()), data_mean, atol=1e-6)\n    return x\n</code></pre>"},{"location":"api/#fl4health.model_bases.pca.PcaModule.project_lower_dim","title":"<code>project_lower_dim(x, k=None, center_data=False)</code>","text":"<p>Project input data <code>X</code> onto the top k principal components.</p> <p>NOTE*: The result of projection (after centering) is <code>X @ U</code> because this method assumes that the rows of <code>X</code> are the data points while the columns of <code>U</code> are the principal components.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data matrix whose rows are the data points.</p> required <code>k</code> <code>int | None</code> <p>The number of principal components onto which projection is done. If <code>k</code> is None, then all principal components will be used in the projection. Defaults to None.</p> <code>None</code> <code>center_data</code> <code>bool</code> <p>If true, then the training data mean (learned in the forward pass) will be subtracted from all data points prior to projection. If <code>center_data</code> is false, it is expected that the data has already been centered in this manner by the user. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Projection result.</p> Source code in <code>fl4health/model_bases/pca.py</code> <pre><code>def project_lower_dim(self, x: Tensor, k: int | None = None, center_data: bool = False) -&gt; Tensor:\n    \"\"\"\n    Project input data ``X`` onto the top k principal components.\n\n    *NOTE**: The result of projection (after centering) is ``X @ U`` because this method assumes that the rows of\n    ``X`` are the data points while the columns of ``U`` are the principal components.\n\n    Args:\n        x (Tensor): Input data matrix whose rows are the data points.\n        k (int | None, optional): The number of principal components onto which projection is done. If ``k`` is\n            None, then all principal components will be used in the projection. Defaults to None.\n        center_data (bool, optional): If true, then the *training* data mean (learned in the forward pass)\n            will be subtracted from all data points prior to projection. If ``center_data`` is false, it is\n            expected that the data has already been centered in this manner by the user. Defaults to False.\n\n    Returns:\n        (Tensor): Projection result.\n    \"\"\"\n    x_prime = self.maybe_reshape(x)\n    if center_data:\n        x_prime = self.center_data(x)\n    if k:\n        return torch.matmul(x_prime, self.principal_components[:, :k])\n    return torch.matmul(x_prime, self.principal_components)\n</code></pre>"},{"location":"api/#fl4health.model_bases.pca.PcaModule.project_back","title":"<code>project_back(x_lower_dim, add_mean=False)</code>","text":"<p>Project low-dimensional principal representations back into the original space to recover the reconstruction of data points.</p> <p>Parameters:</p> Name Type Description Default <code>x_lower_dim</code> <code>Tensor</code> <p>Matrix whose rows are low-dimensional principal representations of the original data.</p> required <code>add_mean</code> <code>bool</code> <p>Indicates whether the training data mean should be added to the projection result. This can be set to True if the user centered the data prior to dimensionality reduction and now wish to add back the data mean. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Reconstruction of data points.</p> Source code in <code>fl4health/model_bases/pca.py</code> <pre><code>def project_back(self, x_lower_dim: Tensor, add_mean: bool = False) -&gt; Tensor:\n    \"\"\"\n    Project low-dimensional principal representations back into the original space to recover the reconstruction\n    of data points.\n\n    Args:\n        x_lower_dim (Tensor): Matrix whose rows are low-dimensional principal representations of the original data.\n        add_mean (bool, optional): Indicates whether the training data mean should be added to the projection\n            result. This can be set to True if the user centered the data prior to dimensionality reduction and\n            now wish to add back the data mean. Defaults to False.\n\n    Returns:\n        (Tensor): Reconstruction of data points.\n    \"\"\"\n    x_lower_dim_prime = self.maybe_reshape(x_lower_dim)\n    k = x_lower_dim.size(1)\n    if add_mean:\n        assert self.data_mean is not None\n        return torch.matmul(x_lower_dim_prime, self.principal_components[:, :k].T) + self.data_mean\n    return torch.matmul(x_lower_dim_prime, self.principal_components[:, :k].T)\n</code></pre>"},{"location":"api/#fl4health.model_bases.pca.PcaModule.compute_reconstruction_error","title":"<code>compute_reconstruction_error(x, k, center_data=False)</code>","text":"<p>Compute the reconstruction error of X under PCA reconstruction.</p> <p>More precisely, if <code>X</code> is an <code>N</code> by <code>d</code> data matrix whose rows are the data points, and U is the matrix whose columns are the principal components of X, then the reconstruction loss is defined as <code>1 / N * | X @ U @ U.T - X| ** 2</code>.</p> <p>NOTE: The reconstruction (after centering) is <code>X @ U @ U.T</code> because this method assumes that the rows of <code>X</code> are the data points while the columns of U are the principal components.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data tensor whose rows represent data points.</p> required <code>k</code> <code>int | None</code> <p>The number of principal components onto which projection is applied.</p> required <code>center_data</code> <code>bool</code> <p>Indicates whether to subtract data mean prior to projecting the data into a lower-dimensional subspace, and whether to add the data mean after projecting back. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>Reconstruction loss as defined above.</p> Source code in <code>fl4health/model_bases/pca.py</code> <pre><code>def compute_reconstruction_error(self, x: Tensor, k: int | None, center_data: bool = False) -&gt; float:\n    \"\"\"\n    Compute the reconstruction error of X under PCA reconstruction.\n\n    More precisely, if ``X`` is an ``N`` by ``d`` data matrix whose *rows* are the data points,\n    and U is the matrix whose *columns* are the principal components of X, then the reconstruction\n    loss is defined as ``1 / N * | X @ U @ U.T - X| ** 2``.\n\n    **NOTE**: The reconstruction (after centering) is ``X @ U @ U.T`` because this method assumes that the rows\n    of ``X`` are the data points while the columns of U are the principal components.\n\n    Args:\n        x (Tensor): Input data tensor whose rows represent data points.\n        k (int | None): The number of principal components onto which projection is applied.\n        center_data (bool, optional): Indicates whether to subtract data mean prior to projecting the data into a\n            lower-dimensional subspace, and whether to add the data mean after projecting back. Defaults to False.\n\n    Returns:\n        (float): Reconstruction loss as defined above.\n    \"\"\"\n    n = x.size(0)\n    x_lower_dim = self.project_lower_dim(x, k, center_data=center_data)\n    reconstruction = self.project_back(x_lower_dim, add_mean=center_data)\n    return (torch.linalg.norm(reconstruction - x) ** 2).item() / n\n</code></pre>"},{"location":"api/#fl4health.model_bases.pca.PcaModule.compute_projection_variance","title":"<code>compute_projection_variance(x, k, center_data=False)</code>","text":"<p>Compute the variance of the data matrix X after projection via PCA.</p> <p>The variance is defined as <code>| X @ U |_F ** 2</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input data tensor whose rows represent data points.</p> required <code>k</code> <code>int | None</code> <p>the number of principal components onto which projection is applied.</p> required <code>center_data</code> <code>bool</code> <p>Indicates whether to subtract data mean prior to projecting the data into a lower-dimensional subspace, and whether to add the data mean after projecting back. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>Variance after projection as defined above.</p> Source code in <code>fl4health/model_bases/pca.py</code> <pre><code>def compute_projection_variance(self, x: Tensor, k: int | None, center_data: bool = False) -&gt; float:\n    \"\"\"\n    Compute the variance of the data matrix X after projection via PCA.\n\n    The variance is defined as ``| X @ U |_F ** 2``\n\n    Args:\n        x (Tensor): input data tensor whose rows represent data points.\n        k (int | None): the number of principal components onto which projection is applied.\n        center_data (bool, optional): Indicates whether to subtract data mean prior to projecting the data into a\n            lower-dimensional subspace, and whether to add the data mean after projecting back. Defaults to False.\n\n    Returns:\n        (float): Variance after projection as defined above.\n    \"\"\"\n    return (torch.linalg.norm(self.project_lower_dim(x, k, center_data)) ** 2).item()\n</code></pre>"},{"location":"api/#fl4health.model_bases.perfcl_base","title":"<code>perfcl_base</code>","text":""},{"location":"api/#fl4health.model_bases.perfcl_base.PerFclModel","title":"<code>PerFclModel</code>","text":"<p>               Bases: <code>PartialLayerExchangeModel</code>, <code>ParallelSplitModel</code></p> Source code in <code>fl4health/model_bases/perfcl_base.py</code> <pre><code>class PerFclModel(PartialLayerExchangeModel, ParallelSplitModel):\n    def __init__(self, local_module: nn.Module, global_module: nn.Module, model_head: ParallelSplitHeadModule) -&gt; None:\n        \"\"\"\n        Model to be used by PerFCL clients to train models with the PerFCL approach. These models are of type\n        ``ParallelSplitModel`` and have distinct feature extractors. One of the feature extractors is exchanged with\n        the server and aggregated while the other remains local. Each of the extractors produces latent features which\n        are flattened and stored with the keys \"local_features\" and \"global_features\" along with the predictions.\n\n        Args:\n            local_module (nn.Module): Feature extraction module that is **NOT** exchanged with the server.\n            global_module (nn.Module): Feature extraction module that is exchanged with the server and aggregated with\n                other client modules.\n            model_head (ParallelSplitHeadModule): The model head that takes the output features from both the local\n                and global modules to produce a prediction.\n        \"\"\"\n        ParallelSplitModel.__init__(\n            self, first_feature_extractor=local_module, second_feature_extractor=global_module, model_head=model_head\n        )\n\n    def layers_to_exchange(self) -&gt; list[str]:\n        \"\"\"\n        Fixes the set of layers to be exchanged with a server. The ``second_feature_extractor`` is assumed to be the\n        **GLOBAL** feature extractor for the PerFCL model.\n\n        Returns:\n            (list[str]): List of layers associated with the global model (``second_feature_extractor``) corresponding\n                to keys in the state dictionary.\n        \"\"\"\n        return [layer_name for layer_name in self.state_dict() if layer_name.startswith(\"second_feature_extractor.\")]\n\n    def forward(self, input: torch.Tensor) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n        \"\"\"\n        Mapping input through the PerFCL model local and global feature extractors and the classification head.\n\n        Args:\n            input (torch.Tensor): input is expected to be of shape (``batch_size``, \\\\*)\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): Tuple of predictions and feature maps. PerFCL\n                predictions are simply stored under the key \"prediction.\" The features for the local and global feature\n                extraction modules are stored under keys \"local_features\" and \"global_features,\" respectively.\n        \"\"\"\n        local_output = self.first_feature_extractor.forward(input)\n        global_output = self.second_feature_extractor.forward(input)\n        preds = {\"prediction\": self.model_head.forward(local_output, global_output)}\n        # PerFCL models always store features from the feature extractors\n        features = {\n            \"local_features\": local_output.reshape(len(local_output), -1),\n            \"global_features\": global_output.reshape(len(global_output), -1),\n        }\n        return preds, features\n</code></pre>"},{"location":"api/#fl4health.model_bases.perfcl_base.PerFclModel.__init__","title":"<code>__init__(local_module, global_module, model_head)</code>","text":"<p>Model to be used by PerFCL clients to train models with the PerFCL approach. These models are of type <code>ParallelSplitModel</code> and have distinct feature extractors. One of the feature extractors is exchanged with the server and aggregated while the other remains local. Each of the extractors produces latent features which are flattened and stored with the keys \"local_features\" and \"global_features\" along with the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>local_module</code> <code>Module</code> <p>Feature extraction module that is NOT exchanged with the server.</p> required <code>global_module</code> <code>Module</code> <p>Feature extraction module that is exchanged with the server and aggregated with other client modules.</p> required <code>model_head</code> <code>ParallelSplitHeadModule</code> <p>The model head that takes the output features from both the local and global modules to produce a prediction.</p> required Source code in <code>fl4health/model_bases/perfcl_base.py</code> <pre><code>def __init__(self, local_module: nn.Module, global_module: nn.Module, model_head: ParallelSplitHeadModule) -&gt; None:\n    \"\"\"\n    Model to be used by PerFCL clients to train models with the PerFCL approach. These models are of type\n    ``ParallelSplitModel`` and have distinct feature extractors. One of the feature extractors is exchanged with\n    the server and aggregated while the other remains local. Each of the extractors produces latent features which\n    are flattened and stored with the keys \"local_features\" and \"global_features\" along with the predictions.\n\n    Args:\n        local_module (nn.Module): Feature extraction module that is **NOT** exchanged with the server.\n        global_module (nn.Module): Feature extraction module that is exchanged with the server and aggregated with\n            other client modules.\n        model_head (ParallelSplitHeadModule): The model head that takes the output features from both the local\n            and global modules to produce a prediction.\n    \"\"\"\n    ParallelSplitModel.__init__(\n        self, first_feature_extractor=local_module, second_feature_extractor=global_module, model_head=model_head\n    )\n</code></pre>"},{"location":"api/#fl4health.model_bases.perfcl_base.PerFclModel.layers_to_exchange","title":"<code>layers_to_exchange()</code>","text":"<p>Fixes the set of layers to be exchanged with a server. The <code>second_feature_extractor</code> is assumed to be the GLOBAL feature extractor for the PerFCL model.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of layers associated with the global model (<code>second_feature_extractor</code>) corresponding to keys in the state dictionary.</p> Source code in <code>fl4health/model_bases/perfcl_base.py</code> <pre><code>def layers_to_exchange(self) -&gt; list[str]:\n    \"\"\"\n    Fixes the set of layers to be exchanged with a server. The ``second_feature_extractor`` is assumed to be the\n    **GLOBAL** feature extractor for the PerFCL model.\n\n    Returns:\n        (list[str]): List of layers associated with the global model (``second_feature_extractor``) corresponding\n            to keys in the state dictionary.\n    \"\"\"\n    return [layer_name for layer_name in self.state_dict() if layer_name.startswith(\"second_feature_extractor.\")]\n</code></pre>"},{"location":"api/#fl4health.model_bases.perfcl_base.PerFclModel.forward","title":"<code>forward(input)</code>","text":"<p>Mapping input through the PerFCL model local and global feature extractors and the classification head.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>input is expected to be of shape (<code>batch_size</code>, *)</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>Tuple of predictions and feature maps. PerFCL predictions are simply stored under the key \"prediction.\" The features for the local and global feature extraction modules are stored under keys \"local_features\" and \"global_features,\" respectively.</p> Source code in <code>fl4health/model_bases/perfcl_base.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n    \"\"\"\n    Mapping input through the PerFCL model local and global feature extractors and the classification head.\n\n    Args:\n        input (torch.Tensor): input is expected to be of shape (``batch_size``, \\\\*)\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): Tuple of predictions and feature maps. PerFCL\n            predictions are simply stored under the key \"prediction.\" The features for the local and global feature\n            extraction modules are stored under keys \"local_features\" and \"global_features,\" respectively.\n    \"\"\"\n    local_output = self.first_feature_extractor.forward(input)\n    global_output = self.second_feature_extractor.forward(input)\n    preds = {\"prediction\": self.model_head.forward(local_output, global_output)}\n    # PerFCL models always store features from the feature extractors\n    features = {\n        \"local_features\": local_output.reshape(len(local_output), -1),\n        \"global_features\": global_output.reshape(len(global_output), -1),\n    }\n    return preds, features\n</code></pre>"},{"location":"api/#fl4health.model_bases.sequential_split_models","title":"<code>sequential_split_models</code>","text":""},{"location":"api/#fl4health.model_bases.sequential_split_models.SequentiallySplitModel","title":"<code>SequentiallySplitModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>fl4health/model_bases/sequential_split_models.py</code> <pre><code>class SequentiallySplitModel(nn.Module):\n    def __init__(self, base_module: nn.Module, head_module: nn.Module, flatten_features: bool = False) -&gt; None:\n        \"\"\"\n        These models are split into two sequential stages. The first is ``base_module``, used as a feature extractor.\n        The second is the ``head_module``, used as a classifier. Features are extracted from the ``base_module`` and\n        stored for later use, if required.\n\n        Args:\n            base_module (nn.Module): Feature extraction module.\n            head_module (nn.Module): Classification (or other type) of head that acts on the output from the base\n                module.\n            flatten_features (bool, optional): Whether the feature tensor shapes are to be preserved (false) or if\n                they should be flattened to be of shape (``batch_size``, -1). Flattening may be necessary when using\n                certain loss functions, as in MOON, for example. Defaults to False.\n        \"\"\"\n        super().__init__()\n        self.base_module = base_module\n        self.head_module = head_module\n        self.flatten_features = flatten_features\n\n    def _flatten_features(self, features: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        The features tensor is flattened to be of shape are flattened to be of shape (``batch_size``, -1). It is\n        expected that the feature tensor is **BATCH FIRST**.\n\n        Args:\n            features (torch.Tensor): Features tensor to be flattened. It is assumed that this tensor is\n                **BATCH FIRST.**\n\n        Returns:\n            (torch.Tensor): Flattened feature tensor of shape (``batch_size``, -1)\n        \"\"\"\n        return features.reshape(len(features), -1)\n\n    def sequential_forward(self, input: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Run a forward pass using the sequentially split modules ``base_module`` -&gt; ``head_module``.\n\n        Args:\n            input (torch.Tensor): Input to the model forward pass. Expected to be of shape (``batch_size``, \\\\*)\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Returns the predictions and features tensor from the sequential\n                forward.\n        \"\"\"\n        features = self.base_module.forward(input)\n        predictions = self.head_module.forward(features)\n        return predictions, features\n\n    def features_forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Run a forward pass using the ``base_module`` only, returning the features extracted from it.\n\n        Args:\n            input (torch.Tensor): Input to the model forward pass. Expected to be of shape (``batch_size``, \\\\*)\n\n        Returns:\n            (torch.Tensor): Returns the potentially flatten features tensor from the base module.\n        \"\"\"\n        features = self.base_module.forward(input)\n\n        return self._flatten_features(features) if self.flatten_features else features\n\n    def forward(self, input: torch.Tensor) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n        \"\"\"\n        Run a forward pass using the sequentially split modules ``base_module`` -&gt; ``head_module``. Features from the\n        ``base_module`` are stored either in their original shapes are flattened to be of shape (``batch_size``, -1)\n        depending on ``self.flatten_features``.\n\n        Args:\n            input (torch.Tensor): Input to the model forward pass. Expected to be of shape (``batch_size``, \\\\*)\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): Return the prediction dictionary and a features\n                dictionaries representing the output of the ``base_module`` either in the standard tensor shape or\n                flattened,  to be compatible, for example, with MOON contrastive losses.\n        \"\"\"\n        predictions, features = self.sequential_forward(input)\n        predictions_dict = {\"prediction\": predictions}\n        features_dict = (\n            {\"features\": self._flatten_features(features)} if self.flatten_features else {\"features\": features}\n        )\n        return predictions_dict, features_dict\n</code></pre>"},{"location":"api/#fl4health.model_bases.sequential_split_models.SequentiallySplitModel.__init__","title":"<code>__init__(base_module, head_module, flatten_features=False)</code>","text":"<p>These models are split into two sequential stages. The first is <code>base_module</code>, used as a feature extractor. The second is the <code>head_module</code>, used as a classifier. Features are extracted from the <code>base_module</code> and stored for later use, if required.</p> <p>Parameters:</p> Name Type Description Default <code>base_module</code> <code>Module</code> <p>Feature extraction module.</p> required <code>head_module</code> <code>Module</code> <p>Classification (or other type) of head that acts on the output from the base module.</p> required <code>flatten_features</code> <code>bool</code> <p>Whether the feature tensor shapes are to be preserved (false) or if they should be flattened to be of shape (<code>batch_size</code>, -1). Flattening may be necessary when using certain loss functions, as in MOON, for example. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/model_bases/sequential_split_models.py</code> <pre><code>def __init__(self, base_module: nn.Module, head_module: nn.Module, flatten_features: bool = False) -&gt; None:\n    \"\"\"\n    These models are split into two sequential stages. The first is ``base_module``, used as a feature extractor.\n    The second is the ``head_module``, used as a classifier. Features are extracted from the ``base_module`` and\n    stored for later use, if required.\n\n    Args:\n        base_module (nn.Module): Feature extraction module.\n        head_module (nn.Module): Classification (or other type) of head that acts on the output from the base\n            module.\n        flatten_features (bool, optional): Whether the feature tensor shapes are to be preserved (false) or if\n            they should be flattened to be of shape (``batch_size``, -1). Flattening may be necessary when using\n            certain loss functions, as in MOON, for example. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self.base_module = base_module\n    self.head_module = head_module\n    self.flatten_features = flatten_features\n</code></pre>"},{"location":"api/#fl4health.model_bases.sequential_split_models.SequentiallySplitModel.sequential_forward","title":"<code>sequential_forward(input)</code>","text":"<p>Run a forward pass using the sequentially split modules <code>base_module</code> -&gt; <code>head_module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input to the model forward pass. Expected to be of shape (<code>batch_size</code>, *)</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Returns the predictions and features tensor from the sequential forward.</p> Source code in <code>fl4health/model_bases/sequential_split_models.py</code> <pre><code>def sequential_forward(self, input: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Run a forward pass using the sequentially split modules ``base_module`` -&gt; ``head_module``.\n\n    Args:\n        input (torch.Tensor): Input to the model forward pass. Expected to be of shape (``batch_size``, \\\\*)\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Returns the predictions and features tensor from the sequential\n            forward.\n    \"\"\"\n    features = self.base_module.forward(input)\n    predictions = self.head_module.forward(features)\n    return predictions, features\n</code></pre>"},{"location":"api/#fl4health.model_bases.sequential_split_models.SequentiallySplitModel.features_forward","title":"<code>features_forward(input)</code>","text":"<p>Run a forward pass using the <code>base_module</code> only, returning the features extracted from it.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input to the model forward pass. Expected to be of shape (<code>batch_size</code>, *)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Returns the potentially flatten features tensor from the base module.</p> Source code in <code>fl4health/model_bases/sequential_split_models.py</code> <pre><code>def features_forward(self, input: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Run a forward pass using the ``base_module`` only, returning the features extracted from it.\n\n    Args:\n        input (torch.Tensor): Input to the model forward pass. Expected to be of shape (``batch_size``, \\\\*)\n\n    Returns:\n        (torch.Tensor): Returns the potentially flatten features tensor from the base module.\n    \"\"\"\n    features = self.base_module.forward(input)\n\n    return self._flatten_features(features) if self.flatten_features else features\n</code></pre>"},{"location":"api/#fl4health.model_bases.sequential_split_models.SequentiallySplitModel.forward","title":"<code>forward(input)</code>","text":"<p>Run a forward pass using the sequentially split modules <code>base_module</code> -&gt; <code>head_module</code>. Features from the <code>base_module</code> are stored either in their original shapes are flattened to be of shape (<code>batch_size</code>, -1) depending on <code>self.flatten_features</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Input to the model forward pass. Expected to be of shape (<code>batch_size</code>, *)</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], dict[str, Tensor]]</code> <p>Return the prediction dictionary and a features dictionaries representing the output of the <code>base_module</code> either in the standard tensor shape or flattened,  to be compatible, for example, with MOON contrastive losses.</p> Source code in <code>fl4health/model_bases/sequential_split_models.py</code> <pre><code>def forward(self, input: torch.Tensor) -&gt; tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n    \"\"\"\n    Run a forward pass using the sequentially split modules ``base_module`` -&gt; ``head_module``. Features from the\n    ``base_module`` are stored either in their original shapes are flattened to be of shape (``batch_size``, -1)\n    depending on ``self.flatten_features``.\n\n    Args:\n        input (torch.Tensor): Input to the model forward pass. Expected to be of shape (``batch_size``, \\\\*)\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]): Return the prediction dictionary and a features\n            dictionaries representing the output of the ``base_module`` either in the standard tensor shape or\n            flattened,  to be compatible, for example, with MOON contrastive losses.\n    \"\"\"\n    predictions, features = self.sequential_forward(input)\n    predictions_dict = {\"prediction\": predictions}\n    features_dict = (\n        {\"features\": self._flatten_features(features)} if self.flatten_features else {\"features\": features}\n    )\n    return predictions_dict, features_dict\n</code></pre>"},{"location":"api/#fl4health.model_bases.sequential_split_models.SequentiallySplitExchangeBaseModel","title":"<code>SequentiallySplitExchangeBaseModel</code>","text":"<p>               Bases: <code>SequentiallySplitModel</code>, <code>PartialLayerExchangeModel</code></p> <p>This model is a specific type of sequentially split model, where we specify the layers to be exchanged as being those belonging to the <code>base_module</code>.</p> Source code in <code>fl4health/model_bases/sequential_split_models.py</code> <pre><code>class SequentiallySplitExchangeBaseModel(SequentiallySplitModel, PartialLayerExchangeModel):\n    \"\"\"\n    This model is a specific type of sequentially split model, where we specify the layers to be exchanged as being\n    those belonging to the ``base_module``.\n    \"\"\"\n\n    def layers_to_exchange(self) -&gt; list[str]:\n        \"\"\"\n        Names of the layers of the model to be exchanged with the server. For these models, we only exchange layers\n        associated with the ``base_model``.\n\n        Returns:\n            (list[str]): The names of the layers to be exchanged with the server. This is used by the\n                ``FixedLayerExchanger`` class\n        \"\"\"\n        return [layer_name for layer_name in self.state_dict() if layer_name.startswith(\"base_module.\")]\n</code></pre>"},{"location":"api/#fl4health.model_bases.sequential_split_models.SequentiallySplitExchangeBaseModel.layers_to_exchange","title":"<code>layers_to_exchange()</code>","text":"<p>Names of the layers of the model to be exchanged with the server. For these models, we only exchange layers associated with the <code>base_model</code>.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>The names of the layers to be exchanged with the server. This is used by the <code>FixedLayerExchanger</code> class</p> Source code in <code>fl4health/model_bases/sequential_split_models.py</code> <pre><code>def layers_to_exchange(self) -&gt; list[str]:\n    \"\"\"\n    Names of the layers of the model to be exchanged with the server. For these models, we only exchange layers\n    associated with the ``base_model``.\n\n    Returns:\n        (list[str]): The names of the layers to be exchanged with the server. This is used by the\n            ``FixedLayerExchanger`` class\n    \"\"\"\n    return [layer_name for layer_name in self.state_dict() if layer_name.startswith(\"base_module.\")]\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange","title":"<code>parameter_exchange</code>","text":""},{"location":"api/#fl4health.parameter_exchange.fedpm_exchanger","title":"<code>fedpm_exchanger</code>","text":""},{"location":"api/#fl4health.parameter_exchange.fedpm_exchanger.FedPmExchanger","title":"<code>FedPmExchanger</code>","text":"<p>               Bases: <code>DynamicLayerExchanger</code></p> Source code in <code>fl4health/parameter_exchange/fedpm_exchanger.py</code> <pre><code>class FedPmExchanger(DynamicLayerExchanger):\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Exchanger specifically tailored to exchange parameters and other information between FedPM servers and\n        clients. FedPM has a special set of information that needs to be exchanged, which is handled by this class.\n        \"\"\"\n        super().__init__(select_scores_and_sample_masks)\n\n    def pull_parameters(self, parameters: NDArrays, model: nn.Module, config: Config | None = None) -&gt; None:\n        current_state = model.state_dict()\n        layer_params, layer_names = self.unpack_parameters(parameters)\n        for layer_name, layer_param in zip(layer_names, layer_params):\n            # Apply the inverse of the Sigmoid function\n            # since the scores for masked layers are supposed to be unbounded.\n            with torch.no_grad():\n                current_state[layer_name] = sigmoid_inverse(torch.tensor(layer_param))\n        model.load_state_dict(current_state, strict=True)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.fedpm_exchanger.FedPmExchanger.__init__","title":"<code>__init__()</code>","text":"<p>Exchanger specifically tailored to exchange parameters and other information between FedPM servers and clients. FedPM has a special set of information that needs to be exchanged, which is handled by this class.</p> Source code in <code>fl4health/parameter_exchange/fedpm_exchanger.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Exchanger specifically tailored to exchange parameters and other information between FedPM servers and\n    clients. FedPM has a special set of information that needs to be exchanged, which is handled by this class.\n    \"\"\"\n    super().__init__(select_scores_and_sample_masks)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.full_exchanger","title":"<code>full_exchanger</code>","text":""},{"location":"api/#fl4health.parameter_exchange.full_exchanger.FullParameterExchanger","title":"<code>FullParameterExchanger</code>","text":"<p>               Bases: <code>ParameterExchanger</code></p> Source code in <code>fl4health/parameter_exchange/full_exchanger.py</code> <pre><code>class FullParameterExchanger(ParameterExchanger):\n    def push_parameters(\n        self, model: nn.Module, initial_model: nn.Module | None = None, config: Config | None = None\n    ) -&gt; NDArrays:\n        \"\"\"\n        Sending all of parameters ordered by ``state_dict`` keys.\n\n        **NOTE**: Order matters, because it is relied upon by ``pull_parameters`` below.\n\n        Args:\n            model (nn.Module): Model containing the weights to be sent.\n            initial_model (nn.Module | None, optional): Not Used. Defaults to None.\n            config (Config | None, optional): Not Used. Defaults to None.\n\n        Returns:\n            (NDArrays): All parameters contained in the ``state_dict`` of the model parameter. The ``state_dict``\n                maintains a specific order.\n        \"\"\"\n        # Sending all of parameters ordered by state_dict keys\n        # NOTE: Order matters, because it is relied upon by pull_parameters below\n        return [val.cpu().numpy() for _, val in model.state_dict().items()]\n\n    def pull_parameters(self, parameters: NDArrays, model: nn.Module, config: Config | None = None) -&gt; None:\n        \"\"\"\n        Takes in a set of parameters in the form of ``NDArrays`` (list of numpy arrays) and injects them into the\n        provided model.\n\n        Assumes all model parameters are contained in parameters. The ``state_dict`` is reconstituted because\n        parameters is simply a list of arrays.\n\n        Args:\n            parameters (NDArrays): Parameter to inject into the provided model.\n            model (nn.Module): Model to inject the parameters into.\n            config (Config | None, optional): Not used. Defaults to None.\n        \"\"\"\n        params_dict = zip(model.state_dict().keys(), parameters)\n        state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n        model.load_state_dict(state_dict, strict=True)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.full_exchanger.FullParameterExchanger.push_parameters","title":"<code>push_parameters(model, initial_model=None, config=None)</code>","text":"<p>Sending all of parameters ordered by <code>state_dict</code> keys.</p> <p>NOTE: Order matters, because it is relied upon by <code>pull_parameters</code> below.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model containing the weights to be sent.</p> required <code>initial_model</code> <code>Module | None</code> <p>Not Used. Defaults to None.</p> <code>None</code> <code>config</code> <code>Config | None</code> <p>Not Used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>NDArrays</code> <p>All parameters contained in the <code>state_dict</code> of the model parameter. The <code>state_dict</code> maintains a specific order.</p> Source code in <code>fl4health/parameter_exchange/full_exchanger.py</code> <pre><code>def push_parameters(\n    self, model: nn.Module, initial_model: nn.Module | None = None, config: Config | None = None\n) -&gt; NDArrays:\n    \"\"\"\n    Sending all of parameters ordered by ``state_dict`` keys.\n\n    **NOTE**: Order matters, because it is relied upon by ``pull_parameters`` below.\n\n    Args:\n        model (nn.Module): Model containing the weights to be sent.\n        initial_model (nn.Module | None, optional): Not Used. Defaults to None.\n        config (Config | None, optional): Not Used. Defaults to None.\n\n    Returns:\n        (NDArrays): All parameters contained in the ``state_dict`` of the model parameter. The ``state_dict``\n            maintains a specific order.\n    \"\"\"\n    # Sending all of parameters ordered by state_dict keys\n    # NOTE: Order matters, because it is relied upon by pull_parameters below\n    return [val.cpu().numpy() for _, val in model.state_dict().items()]\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.full_exchanger.FullParameterExchanger.pull_parameters","title":"<code>pull_parameters(parameters, model, config=None)</code>","text":"<p>Takes in a set of parameters in the form of <code>NDArrays</code> (list of numpy arrays) and injects them into the provided model.</p> <p>Assumes all model parameters are contained in parameters. The <code>state_dict</code> is reconstituted because parameters is simply a list of arrays.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>NDArrays</code> <p>Parameter to inject into the provided model.</p> required <code>model</code> <code>Module</code> <p>Model to inject the parameters into.</p> required <code>config</code> <code>Config | None</code> <p>Not used. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/parameter_exchange/full_exchanger.py</code> <pre><code>def pull_parameters(self, parameters: NDArrays, model: nn.Module, config: Config | None = None) -&gt; None:\n    \"\"\"\n    Takes in a set of parameters in the form of ``NDArrays`` (list of numpy arrays) and injects them into the\n    provided model.\n\n    Assumes all model parameters are contained in parameters. The ``state_dict`` is reconstituted because\n    parameters is simply a list of arrays.\n\n    Args:\n        parameters (NDArrays): Parameter to inject into the provided model.\n        model (nn.Module): Model to inject the parameters into.\n        config (Config | None, optional): Not used. Defaults to None.\n    \"\"\"\n    params_dict = zip(model.state_dict().keys(), parameters)\n    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n    model.load_state_dict(state_dict, strict=True)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.layer_exchanger","title":"<code>layer_exchanger</code>","text":""},{"location":"api/#fl4health.parameter_exchange.layer_exchanger.FixedLayerExchanger","title":"<code>FixedLayerExchanger</code>","text":"<p>               Bases: <code>ParameterExchanger</code></p> Source code in <code>fl4health/parameter_exchange/layer_exchanger.py</code> <pre><code>class FixedLayerExchanger(ParameterExchanger):\n    def __init__(self, layers_to_transfer: list[str]) -&gt; None:\n        \"\"\"\n        Exchanger that only exchanges a static set of layers at each round of FL.\n\n        Args:\n            layers_to_transfer (list[str]): Names of the layers to be exchanged. These should correspond to the\n                names of the layers in the ``state_dict`` of the pytorch module.\n        \"\"\"\n        self.layers_to_transfer = layers_to_transfer\n\n    def apply_layer_filter(self, model: nn.Module) -&gt; NDArrays:\n        \"\"\"\n        Filter layers to the specific set of layers to be transferred using the ``layers_to_transfer`` property.\n\n        **NOTE**: Filtering layers only works if each client exchanges exactly the same layers\n\n        Args:\n            model (nn.Module): Model whose parameters are to be filtered then transferred.\n\n        Returns:\n            (NDArrays): Filter set of model parameters.\n        \"\"\"\n        model_state_dict = model.state_dict()\n        return [model_state_dict[layer_to_transfer].cpu().numpy() for layer_to_transfer in self.layers_to_transfer]\n\n    def push_parameters(\n        self, model: nn.Module, initial_model: nn.Module | None = None, config: Config | None = None\n    ) -&gt; NDArrays:\n        return self.apply_layer_filter(model)\n\n    def pull_parameters(self, parameters: NDArrays, model: nn.Module, config: Config | None = None) -&gt; None:\n        current_state = model.state_dict()\n        # update the correct layers to new parameters\n        for layer_name, layer_parameters in zip(self.layers_to_transfer, parameters):\n            current_state[layer_name] = torch.tensor(layer_parameters)\n        model.load_state_dict(current_state, strict=True)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.layer_exchanger.FixedLayerExchanger.__init__","title":"<code>__init__(layers_to_transfer)</code>","text":"<p>Exchanger that only exchanges a static set of layers at each round of FL.</p> <p>Parameters:</p> Name Type Description Default <code>layers_to_transfer</code> <code>list[str]</code> <p>Names of the layers to be exchanged. These should correspond to the names of the layers in the <code>state_dict</code> of the pytorch module.</p> required Source code in <code>fl4health/parameter_exchange/layer_exchanger.py</code> <pre><code>def __init__(self, layers_to_transfer: list[str]) -&gt; None:\n    \"\"\"\n    Exchanger that only exchanges a static set of layers at each round of FL.\n\n    Args:\n        layers_to_transfer (list[str]): Names of the layers to be exchanged. These should correspond to the\n            names of the layers in the ``state_dict`` of the pytorch module.\n    \"\"\"\n    self.layers_to_transfer = layers_to_transfer\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.layer_exchanger.FixedLayerExchanger.apply_layer_filter","title":"<code>apply_layer_filter(model)</code>","text":"<p>Filter layers to the specific set of layers to be transferred using the <code>layers_to_transfer</code> property.</p> <p>NOTE: Filtering layers only works if each client exchanges exactly the same layers</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model whose parameters are to be filtered then transferred.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Filter set of model parameters.</p> Source code in <code>fl4health/parameter_exchange/layer_exchanger.py</code> <pre><code>def apply_layer_filter(self, model: nn.Module) -&gt; NDArrays:\n    \"\"\"\n    Filter layers to the specific set of layers to be transferred using the ``layers_to_transfer`` property.\n\n    **NOTE**: Filtering layers only works if each client exchanges exactly the same layers\n\n    Args:\n        model (nn.Module): Model whose parameters are to be filtered then transferred.\n\n    Returns:\n        (NDArrays): Filter set of model parameters.\n    \"\"\"\n    model_state_dict = model.state_dict()\n    return [model_state_dict[layer_to_transfer].cpu().numpy() for layer_to_transfer in self.layers_to_transfer]\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.layer_exchanger.LayerExchangerWithExclusions","title":"<code>LayerExchangerWithExclusions</code>","text":"<p>               Bases: <code>ParameterExchanger</code></p> Source code in <code>fl4health/parameter_exchange/layer_exchanger.py</code> <pre><code>class LayerExchangerWithExclusions(ParameterExchanger):\n    def __init__(self, model: nn.Module, module_exclusions: Set[type[TorchModule]]) -&gt; None:\n        \"\"\"\n        This class implements exchanging all model layers except those matching a specified set of types. The\n        constructor is provided with the model in order to extract the proper layers to be exchanged based on the\n        exclusion criteria.\n\n        Args:\n            model (nn.Module): Model whose layers are to be exchanged.\n            module_exclusions (Set[type[TorchModule]]): modules within the model to **EXCLUDE** from exchange with the\n                server. It should correspond to a torch module or set of modules.\n        \"\"\"\n        # module_exclusion is a set of nn.Module types that should NOT be exchanged with the server.\n        # {nn.BatchNorm1d}\n        self.module_exclusions = module_exclusions\n        # In order to filter out all weights associated with a module, we run through all of the named modules and\n        # store the names of those that have a type matching the provided exclusions\n        self.modules_to_filter: Set[str] = {\n            # Note: Remove duplicate needs to be false in case modules have been tied together with shared objects.\n            name\n            for name, module in model.named_modules(remove_duplicate=False)\n            # We store only if the module should be exclude and name is not the empty string\n            if self.should_module_be_excluded(module) and name\n        }\n        # Needs to be an ordered collection to facilitate exchange consistency between server and client\n        # NOTE: Layers here refers to a collection of parameters in the state dictionary\n        self.layers_to_transfer: list[str] = self.get_layers_to_transfer(model)\n\n    def should_module_be_excluded(self, module: type[TorchModule]) -&gt; bool:\n        return type(module) in self.module_exclusions\n\n    def should_layer_be_excluded(self, layer_name: str) -&gt; bool:\n        # The model state_dict prefixes the weights and/or state associated with a named module with the name of that\n        # module and then an identifier for the specific parameters.\n        # Ex. named module: name: \"fc1\" module: nn.Linear(10, 10, bias=True)\n        # The state_dict has keys fc1.weight and fc1.bias with associated parameters\n        # We filter out any parameters prefixed with the name of an excluded module, as stored in modules_to_filter\n        return any(layer_name.startswith(module_to_filter) for module_to_filter in self.modules_to_filter)\n\n    def get_layers_to_transfer(self, model: nn.Module) -&gt; list[str]:\n        # We store the state dictionary keys that do not correspond to excluded modules as held in modules_to_filter\n        return [name for name in model.state_dict() if not self.should_layer_be_excluded(name)]\n\n    def apply_layer_filter(self, model: nn.Module) -&gt; NDArrays:\n        # NOTE: Filtering layers only works if each client exchanges exactly the same layers\n        model_state_dict = model.state_dict()\n        # The order of the parameters is determined by the order of layers to transfer, this ensures that they\n        # always have the same order, which can be relied upon in weight reconstruction done by pull_parameters\n        return [model_state_dict[layer_to_transfer].cpu().numpy() for layer_to_transfer in self.layers_to_transfer]\n\n    def push_parameters(\n        self, model: nn.Module, initial_model: nn.Module | None = None, config: Config | None = None\n    ) -&gt; NDArrays:\n        return self.apply_layer_filter(model)\n\n    def pull_parameters(self, parameters: NDArrays, model: nn.Module, config: Config | None = None) -&gt; None:\n        current_state = model.state_dict()\n        # update the correct layers to new parameters. Assumes order of parameters is the same as in push_parameters\n        for layer_name, layer_parameters in zip(self.layers_to_transfer, parameters):\n            current_state[layer_name] = torch.tensor(layer_parameters)\n        model.load_state_dict(current_state, strict=True)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.layer_exchanger.LayerExchangerWithExclusions.__init__","title":"<code>__init__(model, module_exclusions)</code>","text":"<p>This class implements exchanging all model layers except those matching a specified set of types. The constructor is provided with the model in order to extract the proper layers to be exchanged based on the exclusion criteria.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model whose layers are to be exchanged.</p> required <code>module_exclusions</code> <code>Set[type[TorchModule]]</code> <p>modules within the model to EXCLUDE from exchange with the server. It should correspond to a torch module or set of modules.</p> required Source code in <code>fl4health/parameter_exchange/layer_exchanger.py</code> <pre><code>def __init__(self, model: nn.Module, module_exclusions: Set[type[TorchModule]]) -&gt; None:\n    \"\"\"\n    This class implements exchanging all model layers except those matching a specified set of types. The\n    constructor is provided with the model in order to extract the proper layers to be exchanged based on the\n    exclusion criteria.\n\n    Args:\n        model (nn.Module): Model whose layers are to be exchanged.\n        module_exclusions (Set[type[TorchModule]]): modules within the model to **EXCLUDE** from exchange with the\n            server. It should correspond to a torch module or set of modules.\n    \"\"\"\n    # module_exclusion is a set of nn.Module types that should NOT be exchanged with the server.\n    # {nn.BatchNorm1d}\n    self.module_exclusions = module_exclusions\n    # In order to filter out all weights associated with a module, we run through all of the named modules and\n    # store the names of those that have a type matching the provided exclusions\n    self.modules_to_filter: Set[str] = {\n        # Note: Remove duplicate needs to be false in case modules have been tied together with shared objects.\n        name\n        for name, module in model.named_modules(remove_duplicate=False)\n        # We store only if the module should be exclude and name is not the empty string\n        if self.should_module_be_excluded(module) and name\n    }\n    # Needs to be an ordered collection to facilitate exchange consistency between server and client\n    # NOTE: Layers here refers to a collection of parameters in the state dictionary\n    self.layers_to_transfer: list[str] = self.get_layers_to_transfer(model)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.layer_exchanger.DynamicLayerExchanger","title":"<code>DynamicLayerExchanger</code>","text":"<p>               Bases: <code>PartialParameterExchanger[list[str]]</code></p> Source code in <code>fl4health/parameter_exchange/layer_exchanger.py</code> <pre><code>class DynamicLayerExchanger(PartialParameterExchanger[list[str]]):\n    def __init__(\n        self,\n        layer_selection_function: LayerSelectionFunction,\n    ) -&gt; None:\n        \"\"\"\n        This exchanger uses ``layer_selection_function`` to select a subset of a model's layers\n        at the end of each training round. Only the selected layers are exchanged with the server.\n\n        Args:\n            layer_selection_function (LayerSelectionFunction): Function responsible for selecting the layers to be\n                exchanged. This function relies on extra parameters such as norm threshold or exchange percentage,\n                but we assume that it has already been pre-constructed using the class\n                ``LayerSelectionFunctionConstructor``, so it only needs to take in two ``nn.Module`` objects as inputs.\n                For more details, please see the docstring of ``LayerSelectionFunctionConstructor``.\n        \"\"\"\n        self.layer_selection_function = layer_selection_function\n        self.parameter_packer = ParameterPackerWithLayerNames()\n\n    def select_parameters(\n        self, model: nn.Module, initial_model: nn.Module | None = None\n    ) -&gt; tuple[NDArrays, list[str]]:\n        return self.layer_selection_function(model, initial_model)\n\n    def push_parameters(\n        self, model: nn.Module, initial_model: nn.Module | None = None, config: Config | None = None\n    ) -&gt; NDArrays:\n        layers_to_transfer, layer_names = self.select_parameters(model, initial_model)\n        return self.pack_parameters(layers_to_transfer, layer_names)\n\n    def pull_parameters(self, parameters: NDArrays, model: nn.Module, config: Config | None = None) -&gt; None:\n        current_state = model.state_dict()\n        # update the correct layers to new parameters\n        layer_params, layer_names = self.unpack_parameters(parameters)\n        for layer_name, layer_param in zip(layer_names, layer_params):\n            current_state[layer_name] = torch.tensor(layer_param)\n        model.load_state_dict(current_state, strict=True)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.layer_exchanger.DynamicLayerExchanger.__init__","title":"<code>__init__(layer_selection_function)</code>","text":"<p>This exchanger uses <code>layer_selection_function</code> to select a subset of a model's layers at the end of each training round. Only the selected layers are exchanged with the server.</p> <p>Parameters:</p> Name Type Description Default <code>layer_selection_function</code> <code>LayerSelectionFunction</code> <p>Function responsible for selecting the layers to be exchanged. This function relies on extra parameters such as norm threshold or exchange percentage, but we assume that it has already been pre-constructed using the class <code>LayerSelectionFunctionConstructor</code>, so it only needs to take in two <code>nn.Module</code> objects as inputs. For more details, please see the docstring of <code>LayerSelectionFunctionConstructor</code>.</p> required Source code in <code>fl4health/parameter_exchange/layer_exchanger.py</code> <pre><code>def __init__(\n    self,\n    layer_selection_function: LayerSelectionFunction,\n) -&gt; None:\n    \"\"\"\n    This exchanger uses ``layer_selection_function`` to select a subset of a model's layers\n    at the end of each training round. Only the selected layers are exchanged with the server.\n\n    Args:\n        layer_selection_function (LayerSelectionFunction): Function responsible for selecting the layers to be\n            exchanged. This function relies on extra parameters such as norm threshold or exchange percentage,\n            but we assume that it has already been pre-constructed using the class\n            ``LayerSelectionFunctionConstructor``, so it only needs to take in two ``nn.Module`` objects as inputs.\n            For more details, please see the docstring of ``LayerSelectionFunctionConstructor``.\n    \"\"\"\n    self.layer_selection_function = layer_selection_function\n    self.parameter_packer = ParameterPackerWithLayerNames()\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.packing_exchanger","title":"<code>packing_exchanger</code>","text":""},{"location":"api/#fl4health.parameter_exchange.packing_exchanger.FullParameterExchangerWithPacking","title":"<code>FullParameterExchangerWithPacking</code>","text":"<p>               Bases: <code>FullParameterExchanger</code>, <code>Generic[T]</code></p> Source code in <code>fl4health/parameter_exchange/packing_exchanger.py</code> <pre><code>class FullParameterExchangerWithPacking(FullParameterExchanger, Generic[T]):\n    def __init__(self, parameter_packer: ParameterPacker[T]) -&gt; None:\n        \"\"\"\n        Parameter exchanger for when sending the entire set of model weights between the client and server with\n        potential side information packed in as well.\n\n        Args:\n            parameter_packer (ParameterPacker[T]): Parameter packer used to pack and unpack auxiliary information\n                alongside the model weights.\n        \"\"\"\n        super().__init__()\n        self.parameter_packer = parameter_packer\n\n    def pack_parameters(self, model_weights: NDArrays, additional_parameters: T) -&gt; NDArrays:\n        return self.parameter_packer.pack_parameters(model_weights, additional_parameters)\n\n    def unpack_parameters(self, packed_parameters: NDArrays) -&gt; tuple[NDArrays, T]:\n        return self.parameter_packer.unpack_parameters(packed_parameters)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.packing_exchanger.FullParameterExchangerWithPacking.__init__","title":"<code>__init__(parameter_packer)</code>","text":"<p>Parameter exchanger for when sending the entire set of model weights between the client and server with potential side information packed in as well.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_packer</code> <code>ParameterPacker[T]</code> <p>Parameter packer used to pack and unpack auxiliary information alongside the model weights.</p> required Source code in <code>fl4health/parameter_exchange/packing_exchanger.py</code> <pre><code>def __init__(self, parameter_packer: ParameterPacker[T]) -&gt; None:\n    \"\"\"\n    Parameter exchanger for when sending the entire set of model weights between the client and server with\n    potential side information packed in as well.\n\n    Args:\n        parameter_packer (ParameterPacker[T]): Parameter packer used to pack and unpack auxiliary information\n            alongside the model weights.\n    \"\"\"\n    super().__init__()\n    self.parameter_packer = parameter_packer\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.parameter_packer","title":"<code>parameter_packer</code>","text":""},{"location":"api/#fl4health.parameter_exchange.parameter_packer.ParameterPackerWithControlVariates","title":"<code>ParameterPackerWithControlVariates</code>","text":"<p>               Bases: <code>ParameterPacker[NDArrays]</code></p> Source code in <code>fl4health/parameter_exchange/parameter_packer.py</code> <pre><code>class ParameterPackerWithControlVariates(ParameterPacker[NDArrays]):\n    def __init__(self, size_of_model_params: int) -&gt; None:\n        \"\"\"\n        Class to handle the exchange of control variates for the SCAFFOLD FL method.\n\n        **NOTE** model params exchanged and control variates can be different sizes, for example, when layers are\n        frozen or the state dictionary contains things like Batch Normalization layers.\n\n        Args:\n            size_of_model_params (int): This is the number of layers that are associated with the parameters of the\n                model itself. This is used to split the covariates from the model parameters during unpacking.\n        \"\"\"\n        self.size_of_model_params = size_of_model_params\n        super().__init__()\n\n    def pack_parameters(self, model_weights: NDArrays, additional_parameters: NDArrays) -&gt; NDArrays:\n        return model_weights + additional_parameters\n\n    def unpack_parameters(self, packed_parameters: NDArrays) -&gt; tuple[NDArrays, NDArrays]:\n        return packed_parameters[: self.size_of_model_params], packed_parameters[self.size_of_model_params :]\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.parameter_packer.ParameterPackerWithControlVariates.__init__","title":"<code>__init__(size_of_model_params)</code>","text":"<p>Class to handle the exchange of control variates for the SCAFFOLD FL method.</p> <p>NOTE model params exchanged and control variates can be different sizes, for example, when layers are frozen or the state dictionary contains things like Batch Normalization layers.</p> <p>Parameters:</p> Name Type Description Default <code>size_of_model_params</code> <code>int</code> <p>This is the number of layers that are associated with the parameters of the model itself. This is used to split the covariates from the model parameters during unpacking.</p> required Source code in <code>fl4health/parameter_exchange/parameter_packer.py</code> <pre><code>def __init__(self, size_of_model_params: int) -&gt; None:\n    \"\"\"\n    Class to handle the exchange of control variates for the SCAFFOLD FL method.\n\n    **NOTE** model params exchanged and control variates can be different sizes, for example, when layers are\n    frozen or the state dictionary contains things like Batch Normalization layers.\n\n    Args:\n        size_of_model_params (int): This is the number of layers that are associated with the parameters of the\n            model itself. This is used to split the covariates from the model parameters during unpacking.\n    \"\"\"\n    self.size_of_model_params = size_of_model_params\n    super().__init__()\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.parameter_packer.ParameterPackerWithLayerNames","title":"<code>ParameterPackerWithLayerNames</code>","text":"<p>               Bases: <code>ParameterPacker[list[str]]</code></p> Source code in <code>fl4health/parameter_exchange/parameter_packer.py</code> <pre><code>class ParameterPackerWithLayerNames(ParameterPacker[list[str]]):\n    def pack_parameters(self, model_weights: NDArrays, weights_names: list[str]) -&gt; NDArrays:\n        return model_weights + [np.array(weights_names)]\n\n    def unpack_parameters(self, packed_parameters: NDArrays) -&gt; tuple[NDArrays, list[str]]:\n        \"\"\"\n        Function to separate the model parameters from the layer names that have been packed with them.\n\n        Args:\n            packed_parameters (NDArrays): packed_parameters is a list containing model parameters followed by an\n                ``NDArray`` that contains the corresponding names of those parameters.\n\n        Returns:\n            (tuple[NDArrays, list[str]]): tuple of model parameters and the names of the layers to which they\n                correspond.\n        \"\"\"\n        split_size = len(packed_parameters) - 1\n        model_parameters = packed_parameters[:split_size]\n        param_names = packed_parameters[split_size:][0].tolist()\n        return model_parameters, param_names\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.parameter_packer.ParameterPackerWithLayerNames.unpack_parameters","title":"<code>unpack_parameters(packed_parameters)</code>","text":"<p>Function to separate the model parameters from the layer names that have been packed with them.</p> <p>Parameters:</p> Name Type Description Default <code>packed_parameters</code> <code>NDArrays</code> <p>packed_parameters is a list containing model parameters followed by an <code>NDArray</code> that contains the corresponding names of those parameters.</p> required <p>Returns:</p> Type Description <code>tuple[NDArrays, list[str]]</code> <p>tuple of model parameters and the names of the layers to which they correspond.</p> Source code in <code>fl4health/parameter_exchange/parameter_packer.py</code> <pre><code>def unpack_parameters(self, packed_parameters: NDArrays) -&gt; tuple[NDArrays, list[str]]:\n    \"\"\"\n    Function to separate the model parameters from the layer names that have been packed with them.\n\n    Args:\n        packed_parameters (NDArrays): packed_parameters is a list containing model parameters followed by an\n            ``NDArray`` that contains the corresponding names of those parameters.\n\n    Returns:\n        (tuple[NDArrays, list[str]]): tuple of model parameters and the names of the layers to which they\n            correspond.\n    \"\"\"\n    split_size = len(packed_parameters) - 1\n    model_parameters = packed_parameters[:split_size]\n    param_names = packed_parameters[split_size:][0].tolist()\n    return model_parameters, param_names\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.parameter_packer.SparseCooParameterPacker","title":"<code>SparseCooParameterPacker</code>","text":"<p>               Bases: <code>ParameterPacker[tuple[NDArrays, NDArrays, list[str]]]</code></p> <p>This parameter packer is responsible for selecting an arbitrary set of parameters and then representing them in the sparse COO tensor format, which requires knowing the indices of the parameters within the tensor to which they belong, the shape of that tensor, and also the name of it.</p> <p>For more information on the sparse COO format and sparse tensors in PyTorch, please see the following two pages:</p> <ol> <li>https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html</li> <li>https://pytorch.org/docs/stable/sparse.html</li> </ol> Source code in <code>fl4health/parameter_exchange/parameter_packer.py</code> <pre><code>class SparseCooParameterPacker(ParameterPacker[tuple[NDArrays, NDArrays, list[str]]]):\n    \"\"\"\n    This parameter packer is responsible for selecting an arbitrary set of parameters and then representing them in\n    the sparse COO tensor format, which requires knowing the indices of the parameters within the tensor to which they\n    belong, the shape of that tensor, and also the name of it.\n\n    For more information on the sparse COO format and sparse tensors in PyTorch, please see the following\n    two pages:\n\n    1. https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html\n    2. https://pytorch.org/docs/stable/sparse.html\n    \"\"\"\n\n    def pack_parameters(\n        self, model_parameters: NDArrays, additional_parameters: tuple[NDArrays, NDArrays, list[str]]\n    ) -&gt; NDArrays:\n        parameter_indices, tensor_shapes, tensor_names = additional_parameters\n        return model_parameters + parameter_indices + tensor_shapes + [np.array(tensor_names)]\n\n    def unpack_parameters(self, packed_parameters: NDArrays) -&gt; tuple[NDArrays, tuple[NDArrays, NDArrays, list[str]]]:\n        # The names of the tensors is wrapped in a list, which is then transformed into an NDArrays of length 1\n        # before packing.\n        assert len(packed_parameters) % 3 == 1\n        split_size = (len(packed_parameters) - 1) // 3\n        model_parameters = packed_parameters[:split_size]\n        parameter_indices = packed_parameters[split_size : (2 * split_size)]\n        tensor_shapes = packed_parameters[(2 * split_size) : (3 * split_size)]\n        tensor_names = packed_parameters[(3 * split_size) :][0].tolist()\n        return model_parameters, (parameter_indices, tensor_shapes, tensor_names)\n\n    @staticmethod\n    def extract_coo_info_from_dense(x: Tensor) -&gt; tuple[NDArray, NDArray, NDArray]:\n        \"\"\"\n        Take a dense tensor x and extract the information required (namely, its nonzero values, their indices within\n        the tensor, and the shape of x) in order to represent it in the sparse coo format.\n\n        The results are converted to numpy arrays.\n\n        Args:\n            x (Tensor): Input dense tensor.\n\n        Returns:\n            (tuple[NDArray, NDArray, NDArray]): The nonzero values of x, the indices of those values within x, and the\n                shape of x.\n        \"\"\"\n        selected_parameters = x[torch.nonzero(x, as_tuple=True)].cpu().numpy()\n        selected_indices = torch.nonzero(x, as_tuple=False).cpu().numpy()\n        tensor_shape = np.array(list(x.shape))\n        return selected_parameters, selected_indices, tensor_shape\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.parameter_packer.SparseCooParameterPacker.extract_coo_info_from_dense","title":"<code>extract_coo_info_from_dense(x)</code>  <code>staticmethod</code>","text":"<p>Take a dense tensor x and extract the information required (namely, its nonzero values, their indices within the tensor, and the shape of x) in order to represent it in the sparse coo format.</p> <p>The results are converted to numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input dense tensor.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray, NDArray, NDArray]</code> <p>The nonzero values of x, the indices of those values within x, and the shape of x.</p> Source code in <code>fl4health/parameter_exchange/parameter_packer.py</code> <pre><code>@staticmethod\ndef extract_coo_info_from_dense(x: Tensor) -&gt; tuple[NDArray, NDArray, NDArray]:\n    \"\"\"\n    Take a dense tensor x and extract the information required (namely, its nonzero values, their indices within\n    the tensor, and the shape of x) in order to represent it in the sparse coo format.\n\n    The results are converted to numpy arrays.\n\n    Args:\n        x (Tensor): Input dense tensor.\n\n    Returns:\n        (tuple[NDArray, NDArray, NDArray]): The nonzero values of x, the indices of those values within x, and the\n            shape of x.\n    \"\"\"\n    selected_parameters = x[torch.nonzero(x, as_tuple=True)].cpu().numpy()\n    selected_indices = torch.nonzero(x, as_tuple=False).cpu().numpy()\n    tensor_shape = np.array(list(x.shape))\n    return selected_parameters, selected_indices, tensor_shape\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.parameter_selection_criteria","title":"<code>parameter_selection_criteria</code>","text":""},{"location":"api/#fl4health.parameter_exchange.parameter_selection_criteria.LayerSelectionFunctionConstructor","title":"<code>LayerSelectionFunctionConstructor</code>","text":"Source code in <code>fl4health/parameter_exchange/parameter_selection_criteria.py</code> <pre><code>class LayerSelectionFunctionConstructor:\n    def __init__(\n        self, norm_threshold: float, exchange_percentage: float, normalize: bool = True, select_drift_more: bool = True\n    ) -&gt; None:\n        \"\"\"\n        This class leverages ``functools.partial`` to construct layer selection functions, which are meant to be used\n        by the ``DynamicLayerExchanger`` class.\n\n        Args:\n            norm_threshold (float): A nonnegative real number used to select those layers whose drift in\n                \\\\(\\\\ell^2\\\\) norm exceeds (or falls short of) it.\n            exchange_percentage (float): Indicates the percentage of layers that are selected.\n            normalize (bool, optional): Indicates whether when calculating the norm of a layer, we also divide by the\n                number of parameters in that layer. Defaults to True.\n            select_drift_more (bool, optional): Indicates whether layers with larger drift norm are selected.\n                Defaults to True.\n        \"\"\"\n        assert 0 &lt; exchange_percentage &lt;= 1\n        assert norm_threshold &gt; 0\n        self.norm_threshold = norm_threshold\n        self.exchange_percentage = exchange_percentage\n        self.normalize = normalize\n        self.select_drift_more = select_drift_more\n\n    def select_by_threshold(self) -&gt; LayerSelectionFunction:\n        return partial(\n            select_layers_by_threshold,\n            self.norm_threshold,\n            self.normalize,\n            self.select_drift_more,\n        )\n\n    def select_by_percentage(self) -&gt; LayerSelectionFunction:\n        return partial(\n            select_layers_by_percentage,\n            self.exchange_percentage,\n            self.normalize,\n            self.select_drift_more,\n        )\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.parameter_selection_criteria.LayerSelectionFunctionConstructor.__init__","title":"<code>__init__(norm_threshold, exchange_percentage, normalize=True, select_drift_more=True)</code>","text":"<p>This class leverages <code>functools.partial</code> to construct layer selection functions, which are meant to be used by the <code>DynamicLayerExchanger</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>norm_threshold</code> <code>float</code> <p>A nonnegative real number used to select those layers whose drift in \\(\\ell^2\\) norm exceeds (or falls short of) it.</p> required <code>exchange_percentage</code> <code>float</code> <p>Indicates the percentage of layers that are selected.</p> required <code>normalize</code> <code>bool</code> <p>Indicates whether when calculating the norm of a layer, we also divide by the number of parameters in that layer. Defaults to True.</p> <code>True</code> <code>select_drift_more</code> <code>bool</code> <p>Indicates whether layers with larger drift norm are selected. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/parameter_exchange/parameter_selection_criteria.py</code> <pre><code>def __init__(\n    self, norm_threshold: float, exchange_percentage: float, normalize: bool = True, select_drift_more: bool = True\n) -&gt; None:\n    \"\"\"\n    This class leverages ``functools.partial`` to construct layer selection functions, which are meant to be used\n    by the ``DynamicLayerExchanger`` class.\n\n    Args:\n        norm_threshold (float): A nonnegative real number used to select those layers whose drift in\n            \\\\(\\\\ell^2\\\\) norm exceeds (or falls short of) it.\n        exchange_percentage (float): Indicates the percentage of layers that are selected.\n        normalize (bool, optional): Indicates whether when calculating the norm of a layer, we also divide by the\n            number of parameters in that layer. Defaults to True.\n        select_drift_more (bool, optional): Indicates whether layers with larger drift norm are selected.\n            Defaults to True.\n    \"\"\"\n    assert 0 &lt; exchange_percentage &lt;= 1\n    assert norm_threshold &gt; 0\n    self.norm_threshold = norm_threshold\n    self.exchange_percentage = exchange_percentage\n    self.normalize = normalize\n    self.select_drift_more = select_drift_more\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.parameter_selection_criteria.select_layers_by_threshold","title":"<code>select_layers_by_threshold(threshold, normalize, select_drift_more, model, initial_model)</code>","text":"<p>Return those layers of model that deviate (in \\(\\ell^2\\) norm) away from corresponding layers of <code>self.initial_model</code> by at least (or at most) <code>self.threshold</code>.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Drift threshold to be used for selection. It is an fixed value.</p> required <code>normalize</code> <code>bool</code> <p>Whether to divide the difference between the tensors by their number of elements.</p> required <code>select_drift_more</code> <code>bool</code> <p>Whether we are selecting parameters that have drifted further (True) or less far from their comparison values</p> required <code>model</code> <code>Module</code> <p>Model after training/modification.</p> required <code>initial_model</code> <code>Module</code> <p>Model that we started with to which we are comparing parameters.</p> required <p>Returns:</p> Type Description <code>tuple[NDArrays, list[str]]</code> <p>Layers selected by the process and their corresponding names in the model's</p> <code>list[str]</code> <p><code>state_dict</code>.</p> Source code in <code>fl4health/parameter_exchange/parameter_selection_criteria.py</code> <pre><code>def select_layers_by_threshold(\n    threshold: float,\n    normalize: bool,\n    select_drift_more: bool,\n    model: nn.Module,\n    initial_model: nn.Module,\n) -&gt; tuple[NDArrays, list[str]]:\n    \"\"\"\n    Return those layers of model that deviate (in \\\\(\\\\ell^2\\\\) norm) away from corresponding layers of\n    ``self.initial_model`` by at least (or at most) ``self.threshold``.\n\n    Args:\n        threshold (float): Drift threshold to be used for selection. It is an fixed value.\n        normalize (bool): Whether to divide the difference between the tensors by their number of elements.\n        select_drift_more (bool): Whether we are selecting parameters that have drifted further (True) or less far\n            from their comparison values\n        model (nn.Module): Model after training/modification.\n        initial_model (nn.Module): Model that we started with to which we are comparing parameters.\n\n    Returns:\n        (tuple[NDArrays, list[str]]): Layers selected by the process and their corresponding names in the model's\n        ``state_dict``.\n    \"\"\"\n    layer_names = []\n    layers_to_transfer = []\n    initial_model_states = initial_model.state_dict()\n    model_states = model.state_dict()\n    for layer_name, layer_param in model_states.items():\n        ghost_of_layer_params_past = initial_model_states[layer_name]\n        drift_norm = _calculate_drift_norm(layer_param, ghost_of_layer_params_past, normalize)\n        if select_drift_more:\n            if drift_norm &gt; threshold:\n                layers_to_transfer.append(layer_param.cpu().numpy())\n                layer_names.append(layer_name)\n        elif drift_norm &lt;= threshold:\n            layers_to_transfer.append(layer_param.cpu().numpy())\n            layer_names.append(layer_name)\n    return layers_to_transfer, layer_names\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.parameter_selection_criteria.select_scores_and_sample_masks","title":"<code>select_scores_and_sample_masks(model, initial_model)</code>","text":"<p>Selection function that first selects the <code>weight_scores</code> and <code>bias_scores</code> parameters for the masked layers, and then samples binary masks based on those scores to send to the server. This function is meant to be used for the FedPM algorithm.</p> <p>NOTE: in the current implementation, we always exchange the score tensors for all layers. In the future, we might support exchanging a subset of the layers (for example, filtering out the masks that are all zeros).</p> Source code in <code>fl4health/parameter_exchange/parameter_selection_criteria.py</code> <pre><code>def select_scores_and_sample_masks(model: nn.Module, initial_model: nn.Module | None) -&gt; tuple[NDArrays, list[str]]:\n    \"\"\"\n    Selection function that first selects the ``weight_scores`` and ``bias_scores`` parameters for the masked layers,\n    and then samples binary masks based on those scores to send to the server. This function is meant to be used for\n    the FedPM algorithm.\n\n    **NOTE**: in the current implementation, we always exchange the score tensors for all layers. In the future, we\n    might support exchanging a subset of the layers (for example, filtering out the masks that are all zeros).\n    \"\"\"\n    model_states = model.state_dict()\n    with torch.no_grad():\n        if is_masked_module(model):\n            return _process_masked_module(module=model, model_state_dict=model_states)\n        masks_to_exchange = []\n        score_tensor_names = []\n        for name, module in model.named_modules():\n            if is_masked_module(module):\n                module_masks, module_score_tensor_names = _process_masked_module(\n                    module=module, model_state_dict=model_states, module_name=name\n                )\n                masks_to_exchange.extend(module_masks)\n                score_tensor_names.extend(module_score_tensor_names)\n        return masks_to_exchange, score_tensor_names\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.partial_parameter_exchanger","title":"<code>partial_parameter_exchanger</code>","text":""},{"location":"api/#fl4health.parameter_exchange.partial_parameter_exchanger.PartialParameterExchanger","title":"<code>PartialParameterExchanger</code>","text":"<p>               Bases: <code>ParameterExchanger</code>, <code>Generic[T]</code></p> Source code in <code>fl4health/parameter_exchange/partial_parameter_exchanger.py</code> <pre><code>class PartialParameterExchanger(ParameterExchanger, Generic[T]):\n    def __init__(self, parameter_packer: ParameterPacker[T]) -&gt; None:\n        \"\"\"\n        Base class meant to properly facilitate partial parameter exchange through a selection criterion. This\n        mechanism is more complicated than, for example, that used by the ``FixedLayerExchanger`` where the subset\n        parameters to exchange do not change dynamically from round to round.\n\n        Args:\n            parameter_packer (ParameterPacker[T]): Parameter packer that can be used to pack in more information\n                than just the parameters being exchange. This is important, for example, when exchanging different\n                sets of layers in each round.\n        \"\"\"\n        super().__init__()\n        self.parameter_packer = parameter_packer\n\n    def pack_parameters(self, model_weights: NDArrays, additional_parameters: T) -&gt; NDArrays:\n        return self.parameter_packer.pack_parameters(model_weights, additional_parameters)\n\n    def unpack_parameters(self, packed_parameters: NDArrays) -&gt; tuple[NDArrays, T]:\n        return self.parameter_packer.unpack_parameters(packed_parameters)\n\n    @abstractmethod\n    def select_parameters(\n        self,\n        model: nn.Module,\n        initial_model: nn.Module | None = None,\n    ) -&gt; tuple[NDArrays, T]:\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.partial_parameter_exchanger.PartialParameterExchanger.__init__","title":"<code>__init__(parameter_packer)</code>","text":"<p>Base class meant to properly facilitate partial parameter exchange through a selection criterion. This mechanism is more complicated than, for example, that used by the <code>FixedLayerExchanger</code> where the subset parameters to exchange do not change dynamically from round to round.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_packer</code> <code>ParameterPacker[T]</code> <p>Parameter packer that can be used to pack in more information than just the parameters being exchange. This is important, for example, when exchanging different sets of layers in each round.</p> required Source code in <code>fl4health/parameter_exchange/partial_parameter_exchanger.py</code> <pre><code>def __init__(self, parameter_packer: ParameterPacker[T]) -&gt; None:\n    \"\"\"\n    Base class meant to properly facilitate partial parameter exchange through a selection criterion. This\n    mechanism is more complicated than, for example, that used by the ``FixedLayerExchanger`` where the subset\n    parameters to exchange do not change dynamically from round to round.\n\n    Args:\n        parameter_packer (ParameterPacker[T]): Parameter packer that can be used to pack in more information\n            than just the parameters being exchange. This is important, for example, when exchanging different\n            sets of layers in each round.\n    \"\"\"\n    super().__init__()\n    self.parameter_packer = parameter_packer\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.sparse_coo_parameter_exchanger","title":"<code>sparse_coo_parameter_exchanger</code>","text":""},{"location":"api/#fl4health.parameter_exchange.sparse_coo_parameter_exchanger.SparseCooParameterExchanger","title":"<code>SparseCooParameterExchanger</code>","text":"<p>               Bases: <code>PartialParameterExchanger[tuple[NDArrays, NDArrays, list[str]]]</code></p> Source code in <code>fl4health/parameter_exchange/sparse_coo_parameter_exchanger.py</code> <pre><code>class SparseCooParameterExchanger(PartialParameterExchanger[tuple[NDArrays, NDArrays, list[str]]]):\n    def __init__(self, sparsity_level: float, score_gen_function: ScoreGenFunction) -&gt; None:\n        \"\"\"\n        Parameter exchanger for sparse tensors.\n\n        This exchanger is responsible for selecting an arbitrary subset of a model's parameters\n        via some selection criterion and then packaging them into the COO sparse tensor format for exchanging.\n\n        For more information on the sparse COO format and sparse tensors in PyTorch, please see the following\n        two pages:\n\n        1. https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html\n        2. https://pytorch.org/docs/stable/sparse.html\n\n        In most cases, this function takes as inputs a current model and an initial model, and it returns a dictionary\n        that maps the name of each of the current model's tensors to another tensor which contains the parameter\n        scores.\n\n        Args:\n            sparsity_level (float): The level of sparsity. Must be between 0 and 1.\n            score_gen_function (ScoreGenFunction): Function that is responsible for generating a score for every\n                parameter inside a model in order to facilitate parameter selection.\n        \"\"\"\n        assert 0 &lt; sparsity_level &lt;= 1\n        self.sparsity_level = sparsity_level\n        self.parameter_packer: SparseCooParameterPacker = SparseCooParameterPacker()\n        self.score_gen_function = score_gen_function\n\n    def generate_parameter_scores(self, model: nn.Module, initial_model: nn.Module | None) -&gt; dict[str, Tensor]:\n        \"\"\"\n        Calling the score generating function to produce parameter scores.\n\n        Args:\n            model (nn.Module): Current model\n            initial_model (nn.Module | None): Model to which the weights will be compared/scored\n\n        Returns:\n            (dict[str, Tensor]): scores associated with each layer of the model.\n        \"\"\"\n        return self.score_gen_function(model, initial_model)\n\n    def _check_unique_score(self, param_scores: Tensor) -&gt; None:\n        unique_score_values = torch.unique(input=param_scores, sorted=False, return_inverse=False, return_counts=False)\n        if len(unique_score_values) == 1:\n            log(\n                WARNING,\n                \"All parameters have the same score.\\nThe number of parameters selected may not match the intended\"\n                \" sparsity level.\",\n            )\n\n    def select_parameters(\n        self, model: nn.Module, initial_model: nn.Module | None = None\n    ) -&gt; tuple[NDArrays, tuple[NDArrays, NDArrays, list[str]]]:\n        \"\"\"\n        Select model parameters according to the sparsity level and pack them into the sparse COO format to be\n        exchanged.\n\n        First, this method leverages a score generating function to generate scores for all parameters of model.\n\n        Next, these scores are used to select the parameters to be exchanged by performing a thresholding operation\n        on each of the model's tensors. A threshold is determined according to the desired sparsity level, then for\n        each model tensor, parameters whose scores are less than this threshold are set to zero, while parameters\n        whose scores are greater than or equal to this threshold retain their values.\n\n        Finally, the method extracts all the information required to represent the selected parameters in the\n        sparse COO tensor format. More specifically, the information consists of the indices of the parameters\n        within the tensor to which they belong, the shape of that tensor, and also the name of it.\n\n        Args:\n            model (nn.Module): Current model.\n            initial_model (nn.Module): Initial model.\n\n        Returns:\n            (tuple[NDArrays, tuple[NDArrays, NDArrays, list[str]]]): The selected parameters and other information,\n                as detailed above.\n        \"\"\"\n        all_parameter_scores = self.generate_parameter_scores(model, initial_model)\n        all_scores = torch.cat([val.flatten() for _, val in all_parameter_scores.items()])\n        # Sorting all scores and determining the threshold.\n        sorted_scores, _ = torch.sort(all_scores, descending=True)\n        n_top_scores = math.ceil(len(sorted_scores) * self.sparsity_level)\n        # Sanity check.\n        assert n_top_scores &gt;= 1\n        score_threshold = sorted_scores[(n_top_scores - 1)].item()\n\n        # Apply the score threshold to each model tensor to obtain the corresponding sparse tensor.\n        selected_parameters_all_tensors = []\n        selected_indices_all_tensors = []\n        tensor_shapes = []\n        tensor_names = []\n        model_states = model.state_dict()\n        for tensor_name, param_scores in all_parameter_scores.items():\n            model_tensor = model_states[tensor_name]\n            # Sanity check.\n            assert model_tensor.shape == param_scores.shape\n\n            self._check_unique_score(param_scores=param_scores)\n\n            # Use score_threshold to produce sparse tensors.\n            model_tensor_sparse = torch.where(param_scores &gt;= score_threshold, input=model_tensor, other=0)\n            # Tensors without any parameter or whose parameter values are all zero after thresholding\n            # will not be exchanged, so we discard them.\n            if not (model_tensor_sparse.shape == torch.Size([]) or (model_tensor_sparse == 0).all()):\n                (\n                    selected_parameters,\n                    selected_indices,\n                    tensor_shape,\n                ) = self.parameter_packer.extract_coo_info_from_dense(model_tensor_sparse)\n                selected_parameters_all_tensors.append(selected_parameters)\n                selected_indices_all_tensors.append(selected_indices)\n                tensor_shapes.append(tensor_shape)\n                tensor_names.append(tensor_name)\n\n        log(INFO, f\"Sparsity level used to select parameters for exchange: {self.sparsity_level}\")\n        return (selected_parameters_all_tensors, (selected_indices_all_tensors, tensor_shapes, tensor_names))\n\n    def push_parameters(\n        self, model: nn.Module, initial_model: nn.Module | None = None, config: Config | None = None\n    ) -&gt; NDArrays:\n        selected_parameters, additional_parameters = self.select_parameters(model, initial_model)\n        return self.pack_parameters(\n            model_weights=selected_parameters,\n            additional_parameters=additional_parameters,\n        )\n\n    def pull_parameters(self, parameters: NDArrays, model: Module, config: Config | None = None) -&gt; None:\n        selected_parameters, additional_info = self.parameter_packer.unpack_parameters(parameters)\n        indices, shapes, names = additional_info\n        current_state = model.state_dict()\n\n        # Sanity check.\n        assert len(selected_parameters) == len(indices) == len(shapes) == len(names) and len(names) &gt; 0\n        for param_values, param_indices, param_shape, param_name in zip(selected_parameters, indices, shapes, names):\n            # Use parameter values, indices, and shape to create\n            # a sparse coo tensor, which is then converted to a dense tensor\n            # to allow for loading.\n            param_coo = torch.sparse_coo_tensor(\n                indices=torch.tensor(param_indices.T),\n                values=torch.tensor(param_values),\n                size=torch.Size(param_shape),\n            )\n            param_dense = param_coo.to_dense()\n            current_state[param_name] = param_dense\n\n        model.load_state_dict(current_state, strict=True)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.sparse_coo_parameter_exchanger.SparseCooParameterExchanger.__init__","title":"<code>__init__(sparsity_level, score_gen_function)</code>","text":"<p>Parameter exchanger for sparse tensors.</p> <p>This exchanger is responsible for selecting an arbitrary subset of a model's parameters via some selection criterion and then packaging them into the COO sparse tensor format for exchanging.</p> <p>For more information on the sparse COO format and sparse tensors in PyTorch, please see the following two pages:</p> <ol> <li>https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html</li> <li>https://pytorch.org/docs/stable/sparse.html</li> </ol> <p>In most cases, this function takes as inputs a current model and an initial model, and it returns a dictionary that maps the name of each of the current model's tensors to another tensor which contains the parameter scores.</p> <p>Parameters:</p> Name Type Description Default <code>sparsity_level</code> <code>float</code> <p>The level of sparsity. Must be between 0 and 1.</p> required <code>score_gen_function</code> <code>ScoreGenFunction</code> <p>Function that is responsible for generating a score for every parameter inside a model in order to facilitate parameter selection.</p> required Source code in <code>fl4health/parameter_exchange/sparse_coo_parameter_exchanger.py</code> <pre><code>def __init__(self, sparsity_level: float, score_gen_function: ScoreGenFunction) -&gt; None:\n    \"\"\"\n    Parameter exchanger for sparse tensors.\n\n    This exchanger is responsible for selecting an arbitrary subset of a model's parameters\n    via some selection criterion and then packaging them into the COO sparse tensor format for exchanging.\n\n    For more information on the sparse COO format and sparse tensors in PyTorch, please see the following\n    two pages:\n\n    1. https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html\n    2. https://pytorch.org/docs/stable/sparse.html\n\n    In most cases, this function takes as inputs a current model and an initial model, and it returns a dictionary\n    that maps the name of each of the current model's tensors to another tensor which contains the parameter\n    scores.\n\n    Args:\n        sparsity_level (float): The level of sparsity. Must be between 0 and 1.\n        score_gen_function (ScoreGenFunction): Function that is responsible for generating a score for every\n            parameter inside a model in order to facilitate parameter selection.\n    \"\"\"\n    assert 0 &lt; sparsity_level &lt;= 1\n    self.sparsity_level = sparsity_level\n    self.parameter_packer: SparseCooParameterPacker = SparseCooParameterPacker()\n    self.score_gen_function = score_gen_function\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.sparse_coo_parameter_exchanger.SparseCooParameterExchanger.generate_parameter_scores","title":"<code>generate_parameter_scores(model, initial_model)</code>","text":"<p>Calling the score generating function to produce parameter scores.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Current model</p> required <code>initial_model</code> <code>Module | None</code> <p>Model to which the weights will be compared/scored</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>scores associated with each layer of the model.</p> Source code in <code>fl4health/parameter_exchange/sparse_coo_parameter_exchanger.py</code> <pre><code>def generate_parameter_scores(self, model: nn.Module, initial_model: nn.Module | None) -&gt; dict[str, Tensor]:\n    \"\"\"\n    Calling the score generating function to produce parameter scores.\n\n    Args:\n        model (nn.Module): Current model\n        initial_model (nn.Module | None): Model to which the weights will be compared/scored\n\n    Returns:\n        (dict[str, Tensor]): scores associated with each layer of the model.\n    \"\"\"\n    return self.score_gen_function(model, initial_model)\n</code></pre>"},{"location":"api/#fl4health.parameter_exchange.sparse_coo_parameter_exchanger.SparseCooParameterExchanger.select_parameters","title":"<code>select_parameters(model, initial_model=None)</code>","text":"<p>Select model parameters according to the sparsity level and pack them into the sparse COO format to be exchanged.</p> <p>First, this method leverages a score generating function to generate scores for all parameters of model.</p> <p>Next, these scores are used to select the parameters to be exchanged by performing a thresholding operation on each of the model's tensors. A threshold is determined according to the desired sparsity level, then for each model tensor, parameters whose scores are less than this threshold are set to zero, while parameters whose scores are greater than or equal to this threshold retain their values.</p> <p>Finally, the method extracts all the information required to represent the selected parameters in the sparse COO tensor format. More specifically, the information consists of the indices of the parameters within the tensor to which they belong, the shape of that tensor, and also the name of it.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Current model.</p> required <code>initial_model</code> <code>Module</code> <p>Initial model.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[NDArrays, tuple[NDArrays, NDArrays, list[str]]]</code> <p>The selected parameters and other information, as detailed above.</p> Source code in <code>fl4health/parameter_exchange/sparse_coo_parameter_exchanger.py</code> <pre><code>def select_parameters(\n    self, model: nn.Module, initial_model: nn.Module | None = None\n) -&gt; tuple[NDArrays, tuple[NDArrays, NDArrays, list[str]]]:\n    \"\"\"\n    Select model parameters according to the sparsity level and pack them into the sparse COO format to be\n    exchanged.\n\n    First, this method leverages a score generating function to generate scores for all parameters of model.\n\n    Next, these scores are used to select the parameters to be exchanged by performing a thresholding operation\n    on each of the model's tensors. A threshold is determined according to the desired sparsity level, then for\n    each model tensor, parameters whose scores are less than this threshold are set to zero, while parameters\n    whose scores are greater than or equal to this threshold retain their values.\n\n    Finally, the method extracts all the information required to represent the selected parameters in the\n    sparse COO tensor format. More specifically, the information consists of the indices of the parameters\n    within the tensor to which they belong, the shape of that tensor, and also the name of it.\n\n    Args:\n        model (nn.Module): Current model.\n        initial_model (nn.Module): Initial model.\n\n    Returns:\n        (tuple[NDArrays, tuple[NDArrays, NDArrays, list[str]]]): The selected parameters and other information,\n            as detailed above.\n    \"\"\"\n    all_parameter_scores = self.generate_parameter_scores(model, initial_model)\n    all_scores = torch.cat([val.flatten() for _, val in all_parameter_scores.items()])\n    # Sorting all scores and determining the threshold.\n    sorted_scores, _ = torch.sort(all_scores, descending=True)\n    n_top_scores = math.ceil(len(sorted_scores) * self.sparsity_level)\n    # Sanity check.\n    assert n_top_scores &gt;= 1\n    score_threshold = sorted_scores[(n_top_scores - 1)].item()\n\n    # Apply the score threshold to each model tensor to obtain the corresponding sparse tensor.\n    selected_parameters_all_tensors = []\n    selected_indices_all_tensors = []\n    tensor_shapes = []\n    tensor_names = []\n    model_states = model.state_dict()\n    for tensor_name, param_scores in all_parameter_scores.items():\n        model_tensor = model_states[tensor_name]\n        # Sanity check.\n        assert model_tensor.shape == param_scores.shape\n\n        self._check_unique_score(param_scores=param_scores)\n\n        # Use score_threshold to produce sparse tensors.\n        model_tensor_sparse = torch.where(param_scores &gt;= score_threshold, input=model_tensor, other=0)\n        # Tensors without any parameter or whose parameter values are all zero after thresholding\n        # will not be exchanged, so we discard them.\n        if not (model_tensor_sparse.shape == torch.Size([]) or (model_tensor_sparse == 0).all()):\n            (\n                selected_parameters,\n                selected_indices,\n                tensor_shape,\n            ) = self.parameter_packer.extract_coo_info_from_dense(model_tensor_sparse)\n            selected_parameters_all_tensors.append(selected_parameters)\n            selected_indices_all_tensors.append(selected_indices)\n            tensor_shapes.append(tensor_shape)\n            tensor_names.append(tensor_name)\n\n    log(INFO, f\"Sparsity level used to select parameters for exchange: {self.sparsity_level}\")\n    return (selected_parameters_all_tensors, (selected_indices_all_tensors, tensor_shapes, tensor_names))\n</code></pre>"},{"location":"api/#fl4health.preprocessing","title":"<code>preprocessing</code>","text":""},{"location":"api/#fl4health.preprocessing.autoencoders","title":"<code>autoencoders</code>","text":""},{"location":"api/#fl4health.preprocessing.autoencoders.dim_reduction","title":"<code>dim_reduction</code>","text":""},{"location":"api/#fl4health.preprocessing.autoencoders.dim_reduction.AutoEncoderProcessing","title":"<code>AutoEncoderProcessing</code>","text":"Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>class AutoEncoderProcessing:\n    def __init__(\n        self,\n        checkpointing_path: Path,\n        device: torch.device = DEVICE,\n    ) -&gt; None:\n        \"\"\"\n        Abstract class for processors that work with a pre-trained and saved autoencoder model.\n\n        Args:\n            checkpointing_path (Path): Path to the saved model.\n            device (torch.device, optional): Device indicator for where to send the model and data samples\n                for preprocessing.\n        \"\"\"\n        self.checkpointing_path = checkpointing_path\n        self.device = device\n        self.load_autoencoder()\n\n    def load_autoencoder(self) -&gt; None:\n        autoencoder = torch.load(self.checkpointing_path, weights_only=False)\n        autoencoder.eval()\n        self.autoencoder = autoencoder.to(self.device)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Printable representation of the object.\n\n        Returns:\n            (str): Printable representation of the object.\n        \"\"\"\n        return f\"{self.__class__.__name__}\"\n</code></pre> <code></code> <code>__init__(checkpointing_path, device=DEVICE)</code> \u00b6 <p>Abstract class for processors that work with a pre-trained and saved autoencoder model.</p> <p>Parameters:</p> Name Type Description Default <code>checkpointing_path</code> <code>Path</code> <p>Path to the saved model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model and data samples for preprocessing.</p> <code>DEVICE</code> Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>def __init__(\n    self,\n    checkpointing_path: Path,\n    device: torch.device = DEVICE,\n) -&gt; None:\n    \"\"\"\n    Abstract class for processors that work with a pre-trained and saved autoencoder model.\n\n    Args:\n        checkpointing_path (Path): Path to the saved model.\n        device (torch.device, optional): Device indicator for where to send the model and data samples\n            for preprocessing.\n    \"\"\"\n    self.checkpointing_path = checkpointing_path\n    self.device = device\n    self.load_autoencoder()\n</code></pre> <code></code> <code>__repr__()</code> \u00b6 <p>Printable representation of the object.</p> <p>Returns:</p> Type Description <code>str</code> <p>Printable representation of the object.</p> Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Printable representation of the object.\n\n    Returns:\n        (str): Printable representation of the object.\n    \"\"\"\n    return f\"{self.__class__.__name__}\"\n</code></pre>"},{"location":"api/#fl4health.preprocessing.autoencoders.dim_reduction.AeProcessor","title":"<code>AeProcessor</code>","text":"<p>               Bases: <code>AutoEncoderProcessing</code></p> <p>Transformer processor to encode the data using basic autoencoder.</p> Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>class AeProcessor(AutoEncoderProcessing):\n    \"\"\"Transformer processor to encode the data using basic autoencoder.\"\"\"\n\n    def __call__(self, sample: torch.Tensor) -&gt; torch.Tensor:\n        # This transformer is called for the input samples after they are transferred into torch tensors.\n        embedding_vector = self.autoencoder.encode(sample.to(self.device))\n        return embedding_vector.clone().detach()\n</code></pre>"},{"location":"api/#fl4health.preprocessing.autoencoders.dim_reduction.VaeProcessor","title":"<code>VaeProcessor</code>","text":"<p>               Bases: <code>AutoEncoderProcessing</code></p> Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>class VaeProcessor(AutoEncoderProcessing):\n    def __init__(\n        self,\n        checkpointing_path: Path,\n        device: torch.device = DEVICE,\n        return_mu_only: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Transformer processor to encode the data using VAE encoder.\n\n        Args:\n            checkpointing_path (Path): Path to the saved model.\n            device (torch.device, optional): Device indicator for where to send the model and data samples\n                for preprocessing.\n            return_mu_only (bool, optional): If true, only mu is returned. Defaults to False.\n        \"\"\"\n        super().__init__(checkpointing_path, device)\n        self.return_mu_only = return_mu_only\n\n    def __call__(self, sample: torch.Tensor) -&gt; torch.Tensor:\n        # This transformer is called for the input samples after they are transferred into torch tensors.\n        mu, logvar = self.autoencoder.encode(sample.to(self.device))\n        if self.return_mu_only:\n            return mu.clone().detach()\n        # By default returns cat(mu,logvar).\n        # Concatenation is performed on the last dimension which for both the batched data and single data\n        # is the latent space dimension.\n        return torch.cat((mu.clone().detach(), logvar.clone().detach()), dim=-1)\n</code></pre> <code></code> <code>__init__(checkpointing_path, device=DEVICE, return_mu_only=False)</code> \u00b6 <p>Transformer processor to encode the data using VAE encoder.</p> <p>Parameters:</p> Name Type Description Default <code>checkpointing_path</code> <code>Path</code> <p>Path to the saved model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model and data samples for preprocessing.</p> <code>DEVICE</code> <code>return_mu_only</code> <code>bool</code> <p>If true, only mu is returned. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>def __init__(\n    self,\n    checkpointing_path: Path,\n    device: torch.device = DEVICE,\n    return_mu_only: bool = False,\n) -&gt; None:\n    \"\"\"\n    Transformer processor to encode the data using VAE encoder.\n\n    Args:\n        checkpointing_path (Path): Path to the saved model.\n        device (torch.device, optional): Device indicator for where to send the model and data samples\n            for preprocessing.\n        return_mu_only (bool, optional): If true, only mu is returned. Defaults to False.\n    \"\"\"\n    super().__init__(checkpointing_path, device)\n    self.return_mu_only = return_mu_only\n</code></pre>"},{"location":"api/#fl4health.preprocessing.autoencoders.dim_reduction.CvaeFixedConditionProcessor","title":"<code>CvaeFixedConditionProcessor</code>","text":"<p>               Bases: <code>AutoEncoderProcessing</code></p> Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>class CvaeFixedConditionProcessor(AutoEncoderProcessing):\n    def __init__(\n        self,\n        checkpointing_path: Path,\n        condition: torch.Tensor,\n        device: torch.device = DEVICE,\n        return_mu_only: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Transformer processor to encode the data using a CVAE encoder with a fixed condition.\n\n        Args:\n            checkpointing_path (Path): Path to the saved model.\n            condition (torch.Tensor): Fixed condition tensor.\n            device (torch.device, optional):  Device indicator for where to send the model and data samples\n                for preprocessing.\n            return_mu_only (bool, optional): If true, only mu is returned. Defaults to False.\n        \"\"\"\n        super().__init__(checkpointing_path, device)\n        self.condition = condition\n        self.return_mu_only = return_mu_only\n        assert self.condition.dim() == 1, (\n            f\"Error: condition should be a 1D vector instead of a {self.condition.dim()}D tensor.\"\n        )\n\n    def __call__(self, sample: torch.Tensor) -&gt; torch.Tensor:\n        # Assuming batch is the first dimension\n        if sample.dim() == 1:\n            batch_condition = self.condition\n        else:\n            # If we are processing a batch of data, condition should be repeated for each sample.\n            sample_batch_size = sample.shape[0]\n            batch_condition = self.condition.repeat(sample_batch_size, 1)\n        # This transformer is called for the input samples after they are transformed into torch tensors.\n        mu, logvar = self.autoencoder.encode(sample.to(self.device), batch_condition.to(self.device))\n        if self.return_mu_only:\n            return mu.clone().detach()\n        # By default returns cat(mu,logvar)\n        # Concatenation is performed on the last dimension which for both the batched data and single data\n        # is the latent space dimension.\n        return torch.cat((mu.clone().detach(), logvar.clone().detach()), dim=-1)\n</code></pre> <code></code> <code>__init__(checkpointing_path, condition, device=DEVICE, return_mu_only=False)</code> \u00b6 <p>Transformer processor to encode the data using a CVAE encoder with a fixed condition.</p> <p>Parameters:</p> Name Type Description Default <code>checkpointing_path</code> <code>Path</code> <p>Path to the saved model.</p> required <code>condition</code> <code>Tensor</code> <p>Fixed condition tensor.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model and data samples for preprocessing.</p> <code>DEVICE</code> <code>return_mu_only</code> <code>bool</code> <p>If true, only mu is returned. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>def __init__(\n    self,\n    checkpointing_path: Path,\n    condition: torch.Tensor,\n    device: torch.device = DEVICE,\n    return_mu_only: bool = False,\n) -&gt; None:\n    \"\"\"\n    Transformer processor to encode the data using a CVAE encoder with a fixed condition.\n\n    Args:\n        checkpointing_path (Path): Path to the saved model.\n        condition (torch.Tensor): Fixed condition tensor.\n        device (torch.device, optional):  Device indicator for where to send the model and data samples\n            for preprocessing.\n        return_mu_only (bool, optional): If true, only mu is returned. Defaults to False.\n    \"\"\"\n    super().__init__(checkpointing_path, device)\n    self.condition = condition\n    self.return_mu_only = return_mu_only\n    assert self.condition.dim() == 1, (\n        f\"Error: condition should be a 1D vector instead of a {self.condition.dim()}D tensor.\"\n    )\n</code></pre>"},{"location":"api/#fl4health.preprocessing.autoencoders.dim_reduction.CvaeVariableConditionProcessor","title":"<code>CvaeVariableConditionProcessor</code>","text":"<p>               Bases: <code>AutoEncoderProcessing</code></p> Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>class CvaeVariableConditionProcessor(AutoEncoderProcessing):\n    def __init__(\n        self,\n        checkpointing_path: Path,\n        device: torch.device = DEVICE,\n        return_mu_only: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Transformer processor to encode the data using CVAE encoder with variable condition, that is each data sample\n        can have a specific condition.\n\n        Args:\n            checkpointing_path (Path): Path to the saved model.\n            device (torch.device, optional): Device indicator for where to send the model and data samples\n                for preprocessing.\n            return_mu_only (bool, optional): If true, only mu is returned. Defaults to False.\n        \"\"\"\n        super().__init__(checkpointing_path, device)\n        self.return_mu_only = return_mu_only\n\n    def __call__(self, sample: torch.Tensor, condition: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Performs encoding.\n\n        Args:\n            sample (torch.Tensor): A single data sample or a batch of data.\n            condition (torch.Tensor): A single condition for the given sample, or a batch of condition for a batch of\n                data.\n\n        Returns:\n            (torch.Tensor): Encoded sample(s).\n        \"\"\"\n        # This transformer is called for the input samples after they are transformed into torch tensors.\n        # We assume condition and data are \"batch first\".\n        if condition.size(0) &gt; 1:  # If condition is a batch\n            assert condition.size(0) == sample.size(0), (\n                f\"Error: Condition shape: {condition.shape} does not match the data shape: {sample.shape}\"\n            )\n        mu, logvar = self.autoencoder.encode(sample.to(self.device), condition.to(self.device))\n        if self.return_mu_only:\n            return mu.clone().detach()\n        # By default returns cat(mu,logvar)\n        # Concatenation is performed on the last dimension which for both the batched data and single data\n        # is the latent space dimension.\n        return torch.cat((mu.clone().detach(), logvar.clone().detach()), dim=-1)\n</code></pre> <code></code> <code>__init__(checkpointing_path, device=DEVICE, return_mu_only=False)</code> \u00b6 <p>Transformer processor to encode the data using CVAE encoder with variable condition, that is each data sample can have a specific condition.</p> <p>Parameters:</p> Name Type Description Default <code>checkpointing_path</code> <code>Path</code> <p>Path to the saved model.</p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model and data samples for preprocessing.</p> <code>DEVICE</code> <code>return_mu_only</code> <code>bool</code> <p>If true, only mu is returned. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>def __init__(\n    self,\n    checkpointing_path: Path,\n    device: torch.device = DEVICE,\n    return_mu_only: bool = False,\n) -&gt; None:\n    \"\"\"\n    Transformer processor to encode the data using CVAE encoder with variable condition, that is each data sample\n    can have a specific condition.\n\n    Args:\n        checkpointing_path (Path): Path to the saved model.\n        device (torch.device, optional): Device indicator for where to send the model and data samples\n            for preprocessing.\n        return_mu_only (bool, optional): If true, only mu is returned. Defaults to False.\n    \"\"\"\n    super().__init__(checkpointing_path, device)\n    self.return_mu_only = return_mu_only\n</code></pre> <code></code> <code>__call__(sample, condition)</code> \u00b6 <p>Performs encoding.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Tensor</code> <p>A single data sample or a batch of data.</p> required <code>condition</code> <code>Tensor</code> <p>A single condition for the given sample, or a batch of condition for a batch of data.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Encoded sample(s).</p> Source code in <code>fl4health/preprocessing/autoencoders/dim_reduction.py</code> <pre><code>def __call__(self, sample: torch.Tensor, condition: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Performs encoding.\n\n    Args:\n        sample (torch.Tensor): A single data sample or a batch of data.\n        condition (torch.Tensor): A single condition for the given sample, or a batch of condition for a batch of\n            data.\n\n    Returns:\n        (torch.Tensor): Encoded sample(s).\n    \"\"\"\n    # This transformer is called for the input samples after they are transformed into torch tensors.\n    # We assume condition and data are \"batch first\".\n    if condition.size(0) &gt; 1:  # If condition is a batch\n        assert condition.size(0) == sample.size(0), (\n            f\"Error: Condition shape: {condition.shape} does not match the data shape: {sample.shape}\"\n        )\n    mu, logvar = self.autoencoder.encode(sample.to(self.device), condition.to(self.device))\n    if self.return_mu_only:\n        return mu.clone().detach()\n    # By default returns cat(mu,logvar)\n    # Concatenation is performed on the last dimension which for both the batched data and single data\n    # is the latent space dimension.\n    return torch.cat((mu.clone().detach(), logvar.clone().detach()), dim=-1)\n</code></pre>"},{"location":"api/#fl4health.preprocessing.autoencoders.loss","title":"<code>loss</code>","text":""},{"location":"api/#fl4health.preprocessing.autoencoders.loss.VaeLoss","title":"<code>VaeLoss</code>","text":"<p>               Bases: <code>_Loss</code></p> Source code in <code>fl4health/preprocessing/autoencoders/loss.py</code> <pre><code>class VaeLoss(_Loss):\n    def __init__(\n        self,\n        latent_dim: int,\n        base_loss: _Loss,\n    ) -&gt; None:\n        \"\"\"\n        The loss function used for training CVAEs and VAEs.\n\n        This loss computes the ``base_loss`` (defined by the user) between the input and generated output.\n        It then adds the KL divergence between the estimated distribution (represented by mu and logvar)\n        and the standard normal distribution.\n\n        Args:\n            latent_dim (int): Dimensionality of the latent space.\n            base_loss (_Loss): Base loss function between the input and reconstruction.\n        \"\"\"\n        super().__init__()\n        # User can define a base_loss to measure the distance between the input and generated output.\n        self.base_loss = base_loss\n        # Latent dimension is used to unpack the model output\n        self.latent_dim = latent_dim\n\n    def standard_normal_kl_divergence_loss(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the analytical KL divergence between the normal distribution and the estimated distribution.\n\n        Args:\n            mu (torch.Tensor): Mean of the estimated distribution.\n            logvar (torch.Tensor): Log variance of the estimated distribution.\n\n        Returns:\n            (torch.Tensor): KL divergence loss.\n        \"\"\"\n        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n    def unpack_model_output(self, preds: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Unpacks the model output tensor.\n\n        Args:\n            preds (torch.Tensor): Model predictions.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor, torch.Tensor]): Unpacked output containing predictions, mu, and logvar.\n        \"\"\"\n        # This methods assumes \"preds\" are batch first, and preds are 2D dimensional (already flattened).\n        assert preds.dim() == REQUIRED_PREDS_DIMENSIONS, (\n            f\"Expected a 2D tensor for VaeLoss, but got {preds.dim()}D tensor with shape {preds.shape}.\"\n        )\n        # The order of logvar and mu in the output tensor is important.\n        # For each model output, the first self.latent_dim indices are used to store the log variance,\n        # the next self.latent_dim indices are allocated to mu, and the remaining indices store the model predictions.\n        logvar = preds[:, 0 : self.latent_dim]\n        mu = preds[:, self.latent_dim : 2 * self.latent_dim]\n        preds = preds[:, 2 * self.latent_dim :]\n        return preds, mu, logvar\n\n    def forward(self, preds: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the total loss.\n\n        Args:\n            preds (torch.Tensor): Model predictions.\n            target (torch.Tensor): Target values.\n\n        Returns:\n            (torch.Tensor): Total loss composed of base loss and KL divergence loss.\n        \"\"\"\n        flattened_output, mu, logvar = self.unpack_model_output(preds)\n        kl_loss = self.standard_normal_kl_divergence_loss(mu, logvar)\n        # Reshaping the flattened output to its original shape.\n        return self.base_loss(flattened_output.view(*target.shape), target) + kl_loss\n</code></pre> <code></code> <code>__init__(latent_dim, base_loss)</code> \u00b6 <p>The loss function used for training CVAEs and VAEs.</p> <p>This loss computes the <code>base_loss</code> (defined by the user) between the input and generated output. It then adds the KL divergence between the estimated distribution (represented by mu and logvar) and the standard normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>latent_dim</code> <code>int</code> <p>Dimensionality of the latent space.</p> required <code>base_loss</code> <code>_Loss</code> <p>Base loss function between the input and reconstruction.</p> required Source code in <code>fl4health/preprocessing/autoencoders/loss.py</code> <pre><code>def __init__(\n    self,\n    latent_dim: int,\n    base_loss: _Loss,\n) -&gt; None:\n    \"\"\"\n    The loss function used for training CVAEs and VAEs.\n\n    This loss computes the ``base_loss`` (defined by the user) between the input and generated output.\n    It then adds the KL divergence between the estimated distribution (represented by mu and logvar)\n    and the standard normal distribution.\n\n    Args:\n        latent_dim (int): Dimensionality of the latent space.\n        base_loss (_Loss): Base loss function between the input and reconstruction.\n    \"\"\"\n    super().__init__()\n    # User can define a base_loss to measure the distance between the input and generated output.\n    self.base_loss = base_loss\n    # Latent dimension is used to unpack the model output\n    self.latent_dim = latent_dim\n</code></pre> <code></code> <code>standard_normal_kl_divergence_loss(mu, logvar)</code> \u00b6 <p>Calculates the analytical KL divergence between the normal distribution and the estimated distribution.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>Mean of the estimated distribution.</p> required <code>logvar</code> <code>Tensor</code> <p>Log variance of the estimated distribution.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>KL divergence loss.</p> Source code in <code>fl4health/preprocessing/autoencoders/loss.py</code> <pre><code>def standard_normal_kl_divergence_loss(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the analytical KL divergence between the normal distribution and the estimated distribution.\n\n    Args:\n        mu (torch.Tensor): Mean of the estimated distribution.\n        logvar (torch.Tensor): Log variance of the estimated distribution.\n\n    Returns:\n        (torch.Tensor): KL divergence loss.\n    \"\"\"\n    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n</code></pre> <code></code> <code>unpack_model_output(preds)</code> \u00b6 <p>Unpacks the model output tensor.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Model predictions.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor, Tensor]</code> <p>Unpacked output containing predictions, mu, and logvar.</p> Source code in <code>fl4health/preprocessing/autoencoders/loss.py</code> <pre><code>def unpack_model_output(self, preds: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Unpacks the model output tensor.\n\n    Args:\n        preds (torch.Tensor): Model predictions.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor, torch.Tensor]): Unpacked output containing predictions, mu, and logvar.\n    \"\"\"\n    # This methods assumes \"preds\" are batch first, and preds are 2D dimensional (already flattened).\n    assert preds.dim() == REQUIRED_PREDS_DIMENSIONS, (\n        f\"Expected a 2D tensor for VaeLoss, but got {preds.dim()}D tensor with shape {preds.shape}.\"\n    )\n    # The order of logvar and mu in the output tensor is important.\n    # For each model output, the first self.latent_dim indices are used to store the log variance,\n    # the next self.latent_dim indices are allocated to mu, and the remaining indices store the model predictions.\n    logvar = preds[:, 0 : self.latent_dim]\n    mu = preds[:, self.latent_dim : 2 * self.latent_dim]\n    preds = preds[:, 2 * self.latent_dim :]\n    return preds, mu, logvar\n</code></pre> <code></code> <code>forward(preds, target)</code> \u00b6 <p>Calculates the total loss.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Model predictions.</p> required <code>target</code> <code>Tensor</code> <p>Target values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Total loss composed of base loss and KL divergence loss.</p> Source code in <code>fl4health/preprocessing/autoencoders/loss.py</code> <pre><code>def forward(self, preds: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the total loss.\n\n    Args:\n        preds (torch.Tensor): Model predictions.\n        target (torch.Tensor): Target values.\n\n    Returns:\n        (torch.Tensor): Total loss composed of base loss and KL divergence loss.\n    \"\"\"\n    flattened_output, mu, logvar = self.unpack_model_output(preds)\n    kl_loss = self.standard_normal_kl_divergence_loss(mu, logvar)\n    # Reshaping the flattened output to its original shape.\n    return self.base_loss(flattened_output.view(*target.shape), target) + kl_loss\n</code></pre>"},{"location":"api/#fl4health.preprocessing.pca_preprocessor","title":"<code>pca_preprocessor</code>","text":""},{"location":"api/#fl4health.preprocessing.pca_preprocessor.PcaPreprocessor","title":"<code>PcaPreprocessor</code>","text":"Source code in <code>fl4health/preprocessing/pca_preprocessor.py</code> <pre><code>class PcaPreprocessor:\n    def __init__(self, checkpointing_path: Path) -&gt; None:\n        \"\"\"\n        Class that leverages pre-computed principal components of a dataset to perform data-preprocessing.\n\n        Args:\n            checkpointing_path (Path): Path to saved principal components.\n        \"\"\"\n        self.checkpointing_path = checkpointing_path\n        self.pca_module: PcaModule = self.load_pca_module()\n\n    def load_pca_module(self) -&gt; PcaModule:\n        pca_module = torch.load(self.checkpointing_path, weights_only=False)\n        pca_module.eval()\n        return pca_module\n\n    def reduce_dimension(\n        self,\n        new_dimension: int,\n        dataset: TensorDataset,\n    ) -&gt; TensorDataset:\n        \"\"\"\n        Perform dimensionality reduction on a dataset by projecting the data onto a set of pre-computed principal\n        components.\n\n        **NOTE** that PyTorch dataloaders perform lazy application of transforms. So in reality, dimensionality\n        reduction is applied in real-time as the user iterates through the dataloader created from the dataset\n        returned here.\n\n        Args:\n            new_dimension (int): New data dimension after dimensionality reduction. Equals the number of principal\n                components onto which projection is performed.\n            dataset (BaseDataset): Dataset containing data whose dimension is to be reduced.\n\n        Returns:\n            (BaseDataset): Dataset consisting of data with reduced dimension.\n        \"\"\"\n        projection = partial(self.pca_module.project_lower_dim, k=new_dimension)\n        dataset.update_transform(projection)\n        return dataset\n</code></pre>"},{"location":"api/#fl4health.preprocessing.pca_preprocessor.PcaPreprocessor.__init__","title":"<code>__init__(checkpointing_path)</code>","text":"<p>Class that leverages pre-computed principal components of a dataset to perform data-preprocessing.</p> <p>Parameters:</p> Name Type Description Default <code>checkpointing_path</code> <code>Path</code> <p>Path to saved principal components.</p> required Source code in <code>fl4health/preprocessing/pca_preprocessor.py</code> <pre><code>def __init__(self, checkpointing_path: Path) -&gt; None:\n    \"\"\"\n    Class that leverages pre-computed principal components of a dataset to perform data-preprocessing.\n\n    Args:\n        checkpointing_path (Path): Path to saved principal components.\n    \"\"\"\n    self.checkpointing_path = checkpointing_path\n    self.pca_module: PcaModule = self.load_pca_module()\n</code></pre>"},{"location":"api/#fl4health.preprocessing.pca_preprocessor.PcaPreprocessor.reduce_dimension","title":"<code>reduce_dimension(new_dimension, dataset)</code>","text":"<p>Perform dimensionality reduction on a dataset by projecting the data onto a set of pre-computed principal components.</p> <p>NOTE that PyTorch dataloaders perform lazy application of transforms. So in reality, dimensionality reduction is applied in real-time as the user iterates through the dataloader created from the dataset returned here.</p> <p>Parameters:</p> Name Type Description Default <code>new_dimension</code> <code>int</code> <p>New data dimension after dimensionality reduction. Equals the number of principal components onto which projection is performed.</p> required <code>dataset</code> <code>BaseDataset</code> <p>Dataset containing data whose dimension is to be reduced.</p> required <p>Returns:</p> Type Description <code>BaseDataset</code> <p>Dataset consisting of data with reduced dimension.</p> Source code in <code>fl4health/preprocessing/pca_preprocessor.py</code> <pre><code>def reduce_dimension(\n    self,\n    new_dimension: int,\n    dataset: TensorDataset,\n) -&gt; TensorDataset:\n    \"\"\"\n    Perform dimensionality reduction on a dataset by projecting the data onto a set of pre-computed principal\n    components.\n\n    **NOTE** that PyTorch dataloaders perform lazy application of transforms. So in reality, dimensionality\n    reduction is applied in real-time as the user iterates through the dataloader created from the dataset\n    returned here.\n\n    Args:\n        new_dimension (int): New data dimension after dimensionality reduction. Equals the number of principal\n            components onto which projection is performed.\n        dataset (BaseDataset): Dataset containing data whose dimension is to be reduced.\n\n    Returns:\n        (BaseDataset): Dataset consisting of data with reduced dimension.\n    \"\"\"\n    projection = partial(self.pca_module.project_lower_dim, k=new_dimension)\n    dataset.update_transform(projection)\n    return dataset\n</code></pre>"},{"location":"api/#fl4health.preprocessing.warmed_up_module","title":"<code>warmed_up_module</code>","text":""},{"location":"api/#fl4health.preprocessing.warmed_up_module.WarmedUpModule","title":"<code>WarmedUpModule</code>","text":"Source code in <code>fl4health/preprocessing/warmed_up_module.py</code> <pre><code>class WarmedUpModule:\n    def __init__(\n        self,\n        pretrained_model: torch.nn.Module | None = None,\n        pretrained_model_path: Path | None = None,\n        weights_mapping_path: Path | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This class is used to load a pretrained model into the target model.\n\n        Initialize the ``WarmedUpModule`` with the pretrained model states and weights mapping dict.\n\n        Args:\n            pretrained_model (torch.nn.Module | None): Pretrained model. This is mutually exclusive with\n                ``pretrained_model_path``.\n            pretrained_model_path (Path | None): Path of the pretrained model. This is mutually exclusive with\n                ``pretrained_model``.\n            weights_mapping_path (str | None, optional): Path of to json file of the weights mapping dict. If models\n                are not exactly the same, a weights mapping dict is needed to map the weights of the pretrained\n                model to the target model.\n        \"\"\"\n        if pretrained_model is not None and pretrained_model_path is not None:\n            raise AssertionError(\n                \"pretrained_model_path and pretrained_model is mutually exclusive. Please provide one of them.\"\n            )\n\n        if pretrained_model is not None:\n            log(INFO, \"Pretrained model is provided.\")\n            self.pretrained_model_state = pretrained_model.state_dict()\n\n        elif pretrained_model_path is not None:\n            assert os.path.exists(pretrained_model_path), (\n                f\"Pretrained model path {pretrained_model_path} does not exist.\"\n            )\n            log(INFO, f\"Loading pretrained model from {pretrained_model_path}\")\n            self.pretrained_model_state = torch.load(pretrained_model_path, weights_only=False).state_dict()\n\n        else:\n            raise AssertionError(\"At least one of pretrained_model_path and pretrained_model should be provided.\")\n\n        if weights_mapping_path is not None:\n            with open(weights_mapping_path, \"r\") as file:\n                self.weights_mapping_dict = json.load(file)\n        else:\n            log(INFO, \"Weights mapping dict is not provided. Matching states directly, based on target model's keys.\")\n            self.weights_mapping_dict = None\n\n    def get_matching_component(self, key: str) -&gt; str | None:\n        \"\"\"\n        Get the matching component of the key from the weights mapping dictionary. Since the provided mapping\n        can contain partial names of the keys, this function is used to split the key of the target model and\n        match it with the partial key in the mapping, returning the complete name of the key in the pretrained model.\n\n        This allows users to provide one mapping for multiple states that share the same prefix. For example, if the\n        mapping is ``{\"model\": \"global_model\"}`` and the input key of the target model is ``model.layer1.weight``,\n        then the returned matching component is ``global_model.layer1.weight``.\n\n        Args:\n            key (str): Key to be matched in pretrained model.\n\n        Returns:\n            (str | None): If no weights mapping dict is provided, returns the key. Otherwise, if the key is in the\n                weights mapping dict, returns the matching component of the key. Otherwise, returns None.\n        \"\"\"\n        if self.weights_mapping_dict is None:\n            return key\n\n        components = key.split(\".\")\n        matching_component = \"\"\n        for i, component in enumerate(components):\n            matching_component = component if i == 0 else f\"{matching_component}.{component}\"\n            if matching_component in self.weights_mapping_dict:\n                return self.weights_mapping_dict[matching_component] + key[len(matching_component) :]\n        return None\n\n    def load_from_pretrained(self, model: torch.nn.Module) -&gt; torch.nn.Module:\n        \"\"\"\n        Load the pretrained model into the target model.\n\n        Args:\n            model (torch.nn.Module): target model.\n        \"\"\"\n        assert self.pretrained_model_state is not None\n\n        target_model_state = model.state_dict()\n\n        matching_state = {}\n        for key in target_model_state:\n            original_state = target_model_state[key]\n\n            pretrained_key = self.get_matching_component(key)\n            log(INFO, f\"Matching: {key} -&gt; {pretrained_key}\")\n            if pretrained_key is not None:\n                if pretrained_key in self.pretrained_model_state:\n                    pretrained_state = self.pretrained_model_state[pretrained_key]\n                    if original_state.size() == pretrained_state.size():\n                        matching_state[key] = pretrained_state\n                        log(INFO, \"Successful matching states.\")\n                    else:\n                        log(\n                            WARNING,\n                            f\"State won't be loaded. Mismatched sizes {original_state.size()}) -&gt; ({pretrained_key}).\",\n                        )\n                else:\n                    log(\n                        WARNING,\n                        f\"state won't be loaded. Key {pretrained_key} not found in the pretrained model states.\",\n                    )\n\n        log(INFO, f\"{len(matching_state)}/{len(target_model_state)} states were matched.\")\n\n        target_model_state.update(matching_state)\n        model.load_state_dict(target_model_state)\n        return model\n</code></pre>"},{"location":"api/#fl4health.preprocessing.warmed_up_module.WarmedUpModule.__init__","title":"<code>__init__(pretrained_model=None, pretrained_model_path=None, weights_mapping_path=None)</code>","text":"<p>This class is used to load a pretrained model into the target model.</p> <p>Initialize the <code>WarmedUpModule</code> with the pretrained model states and weights mapping dict.</p> <p>Parameters:</p> Name Type Description Default <code>pretrained_model</code> <code>Module | None</code> <p>Pretrained model. This is mutually exclusive with <code>pretrained_model_path</code>.</p> <code>None</code> <code>pretrained_model_path</code> <code>Path | None</code> <p>Path of the pretrained model. This is mutually exclusive with <code>pretrained_model</code>.</p> <code>None</code> <code>weights_mapping_path</code> <code>str | None</code> <p>Path of to json file of the weights mapping dict. If models are not exactly the same, a weights mapping dict is needed to map the weights of the pretrained model to the target model.</p> <code>None</code> Source code in <code>fl4health/preprocessing/warmed_up_module.py</code> <pre><code>def __init__(\n    self,\n    pretrained_model: torch.nn.Module | None = None,\n    pretrained_model_path: Path | None = None,\n    weights_mapping_path: Path | None = None,\n) -&gt; None:\n    \"\"\"\n    This class is used to load a pretrained model into the target model.\n\n    Initialize the ``WarmedUpModule`` with the pretrained model states and weights mapping dict.\n\n    Args:\n        pretrained_model (torch.nn.Module | None): Pretrained model. This is mutually exclusive with\n            ``pretrained_model_path``.\n        pretrained_model_path (Path | None): Path of the pretrained model. This is mutually exclusive with\n            ``pretrained_model``.\n        weights_mapping_path (str | None, optional): Path of to json file of the weights mapping dict. If models\n            are not exactly the same, a weights mapping dict is needed to map the weights of the pretrained\n            model to the target model.\n    \"\"\"\n    if pretrained_model is not None and pretrained_model_path is not None:\n        raise AssertionError(\n            \"pretrained_model_path and pretrained_model is mutually exclusive. Please provide one of them.\"\n        )\n\n    if pretrained_model is not None:\n        log(INFO, \"Pretrained model is provided.\")\n        self.pretrained_model_state = pretrained_model.state_dict()\n\n    elif pretrained_model_path is not None:\n        assert os.path.exists(pretrained_model_path), (\n            f\"Pretrained model path {pretrained_model_path} does not exist.\"\n        )\n        log(INFO, f\"Loading pretrained model from {pretrained_model_path}\")\n        self.pretrained_model_state = torch.load(pretrained_model_path, weights_only=False).state_dict()\n\n    else:\n        raise AssertionError(\"At least one of pretrained_model_path and pretrained_model should be provided.\")\n\n    if weights_mapping_path is not None:\n        with open(weights_mapping_path, \"r\") as file:\n            self.weights_mapping_dict = json.load(file)\n    else:\n        log(INFO, \"Weights mapping dict is not provided. Matching states directly, based on target model's keys.\")\n        self.weights_mapping_dict = None\n</code></pre>"},{"location":"api/#fl4health.preprocessing.warmed_up_module.WarmedUpModule.get_matching_component","title":"<code>get_matching_component(key)</code>","text":"<p>Get the matching component of the key from the weights mapping dictionary. Since the provided mapping can contain partial names of the keys, this function is used to split the key of the target model and match it with the partial key in the mapping, returning the complete name of the key in the pretrained model.</p> <p>This allows users to provide one mapping for multiple states that share the same prefix. For example, if the mapping is <code>{\"model\": \"global_model\"}</code> and the input key of the target model is <code>model.layer1.weight</code>, then the returned matching component is <code>global_model.layer1.weight</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key to be matched in pretrained model.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>If no weights mapping dict is provided, returns the key. Otherwise, if the key is in the weights mapping dict, returns the matching component of the key. Otherwise, returns None.</p> Source code in <code>fl4health/preprocessing/warmed_up_module.py</code> <pre><code>def get_matching_component(self, key: str) -&gt; str | None:\n    \"\"\"\n    Get the matching component of the key from the weights mapping dictionary. Since the provided mapping\n    can contain partial names of the keys, this function is used to split the key of the target model and\n    match it with the partial key in the mapping, returning the complete name of the key in the pretrained model.\n\n    This allows users to provide one mapping for multiple states that share the same prefix. For example, if the\n    mapping is ``{\"model\": \"global_model\"}`` and the input key of the target model is ``model.layer1.weight``,\n    then the returned matching component is ``global_model.layer1.weight``.\n\n    Args:\n        key (str): Key to be matched in pretrained model.\n\n    Returns:\n        (str | None): If no weights mapping dict is provided, returns the key. Otherwise, if the key is in the\n            weights mapping dict, returns the matching component of the key. Otherwise, returns None.\n    \"\"\"\n    if self.weights_mapping_dict is None:\n        return key\n\n    components = key.split(\".\")\n    matching_component = \"\"\n    for i, component in enumerate(components):\n        matching_component = component if i == 0 else f\"{matching_component}.{component}\"\n        if matching_component in self.weights_mapping_dict:\n            return self.weights_mapping_dict[matching_component] + key[len(matching_component) :]\n    return None\n</code></pre>"},{"location":"api/#fl4health.preprocessing.warmed_up_module.WarmedUpModule.load_from_pretrained","title":"<code>load_from_pretrained(model)</code>","text":"<p>Load the pretrained model into the target model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>target model.</p> required Source code in <code>fl4health/preprocessing/warmed_up_module.py</code> <pre><code>def load_from_pretrained(self, model: torch.nn.Module) -&gt; torch.nn.Module:\n    \"\"\"\n    Load the pretrained model into the target model.\n\n    Args:\n        model (torch.nn.Module): target model.\n    \"\"\"\n    assert self.pretrained_model_state is not None\n\n    target_model_state = model.state_dict()\n\n    matching_state = {}\n    for key in target_model_state:\n        original_state = target_model_state[key]\n\n        pretrained_key = self.get_matching_component(key)\n        log(INFO, f\"Matching: {key} -&gt; {pretrained_key}\")\n        if pretrained_key is not None:\n            if pretrained_key in self.pretrained_model_state:\n                pretrained_state = self.pretrained_model_state[pretrained_key]\n                if original_state.size() == pretrained_state.size():\n                    matching_state[key] = pretrained_state\n                    log(INFO, \"Successful matching states.\")\n                else:\n                    log(\n                        WARNING,\n                        f\"State won't be loaded. Mismatched sizes {original_state.size()}) -&gt; ({pretrained_key}).\",\n                    )\n            else:\n                log(\n                    WARNING,\n                    f\"state won't be loaded. Key {pretrained_key} not found in the pretrained model states.\",\n                )\n\n    log(INFO, f\"{len(matching_state)}/{len(target_model_state)} states were matched.\")\n\n    target_model_state.update(matching_state)\n    model.load_state_dict(target_model_state)\n    return model\n</code></pre>"},{"location":"api/#fl4health.privacy","title":"<code>privacy</code>","text":""},{"location":"api/#fl4health.privacy.fl_accountants","title":"<code>fl_accountants</code>","text":""},{"location":"api/#fl4health.privacy.fl_accountants.FlInstanceLevelAccountant","title":"<code>FlInstanceLevelAccountant</code>","text":"Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>class FlInstanceLevelAccountant:\n    def __init__(\n        self,\n        client_sampling_rate: float,\n        noise_multiplier: float,\n        epochs_per_round: int,\n        client_batch_sizes: list[int],\n        client_dataset_sizes: list[int],\n        moment_orders: list[float] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This accountant should be used when applying FL and measuring instance-level privacy.\n\n        **NOTE**: This class assumes that all sampling is done via Poisson sampling (client and data point level).\n        Further it assumes that the sampling ratio of clients and noise multiplier are fixed throughout training\n\n        Args:\n            client_sampling_rate (float): Probability that each client will be included in a round\n            noise_multiplier (float):  Multiplier of noise std. dev. on clipping bound.\n            epochs_per_round (int): Number of epochs each client will complete per server round.\n            client_batch_sizes (list[int]): Batch size per client, if a single value it is assumed to be constant\n                across clients.\n            client_dataset_sizes (list[int]): Size of full dataset on a client, if a single value it is assumed to be\n                constant across clients.\n            moment_orders (list[float] | None, optional): Moments orders to be used in computing the approximate\n                epsilon value. Defaults to None.\n        \"\"\"\n        self.noise_multiplier = noise_multiplier\n        self.epochs_per_round = epochs_per_round\n        assert len(client_batch_sizes) == len(client_dataset_sizes)\n\n        self.num_batches_per_client = self._calculate_num_batches(client_batch_sizes, client_dataset_sizes)\n\n        client_batch_ratios = self._calculate_batch_ratios(client_batch_sizes, client_dataset_sizes)\n        self.sampling_strategies_per_client = [\n            PoissonSampling(client_sampling_rate * client_batch_ratio) for client_batch_ratio in client_batch_ratios\n        ]\n\n        self.accountant = MomentsAccountant(moment_orders)\n\n    def _calculate_batch_ratios(self, client_batch_sizes: list[int], client_dataset_sizes: list[int]) -&gt; list[float]:\n        return [batch / dataset for batch, dataset in zip(client_batch_sizes, client_dataset_sizes)]\n\n    def _calculate_num_batches(self, client_batch_sizes: list[int], client_dataset_sizes: list[int]) -&gt; list[int]:\n        return [ceil(dataset / batch) for batch, dataset in zip(client_batch_sizes, client_dataset_sizes)]\n\n    def get_epsilon(self, server_updates: int, delta: float) -&gt; float:\n        \"\"\"\n        Compute the epsilon value for the provided delta and the number of server updates performed.\n\n        Args:\n            server_updates (int): Number of central server updates performed.\n            delta (float): Delta value from which to compute epsilon.\n\n        Returns:\n            (float): Epsilon.\n        \"\"\"\n        epsilons = []\n        for num_batch, sampling_strategy in zip(self.num_batches_per_client, self.sampling_strategies_per_client):\n            # Round up because privacy loss is monotonic wrt total_updates\n            total_updates = ceil(server_updates * self.epochs_per_round * num_batch)\n            epsilon = self.accountant.get_epsilon(sampling_strategy, self.noise_multiplier, total_updates, delta)\n            epsilons.append(epsilon)\n        return max(epsilons)\n\n    def get_delta(self, server_updates: int, epsilon: float) -&gt; float:\n        \"\"\"\n        Compute the delta value for the provided epsilon and the number of server updates performed.\n\n        Args:\n            server_updates (int): Number of central server updates performed.\n            epsilon (float): Epsilon value from which to compute delta.\n\n        Returns:\n            (float): delta.\n        \"\"\"\n        deltas = []\n        for num_batch, sampling_strategy in zip(self.num_batches_per_client, self.sampling_strategies_per_client):\n            # Round up because privacy loss is monotonic wrt total_updates\n            total_updates = ceil(server_updates * self.epochs_per_round * num_batch)\n            delta = self.accountant.get_delta(sampling_strategy, self.noise_multiplier, total_updates, epsilon)\n            deltas.append(delta)\n        return max(deltas)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlInstanceLevelAccountant.__init__","title":"<code>__init__(client_sampling_rate, noise_multiplier, epochs_per_round, client_batch_sizes, client_dataset_sizes, moment_orders=None)</code>","text":"<p>This accountant should be used when applying FL and measuring instance-level privacy.</p> <p>NOTE: This class assumes that all sampling is done via Poisson sampling (client and data point level). Further it assumes that the sampling ratio of clients and noise multiplier are fixed throughout training</p> <p>Parameters:</p> Name Type Description Default <code>client_sampling_rate</code> <code>float</code> <p>Probability that each client will be included in a round</p> required <code>noise_multiplier</code> <code>float</code> <p>Multiplier of noise std. dev. on clipping bound.</p> required <code>epochs_per_round</code> <code>int</code> <p>Number of epochs each client will complete per server round.</p> required <code>client_batch_sizes</code> <code>list[int]</code> <p>Batch size per client, if a single value it is assumed to be constant across clients.</p> required <code>client_dataset_sizes</code> <code>list[int]</code> <p>Size of full dataset on a client, if a single value it is assumed to be constant across clients.</p> required <code>moment_orders</code> <code>list[float] | None</code> <p>Moments orders to be used in computing the approximate epsilon value. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>def __init__(\n    self,\n    client_sampling_rate: float,\n    noise_multiplier: float,\n    epochs_per_round: int,\n    client_batch_sizes: list[int],\n    client_dataset_sizes: list[int],\n    moment_orders: list[float] | None = None,\n) -&gt; None:\n    \"\"\"\n    This accountant should be used when applying FL and measuring instance-level privacy.\n\n    **NOTE**: This class assumes that all sampling is done via Poisson sampling (client and data point level).\n    Further it assumes that the sampling ratio of clients and noise multiplier are fixed throughout training\n\n    Args:\n        client_sampling_rate (float): Probability that each client will be included in a round\n        noise_multiplier (float):  Multiplier of noise std. dev. on clipping bound.\n        epochs_per_round (int): Number of epochs each client will complete per server round.\n        client_batch_sizes (list[int]): Batch size per client, if a single value it is assumed to be constant\n            across clients.\n        client_dataset_sizes (list[int]): Size of full dataset on a client, if a single value it is assumed to be\n            constant across clients.\n        moment_orders (list[float] | None, optional): Moments orders to be used in computing the approximate\n            epsilon value. Defaults to None.\n    \"\"\"\n    self.noise_multiplier = noise_multiplier\n    self.epochs_per_round = epochs_per_round\n    assert len(client_batch_sizes) == len(client_dataset_sizes)\n\n    self.num_batches_per_client = self._calculate_num_batches(client_batch_sizes, client_dataset_sizes)\n\n    client_batch_ratios = self._calculate_batch_ratios(client_batch_sizes, client_dataset_sizes)\n    self.sampling_strategies_per_client = [\n        PoissonSampling(client_sampling_rate * client_batch_ratio) for client_batch_ratio in client_batch_ratios\n    ]\n\n    self.accountant = MomentsAccountant(moment_orders)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlInstanceLevelAccountant.get_epsilon","title":"<code>get_epsilon(server_updates, delta)</code>","text":"<p>Compute the epsilon value for the provided delta and the number of server updates performed.</p> <p>Parameters:</p> Name Type Description Default <code>server_updates</code> <code>int</code> <p>Number of central server updates performed.</p> required <code>delta</code> <code>float</code> <p>Delta value from which to compute epsilon.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Epsilon.</p> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>def get_epsilon(self, server_updates: int, delta: float) -&gt; float:\n    \"\"\"\n    Compute the epsilon value for the provided delta and the number of server updates performed.\n\n    Args:\n        server_updates (int): Number of central server updates performed.\n        delta (float): Delta value from which to compute epsilon.\n\n    Returns:\n        (float): Epsilon.\n    \"\"\"\n    epsilons = []\n    for num_batch, sampling_strategy in zip(self.num_batches_per_client, self.sampling_strategies_per_client):\n        # Round up because privacy loss is monotonic wrt total_updates\n        total_updates = ceil(server_updates * self.epochs_per_round * num_batch)\n        epsilon = self.accountant.get_epsilon(sampling_strategy, self.noise_multiplier, total_updates, delta)\n        epsilons.append(epsilon)\n    return max(epsilons)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlInstanceLevelAccountant.get_delta","title":"<code>get_delta(server_updates, epsilon)</code>","text":"<p>Compute the delta value for the provided epsilon and the number of server updates performed.</p> <p>Parameters:</p> Name Type Description Default <code>server_updates</code> <code>int</code> <p>Number of central server updates performed.</p> required <code>epsilon</code> <code>float</code> <p>Epsilon value from which to compute delta.</p> required <p>Returns:</p> Type Description <code>float</code> <p>delta.</p> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>def get_delta(self, server_updates: int, epsilon: float) -&gt; float:\n    \"\"\"\n    Compute the delta value for the provided epsilon and the number of server updates performed.\n\n    Args:\n        server_updates (int): Number of central server updates performed.\n        epsilon (float): Epsilon value from which to compute delta.\n\n    Returns:\n        (float): delta.\n    \"\"\"\n    deltas = []\n    for num_batch, sampling_strategy in zip(self.num_batches_per_client, self.sampling_strategies_per_client):\n        # Round up because privacy loss is monotonic wrt total_updates\n        total_updates = ceil(server_updates * self.epochs_per_round * num_batch)\n        delta = self.accountant.get_delta(sampling_strategy, self.noise_multiplier, total_updates, epsilon)\n        deltas.append(delta)\n    return max(deltas)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.ClientLevelAccountant","title":"<code>ClientLevelAccountant</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>class ClientLevelAccountant(ABC):\n    def __init__(self, noise_multiplier: float | list[float], moment_orders: list[float] | None = None) -&gt; None:\n        \"\"\"\n        Accountant to be used when measuring Client Level DP in FL training.\n\n        Args:\n            noise_multiplier (float | list[float]): The noise multiplier being applied to weights before transfer to\n                the server.\n            moment_orders (list[float] | None, optional): Basis orders to be used by the accountant for approximation.\n                of the RDP values. Defaults to None.\n        \"\"\"\n        self.noise_multiplier = noise_multiplier\n        self.accountant = MomentsAccountant(moment_orders)\n\n    @abstractmethod\n    def get_epsilon(self, server_updates: int | list[int], delta: float) -&gt; float:\n        pass\n\n    @abstractmethod\n    def get_delta(self, server_updates: int | list[int], epsilon: float) -&gt; float:\n        pass\n\n    def _validate_server_updates(self, server_updates: int | list[int]) -&gt; None:\n        if isinstance(server_updates, list):\n            assert isinstance(self.noise_multiplier, list)\n            assert len(server_updates) == len(self.noise_multiplier)\n        else:\n            assert isinstance(self.noise_multiplier, float)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.ClientLevelAccountant.__init__","title":"<code>__init__(noise_multiplier, moment_orders=None)</code>","text":"<p>Accountant to be used when measuring Client Level DP in FL training.</p> <p>Parameters:</p> Name Type Description Default <code>noise_multiplier</code> <code>float | list[float]</code> <p>The noise multiplier being applied to weights before transfer to the server.</p> required <code>moment_orders</code> <code>list[float] | None</code> <p>Basis orders to be used by the accountant for approximation. of the RDP values. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>def __init__(self, noise_multiplier: float | list[float], moment_orders: list[float] | None = None) -&gt; None:\n    \"\"\"\n    Accountant to be used when measuring Client Level DP in FL training.\n\n    Args:\n        noise_multiplier (float | list[float]): The noise multiplier being applied to weights before transfer to\n            the server.\n        moment_orders (list[float] | None, optional): Basis orders to be used by the accountant for approximation.\n            of the RDP values. Defaults to None.\n    \"\"\"\n    self.noise_multiplier = noise_multiplier\n    self.accountant = MomentsAccountant(moment_orders)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlClientLevelAccountantPoissonSampling","title":"<code>FlClientLevelAccountantPoissonSampling</code>","text":"<p>               Bases: <code>ClientLevelAccountant</code></p> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>class FlClientLevelAccountantPoissonSampling(ClientLevelAccountant):\n    def __init__(\n        self,\n        client_sampling_rate: float | list[float],\n        noise_multiplier: float | list[float],\n        moment_orders: list[float] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This accountant should be used when applying FL with Poisson client sampling and measuring client-level\n        privacy.\n\n        **NOTE**: The above values can be lists, where they are treated as sequences of training with the respective\n        parameters\n\n        Args:\n            client_sampling_rate (float | list[float]): Probability that each client will be included in a round.\n            noise_multiplier (float | list[float]): Multiplier of noise std. dev. on clipping bound.\n            moment_orders (list[float] | None, optional): Moments orders to be used in computing the approximate\n                epsilon value. Defaults to None. Defaults to None.\n        \"\"\"\n        super().__init__(noise_multiplier, moment_orders)\n        self.sampling_strategy: SamplingStrategy | list[PoissonSampling]\n\n        if isinstance(client_sampling_rate, list):\n            self.sampling_strategy = [PoissonSampling(q) for q in client_sampling_rate]\n        else:\n            self.sampling_strategy = PoissonSampling(client_sampling_rate)\n\n    def get_epsilon(self, server_updates: int | list[int], delta: float) -&gt; float:\n        \"\"\"\n        Compute the epsilon value for the provided delta and the number of server updates performed.\n\n        Args:\n            server_updates (int | list[int]): Number of central server updates performed.\n            delta (float): Delta value from which to compute epsilon.\n\n        Returns:\n            (float): epsilon.\n        \"\"\"\n        self._validate_server_updates(server_updates)\n        return self.accountant.get_epsilon(self.sampling_strategy, self.noise_multiplier, server_updates, delta)\n\n    def get_delta(self, server_updates: int | list[int], epsilon: float) -&gt; float:\n        \"\"\"\n        Compute the delta value for the provided epsilon and the number of server updates performed.\n\n        Args:\n            server_updates (int | list[int]): Number of central server updates performed.\n            epsilon (float): Epsilon value from which to compute delta.\n\n        Returns:\n            (float): delta.\n        \"\"\"\n        self._validate_server_updates(server_updates)\n        return self.accountant.get_delta(self.sampling_strategy, self.noise_multiplier, server_updates, epsilon)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlClientLevelAccountantPoissonSampling.__init__","title":"<code>__init__(client_sampling_rate, noise_multiplier, moment_orders=None)</code>","text":"<p>This accountant should be used when applying FL with Poisson client sampling and measuring client-level privacy.</p> <p>NOTE: The above values can be lists, where they are treated as sequences of training with the respective parameters</p> <p>Parameters:</p> Name Type Description Default <code>client_sampling_rate</code> <code>float | list[float]</code> <p>Probability that each client will be included in a round.</p> required <code>noise_multiplier</code> <code>float | list[float]</code> <p>Multiplier of noise std. dev. on clipping bound.</p> required <code>moment_orders</code> <code>list[float] | None</code> <p>Moments orders to be used in computing the approximate epsilon value. Defaults to None. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>def __init__(\n    self,\n    client_sampling_rate: float | list[float],\n    noise_multiplier: float | list[float],\n    moment_orders: list[float] | None = None,\n) -&gt; None:\n    \"\"\"\n    This accountant should be used when applying FL with Poisson client sampling and measuring client-level\n    privacy.\n\n    **NOTE**: The above values can be lists, where they are treated as sequences of training with the respective\n    parameters\n\n    Args:\n        client_sampling_rate (float | list[float]): Probability that each client will be included in a round.\n        noise_multiplier (float | list[float]): Multiplier of noise std. dev. on clipping bound.\n        moment_orders (list[float] | None, optional): Moments orders to be used in computing the approximate\n            epsilon value. Defaults to None. Defaults to None.\n    \"\"\"\n    super().__init__(noise_multiplier, moment_orders)\n    self.sampling_strategy: SamplingStrategy | list[PoissonSampling]\n\n    if isinstance(client_sampling_rate, list):\n        self.sampling_strategy = [PoissonSampling(q) for q in client_sampling_rate]\n    else:\n        self.sampling_strategy = PoissonSampling(client_sampling_rate)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlClientLevelAccountantPoissonSampling.get_epsilon","title":"<code>get_epsilon(server_updates, delta)</code>","text":"<p>Compute the epsilon value for the provided delta and the number of server updates performed.</p> <p>Parameters:</p> Name Type Description Default <code>server_updates</code> <code>int | list[int]</code> <p>Number of central server updates performed.</p> required <code>delta</code> <code>float</code> <p>Delta value from which to compute epsilon.</p> required <p>Returns:</p> Type Description <code>float</code> <p>epsilon.</p> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>def get_epsilon(self, server_updates: int | list[int], delta: float) -&gt; float:\n    \"\"\"\n    Compute the epsilon value for the provided delta and the number of server updates performed.\n\n    Args:\n        server_updates (int | list[int]): Number of central server updates performed.\n        delta (float): Delta value from which to compute epsilon.\n\n    Returns:\n        (float): epsilon.\n    \"\"\"\n    self._validate_server_updates(server_updates)\n    return self.accountant.get_epsilon(self.sampling_strategy, self.noise_multiplier, server_updates, delta)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlClientLevelAccountantPoissonSampling.get_delta","title":"<code>get_delta(server_updates, epsilon)</code>","text":"<p>Compute the delta value for the provided epsilon and the number of server updates performed.</p> <p>Parameters:</p> Name Type Description Default <code>server_updates</code> <code>int | list[int]</code> <p>Number of central server updates performed.</p> required <code>epsilon</code> <code>float</code> <p>Epsilon value from which to compute delta.</p> required <p>Returns:</p> Type Description <code>float</code> <p>delta.</p> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>def get_delta(self, server_updates: int | list[int], epsilon: float) -&gt; float:\n    \"\"\"\n    Compute the delta value for the provided epsilon and the number of server updates performed.\n\n    Args:\n        server_updates (int | list[int]): Number of central server updates performed.\n        epsilon (float): Epsilon value from which to compute delta.\n\n    Returns:\n        (float): delta.\n    \"\"\"\n    self._validate_server_updates(server_updates)\n    return self.accountant.get_delta(self.sampling_strategy, self.noise_multiplier, server_updates, epsilon)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlClientLevelAccountantFixedSamplingNoReplacement","title":"<code>FlClientLevelAccountantFixedSamplingNoReplacement</code>","text":"<p>               Bases: <code>ClientLevelAccountant</code></p> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>class FlClientLevelAccountantFixedSamplingNoReplacement(ClientLevelAccountant):\n    def __init__(\n        self,\n        n_total_clients: int,\n        n_clients_sampled: int | list[int],\n        noise_multiplier: float | list[float],\n        moment_orders: list[float] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        This accountant should be used when applying FL with Fixed Sampling with No Replacement and measuring\n        client-level privacy.\n\n        **NOTE**: The above values can be lists, where they are treated as sequences of training with the respective\n        parameters\n\n        Args:\n            n_total_clients (int): Total number of clients to be sampled from.\n            n_clients_sampled (int | list[int]): Number of clients sampled in a given round.\n            noise_multiplier (float | list[float]): Multiplier of noise std. dev. on clipping bound.\n            moment_orders (list[float] | None, optional): Moments orders to be used in computing the approximate\n                epsilon value. Defaults to None. Defaults to None.\n        \"\"\"\n        super().__init__(noise_multiplier, moment_orders)\n        self.sampling_strategy: SamplingStrategy | list[FixedSamplingWithoutReplacement]\n\n        if isinstance(n_clients_sampled, list):\n            self.sampling_strategy = [\n                FixedSamplingWithoutReplacement(n_total_clients, n_clients) for n_clients in n_clients_sampled\n            ]\n        else:\n            self.sampling_strategy = FixedSamplingWithoutReplacement(n_total_clients, n_clients_sampled)\n\n    def get_epsilon(self, server_updates: int | list[int], delta: float) -&gt; float:\n        \"\"\"\n        Compute the epsilon value for the provided delta and the number of server updates performed.\n\n        Args:\n            server_updates (int | list[int]): Number of central server updates performed.\n            delta (float): Delta value from which to compute epsilon.\n\n        Returns:\n            (float): epsilon.\n        \"\"\"\n        self._validate_server_updates(server_updates)\n        return self.accountant.get_epsilon(self.sampling_strategy, self.noise_multiplier, server_updates, delta)\n\n    def get_delta(self, server_updates: int | list[int], epsilon: float) -&gt; float:\n        \"\"\"\n        Compute the delta value for the provided epsilon and the number of server updates performed.\n\n        Args:\n            server_updates (int | list[int]): Number of central server updates performed.\n            epsilon (float): Epsilon value from which to compute delta.\n\n        Returns:\n            (float): delta.\n        \"\"\"\n        self._validate_server_updates(server_updates)\n        return self.accountant.get_delta(self.sampling_strategy, self.noise_multiplier, server_updates, epsilon)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlClientLevelAccountantFixedSamplingNoReplacement.__init__","title":"<code>__init__(n_total_clients, n_clients_sampled, noise_multiplier, moment_orders=None)</code>","text":"<p>This accountant should be used when applying FL with Fixed Sampling with No Replacement and measuring client-level privacy.</p> <p>NOTE: The above values can be lists, where they are treated as sequences of training with the respective parameters</p> <p>Parameters:</p> Name Type Description Default <code>n_total_clients</code> <code>int</code> <p>Total number of clients to be sampled from.</p> required <code>n_clients_sampled</code> <code>int | list[int]</code> <p>Number of clients sampled in a given round.</p> required <code>noise_multiplier</code> <code>float | list[float]</code> <p>Multiplier of noise std. dev. on clipping bound.</p> required <code>moment_orders</code> <code>list[float] | None</code> <p>Moments orders to be used in computing the approximate epsilon value. Defaults to None. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>def __init__(\n    self,\n    n_total_clients: int,\n    n_clients_sampled: int | list[int],\n    noise_multiplier: float | list[float],\n    moment_orders: list[float] | None = None,\n) -&gt; None:\n    \"\"\"\n    This accountant should be used when applying FL with Fixed Sampling with No Replacement and measuring\n    client-level privacy.\n\n    **NOTE**: The above values can be lists, where they are treated as sequences of training with the respective\n    parameters\n\n    Args:\n        n_total_clients (int): Total number of clients to be sampled from.\n        n_clients_sampled (int | list[int]): Number of clients sampled in a given round.\n        noise_multiplier (float | list[float]): Multiplier of noise std. dev. on clipping bound.\n        moment_orders (list[float] | None, optional): Moments orders to be used in computing the approximate\n            epsilon value. Defaults to None. Defaults to None.\n    \"\"\"\n    super().__init__(noise_multiplier, moment_orders)\n    self.sampling_strategy: SamplingStrategy | list[FixedSamplingWithoutReplacement]\n\n    if isinstance(n_clients_sampled, list):\n        self.sampling_strategy = [\n            FixedSamplingWithoutReplacement(n_total_clients, n_clients) for n_clients in n_clients_sampled\n        ]\n    else:\n        self.sampling_strategy = FixedSamplingWithoutReplacement(n_total_clients, n_clients_sampled)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlClientLevelAccountantFixedSamplingNoReplacement.get_epsilon","title":"<code>get_epsilon(server_updates, delta)</code>","text":"<p>Compute the epsilon value for the provided delta and the number of server updates performed.</p> <p>Parameters:</p> Name Type Description Default <code>server_updates</code> <code>int | list[int]</code> <p>Number of central server updates performed.</p> required <code>delta</code> <code>float</code> <p>Delta value from which to compute epsilon.</p> required <p>Returns:</p> Type Description <code>float</code> <p>epsilon.</p> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>def get_epsilon(self, server_updates: int | list[int], delta: float) -&gt; float:\n    \"\"\"\n    Compute the epsilon value for the provided delta and the number of server updates performed.\n\n    Args:\n        server_updates (int | list[int]): Number of central server updates performed.\n        delta (float): Delta value from which to compute epsilon.\n\n    Returns:\n        (float): epsilon.\n    \"\"\"\n    self._validate_server_updates(server_updates)\n    return self.accountant.get_epsilon(self.sampling_strategy, self.noise_multiplier, server_updates, delta)\n</code></pre>"},{"location":"api/#fl4health.privacy.fl_accountants.FlClientLevelAccountantFixedSamplingNoReplacement.get_delta","title":"<code>get_delta(server_updates, epsilon)</code>","text":"<p>Compute the delta value for the provided epsilon and the number of server updates performed.</p> <p>Parameters:</p> Name Type Description Default <code>server_updates</code> <code>int | list[int]</code> <p>Number of central server updates performed.</p> required <code>epsilon</code> <code>float</code> <p>Epsilon value from which to compute delta.</p> required <p>Returns:</p> Type Description <code>float</code> <p>delta.</p> Source code in <code>fl4health/privacy/fl_accountants.py</code> <pre><code>def get_delta(self, server_updates: int | list[int], epsilon: float) -&gt; float:\n    \"\"\"\n    Compute the delta value for the provided epsilon and the number of server updates performed.\n\n    Args:\n        server_updates (int | list[int]): Number of central server updates performed.\n        epsilon (float): Epsilon value from which to compute delta.\n\n    Returns:\n        (float): delta.\n    \"\"\"\n    self._validate_server_updates(server_updates)\n    return self.accountant.get_delta(self.sampling_strategy, self.noise_multiplier, server_updates, epsilon)\n</code></pre>"},{"location":"api/#fl4health.privacy.moments_accountant","title":"<code>moments_accountant</code>","text":""},{"location":"api/#fl4health.privacy.moments_accountant.SamplingStrategy","title":"<code>SamplingStrategy</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>fl4health/privacy/moments_accountant.py</code> <pre><code>class SamplingStrategy(ABC):\n    def __init__(self, neighbor_relation: NeighborRel) -&gt; None:\n        \"\"\"\n        Abstract base class. It holds information about the privacy accountant NeighborRel applied.\n\n        Args:\n            neighbor_relation (NeighborRel): The kind of neighbor relation that should be used in accounting.\n        \"\"\"\n        self.neighbor_relation = neighbor_relation\n\n    @abstractmethod\n    def get_dp_event(self, noise_event: DpEvent) -&gt; DpEvent:\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.privacy.moments_accountant.SamplingStrategy.__init__","title":"<code>__init__(neighbor_relation)</code>","text":"<p>Abstract base class. It holds information about the privacy accountant NeighborRel applied.</p> <p>Parameters:</p> Name Type Description Default <code>neighbor_relation</code> <code>NeighborRel</code> <p>The kind of neighbor relation that should be used in accounting.</p> required Source code in <code>fl4health/privacy/moments_accountant.py</code> <pre><code>def __init__(self, neighbor_relation: NeighborRel) -&gt; None:\n    \"\"\"\n    Abstract base class. It holds information about the privacy accountant NeighborRel applied.\n\n    Args:\n        neighbor_relation (NeighborRel): The kind of neighbor relation that should be used in accounting.\n    \"\"\"\n    self.neighbor_relation = neighbor_relation\n</code></pre>"},{"location":"api/#fl4health.privacy.moments_accountant.PoissonSampling","title":"<code>PoissonSampling</code>","text":"<p>               Bases: <code>SamplingStrategy</code></p> Source code in <code>fl4health/privacy/moments_accountant.py</code> <pre><code>class PoissonSampling(SamplingStrategy):\n    def __init__(self, sampling_ratio: float) -&gt; None:\n        \"\"\"\n        DP Event type that stores important information about sampling statistics and sets the proper ``NeighborRel``\n        value This class is specific to Poisson sampling.\n\n        Args:\n            sampling_ratio (float): Poisson sampling ratio used.\n        \"\"\"\n        self.sampling_ratio = sampling_ratio\n        super().__init__(NeighborRel.ADD_OR_REMOVE_ONE)\n\n    def get_dp_event(self, noise_event: DpEvent) -&gt; DpEvent:\n        return PoissonSampledDpEvent(self.sampling_ratio, noise_event)\n</code></pre>"},{"location":"api/#fl4health.privacy.moments_accountant.PoissonSampling.__init__","title":"<code>__init__(sampling_ratio)</code>","text":"<p>DP Event type that stores important information about sampling statistics and sets the proper <code>NeighborRel</code> value This class is specific to Poisson sampling.</p> <p>Parameters:</p> Name Type Description Default <code>sampling_ratio</code> <code>float</code> <p>Poisson sampling ratio used.</p> required Source code in <code>fl4health/privacy/moments_accountant.py</code> <pre><code>def __init__(self, sampling_ratio: float) -&gt; None:\n    \"\"\"\n    DP Event type that stores important information about sampling statistics and sets the proper ``NeighborRel``\n    value This class is specific to Poisson sampling.\n\n    Args:\n        sampling_ratio (float): Poisson sampling ratio used.\n    \"\"\"\n    self.sampling_ratio = sampling_ratio\n    super().__init__(NeighborRel.ADD_OR_REMOVE_ONE)\n</code></pre>"},{"location":"api/#fl4health.privacy.moments_accountant.FixedSamplingWithoutReplacement","title":"<code>FixedSamplingWithoutReplacement</code>","text":"<p>               Bases: <code>SamplingStrategy</code></p> Source code in <code>fl4health/privacy/moments_accountant.py</code> <pre><code>class FixedSamplingWithoutReplacement(SamplingStrategy):\n    def __init__(self, population_size: int, sample_size: int) -&gt; None:\n        \"\"\"\n        DP Event type that stores important information about sampling statistics and sets the proper ``NeighborRel``\n        value This class is specific to fixed sampling without replacement.\n\n        Args:\n            population_size (int): Size of the total population from which sampling is performed.\n            sample_size (int): Size of the desired sample.\n        \"\"\"\n        self.population_size = population_size\n        self.sample_size = sample_size\n        super().__init__(NeighborRel.REPLACE_ONE)\n\n    def get_dp_event(self, noise_event: DpEvent) -&gt; DpEvent:\n        return SampledWithoutReplacementDpEvent(self.population_size, self.sample_size, noise_event)\n</code></pre>"},{"location":"api/#fl4health.privacy.moments_accountant.FixedSamplingWithoutReplacement.__init__","title":"<code>__init__(population_size, sample_size)</code>","text":"<p>DP Event type that stores important information about sampling statistics and sets the proper <code>NeighborRel</code> value This class is specific to fixed sampling without replacement.</p> <p>Parameters:</p> Name Type Description Default <code>population_size</code> <code>int</code> <p>Size of the total population from which sampling is performed.</p> required <code>sample_size</code> <code>int</code> <p>Size of the desired sample.</p> required Source code in <code>fl4health/privacy/moments_accountant.py</code> <pre><code>def __init__(self, population_size: int, sample_size: int) -&gt; None:\n    \"\"\"\n    DP Event type that stores important information about sampling statistics and sets the proper ``NeighborRel``\n    value This class is specific to fixed sampling without replacement.\n\n    Args:\n        population_size (int): Size of the total population from which sampling is performed.\n        sample_size (int): Size of the desired sample.\n    \"\"\"\n    self.population_size = population_size\n    self.sample_size = sample_size\n    super().__init__(NeighborRel.REPLACE_ONE)\n</code></pre>"},{"location":"api/#fl4health.privacy.moments_accountant.MomentsAccountant","title":"<code>MomentsAccountant</code>","text":"Source code in <code>fl4health/privacy/moments_accountant.py</code> <pre><code>class MomentsAccountant:\n    def __init__(self, moment_orders: list[float] | None = None) -&gt; None:\n        \"\"\"\n\n        Moment orders are equivalent to lambda from Deep Learning with Differential Privacy (Abadi et. al. 2016).\n        They form the set of moments to estimate the infimum of Theorem 2 part 2. The default values were taken from\n        the tensorflow federated DP tutorial notebook:\n\n        https://github.com/tensorflow/federated/blob/main/docs/tutorials/federated_learning_with_differential_privacy.ipynb\n\n        In the paper above, they state that trying lambda &lt;= 32 is usually sufficient.\n\n        Sampling type is the data point sampling strategy: i.e. examples from dataset for a batch with probability q\n        Noise type specifies whether Gaussian or Laplacian noise is added to the updates\n\n        Args:\n            moment_orders (list[float] | None, optional): See description above. Defaults to None.\n        \"\"\"  # noqa\n        if moment_orders is not None:\n            self.moment_orders = moment_orders\n        else:\n            low_orders = [1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 3.0, 3.5, 4.0, 4.5]\n            medium_orders: list[float] = list(range(5, 64))\n            high_orders = [128.0, 256.0, 512.0]\n            self.moment_orders = low_orders + medium_orders + high_orders\n\n    def _construct_dp_events(\n        self, sampling_strategy: SamplingStrategy, noise_multiplier: float, updates: int\n    ) -&gt; DpEvent:\n        # Type of noise used for DP on gradient updates\n        noise_event = GaussianDpEvent(noise_multiplier)\n        # Type of strategy used to select datapoints (or clients in user-level FL DP)\n        sampling_event = sampling_strategy.get_dp_event(noise_event)\n        # Number of times the above two procedures are performed (e.g epochs*batches for DP-SGD)\n        return SelfComposedDpEvent(sampling_event, updates)\n\n    def _construct_dp_events_trajectory(\n        self,\n        sampling_strategies: Sequence[SamplingStrategy],\n        noise_multipliers: list[float],\n        updates_list: list[int],\n    ) -&gt; DpEvent:\n        # Given a list of parameters this assumes that the DP operations were performed in sequence\n        event_builder = DpEventBuilder()\n        for sampling_strategy, noise_multiplier, updates in zip(sampling_strategies, noise_multipliers, updates_list):\n            event_builder.compose(self._construct_dp_events(sampling_strategy, noise_multiplier, updates), 1)\n        return event_builder.build()\n\n    def _construct_rdp_accountant(\n        self,\n        sampling_strategies: SamplingStrategy | Sequence[SamplingStrategy],\n        noise_multipliers: float | list[float],\n        updates: int | list[int],\n    ) -&gt; RdpAccountant:\n        if isinstance(sampling_strategies, SamplingStrategy):\n            sampling_strategies = [sampling_strategies]\n        if isinstance(noise_multipliers, float):\n            noise_multipliers = [noise_multipliers]\n        if isinstance(updates, int):\n            updates = [updates]\n\n        # First we construct the DP events for the accountant to accumulate\n        dp_events = self._construct_dp_events_trajectory(sampling_strategies, noise_multipliers, updates)\n        # Setup accountant with set of moments to minimize over\n        rdp_accountant = RdpAccountant(self.moment_orders, sampling_strategies[0].neighbor_relation)\n        # Set accountant internal state about applied events\n        rdp_accountant.compose(dp_events)\n        return rdp_accountant\n\n    def _validate_accountant_input(\n        self,\n        sampling_strategies: SamplingStrategy | Sequence[SamplingStrategy],\n        noise_multiplier: float | list[float],\n        updates: int | list[int],\n    ) -&gt; None:\n        all_lists = all(\n            [\n                isinstance(sampling_strategies, Sequence)\n                and isinstance(noise_multiplier, list)\n                and isinstance(updates, list)\n            ]\n        )\n        all_values = all(\n            [\n                isinstance(sampling_strategies, SamplingStrategy)\n                and isinstance(noise_multiplier, float)\n                and isinstance(updates, int)\n            ]\n        )\n        assert all_lists or all_values\n\n    def get_epsilon(\n        self,\n        sampling_strategies: SamplingStrategy | Sequence[SamplingStrategy],\n        noise_multiplier: float | list[float],\n        updates: int | list[int],\n        delta: float,\n    ) -&gt; float:\n        \"\"\"\n        If the parameters are lists, then it is assumed that the training applied the parameters in a sequence of\n        updates.\n\n        Ex.\n\n        ```python\n        sampling_strategies = [PoissonSampling(q_1), PoissonSampling(q_2)]\n\n        noise_multiplier = [z_1, z_2]\n\n        updates = [t_1, t_2]\n        ```\n\n        implies that ``q_1``, ``z_1`` were applied for ``t_1`` updates, followed by ``q_2``, ``z_2`` for ``t_2``\n        updates.\n\n        Args:\n            sampling_strategies (SamplingStrategy | Sequence[SamplingStrategy]): Are the type of sampling done for\n                each datapoint or client in the DP procedure. This is either Poisson sampling with a sampling rate\n                specified or Fixed ratio sampling with a fixed number of selections performed over a specified\n                population size. For non-FL DP-SGD: This is the ratio of batch size to dataset size (``L/N``, from Deep\n                Learning with Differential Privacy). For FL with client-side DP-SGD (no noise on server side, instance\n                level privacy): This is the ratio of client sampling probability to client data point probability\n                ``q*(b_k/n_k)`` For FL with client privacy: This is the sampling of clients from the client population\n\n                **NOTE**: If a sequence of strategies is given, they must be all of the same kind (that is poisson or\n                subset, but may have different parameters)\n            noise_multiplier (float | list[float]): Ratio of the noise standard deviation to clipping bound (sigma in\n                Deep Learning with Differential Privacy, ``z`` in some other implementations).\n            updates (int | list[int]): This is the number of noise applications to the update weights. For non-FL\n                DP-SGD: This is the number of updates run (``epochs*batches`` per epoch) For FL w/ client-side DP-SGD\n                (instance DP): This is the number of batches run per client (if selected every time),\n                ``server_updates*epochs per server update*batches per epoch``. For FL with client privacy: Number of\n                server updates.\n            delta (float): This is the delta in (epsilon, delta)-Privacy, that we require.\n\n        Returns:\n            (float): The corresponding epsilon\n        \"\"\"\n        self._validate_accountant_input(sampling_strategies, noise_multiplier, updates)\n        rdp_accountant = self._construct_rdp_accountant(sampling_strategies, noise_multiplier, updates)\n        # calculate minimum epsilon for fixed delta\n        return rdp_accountant.get_epsilon(delta)\n\n    def get_delta(\n        self,\n        sampling_strategies: SamplingStrategy | Sequence[SamplingStrategy],\n        noise_multiplier: float | list[float],\n        updates: int | list[int],\n        epsilon: float,\n    ) -&gt; float:\n        \"\"\"\n        If the parameters are lists, then it is assumed that the training applied the parameters in a sequence of\n        updates.\n\n        Ex.\n\n        ```python\n        sampling_strategies = [PoissonSampling(q_1), PoissonSampling(q_2)]\n\n        noise_multiplier = [z_1, z_2]\n\n        updates = [t_1, t_2]\n        ```\n\n        implies that ``q_1``, ``z_1`` were applied for ``t_1`` updates, followed by ``q_2``, ``z_2`` for ``t_2``\n        updates.\n\n        Args:\n            sampling_strategies (SamplingStrategy | Sequence[SamplingStrategy]): Are the type of sampling done for\n                each datapoint or client in the DP procedure. This is either Poisson sampling with a sampling rate\n                specified or Fixed ratio sampling with a fixed number of selections performed over a specified\n                population size. For non-FL DP-SGD: This is the ratio of batch size to dataset size (``L/N``, from Deep\n                Learning with Differential Privacy). For FL with client-side DP-SGD (no noise on server side,\n                instance level privacy): This is the ratio of client sampling probability to client data point\n                probability ``q*(b_k/n_k)`` For FL with client privacy: This is the sampling of clients from the client\n                population.\n\n                **NOTE**: If a sequence of strategies is given, they must be all of the same kind (that is poisson or\n                subset, but may have different parameters)\n            noise_multiplier (float | list[float]): Ratio of the noise standard deviation to clipping bound (sigma in\n                Deep Learning with Differential Privacy, z in some other implementations).\n            updates (int | list[int]): This is the number of noise applications to the update weights. For non-FL\n                DP-SGD: This is the number of updates run (``epochs*batches per epoch``) For FL w/ client-side DP-SGD\n                (instance DP): This is the number of batches run per client (if selected every time),\n                ``server_updates*epochs per server update*batches per epoch`` For FL with client privacy: Number of\n                server updates\n            epsilon (float): This is the epsilon in (epsilon, delta)-Privacy, that we require.\n\n        Returns:\n            (float): delta for the provided epsilon\n        \"\"\"\n        self._validate_accountant_input(sampling_strategies, noise_multiplier, updates)\n        rdp_accountant = self._construct_rdp_accountant(sampling_strategies, noise_multiplier, updates)\n        # calculate minimum delta for fixed epsilon\n        return rdp_accountant.get_delta(epsilon)\n</code></pre>"},{"location":"api/#fl4health.privacy.moments_accountant.MomentsAccountant.__init__","title":"<code>__init__(moment_orders=None)</code>","text":"<p>Moment orders are equivalent to lambda from Deep Learning with Differential Privacy (Abadi et. al. 2016). They form the set of moments to estimate the infimum of Theorem 2 part 2. The default values were taken from the tensorflow federated DP tutorial notebook:</p> <p>https://github.com/tensorflow/federated/blob/main/docs/tutorials/federated_learning_with_differential_privacy.ipynb</p> <p>In the paper above, they state that trying lambda &lt;= 32 is usually sufficient.</p> <p>Sampling type is the data point sampling strategy: i.e. examples from dataset for a batch with probability q Noise type specifies whether Gaussian or Laplacian noise is added to the updates</p> <p>Parameters:</p> Name Type Description Default <code>moment_orders</code> <code>list[float] | None</code> <p>See description above. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/privacy/moments_accountant.py</code> <pre><code>def __init__(self, moment_orders: list[float] | None = None) -&gt; None:\n    \"\"\"\n\n    Moment orders are equivalent to lambda from Deep Learning with Differential Privacy (Abadi et. al. 2016).\n    They form the set of moments to estimate the infimum of Theorem 2 part 2. The default values were taken from\n    the tensorflow federated DP tutorial notebook:\n\n    https://github.com/tensorflow/federated/blob/main/docs/tutorials/federated_learning_with_differential_privacy.ipynb\n\n    In the paper above, they state that trying lambda &lt;= 32 is usually sufficient.\n\n    Sampling type is the data point sampling strategy: i.e. examples from dataset for a batch with probability q\n    Noise type specifies whether Gaussian or Laplacian noise is added to the updates\n\n    Args:\n        moment_orders (list[float] | None, optional): See description above. Defaults to None.\n    \"\"\"  # noqa\n    if moment_orders is not None:\n        self.moment_orders = moment_orders\n    else:\n        low_orders = [1.25, 1.5, 1.75, 2.0, 2.25, 2.5, 3.0, 3.5, 4.0, 4.5]\n        medium_orders: list[float] = list(range(5, 64))\n        high_orders = [128.0, 256.0, 512.0]\n        self.moment_orders = low_orders + medium_orders + high_orders\n</code></pre>"},{"location":"api/#fl4health.privacy.moments_accountant.MomentsAccountant.get_epsilon","title":"<code>get_epsilon(sampling_strategies, noise_multiplier, updates, delta)</code>","text":"<p>If the parameters are lists, then it is assumed that the training applied the parameters in a sequence of updates.</p> <p>Ex.</p> <pre><code>sampling_strategies = [PoissonSampling(q_1), PoissonSampling(q_2)]\n\nnoise_multiplier = [z_1, z_2]\n\nupdates = [t_1, t_2]\n</code></pre> <p>implies that <code>q_1</code>, <code>z_1</code> were applied for <code>t_1</code> updates, followed by <code>q_2</code>, <code>z_2</code> for <code>t_2</code> updates.</p> <p>Parameters:</p> Name Type Description Default <code>sampling_strategies</code> <code>SamplingStrategy | Sequence[SamplingStrategy]</code> <p>Are the type of sampling done for each datapoint or client in the DP procedure. This is either Poisson sampling with a sampling rate specified or Fixed ratio sampling with a fixed number of selections performed over a specified population size. For non-FL DP-SGD: This is the ratio of batch size to dataset size (<code>L/N</code>, from Deep Learning with Differential Privacy). For FL with client-side DP-SGD (no noise on server side, instance level privacy): This is the ratio of client sampling probability to client data point probability <code>q*(b_k/n_k)</code> For FL with client privacy: This is the sampling of clients from the client population</p> <p>NOTE: If a sequence of strategies is given, they must be all of the same kind (that is poisson or subset, but may have different parameters)</p> required <code>noise_multiplier</code> <code>float | list[float]</code> <p>Ratio of the noise standard deviation to clipping bound (sigma in Deep Learning with Differential Privacy, <code>z</code> in some other implementations).</p> required <code>updates</code> <code>int | list[int]</code> <p>This is the number of noise applications to the update weights. For non-FL DP-SGD: This is the number of updates run (<code>epochs*batches</code> per epoch) For FL w/ client-side DP-SGD (instance DP): This is the number of batches run per client (if selected every time), <code>server_updates*epochs per server update*batches per epoch</code>. For FL with client privacy: Number of server updates.</p> required <code>delta</code> <code>float</code> <p>This is the delta in (epsilon, delta)-Privacy, that we require.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The corresponding epsilon</p> Source code in <code>fl4health/privacy/moments_accountant.py</code> <pre><code>def get_epsilon(\n    self,\n    sampling_strategies: SamplingStrategy | Sequence[SamplingStrategy],\n    noise_multiplier: float | list[float],\n    updates: int | list[int],\n    delta: float,\n) -&gt; float:\n    \"\"\"\n    If the parameters are lists, then it is assumed that the training applied the parameters in a sequence of\n    updates.\n\n    Ex.\n\n    ```python\n    sampling_strategies = [PoissonSampling(q_1), PoissonSampling(q_2)]\n\n    noise_multiplier = [z_1, z_2]\n\n    updates = [t_1, t_2]\n    ```\n\n    implies that ``q_1``, ``z_1`` were applied for ``t_1`` updates, followed by ``q_2``, ``z_2`` for ``t_2``\n    updates.\n\n    Args:\n        sampling_strategies (SamplingStrategy | Sequence[SamplingStrategy]): Are the type of sampling done for\n            each datapoint or client in the DP procedure. This is either Poisson sampling with a sampling rate\n            specified or Fixed ratio sampling with a fixed number of selections performed over a specified\n            population size. For non-FL DP-SGD: This is the ratio of batch size to dataset size (``L/N``, from Deep\n            Learning with Differential Privacy). For FL with client-side DP-SGD (no noise on server side, instance\n            level privacy): This is the ratio of client sampling probability to client data point probability\n            ``q*(b_k/n_k)`` For FL with client privacy: This is the sampling of clients from the client population\n\n            **NOTE**: If a sequence of strategies is given, they must be all of the same kind (that is poisson or\n            subset, but may have different parameters)\n        noise_multiplier (float | list[float]): Ratio of the noise standard deviation to clipping bound (sigma in\n            Deep Learning with Differential Privacy, ``z`` in some other implementations).\n        updates (int | list[int]): This is the number of noise applications to the update weights. For non-FL\n            DP-SGD: This is the number of updates run (``epochs*batches`` per epoch) For FL w/ client-side DP-SGD\n            (instance DP): This is the number of batches run per client (if selected every time),\n            ``server_updates*epochs per server update*batches per epoch``. For FL with client privacy: Number of\n            server updates.\n        delta (float): This is the delta in (epsilon, delta)-Privacy, that we require.\n\n    Returns:\n        (float): The corresponding epsilon\n    \"\"\"\n    self._validate_accountant_input(sampling_strategies, noise_multiplier, updates)\n    rdp_accountant = self._construct_rdp_accountant(sampling_strategies, noise_multiplier, updates)\n    # calculate minimum epsilon for fixed delta\n    return rdp_accountant.get_epsilon(delta)\n</code></pre>"},{"location":"api/#fl4health.privacy.moments_accountant.MomentsAccountant.get_delta","title":"<code>get_delta(sampling_strategies, noise_multiplier, updates, epsilon)</code>","text":"<p>If the parameters are lists, then it is assumed that the training applied the parameters in a sequence of updates.</p> <p>Ex.</p> <pre><code>sampling_strategies = [PoissonSampling(q_1), PoissonSampling(q_2)]\n\nnoise_multiplier = [z_1, z_2]\n\nupdates = [t_1, t_2]\n</code></pre> <p>implies that <code>q_1</code>, <code>z_1</code> were applied for <code>t_1</code> updates, followed by <code>q_2</code>, <code>z_2</code> for <code>t_2</code> updates.</p> <p>Parameters:</p> Name Type Description Default <code>sampling_strategies</code> <code>SamplingStrategy | Sequence[SamplingStrategy]</code> <p>Are the type of sampling done for each datapoint or client in the DP procedure. This is either Poisson sampling with a sampling rate specified or Fixed ratio sampling with a fixed number of selections performed over a specified population size. For non-FL DP-SGD: This is the ratio of batch size to dataset size (<code>L/N</code>, from Deep Learning with Differential Privacy). For FL with client-side DP-SGD (no noise on server side, instance level privacy): This is the ratio of client sampling probability to client data point probability <code>q*(b_k/n_k)</code> For FL with client privacy: This is the sampling of clients from the client population.</p> <p>NOTE: If a sequence of strategies is given, they must be all of the same kind (that is poisson or subset, but may have different parameters)</p> required <code>noise_multiplier</code> <code>float | list[float]</code> <p>Ratio of the noise standard deviation to clipping bound (sigma in Deep Learning with Differential Privacy, z in some other implementations).</p> required <code>updates</code> <code>int | list[int]</code> <p>This is the number of noise applications to the update weights. For non-FL DP-SGD: This is the number of updates run (<code>epochs*batches per epoch</code>) For FL w/ client-side DP-SGD (instance DP): This is the number of batches run per client (if selected every time), <code>server_updates*epochs per server update*batches per epoch</code> For FL with client privacy: Number of server updates</p> required <code>epsilon</code> <code>float</code> <p>This is the epsilon in (epsilon, delta)-Privacy, that we require.</p> required <p>Returns:</p> Type Description <code>float</code> <p>delta for the provided epsilon</p> Source code in <code>fl4health/privacy/moments_accountant.py</code> <pre><code>def get_delta(\n    self,\n    sampling_strategies: SamplingStrategy | Sequence[SamplingStrategy],\n    noise_multiplier: float | list[float],\n    updates: int | list[int],\n    epsilon: float,\n) -&gt; float:\n    \"\"\"\n    If the parameters are lists, then it is assumed that the training applied the parameters in a sequence of\n    updates.\n\n    Ex.\n\n    ```python\n    sampling_strategies = [PoissonSampling(q_1), PoissonSampling(q_2)]\n\n    noise_multiplier = [z_1, z_2]\n\n    updates = [t_1, t_2]\n    ```\n\n    implies that ``q_1``, ``z_1`` were applied for ``t_1`` updates, followed by ``q_2``, ``z_2`` for ``t_2``\n    updates.\n\n    Args:\n        sampling_strategies (SamplingStrategy | Sequence[SamplingStrategy]): Are the type of sampling done for\n            each datapoint or client in the DP procedure. This is either Poisson sampling with a sampling rate\n            specified or Fixed ratio sampling with a fixed number of selections performed over a specified\n            population size. For non-FL DP-SGD: This is the ratio of batch size to dataset size (``L/N``, from Deep\n            Learning with Differential Privacy). For FL with client-side DP-SGD (no noise on server side,\n            instance level privacy): This is the ratio of client sampling probability to client data point\n            probability ``q*(b_k/n_k)`` For FL with client privacy: This is the sampling of clients from the client\n            population.\n\n            **NOTE**: If a sequence of strategies is given, they must be all of the same kind (that is poisson or\n            subset, but may have different parameters)\n        noise_multiplier (float | list[float]): Ratio of the noise standard deviation to clipping bound (sigma in\n            Deep Learning with Differential Privacy, z in some other implementations).\n        updates (int | list[int]): This is the number of noise applications to the update weights. For non-FL\n            DP-SGD: This is the number of updates run (``epochs*batches per epoch``) For FL w/ client-side DP-SGD\n            (instance DP): This is the number of batches run per client (if selected every time),\n            ``server_updates*epochs per server update*batches per epoch`` For FL with client privacy: Number of\n            server updates\n        epsilon (float): This is the epsilon in (epsilon, delta)-Privacy, that we require.\n\n    Returns:\n        (float): delta for the provided epsilon\n    \"\"\"\n    self._validate_accountant_input(sampling_strategies, noise_multiplier, updates)\n    rdp_accountant = self._construct_rdp_accountant(sampling_strategies, noise_multiplier, updates)\n    # calculate minimum delta for fixed epsilon\n    return rdp_accountant.get_delta(epsilon)\n</code></pre>"},{"location":"api/#fl4health.reporting","title":"<code>reporting</code>","text":""},{"location":"api/#fl4health.reporting.JsonReporter","title":"<code>JsonReporter</code>","text":"<p>               Bases: <code>FileReporter</code></p> Source code in <code>fl4health/reporting/json_reporter.py</code> <pre><code>class JsonReporter(FileReporter):\n    def dump(self) -&gt; None:\n        \"\"\"Dumps the current metrics to a JSON file at ``{output_folder}/{run_id.json}``.\"\"\"\n        assert self.run_id is not None\n        output_file_path = Path(self.output_folder, self.run_id).with_suffix(\".json\")\n        log(INFO, f\"Dumping metrics to {str(output_file_path)}\")\n\n        with open(output_file_path, \"w\") as output_file:\n            json.dump(self.metrics, output_file, indent=4)\n</code></pre>"},{"location":"api/#fl4health.reporting.JsonReporter.dump","title":"<code>dump()</code>","text":"<p>Dumps the current metrics to a JSON file at <code>{output_folder}/{run_id.json}</code>.</p> Source code in <code>fl4health/reporting/json_reporter.py</code> <pre><code>def dump(self) -&gt; None:\n    \"\"\"Dumps the current metrics to a JSON file at ``{output_folder}/{run_id.json}``.\"\"\"\n    assert self.run_id is not None\n    output_file_path = Path(self.output_folder, self.run_id).with_suffix(\".json\")\n    log(INFO, f\"Dumping metrics to {str(output_file_path)}\")\n\n    with open(output_file_path, \"w\") as output_file:\n        json.dump(self.metrics, output_file, indent=4)\n</code></pre>"},{"location":"api/#fl4health.reporting.WandBReporter","title":"<code>WandBReporter</code>","text":"<p>               Bases: <code>BaseReporter</code></p> Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>class WandBReporter(BaseReporter):\n    def __init__(\n        self,\n        wandb_step_type: WandBStepType | str = WandBStepType.ROUND,\n        project: str | None = None,\n        entity: str | None = None,\n        config: dict | str | None = None,\n        group: str | None = None,\n        job_type: str | None = None,\n        tags: list[str] | None = None,\n        name: str | None = None,\n        id: str | None = None,\n        resume: Literal[\"allow\", \"never\", \"must\", \"auto\"] | bool | None = \"allow\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Weights and Biases Reporter for logging experimental results associated with FL runs.\n\n        Args:\n            wandb_step_type (StepType | str, optional): Whether to use the \"round\", \"epoch\" or \"step\" as the\n                ``wandb_step`` value when logging information to the wandb server.\n            project (str | None, optional): The name of the project where you're sending the new run. If unspecified,\n                wandb will try to infer or set to \"uncategorized\"\n            entity (str | None, optional): An entity is a username or team name where you're sending runs. This entity\n                must exist before you can send runs there, so make sure to create your account or team in the UI before\n                starting to log runs. If you do not specify an entity, the run will be sent to your default entity.\n                Change your default entity in your settings under \"default location to create new projects\".\n            config (str | None, optional): This sets ``wandb.config``, a dictionary-like object for saving inputs to\n                your job such as hyperparameters for a model.\n\n                - If ``dict``: will load the key value pairs into the  ``wandb.config`` object.\n                - If ``str``: will look for a yaml file by that name, and load config from that file into the\n                  ``wandb.config`` object.\n            group (str | None, optional): Specify a group to organize individual runs into a larger experiment.\n            job_type (str | None, optional): Specify the type of run, useful when grouping runs.\n            tags (list[str] |None, optional): A list of strings, which will populate the list of tags on this run. If\n                you want to add tags to a resumed run without overwriting its existing tags, use ``run.tags +=\n                [\"new_tag\"]`` after ``wandb.init()``.\n            name (str | None, optional): A short display name for this run. Default generates a random two-word name.\n            id (str | None, optional): A unique ID for this run. It must be unique in the project, and if you delete a\n                run you cannot reuse the ID.\n            resume (str): Indicates how to handle the case when a run has the same entity, project and run id as\n                a previous run. \"must\" enforces the run must resume from the run with same id and throws an error\n                if it does not exist. \"never\" enforces that a run will not resume and throws an error if run id exists.\n                \"allow\" resumes if the run id already exists. Defaults to \"allow\".\n            kwargs (Any): Keyword arguments to ``wandb.init`` excluding the ones explicitly described above.\n                Documentation here: https://docs.wandb.ai/ref/python/init/\n        \"\"\"\n        # Create wandb metadata dir if necessary\n        if kwargs.get(\"dir\") is not None:\n            Path(kwargs[\"dir\"]).mkdir(exist_ok=True)\n\n        # Set attributes\n        self.wandb_init_kwargs = kwargs\n        self.wandb_step_type = WandBStepType(wandb_step_type)\n        self.run_started = False\n        self.initialized = False\n        self.project = project\n        self.entity = entity\n        self.config = config\n        self.group = group\n        self.job_type = job_type\n        self.tags = tags\n        self.name = name\n        self.id = id\n        self.resume = resume\n\n        # Keep track of epoch and step. Initialize as 0.\n        self.current_epoch = 0\n        self.current_step = 0\n\n        # Initialize run later to avoid creating runs while debugging\n        self.run: wandb.wandb_run.Run\n\n    def initialize(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Checks if an id was provided by the client or server.\n\n        If an id was passed to the ``WandBReporter`` on init then it takes priority over the one passed by the\n        client/server.\n        \"\"\"\n        if self.id is None:\n            self.id = kwargs.get(\"id\")\n\n        if self.name is None:\n            self.name = kwargs.get(\"name\")\n\n        self.initialized = True\n\n    def define_metrics(self) -&gt; None:\n        \"\"\"\n        This method defines some of the metrics we expect to see from ``BasicClient`` and server.\n\n        **NOTE** that you do not have to define metrics, but it can be useful for determining what should and\n        shouldn't go into the run summary.\n        \"\"\"\n        # Note that the hidden argument is not working. Raised issue here: https://github.com/wandb/wandb/issues/8890\n        # Round, epoch and step\n        self.run.define_metric(\"fit_step\", summary=\"none\", hidden=True)  # Current fit step\n        self.run.define_metric(\"fit_epoch\", summary=\"none\", hidden=True)  # Current fit epoch\n        self.run.define_metric(\"round\", summary=\"none\", hidden=True)  # Current server round\n        self.run.define_metric(\"round_start\", summary=\"none\", hidden=True)\n        self.run.define_metric(\"round_end\", summary=\"none\", hidden=True)\n        # A server round contains a fit_round and maybe also an evaluate round\n        self.run.define_metric(\"fit_round_start\", summary=\"none\", hidden=True)\n        self.run.define_metric(\"fit_round_end\", summary=\"none\", hidden=True)\n        self.run.define_metric(\"eval_round_start\", summary=\"none\", hidden=True)\n        self.run.define_metric(\"eval_round_end\", summary=\"none\", hidden=True)\n        # The metrics computed on all the samples from the final epoch, or the entire round if training by steps\n        self.run.define_metric(\"fit_round_time_elapsed\", summary=\"none\")\n        self.run.define_metric(\"eval_round_time_elapsed\", summary=\"none\")\n        self.run.define_metric(\"fit_round_metrics\", step_metric=\"round\", summary=\"best\")\n        self.run.define_metric(\"eval_round_metrics\", step_metric=\"round\", summary=\"best\")\n        # Average of the losses for each step in the final epoch, or the entire round if training by steps.\n        self.run.define_metric(\"fit_round_losses\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n        self.run.define_metric(\"eval_round_loss\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n        # The metrics computed  at the end of the epoch on all the samples from the epoch\n        self.run.define_metric(\"fit_round_metrics\", step_metric=\"fit_epoch\", summary=\"best\")\n        # Average of the losses for each step in the epoch\n        self.run.define_metric(\"fit_epoch_losses\", step_metric=\"fit_epoch\", summary=\"best\", goal=\"minimize\")\n        # The loss and metrics for each individual step\n        self.run.define_metric(\"fit_step_metrics\", step_metric=\"fit_step\", summary=\"best\")\n        self.run.define_metric(\"fit_step_losses\", step_metric=\"fit_step\", summary=\"best\", goal=\"minimize\")\n        # FlServer (Base Server) specific metrics\n        self.run.define_metric(\"val - loss - aggregated\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n        self.run.define_metric(\"eval_round_metrics_aggregated\", step_metric=\"round\", summary=\"best\")\n        # The following metrics don't work with wandb since they are currently obtained after training instead of live\n        self.run.define_metric(\"val - loss - centralized\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n        self.run.define_metric(\"eval_round_metrics_centralized\", step_metric=\"round\", summary=\"best\")\n\n    def start_run(self, wandb_init_kwargs: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Initializes the wandb run.\n\n        We avoid doing this in the ``self.init`` function so that when debugging, jobs that fail before training\n        starts do not get uploaded to wandb.\n\n        Args:\n            wandb_init_kwargs (dict[str, Any]): Keyword arguments for ``wandb.init()`` excluding the ones explicitly\n                accessible through ``WandBReporter.init()``.\n        \"\"\"\n        if not self.initialized:\n            self.initialize()\n\n        self.run = wandb.init(\n            project=self.project,\n            entity=self.entity,\n            config=self.config,\n            group=self.group,\n            job_type=self.job_type,\n            tags=self.tags,\n            name=self.name,\n            id=self.id,\n            resume=self.resume,\n            **wandb_init_kwargs,  # Other less commonly used kwargs\n        )\n        self.run_id = self.run.id  # If run_id was None, we need to reset run id\n        self.run_started = True\n\n        # Wandb metric definitions\n        self.define_metrics()\n\n    def report(\n        self,\n        data: dict[str, Any],\n        round: int | None = None,\n        epoch: int | None = None,\n        step: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Reports wandb compatible data to the wandb server.\n\n        Data passed to ``self.report`` is always reported. If round is None, the data is reported as config\n        information. If round is specified, the data is logged to the wandb run at the current wandb step which is\n        either the current round, epoch or step depending on the ``wandb_step_type`` passed on initialization. The\n        current epoch  and step are initialized at 0 and updated internally when specified as arguments to report.\n        Therefore leaving  epoch or step as None will overwrite the data for the previous epoch/step if the key is the\n        same, otherwise  the new key-value pairs are added. For example, if ``{\"loss\": value}`` is logged every epoch\n        but  ``wandb_step_type`` is \"round\", then the value for \"loss\" at round 1 will be it's value at the last epoch\n        of  that round. You can only update or overwrite the current wandb step, previous steps can not be modified.\n\n        Args:\n            data (dict[str, Any]): Dictionary of wandb compatible data to log.\n            round (int | None, optional): The current FL round. If None, this indicates that the method was called\n                outside of a round (e.g. for summary information). Defaults to None.\n            epoch (int | None, optional): The current epoch (In total across all rounds). If None then this method was\n                not called at or within the scope of an epoch. Defaults to None.\n            step (int | None, optional): The current step (In total across all rounds and epochs). If None then this\n                method was called outside the scope of a training or evaluation step (e.g. at the end of an epoch or\n                round) Defaults to None.\n        \"\"\"\n        # Now that report has been called we are finally forced to start the run.\n        if not self.run_started:\n            self.start_run(self.wandb_init_kwargs)\n\n        # If round is None, assume data is summary information.\n        if round is None:\n            wandb.config.update(data)\n            return\n\n        # Update current epoch and step if they were specified\n        if epoch is not None:\n            if epoch &lt; self.current_epoch:\n                log(\n                    WARNING,\n                    f\"The specified current epoch ({epoch}) is less than a previous \\\n                        current epoch ({self.current_epoch})\",\n                )\n            self.current_epoch = epoch\n\n        if step is not None:\n            if step &lt; self.current_step:\n                log(\n                    WARNING,\n                    f\"The specified current step ({step}) is less than a previous current step ({self.current_step})\",\n                )\n            self.current_step = step\n\n        # Log based on step type\n        if self.wandb_step_type == WandBStepType.ROUND:\n            self.run.log(data, step=round)\n        elif self.wandb_step_type == WandBStepType.EPOCH:\n            self.run.log(data, step=self.current_epoch)\n        elif self.wandb_step_type == WandBStepType.STEP:\n            self.run.log(data, step=self.current_step)\n\n    def shutdown(self) -&gt; None:\n        self.run.finish()\n</code></pre>"},{"location":"api/#fl4health.reporting.WandBReporter.__init__","title":"<code>__init__(wandb_step_type=WandBStepType.ROUND, project=None, entity=None, config=None, group=None, job_type=None, tags=None, name=None, id=None, resume='allow', **kwargs)</code>","text":"<p>Weights and Biases Reporter for logging experimental results associated with FL runs.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_step_type</code> <code>StepType | str</code> <p>Whether to use the \"round\", \"epoch\" or \"step\" as the <code>wandb_step</code> value when logging information to the wandb server.</p> <code>ROUND</code> <code>project</code> <code>str | None</code> <p>The name of the project where you're sending the new run. If unspecified, wandb will try to infer or set to \"uncategorized\"</p> <code>None</code> <code>entity</code> <code>str | None</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you do not specify an entity, the run will be sent to your default entity. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>config</code> <code>str | None</code> <p>This sets <code>wandb.config</code>, a dictionary-like object for saving inputs to your job such as hyperparameters for a model.</p> <ul> <li>If <code>dict</code>: will load the key value pairs into the  <code>wandb.config</code> object.</li> <li>If <code>str</code>: will look for a yaml file by that name, and load config from that file into the   <code>wandb.config</code> object.</li> </ul> <code>None</code> <code>group</code> <code>str | None</code> <p>Specify a group to organize individual runs into a larger experiment.</p> <code>None</code> <code>job_type</code> <code>str | None</code> <p>Specify the type of run, useful when grouping runs.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>A list of strings, which will populate the list of tags on this run. If you want to add tags to a resumed run without overwriting its existing tags, use <code>run.tags += [\"new_tag\"]</code> after <code>wandb.init()</code>.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>A short display name for this run. Default generates a random two-word name.</p> <code>None</code> <code>id</code> <code>str | None</code> <p>A unique ID for this run. It must be unique in the project, and if you delete a run you cannot reuse the ID.</p> <code>None</code> <code>resume</code> <code>str</code> <p>Indicates how to handle the case when a run has the same entity, project and run id as a previous run. \"must\" enforces the run must resume from the run with same id and throws an error if it does not exist. \"never\" enforces that a run will not resume and throws an error if run id exists. \"allow\" resumes if the run id already exists. Defaults to \"allow\".</p> <code>'allow'</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments to <code>wandb.init</code> excluding the ones explicitly described above. Documentation here: https://docs.wandb.ai/ref/python/init/</p> <code>{}</code> Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>def __init__(\n    self,\n    wandb_step_type: WandBStepType | str = WandBStepType.ROUND,\n    project: str | None = None,\n    entity: str | None = None,\n    config: dict | str | None = None,\n    group: str | None = None,\n    job_type: str | None = None,\n    tags: list[str] | None = None,\n    name: str | None = None,\n    id: str | None = None,\n    resume: Literal[\"allow\", \"never\", \"must\", \"auto\"] | bool | None = \"allow\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Weights and Biases Reporter for logging experimental results associated with FL runs.\n\n    Args:\n        wandb_step_type (StepType | str, optional): Whether to use the \"round\", \"epoch\" or \"step\" as the\n            ``wandb_step`` value when logging information to the wandb server.\n        project (str | None, optional): The name of the project where you're sending the new run. If unspecified,\n            wandb will try to infer or set to \"uncategorized\"\n        entity (str | None, optional): An entity is a username or team name where you're sending runs. This entity\n            must exist before you can send runs there, so make sure to create your account or team in the UI before\n            starting to log runs. If you do not specify an entity, the run will be sent to your default entity.\n            Change your default entity in your settings under \"default location to create new projects\".\n        config (str | None, optional): This sets ``wandb.config``, a dictionary-like object for saving inputs to\n            your job such as hyperparameters for a model.\n\n            - If ``dict``: will load the key value pairs into the  ``wandb.config`` object.\n            - If ``str``: will look for a yaml file by that name, and load config from that file into the\n              ``wandb.config`` object.\n        group (str | None, optional): Specify a group to organize individual runs into a larger experiment.\n        job_type (str | None, optional): Specify the type of run, useful when grouping runs.\n        tags (list[str] |None, optional): A list of strings, which will populate the list of tags on this run. If\n            you want to add tags to a resumed run without overwriting its existing tags, use ``run.tags +=\n            [\"new_tag\"]`` after ``wandb.init()``.\n        name (str | None, optional): A short display name for this run. Default generates a random two-word name.\n        id (str | None, optional): A unique ID for this run. It must be unique in the project, and if you delete a\n            run you cannot reuse the ID.\n        resume (str): Indicates how to handle the case when a run has the same entity, project and run id as\n            a previous run. \"must\" enforces the run must resume from the run with same id and throws an error\n            if it does not exist. \"never\" enforces that a run will not resume and throws an error if run id exists.\n            \"allow\" resumes if the run id already exists. Defaults to \"allow\".\n        kwargs (Any): Keyword arguments to ``wandb.init`` excluding the ones explicitly described above.\n            Documentation here: https://docs.wandb.ai/ref/python/init/\n    \"\"\"\n    # Create wandb metadata dir if necessary\n    if kwargs.get(\"dir\") is not None:\n        Path(kwargs[\"dir\"]).mkdir(exist_ok=True)\n\n    # Set attributes\n    self.wandb_init_kwargs = kwargs\n    self.wandb_step_type = WandBStepType(wandb_step_type)\n    self.run_started = False\n    self.initialized = False\n    self.project = project\n    self.entity = entity\n    self.config = config\n    self.group = group\n    self.job_type = job_type\n    self.tags = tags\n    self.name = name\n    self.id = id\n    self.resume = resume\n\n    # Keep track of epoch and step. Initialize as 0.\n    self.current_epoch = 0\n    self.current_step = 0\n\n    # Initialize run later to avoid creating runs while debugging\n    self.run: wandb.wandb_run.Run\n</code></pre>"},{"location":"api/#fl4health.reporting.WandBReporter.initialize","title":"<code>initialize(**kwargs)</code>","text":"<p>Checks if an id was provided by the client or server.</p> <p>If an id was passed to the <code>WandBReporter</code> on init then it takes priority over the one passed by the client/server.</p> Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>def initialize(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Checks if an id was provided by the client or server.\n\n    If an id was passed to the ``WandBReporter`` on init then it takes priority over the one passed by the\n    client/server.\n    \"\"\"\n    if self.id is None:\n        self.id = kwargs.get(\"id\")\n\n    if self.name is None:\n        self.name = kwargs.get(\"name\")\n\n    self.initialized = True\n</code></pre>"},{"location":"api/#fl4health.reporting.WandBReporter.define_metrics","title":"<code>define_metrics()</code>","text":"<p>This method defines some of the metrics we expect to see from <code>BasicClient</code> and server.</p> <p>NOTE that you do not have to define metrics, but it can be useful for determining what should and shouldn't go into the run summary.</p> Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>def define_metrics(self) -&gt; None:\n    \"\"\"\n    This method defines some of the metrics we expect to see from ``BasicClient`` and server.\n\n    **NOTE** that you do not have to define metrics, but it can be useful for determining what should and\n    shouldn't go into the run summary.\n    \"\"\"\n    # Note that the hidden argument is not working. Raised issue here: https://github.com/wandb/wandb/issues/8890\n    # Round, epoch and step\n    self.run.define_metric(\"fit_step\", summary=\"none\", hidden=True)  # Current fit step\n    self.run.define_metric(\"fit_epoch\", summary=\"none\", hidden=True)  # Current fit epoch\n    self.run.define_metric(\"round\", summary=\"none\", hidden=True)  # Current server round\n    self.run.define_metric(\"round_start\", summary=\"none\", hidden=True)\n    self.run.define_metric(\"round_end\", summary=\"none\", hidden=True)\n    # A server round contains a fit_round and maybe also an evaluate round\n    self.run.define_metric(\"fit_round_start\", summary=\"none\", hidden=True)\n    self.run.define_metric(\"fit_round_end\", summary=\"none\", hidden=True)\n    self.run.define_metric(\"eval_round_start\", summary=\"none\", hidden=True)\n    self.run.define_metric(\"eval_round_end\", summary=\"none\", hidden=True)\n    # The metrics computed on all the samples from the final epoch, or the entire round if training by steps\n    self.run.define_metric(\"fit_round_time_elapsed\", summary=\"none\")\n    self.run.define_metric(\"eval_round_time_elapsed\", summary=\"none\")\n    self.run.define_metric(\"fit_round_metrics\", step_metric=\"round\", summary=\"best\")\n    self.run.define_metric(\"eval_round_metrics\", step_metric=\"round\", summary=\"best\")\n    # Average of the losses for each step in the final epoch, or the entire round if training by steps.\n    self.run.define_metric(\"fit_round_losses\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n    self.run.define_metric(\"eval_round_loss\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n    # The metrics computed  at the end of the epoch on all the samples from the epoch\n    self.run.define_metric(\"fit_round_metrics\", step_metric=\"fit_epoch\", summary=\"best\")\n    # Average of the losses for each step in the epoch\n    self.run.define_metric(\"fit_epoch_losses\", step_metric=\"fit_epoch\", summary=\"best\", goal=\"minimize\")\n    # The loss and metrics for each individual step\n    self.run.define_metric(\"fit_step_metrics\", step_metric=\"fit_step\", summary=\"best\")\n    self.run.define_metric(\"fit_step_losses\", step_metric=\"fit_step\", summary=\"best\", goal=\"minimize\")\n    # FlServer (Base Server) specific metrics\n    self.run.define_metric(\"val - loss - aggregated\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n    self.run.define_metric(\"eval_round_metrics_aggregated\", step_metric=\"round\", summary=\"best\")\n    # The following metrics don't work with wandb since they are currently obtained after training instead of live\n    self.run.define_metric(\"val - loss - centralized\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n    self.run.define_metric(\"eval_round_metrics_centralized\", step_metric=\"round\", summary=\"best\")\n</code></pre>"},{"location":"api/#fl4health.reporting.WandBReporter.start_run","title":"<code>start_run(wandb_init_kwargs)</code>","text":"<p>Initializes the wandb run.</p> <p>We avoid doing this in the <code>self.init</code> function so that when debugging, jobs that fail before training starts do not get uploaded to wandb.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_init_kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for <code>wandb.init()</code> excluding the ones explicitly accessible through <code>WandBReporter.init()</code>.</p> required Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>def start_run(self, wandb_init_kwargs: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Initializes the wandb run.\n\n    We avoid doing this in the ``self.init`` function so that when debugging, jobs that fail before training\n    starts do not get uploaded to wandb.\n\n    Args:\n        wandb_init_kwargs (dict[str, Any]): Keyword arguments for ``wandb.init()`` excluding the ones explicitly\n            accessible through ``WandBReporter.init()``.\n    \"\"\"\n    if not self.initialized:\n        self.initialize()\n\n    self.run = wandb.init(\n        project=self.project,\n        entity=self.entity,\n        config=self.config,\n        group=self.group,\n        job_type=self.job_type,\n        tags=self.tags,\n        name=self.name,\n        id=self.id,\n        resume=self.resume,\n        **wandb_init_kwargs,  # Other less commonly used kwargs\n    )\n    self.run_id = self.run.id  # If run_id was None, we need to reset run id\n    self.run_started = True\n\n    # Wandb metric definitions\n    self.define_metrics()\n</code></pre>"},{"location":"api/#fl4health.reporting.WandBReporter.report","title":"<code>report(data, round=None, epoch=None, step=None)</code>","text":"<p>Reports wandb compatible data to the wandb server.</p> <p>Data passed to <code>self.report</code> is always reported. If round is None, the data is reported as config information. If round is specified, the data is logged to the wandb run at the current wandb step which is either the current round, epoch or step depending on the <code>wandb_step_type</code> passed on initialization. The current epoch  and step are initialized at 0 and updated internally when specified as arguments to report. Therefore leaving  epoch or step as None will overwrite the data for the previous epoch/step if the key is the same, otherwise  the new key-value pairs are added. For example, if <code>{\"loss\": value}</code> is logged every epoch but  <code>wandb_step_type</code> is \"round\", then the value for \"loss\" at round 1 will be it's value at the last epoch of  that round. You can only update or overwrite the current wandb step, previous steps can not be modified.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary of wandb compatible data to log.</p> required <code>round</code> <code>int | None</code> <p>The current FL round. If None, this indicates that the method was called outside of a round (e.g. for summary information). Defaults to None.</p> <code>None</code> <code>epoch</code> <code>int | None</code> <p>The current epoch (In total across all rounds). If None then this method was not called at or within the scope of an epoch. Defaults to None.</p> <code>None</code> <code>step</code> <code>int | None</code> <p>The current step (In total across all rounds and epochs). If None then this method was called outside the scope of a training or evaluation step (e.g. at the end of an epoch or round) Defaults to None.</p> <code>None</code> Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>def report(\n    self,\n    data: dict[str, Any],\n    round: int | None = None,\n    epoch: int | None = None,\n    step: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Reports wandb compatible data to the wandb server.\n\n    Data passed to ``self.report`` is always reported. If round is None, the data is reported as config\n    information. If round is specified, the data is logged to the wandb run at the current wandb step which is\n    either the current round, epoch or step depending on the ``wandb_step_type`` passed on initialization. The\n    current epoch  and step are initialized at 0 and updated internally when specified as arguments to report.\n    Therefore leaving  epoch or step as None will overwrite the data for the previous epoch/step if the key is the\n    same, otherwise  the new key-value pairs are added. For example, if ``{\"loss\": value}`` is logged every epoch\n    but  ``wandb_step_type`` is \"round\", then the value for \"loss\" at round 1 will be it's value at the last epoch\n    of  that round. You can only update or overwrite the current wandb step, previous steps can not be modified.\n\n    Args:\n        data (dict[str, Any]): Dictionary of wandb compatible data to log.\n        round (int | None, optional): The current FL round. If None, this indicates that the method was called\n            outside of a round (e.g. for summary information). Defaults to None.\n        epoch (int | None, optional): The current epoch (In total across all rounds). If None then this method was\n            not called at or within the scope of an epoch. Defaults to None.\n        step (int | None, optional): The current step (In total across all rounds and epochs). If None then this\n            method was called outside the scope of a training or evaluation step (e.g. at the end of an epoch or\n            round) Defaults to None.\n    \"\"\"\n    # Now that report has been called we are finally forced to start the run.\n    if not self.run_started:\n        self.start_run(self.wandb_init_kwargs)\n\n    # If round is None, assume data is summary information.\n    if round is None:\n        wandb.config.update(data)\n        return\n\n    # Update current epoch and step if they were specified\n    if epoch is not None:\n        if epoch &lt; self.current_epoch:\n            log(\n                WARNING,\n                f\"The specified current epoch ({epoch}) is less than a previous \\\n                    current epoch ({self.current_epoch})\",\n            )\n        self.current_epoch = epoch\n\n    if step is not None:\n        if step &lt; self.current_step:\n            log(\n                WARNING,\n                f\"The specified current step ({step}) is less than a previous current step ({self.current_step})\",\n            )\n        self.current_step = step\n\n    # Log based on step type\n    if self.wandb_step_type == WandBStepType.ROUND:\n        self.run.log(data, step=round)\n    elif self.wandb_step_type == WandBStepType.EPOCH:\n        self.run.log(data, step=self.current_epoch)\n    elif self.wandb_step_type == WandBStepType.STEP:\n        self.run.log(data, step=self.current_step)\n</code></pre>"},{"location":"api/#fl4health.reporting.base_reporter","title":"<code>base_reporter</code>","text":"<p>Base Class for Reporters.</p> <p>Super simple for now but keeping it in a separate file in case we add more base methods.</p>"},{"location":"api/#fl4health.reporting.base_reporter.BaseReporter","title":"<code>BaseReporter</code>","text":"Source code in <code>fl4health/reporting/base_reporter.py</code> <pre><code>class BaseReporter:\n    def report(\n        self,\n        data: dict,\n        round: int | None = None,\n        epoch: int | None = None,\n        step: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        A method called by clients or servers to send data to the reporter.\n\n        The report method is called by the client/server at frequent intervals (i.e. step, epoch, round) and sometimes\n        outside of a FL round (for high level summary data). It is up to the reporter to determine when and what to\n        report.\n\n        Args:\n            data (dict): The data to maybe report from the server or client.\n            round (int | None, optional): The current FL round. If None, this indicates that the method was called\n                outside of a round (e.g. for summary information). Defaults to None.\n            epoch (int | None, optional): The current epoch (In total across all rounds). If None then this method was\n                not called at or within the scope of an epoch. Should always be None if training by steps. Defaults to\n                None.\n            step (int | None, optional): The current step (In total across all rounds and epochs). If None then this\n                method was called outside the scope of a training or evaluation step (e.g. at the end of an epoch or\n                round) Defaults to None.\n        \"\"\"\n        raise NotImplementedError\n\n    def initialize(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Method for initializing reporters with client/server information.\n\n        This method is called once by the client or server during initialization.\n\n        Args:\n            kwargs (Any): arbitrary keyword arguments containing information from the client or server that might be\n                useful for initializing the reporter. This information should be treated as optional and this method\n                should work even if no keyword arguments are passed.\n        \"\"\"\n        pass\n\n    def shutdown(self) -&gt; None:\n        \"\"\"Called by the client/server on shutdown.\"\"\"\n        pass\n</code></pre>"},{"location":"api/#fl4health.reporting.base_reporter.BaseReporter.report","title":"<code>report(data, round=None, epoch=None, step=None)</code>","text":"<p>A method called by clients or servers to send data to the reporter.</p> <p>The report method is called by the client/server at frequent intervals (i.e. step, epoch, round) and sometimes outside of a FL round (for high level summary data). It is up to the reporter to determine when and what to report.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The data to maybe report from the server or client.</p> required <code>round</code> <code>int | None</code> <p>The current FL round. If None, this indicates that the method was called outside of a round (e.g. for summary information). Defaults to None.</p> <code>None</code> <code>epoch</code> <code>int | None</code> <p>The current epoch (In total across all rounds). If None then this method was not called at or within the scope of an epoch. Should always be None if training by steps. Defaults to None.</p> <code>None</code> <code>step</code> <code>int | None</code> <p>The current step (In total across all rounds and epochs). If None then this method was called outside the scope of a training or evaluation step (e.g. at the end of an epoch or round) Defaults to None.</p> <code>None</code> Source code in <code>fl4health/reporting/base_reporter.py</code> <pre><code>def report(\n    self,\n    data: dict,\n    round: int | None = None,\n    epoch: int | None = None,\n    step: int | None = None,\n) -&gt; None:\n    \"\"\"\n    A method called by clients or servers to send data to the reporter.\n\n    The report method is called by the client/server at frequent intervals (i.e. step, epoch, round) and sometimes\n    outside of a FL round (for high level summary data). It is up to the reporter to determine when and what to\n    report.\n\n    Args:\n        data (dict): The data to maybe report from the server or client.\n        round (int | None, optional): The current FL round. If None, this indicates that the method was called\n            outside of a round (e.g. for summary information). Defaults to None.\n        epoch (int | None, optional): The current epoch (In total across all rounds). If None then this method was\n            not called at or within the scope of an epoch. Should always be None if training by steps. Defaults to\n            None.\n        step (int | None, optional): The current step (In total across all rounds and epochs). If None then this\n            method was called outside the scope of a training or evaluation step (e.g. at the end of an epoch or\n            round) Defaults to None.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.reporting.base_reporter.BaseReporter.initialize","title":"<code>initialize(**kwargs)</code>","text":"<p>Method for initializing reporters with client/server information.</p> <p>This method is called once by the client or server during initialization.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <code>Any</code> <p>arbitrary keyword arguments containing information from the client or server that might be useful for initializing the reporter. This information should be treated as optional and this method should work even if no keyword arguments are passed.</p> <code>{}</code> Source code in <code>fl4health/reporting/base_reporter.py</code> <pre><code>def initialize(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Method for initializing reporters with client/server information.\n\n    This method is called once by the client or server during initialization.\n\n    Args:\n        kwargs (Any): arbitrary keyword arguments containing information from the client or server that might be\n            useful for initializing the reporter. This information should be treated as optional and this method\n            should work even if no keyword arguments are passed.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.reporting.base_reporter.BaseReporter.shutdown","title":"<code>shutdown()</code>","text":"<p>Called by the client/server on shutdown.</p> Source code in <code>fl4health/reporting/base_reporter.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"Called by the client/server on shutdown.\"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.reporting.json_reporter","title":"<code>json_reporter</code>","text":""},{"location":"api/#fl4health.reporting.json_reporter.FileReporter","title":"<code>FileReporter</code>","text":"<p>               Bases: <code>BaseReporter</code></p> Source code in <code>fl4health/reporting/json_reporter.py</code> <pre><code>class FileReporter(BaseReporter):\n    def __init__(\n        self,\n        run_id: str | None = None,\n        output_folder: str | Path = Path(\"metrics\"),\n    ):\n        \"\"\"\n        Reports data each round and saves as a json.\n\n        Args:\n            run_id (str | None, optional): the identifier for the run which these metrics are from. If left as None\n                will check if an id is provided during initialize, otherwise uses a ``UUID``.\n            output_folder (str | Path): the folder to save the metrics to. The metrics will be saved in a file named\n                ``{output_folder}/{run_id}.json``. Optional, default is \"metrics\".\n        \"\"\"\n        self.run_id = run_id\n\n        self.output_folder = Path(output_folder)\n        self.metrics: dict[str, Any] = {}\n        self.initialized = False\n\n        self.output_folder.mkdir(exist_ok=True)\n\n    def initialize(self, **kwargs: Any) -&gt; None:\n        # If run_id was not specified on init try first to initialize with client name\n        if self.run_id is None:\n            self.run_id = kwargs.get(\"id\")\n        # If client name was not provided, init run id manually\n        if self.run_id is None:\n            self.run_id = str(uuid.uuid4())\n\n        self.initialized = True\n\n    def report(\n        self,\n        data: dict[str, Any],\n        round: int | None = None,\n        epoch: int | None = None,\n        step: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        A method called by clients or servers to send data to the reporter.\n\n        The report method is called by the client/server at frequent intervals (i.e. step, epoch, round) and sometimes\n        outside of a FL round (for high level summary data). The json reporter is hardcoded to report at the \"round\"\n        level and therefore ignores calls to the report method made every epoch or every step.\n\n        Args:\n            data (dict): The data to maybe report from the server or client.\n            round (int | None, optional): The current FL round. If None, this indicates that the method was called\n                outside of a round (e.g. for summary information). Defaults to None.\n            epoch (int | None, optional): The current epoch. If None then this method was not called within the scope\n                of an epoch. Defaults to None.\n            step (int | None, optional): The current step (total). If None then this method was called outside the\n                scope of a training or evaluation step (e.g. at the end of an epoch or round) Defaults to None.\n        \"\"\"\n        if not self.initialized:\n            self.initialize()\n\n        if round is None:  # Reports outside of a fit round\n            self.metrics.update(data)\n        # Ensure we don't report for each epoch or step\n        elif epoch is None and step is None:\n            if \"rounds\" not in self.metrics:\n                self.metrics[\"rounds\"] = {}\n            if round not in self.metrics[\"rounds\"]:\n                self.metrics[\"rounds\"][round] = {}\n\n            self.metrics[\"rounds\"][round].update(data)\n\n    def dump(self) -&gt; None:\n        raise NotImplementedError\n\n    def shutdown(self) -&gt; None:\n        self.dump()\n</code></pre>"},{"location":"api/#fl4health.reporting.json_reporter.FileReporter.__init__","title":"<code>__init__(run_id=None, output_folder=Path('metrics'))</code>","text":"<p>Reports data each round and saves as a json.</p> <p>Parameters:</p> Name Type Description Default <code>run_id</code> <code>str | None</code> <p>the identifier for the run which these metrics are from. If left as None will check if an id is provided during initialize, otherwise uses a <code>UUID</code>.</p> <code>None</code> <code>output_folder</code> <code>str | Path</code> <p>the folder to save the metrics to. The metrics will be saved in a file named <code>{output_folder}/{run_id}.json</code>. Optional, default is \"metrics\".</p> <code>Path('metrics')</code> Source code in <code>fl4health/reporting/json_reporter.py</code> <pre><code>def __init__(\n    self,\n    run_id: str | None = None,\n    output_folder: str | Path = Path(\"metrics\"),\n):\n    \"\"\"\n    Reports data each round and saves as a json.\n\n    Args:\n        run_id (str | None, optional): the identifier for the run which these metrics are from. If left as None\n            will check if an id is provided during initialize, otherwise uses a ``UUID``.\n        output_folder (str | Path): the folder to save the metrics to. The metrics will be saved in a file named\n            ``{output_folder}/{run_id}.json``. Optional, default is \"metrics\".\n    \"\"\"\n    self.run_id = run_id\n\n    self.output_folder = Path(output_folder)\n    self.metrics: dict[str, Any] = {}\n    self.initialized = False\n\n    self.output_folder.mkdir(exist_ok=True)\n</code></pre>"},{"location":"api/#fl4health.reporting.json_reporter.FileReporter.report","title":"<code>report(data, round=None, epoch=None, step=None)</code>","text":"<p>A method called by clients or servers to send data to the reporter.</p> <p>The report method is called by the client/server at frequent intervals (i.e. step, epoch, round) and sometimes outside of a FL round (for high level summary data). The json reporter is hardcoded to report at the \"round\" level and therefore ignores calls to the report method made every epoch or every step.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The data to maybe report from the server or client.</p> required <code>round</code> <code>int | None</code> <p>The current FL round. If None, this indicates that the method was called outside of a round (e.g. for summary information). Defaults to None.</p> <code>None</code> <code>epoch</code> <code>int | None</code> <p>The current epoch. If None then this method was not called within the scope of an epoch. Defaults to None.</p> <code>None</code> <code>step</code> <code>int | None</code> <p>The current step (total). If None then this method was called outside the scope of a training or evaluation step (e.g. at the end of an epoch or round) Defaults to None.</p> <code>None</code> Source code in <code>fl4health/reporting/json_reporter.py</code> <pre><code>def report(\n    self,\n    data: dict[str, Any],\n    round: int | None = None,\n    epoch: int | None = None,\n    step: int | None = None,\n) -&gt; None:\n    \"\"\"\n    A method called by clients or servers to send data to the reporter.\n\n    The report method is called by the client/server at frequent intervals (i.e. step, epoch, round) and sometimes\n    outside of a FL round (for high level summary data). The json reporter is hardcoded to report at the \"round\"\n    level and therefore ignores calls to the report method made every epoch or every step.\n\n    Args:\n        data (dict): The data to maybe report from the server or client.\n        round (int | None, optional): The current FL round. If None, this indicates that the method was called\n            outside of a round (e.g. for summary information). Defaults to None.\n        epoch (int | None, optional): The current epoch. If None then this method was not called within the scope\n            of an epoch. Defaults to None.\n        step (int | None, optional): The current step (total). If None then this method was called outside the\n            scope of a training or evaluation step (e.g. at the end of an epoch or round) Defaults to None.\n    \"\"\"\n    if not self.initialized:\n        self.initialize()\n\n    if round is None:  # Reports outside of a fit round\n        self.metrics.update(data)\n    # Ensure we don't report for each epoch or step\n    elif epoch is None and step is None:\n        if \"rounds\" not in self.metrics:\n            self.metrics[\"rounds\"] = {}\n        if round not in self.metrics[\"rounds\"]:\n            self.metrics[\"rounds\"][round] = {}\n\n        self.metrics[\"rounds\"][round].update(data)\n</code></pre>"},{"location":"api/#fl4health.reporting.json_reporter.JsonReporter","title":"<code>JsonReporter</code>","text":"<p>               Bases: <code>FileReporter</code></p> Source code in <code>fl4health/reporting/json_reporter.py</code> <pre><code>class JsonReporter(FileReporter):\n    def dump(self) -&gt; None:\n        \"\"\"Dumps the current metrics to a JSON file at ``{output_folder}/{run_id.json}``.\"\"\"\n        assert self.run_id is not None\n        output_file_path = Path(self.output_folder, self.run_id).with_suffix(\".json\")\n        log(INFO, f\"Dumping metrics to {str(output_file_path)}\")\n\n        with open(output_file_path, \"w\") as output_file:\n            json.dump(self.metrics, output_file, indent=4)\n</code></pre>"},{"location":"api/#fl4health.reporting.json_reporter.JsonReporter.dump","title":"<code>dump()</code>","text":"<p>Dumps the current metrics to a JSON file at <code>{output_folder}/{run_id.json}</code>.</p> Source code in <code>fl4health/reporting/json_reporter.py</code> <pre><code>def dump(self) -&gt; None:\n    \"\"\"Dumps the current metrics to a JSON file at ``{output_folder}/{run_id.json}``.\"\"\"\n    assert self.run_id is not None\n    output_file_path = Path(self.output_folder, self.run_id).with_suffix(\".json\")\n    log(INFO, f\"Dumping metrics to {str(output_file_path)}\")\n\n    with open(output_file_path, \"w\") as output_file:\n        json.dump(self.metrics, output_file, indent=4)\n</code></pre>"},{"location":"api/#fl4health.reporting.reports_manager","title":"<code>reports_manager</code>","text":""},{"location":"api/#fl4health.reporting.reports_manager.ReportsManager","title":"<code>ReportsManager</code>","text":"Source code in <code>fl4health/reporting/reports_manager.py</code> <pre><code>class ReportsManager:\n    def __init__(self, reporters: Sequence[BaseReporter] | None = None) -&gt; None:\n        \"\"\"\n        Basic class for managing sequences of reporters. Generally, this class orchestrates initializing, calling,\n        and gracefully shutting down all reporters provided to the class.\n\n        Args:\n            reporters (Sequence[BaseReporter] | None, optional): Reporters of ``BaseReporter`` to be managed by\n                this class. Defaults to None.\n        \"\"\"\n        self.reporters = [] if reporters is None else list(reporters)\n\n    def initialize(self, **kwargs: Any) -&gt; None:\n        for r in self.reporters:\n            r.initialize(**kwargs)\n\n    def report(self, data: dict, round: int | None = None, epoch: int | None = None, step: int | None = None) -&gt; None:\n        for r in self.reporters:\n            r.report(data, round, epoch, step)\n\n    def shutdown(self) -&gt; None:\n        for r in self.reporters:\n            r.shutdown()\n</code></pre>"},{"location":"api/#fl4health.reporting.reports_manager.ReportsManager.__init__","title":"<code>__init__(reporters=None)</code>","text":"<p>Basic class for managing sequences of reporters. Generally, this class orchestrates initializing, calling, and gracefully shutting down all reporters provided to the class.</p> <p>Parameters:</p> Name Type Description Default <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>Reporters of <code>BaseReporter</code> to be managed by this class. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/reporting/reports_manager.py</code> <pre><code>def __init__(self, reporters: Sequence[BaseReporter] | None = None) -&gt; None:\n    \"\"\"\n    Basic class for managing sequences of reporters. Generally, this class orchestrates initializing, calling,\n    and gracefully shutting down all reporters provided to the class.\n\n    Args:\n        reporters (Sequence[BaseReporter] | None, optional): Reporters of ``BaseReporter`` to be managed by\n            this class. Defaults to None.\n    \"\"\"\n    self.reporters = [] if reporters is None else list(reporters)\n</code></pre>"},{"location":"api/#fl4health.reporting.wandb_reporter","title":"<code>wandb_reporter</code>","text":""},{"location":"api/#fl4health.reporting.wandb_reporter.WandBReporter","title":"<code>WandBReporter</code>","text":"<p>               Bases: <code>BaseReporter</code></p> Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>class WandBReporter(BaseReporter):\n    def __init__(\n        self,\n        wandb_step_type: WandBStepType | str = WandBStepType.ROUND,\n        project: str | None = None,\n        entity: str | None = None,\n        config: dict | str | None = None,\n        group: str | None = None,\n        job_type: str | None = None,\n        tags: list[str] | None = None,\n        name: str | None = None,\n        id: str | None = None,\n        resume: Literal[\"allow\", \"never\", \"must\", \"auto\"] | bool | None = \"allow\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Weights and Biases Reporter for logging experimental results associated with FL runs.\n\n        Args:\n            wandb_step_type (StepType | str, optional): Whether to use the \"round\", \"epoch\" or \"step\" as the\n                ``wandb_step`` value when logging information to the wandb server.\n            project (str | None, optional): The name of the project where you're sending the new run. If unspecified,\n                wandb will try to infer or set to \"uncategorized\"\n            entity (str | None, optional): An entity is a username or team name where you're sending runs. This entity\n                must exist before you can send runs there, so make sure to create your account or team in the UI before\n                starting to log runs. If you do not specify an entity, the run will be sent to your default entity.\n                Change your default entity in your settings under \"default location to create new projects\".\n            config (str | None, optional): This sets ``wandb.config``, a dictionary-like object for saving inputs to\n                your job such as hyperparameters for a model.\n\n                - If ``dict``: will load the key value pairs into the  ``wandb.config`` object.\n                - If ``str``: will look for a yaml file by that name, and load config from that file into the\n                  ``wandb.config`` object.\n            group (str | None, optional): Specify a group to organize individual runs into a larger experiment.\n            job_type (str | None, optional): Specify the type of run, useful when grouping runs.\n            tags (list[str] |None, optional): A list of strings, which will populate the list of tags on this run. If\n                you want to add tags to a resumed run without overwriting its existing tags, use ``run.tags +=\n                [\"new_tag\"]`` after ``wandb.init()``.\n            name (str | None, optional): A short display name for this run. Default generates a random two-word name.\n            id (str | None, optional): A unique ID for this run. It must be unique in the project, and if you delete a\n                run you cannot reuse the ID.\n            resume (str): Indicates how to handle the case when a run has the same entity, project and run id as\n                a previous run. \"must\" enforces the run must resume from the run with same id and throws an error\n                if it does not exist. \"never\" enforces that a run will not resume and throws an error if run id exists.\n                \"allow\" resumes if the run id already exists. Defaults to \"allow\".\n            kwargs (Any): Keyword arguments to ``wandb.init`` excluding the ones explicitly described above.\n                Documentation here: https://docs.wandb.ai/ref/python/init/\n        \"\"\"\n        # Create wandb metadata dir if necessary\n        if kwargs.get(\"dir\") is not None:\n            Path(kwargs[\"dir\"]).mkdir(exist_ok=True)\n\n        # Set attributes\n        self.wandb_init_kwargs = kwargs\n        self.wandb_step_type = WandBStepType(wandb_step_type)\n        self.run_started = False\n        self.initialized = False\n        self.project = project\n        self.entity = entity\n        self.config = config\n        self.group = group\n        self.job_type = job_type\n        self.tags = tags\n        self.name = name\n        self.id = id\n        self.resume = resume\n\n        # Keep track of epoch and step. Initialize as 0.\n        self.current_epoch = 0\n        self.current_step = 0\n\n        # Initialize run later to avoid creating runs while debugging\n        self.run: wandb.wandb_run.Run\n\n    def initialize(self, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Checks if an id was provided by the client or server.\n\n        If an id was passed to the ``WandBReporter`` on init then it takes priority over the one passed by the\n        client/server.\n        \"\"\"\n        if self.id is None:\n            self.id = kwargs.get(\"id\")\n\n        if self.name is None:\n            self.name = kwargs.get(\"name\")\n\n        self.initialized = True\n\n    def define_metrics(self) -&gt; None:\n        \"\"\"\n        This method defines some of the metrics we expect to see from ``BasicClient`` and server.\n\n        **NOTE** that you do not have to define metrics, but it can be useful for determining what should and\n        shouldn't go into the run summary.\n        \"\"\"\n        # Note that the hidden argument is not working. Raised issue here: https://github.com/wandb/wandb/issues/8890\n        # Round, epoch and step\n        self.run.define_metric(\"fit_step\", summary=\"none\", hidden=True)  # Current fit step\n        self.run.define_metric(\"fit_epoch\", summary=\"none\", hidden=True)  # Current fit epoch\n        self.run.define_metric(\"round\", summary=\"none\", hidden=True)  # Current server round\n        self.run.define_metric(\"round_start\", summary=\"none\", hidden=True)\n        self.run.define_metric(\"round_end\", summary=\"none\", hidden=True)\n        # A server round contains a fit_round and maybe also an evaluate round\n        self.run.define_metric(\"fit_round_start\", summary=\"none\", hidden=True)\n        self.run.define_metric(\"fit_round_end\", summary=\"none\", hidden=True)\n        self.run.define_metric(\"eval_round_start\", summary=\"none\", hidden=True)\n        self.run.define_metric(\"eval_round_end\", summary=\"none\", hidden=True)\n        # The metrics computed on all the samples from the final epoch, or the entire round if training by steps\n        self.run.define_metric(\"fit_round_time_elapsed\", summary=\"none\")\n        self.run.define_metric(\"eval_round_time_elapsed\", summary=\"none\")\n        self.run.define_metric(\"fit_round_metrics\", step_metric=\"round\", summary=\"best\")\n        self.run.define_metric(\"eval_round_metrics\", step_metric=\"round\", summary=\"best\")\n        # Average of the losses for each step in the final epoch, or the entire round if training by steps.\n        self.run.define_metric(\"fit_round_losses\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n        self.run.define_metric(\"eval_round_loss\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n        # The metrics computed  at the end of the epoch on all the samples from the epoch\n        self.run.define_metric(\"fit_round_metrics\", step_metric=\"fit_epoch\", summary=\"best\")\n        # Average of the losses for each step in the epoch\n        self.run.define_metric(\"fit_epoch_losses\", step_metric=\"fit_epoch\", summary=\"best\", goal=\"minimize\")\n        # The loss and metrics for each individual step\n        self.run.define_metric(\"fit_step_metrics\", step_metric=\"fit_step\", summary=\"best\")\n        self.run.define_metric(\"fit_step_losses\", step_metric=\"fit_step\", summary=\"best\", goal=\"minimize\")\n        # FlServer (Base Server) specific metrics\n        self.run.define_metric(\"val - loss - aggregated\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n        self.run.define_metric(\"eval_round_metrics_aggregated\", step_metric=\"round\", summary=\"best\")\n        # The following metrics don't work with wandb since they are currently obtained after training instead of live\n        self.run.define_metric(\"val - loss - centralized\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n        self.run.define_metric(\"eval_round_metrics_centralized\", step_metric=\"round\", summary=\"best\")\n\n    def start_run(self, wandb_init_kwargs: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Initializes the wandb run.\n\n        We avoid doing this in the ``self.init`` function so that when debugging, jobs that fail before training\n        starts do not get uploaded to wandb.\n\n        Args:\n            wandb_init_kwargs (dict[str, Any]): Keyword arguments for ``wandb.init()`` excluding the ones explicitly\n                accessible through ``WandBReporter.init()``.\n        \"\"\"\n        if not self.initialized:\n            self.initialize()\n\n        self.run = wandb.init(\n            project=self.project,\n            entity=self.entity,\n            config=self.config,\n            group=self.group,\n            job_type=self.job_type,\n            tags=self.tags,\n            name=self.name,\n            id=self.id,\n            resume=self.resume,\n            **wandb_init_kwargs,  # Other less commonly used kwargs\n        )\n        self.run_id = self.run.id  # If run_id was None, we need to reset run id\n        self.run_started = True\n\n        # Wandb metric definitions\n        self.define_metrics()\n\n    def report(\n        self,\n        data: dict[str, Any],\n        round: int | None = None,\n        epoch: int | None = None,\n        step: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Reports wandb compatible data to the wandb server.\n\n        Data passed to ``self.report`` is always reported. If round is None, the data is reported as config\n        information. If round is specified, the data is logged to the wandb run at the current wandb step which is\n        either the current round, epoch or step depending on the ``wandb_step_type`` passed on initialization. The\n        current epoch  and step are initialized at 0 and updated internally when specified as arguments to report.\n        Therefore leaving  epoch or step as None will overwrite the data for the previous epoch/step if the key is the\n        same, otherwise  the new key-value pairs are added. For example, if ``{\"loss\": value}`` is logged every epoch\n        but  ``wandb_step_type`` is \"round\", then the value for \"loss\" at round 1 will be it's value at the last epoch\n        of  that round. You can only update or overwrite the current wandb step, previous steps can not be modified.\n\n        Args:\n            data (dict[str, Any]): Dictionary of wandb compatible data to log.\n            round (int | None, optional): The current FL round. If None, this indicates that the method was called\n                outside of a round (e.g. for summary information). Defaults to None.\n            epoch (int | None, optional): The current epoch (In total across all rounds). If None then this method was\n                not called at or within the scope of an epoch. Defaults to None.\n            step (int | None, optional): The current step (In total across all rounds and epochs). If None then this\n                method was called outside the scope of a training or evaluation step (e.g. at the end of an epoch or\n                round) Defaults to None.\n        \"\"\"\n        # Now that report has been called we are finally forced to start the run.\n        if not self.run_started:\n            self.start_run(self.wandb_init_kwargs)\n\n        # If round is None, assume data is summary information.\n        if round is None:\n            wandb.config.update(data)\n            return\n\n        # Update current epoch and step if they were specified\n        if epoch is not None:\n            if epoch &lt; self.current_epoch:\n                log(\n                    WARNING,\n                    f\"The specified current epoch ({epoch}) is less than a previous \\\n                        current epoch ({self.current_epoch})\",\n                )\n            self.current_epoch = epoch\n\n        if step is not None:\n            if step &lt; self.current_step:\n                log(\n                    WARNING,\n                    f\"The specified current step ({step}) is less than a previous current step ({self.current_step})\",\n                )\n            self.current_step = step\n\n        # Log based on step type\n        if self.wandb_step_type == WandBStepType.ROUND:\n            self.run.log(data, step=round)\n        elif self.wandb_step_type == WandBStepType.EPOCH:\n            self.run.log(data, step=self.current_epoch)\n        elif self.wandb_step_type == WandBStepType.STEP:\n            self.run.log(data, step=self.current_step)\n\n    def shutdown(self) -&gt; None:\n        self.run.finish()\n</code></pre>"},{"location":"api/#fl4health.reporting.wandb_reporter.WandBReporter.__init__","title":"<code>__init__(wandb_step_type=WandBStepType.ROUND, project=None, entity=None, config=None, group=None, job_type=None, tags=None, name=None, id=None, resume='allow', **kwargs)</code>","text":"<p>Weights and Biases Reporter for logging experimental results associated with FL runs.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_step_type</code> <code>StepType | str</code> <p>Whether to use the \"round\", \"epoch\" or \"step\" as the <code>wandb_step</code> value when logging information to the wandb server.</p> <code>ROUND</code> <code>project</code> <code>str | None</code> <p>The name of the project where you're sending the new run. If unspecified, wandb will try to infer or set to \"uncategorized\"</p> <code>None</code> <code>entity</code> <code>str | None</code> <p>An entity is a username or team name where you're sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you do not specify an entity, the run will be sent to your default entity. Change your default entity in your settings under \"default location to create new projects\".</p> <code>None</code> <code>config</code> <code>str | None</code> <p>This sets <code>wandb.config</code>, a dictionary-like object for saving inputs to your job such as hyperparameters for a model.</p> <ul> <li>If <code>dict</code>: will load the key value pairs into the  <code>wandb.config</code> object.</li> <li>If <code>str</code>: will look for a yaml file by that name, and load config from that file into the   <code>wandb.config</code> object.</li> </ul> <code>None</code> <code>group</code> <code>str | None</code> <p>Specify a group to organize individual runs into a larger experiment.</p> <code>None</code> <code>job_type</code> <code>str | None</code> <p>Specify the type of run, useful when grouping runs.</p> <code>None</code> <code>tags</code> <code>list[str] | None</code> <p>A list of strings, which will populate the list of tags on this run. If you want to add tags to a resumed run without overwriting its existing tags, use <code>run.tags += [\"new_tag\"]</code> after <code>wandb.init()</code>.</p> <code>None</code> <code>name</code> <code>str | None</code> <p>A short display name for this run. Default generates a random two-word name.</p> <code>None</code> <code>id</code> <code>str | None</code> <p>A unique ID for this run. It must be unique in the project, and if you delete a run you cannot reuse the ID.</p> <code>None</code> <code>resume</code> <code>str</code> <p>Indicates how to handle the case when a run has the same entity, project and run id as a previous run. \"must\" enforces the run must resume from the run with same id and throws an error if it does not exist. \"never\" enforces that a run will not resume and throws an error if run id exists. \"allow\" resumes if the run id already exists. Defaults to \"allow\".</p> <code>'allow'</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments to <code>wandb.init</code> excluding the ones explicitly described above. Documentation here: https://docs.wandb.ai/ref/python/init/</p> <code>{}</code> Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>def __init__(\n    self,\n    wandb_step_type: WandBStepType | str = WandBStepType.ROUND,\n    project: str | None = None,\n    entity: str | None = None,\n    config: dict | str | None = None,\n    group: str | None = None,\n    job_type: str | None = None,\n    tags: list[str] | None = None,\n    name: str | None = None,\n    id: str | None = None,\n    resume: Literal[\"allow\", \"never\", \"must\", \"auto\"] | bool | None = \"allow\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Weights and Biases Reporter for logging experimental results associated with FL runs.\n\n    Args:\n        wandb_step_type (StepType | str, optional): Whether to use the \"round\", \"epoch\" or \"step\" as the\n            ``wandb_step`` value when logging information to the wandb server.\n        project (str | None, optional): The name of the project where you're sending the new run. If unspecified,\n            wandb will try to infer or set to \"uncategorized\"\n        entity (str | None, optional): An entity is a username or team name where you're sending runs. This entity\n            must exist before you can send runs there, so make sure to create your account or team in the UI before\n            starting to log runs. If you do not specify an entity, the run will be sent to your default entity.\n            Change your default entity in your settings under \"default location to create new projects\".\n        config (str | None, optional): This sets ``wandb.config``, a dictionary-like object for saving inputs to\n            your job such as hyperparameters for a model.\n\n            - If ``dict``: will load the key value pairs into the  ``wandb.config`` object.\n            - If ``str``: will look for a yaml file by that name, and load config from that file into the\n              ``wandb.config`` object.\n        group (str | None, optional): Specify a group to organize individual runs into a larger experiment.\n        job_type (str | None, optional): Specify the type of run, useful when grouping runs.\n        tags (list[str] |None, optional): A list of strings, which will populate the list of tags on this run. If\n            you want to add tags to a resumed run without overwriting its existing tags, use ``run.tags +=\n            [\"new_tag\"]`` after ``wandb.init()``.\n        name (str | None, optional): A short display name for this run. Default generates a random two-word name.\n        id (str | None, optional): A unique ID for this run. It must be unique in the project, and if you delete a\n            run you cannot reuse the ID.\n        resume (str): Indicates how to handle the case when a run has the same entity, project and run id as\n            a previous run. \"must\" enforces the run must resume from the run with same id and throws an error\n            if it does not exist. \"never\" enforces that a run will not resume and throws an error if run id exists.\n            \"allow\" resumes if the run id already exists. Defaults to \"allow\".\n        kwargs (Any): Keyword arguments to ``wandb.init`` excluding the ones explicitly described above.\n            Documentation here: https://docs.wandb.ai/ref/python/init/\n    \"\"\"\n    # Create wandb metadata dir if necessary\n    if kwargs.get(\"dir\") is not None:\n        Path(kwargs[\"dir\"]).mkdir(exist_ok=True)\n\n    # Set attributes\n    self.wandb_init_kwargs = kwargs\n    self.wandb_step_type = WandBStepType(wandb_step_type)\n    self.run_started = False\n    self.initialized = False\n    self.project = project\n    self.entity = entity\n    self.config = config\n    self.group = group\n    self.job_type = job_type\n    self.tags = tags\n    self.name = name\n    self.id = id\n    self.resume = resume\n\n    # Keep track of epoch and step. Initialize as 0.\n    self.current_epoch = 0\n    self.current_step = 0\n\n    # Initialize run later to avoid creating runs while debugging\n    self.run: wandb.wandb_run.Run\n</code></pre>"},{"location":"api/#fl4health.reporting.wandb_reporter.WandBReporter.initialize","title":"<code>initialize(**kwargs)</code>","text":"<p>Checks if an id was provided by the client or server.</p> <p>If an id was passed to the <code>WandBReporter</code> on init then it takes priority over the one passed by the client/server.</p> Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>def initialize(self, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Checks if an id was provided by the client or server.\n\n    If an id was passed to the ``WandBReporter`` on init then it takes priority over the one passed by the\n    client/server.\n    \"\"\"\n    if self.id is None:\n        self.id = kwargs.get(\"id\")\n\n    if self.name is None:\n        self.name = kwargs.get(\"name\")\n\n    self.initialized = True\n</code></pre>"},{"location":"api/#fl4health.reporting.wandb_reporter.WandBReporter.define_metrics","title":"<code>define_metrics()</code>","text":"<p>This method defines some of the metrics we expect to see from <code>BasicClient</code> and server.</p> <p>NOTE that you do not have to define metrics, but it can be useful for determining what should and shouldn't go into the run summary.</p> Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>def define_metrics(self) -&gt; None:\n    \"\"\"\n    This method defines some of the metrics we expect to see from ``BasicClient`` and server.\n\n    **NOTE** that you do not have to define metrics, but it can be useful for determining what should and\n    shouldn't go into the run summary.\n    \"\"\"\n    # Note that the hidden argument is not working. Raised issue here: https://github.com/wandb/wandb/issues/8890\n    # Round, epoch and step\n    self.run.define_metric(\"fit_step\", summary=\"none\", hidden=True)  # Current fit step\n    self.run.define_metric(\"fit_epoch\", summary=\"none\", hidden=True)  # Current fit epoch\n    self.run.define_metric(\"round\", summary=\"none\", hidden=True)  # Current server round\n    self.run.define_metric(\"round_start\", summary=\"none\", hidden=True)\n    self.run.define_metric(\"round_end\", summary=\"none\", hidden=True)\n    # A server round contains a fit_round and maybe also an evaluate round\n    self.run.define_metric(\"fit_round_start\", summary=\"none\", hidden=True)\n    self.run.define_metric(\"fit_round_end\", summary=\"none\", hidden=True)\n    self.run.define_metric(\"eval_round_start\", summary=\"none\", hidden=True)\n    self.run.define_metric(\"eval_round_end\", summary=\"none\", hidden=True)\n    # The metrics computed on all the samples from the final epoch, or the entire round if training by steps\n    self.run.define_metric(\"fit_round_time_elapsed\", summary=\"none\")\n    self.run.define_metric(\"eval_round_time_elapsed\", summary=\"none\")\n    self.run.define_metric(\"fit_round_metrics\", step_metric=\"round\", summary=\"best\")\n    self.run.define_metric(\"eval_round_metrics\", step_metric=\"round\", summary=\"best\")\n    # Average of the losses for each step in the final epoch, or the entire round if training by steps.\n    self.run.define_metric(\"fit_round_losses\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n    self.run.define_metric(\"eval_round_loss\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n    # The metrics computed  at the end of the epoch on all the samples from the epoch\n    self.run.define_metric(\"fit_round_metrics\", step_metric=\"fit_epoch\", summary=\"best\")\n    # Average of the losses for each step in the epoch\n    self.run.define_metric(\"fit_epoch_losses\", step_metric=\"fit_epoch\", summary=\"best\", goal=\"minimize\")\n    # The loss and metrics for each individual step\n    self.run.define_metric(\"fit_step_metrics\", step_metric=\"fit_step\", summary=\"best\")\n    self.run.define_metric(\"fit_step_losses\", step_metric=\"fit_step\", summary=\"best\", goal=\"minimize\")\n    # FlServer (Base Server) specific metrics\n    self.run.define_metric(\"val - loss - aggregated\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n    self.run.define_metric(\"eval_round_metrics_aggregated\", step_metric=\"round\", summary=\"best\")\n    # The following metrics don't work with wandb since they are currently obtained after training instead of live\n    self.run.define_metric(\"val - loss - centralized\", step_metric=\"round\", summary=\"best\", goal=\"minimize\")\n    self.run.define_metric(\"eval_round_metrics_centralized\", step_metric=\"round\", summary=\"best\")\n</code></pre>"},{"location":"api/#fl4health.reporting.wandb_reporter.WandBReporter.start_run","title":"<code>start_run(wandb_init_kwargs)</code>","text":"<p>Initializes the wandb run.</p> <p>We avoid doing this in the <code>self.init</code> function so that when debugging, jobs that fail before training starts do not get uploaded to wandb.</p> <p>Parameters:</p> Name Type Description Default <code>wandb_init_kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments for <code>wandb.init()</code> excluding the ones explicitly accessible through <code>WandBReporter.init()</code>.</p> required Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>def start_run(self, wandb_init_kwargs: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Initializes the wandb run.\n\n    We avoid doing this in the ``self.init`` function so that when debugging, jobs that fail before training\n    starts do not get uploaded to wandb.\n\n    Args:\n        wandb_init_kwargs (dict[str, Any]): Keyword arguments for ``wandb.init()`` excluding the ones explicitly\n            accessible through ``WandBReporter.init()``.\n    \"\"\"\n    if not self.initialized:\n        self.initialize()\n\n    self.run = wandb.init(\n        project=self.project,\n        entity=self.entity,\n        config=self.config,\n        group=self.group,\n        job_type=self.job_type,\n        tags=self.tags,\n        name=self.name,\n        id=self.id,\n        resume=self.resume,\n        **wandb_init_kwargs,  # Other less commonly used kwargs\n    )\n    self.run_id = self.run.id  # If run_id was None, we need to reset run id\n    self.run_started = True\n\n    # Wandb metric definitions\n    self.define_metrics()\n</code></pre>"},{"location":"api/#fl4health.reporting.wandb_reporter.WandBReporter.report","title":"<code>report(data, round=None, epoch=None, step=None)</code>","text":"<p>Reports wandb compatible data to the wandb server.</p> <p>Data passed to <code>self.report</code> is always reported. If round is None, the data is reported as config information. If round is specified, the data is logged to the wandb run at the current wandb step which is either the current round, epoch or step depending on the <code>wandb_step_type</code> passed on initialization. The current epoch  and step are initialized at 0 and updated internally when specified as arguments to report. Therefore leaving  epoch or step as None will overwrite the data for the previous epoch/step if the key is the same, otherwise  the new key-value pairs are added. For example, if <code>{\"loss\": value}</code> is logged every epoch but  <code>wandb_step_type</code> is \"round\", then the value for \"loss\" at round 1 will be it's value at the last epoch of  that round. You can only update or overwrite the current wandb step, previous steps can not be modified.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary of wandb compatible data to log.</p> required <code>round</code> <code>int | None</code> <p>The current FL round. If None, this indicates that the method was called outside of a round (e.g. for summary information). Defaults to None.</p> <code>None</code> <code>epoch</code> <code>int | None</code> <p>The current epoch (In total across all rounds). If None then this method was not called at or within the scope of an epoch. Defaults to None.</p> <code>None</code> <code>step</code> <code>int | None</code> <p>The current step (In total across all rounds and epochs). If None then this method was called outside the scope of a training or evaluation step (e.g. at the end of an epoch or round) Defaults to None.</p> <code>None</code> Source code in <code>fl4health/reporting/wandb_reporter.py</code> <pre><code>def report(\n    self,\n    data: dict[str, Any],\n    round: int | None = None,\n    epoch: int | None = None,\n    step: int | None = None,\n) -&gt; None:\n    \"\"\"\n    Reports wandb compatible data to the wandb server.\n\n    Data passed to ``self.report`` is always reported. If round is None, the data is reported as config\n    information. If round is specified, the data is logged to the wandb run at the current wandb step which is\n    either the current round, epoch or step depending on the ``wandb_step_type`` passed on initialization. The\n    current epoch  and step are initialized at 0 and updated internally when specified as arguments to report.\n    Therefore leaving  epoch or step as None will overwrite the data for the previous epoch/step if the key is the\n    same, otherwise  the new key-value pairs are added. For example, if ``{\"loss\": value}`` is logged every epoch\n    but  ``wandb_step_type`` is \"round\", then the value for \"loss\" at round 1 will be it's value at the last epoch\n    of  that round. You can only update or overwrite the current wandb step, previous steps can not be modified.\n\n    Args:\n        data (dict[str, Any]): Dictionary of wandb compatible data to log.\n        round (int | None, optional): The current FL round. If None, this indicates that the method was called\n            outside of a round (e.g. for summary information). Defaults to None.\n        epoch (int | None, optional): The current epoch (In total across all rounds). If None then this method was\n            not called at or within the scope of an epoch. Defaults to None.\n        step (int | None, optional): The current step (In total across all rounds and epochs). If None then this\n            method was called outside the scope of a training or evaluation step (e.g. at the end of an epoch or\n            round) Defaults to None.\n    \"\"\"\n    # Now that report has been called we are finally forced to start the run.\n    if not self.run_started:\n        self.start_run(self.wandb_init_kwargs)\n\n    # If round is None, assume data is summary information.\n    if round is None:\n        wandb.config.update(data)\n        return\n\n    # Update current epoch and step if they were specified\n    if epoch is not None:\n        if epoch &lt; self.current_epoch:\n            log(\n                WARNING,\n                f\"The specified current epoch ({epoch}) is less than a previous \\\n                    current epoch ({self.current_epoch})\",\n            )\n        self.current_epoch = epoch\n\n    if step is not None:\n        if step &lt; self.current_step:\n            log(\n                WARNING,\n                f\"The specified current step ({step}) is less than a previous current step ({self.current_step})\",\n            )\n        self.current_step = step\n\n    # Log based on step type\n    if self.wandb_step_type == WandBStepType.ROUND:\n        self.run.log(data, step=round)\n    elif self.wandb_step_type == WandBStepType.EPOCH:\n        self.run.log(data, step=self.current_epoch)\n    elif self.wandb_step_type == WandBStepType.STEP:\n        self.run.log(data, step=self.current_step)\n</code></pre>"},{"location":"api/#fl4health.servers","title":"<code>servers</code>","text":""},{"location":"api/#fl4health.servers.adaptive_constraint_servers","title":"<code>adaptive_constraint_servers</code>","text":""},{"location":"api/#fl4health.servers.adaptive_constraint_servers.ditto_server","title":"<code>ditto_server</code>","text":""},{"location":"api/#fl4health.servers.adaptive_constraint_servers.ditto_server.DittoServer","title":"<code>DittoServer</code>","text":"<p>               Bases: <code>FlServer</code></p> Source code in <code>fl4health/servers/adaptive_constraint_servers/ditto_server.py</code> <pre><code>class DittoServer(FlServer):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fl_config: Config,\n        strategy: FedAvgWithAdaptiveConstraint,\n        reporters: Sequence[BaseReporter] | None = None,\n        checkpoint_and_state_module: AdaptiveConstraintServerCheckpointAndStateModule | None = None,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        This is a very basic wrapper class over the ``FlServer`` to ensure that the strategy used for Ditto is of type\n        ``FedAvgWithAdaptiveConstraint``.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fl_config (Config): This should be the configuration that was used to setup the federated training.\n                In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n                example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n                strategy.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            strategy (FedAvgWithAdaptiveConstraint): The aggregation strategy to be used by the server to handle.\n                client updates and other information potentially sent by the participating clients. For MR-MTL, the\n                strategy must be a derivative of the ``FedAvgWithAdaptiveConstraint`` class.\n            reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the server should\n                send data to before and after each round. Defaults to None.\n            checkpoint_and_state_module (AdaptiveConstraintServerCheckpointAndStateModule | None, optional): This\n                module is used to handle both model checkpointing and state checkpointing. The former is aimed at\n                saving model artifacts to be used or evaluated after training. The latter is used to preserve training\n                state (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n\n                **NOTE**: For Ditto, the model shared with the server is the **GLOBAL MODEL**, which isn't the target\n                of FL training for this algorithm. However, one may still want to save this model for other purposes.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n                configure how one asks a client to provide parameters from which to initialize all other clients by\n                providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n                request (which is default behavior for flower servers). Defaults to None.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n        \"\"\"\n        assert isinstance(strategy, FedAvgWithAdaptiveConstraint), (\n            \"Strategy must be of base type FedAvgWithAdaptiveConstraint\"\n        )\n        if checkpoint_and_state_module is not None:\n            assert isinstance(\n                checkpoint_and_state_module,\n                AdaptiveConstraintServerCheckpointAndStateModule,\n            ), \"checkpoint_and_state_module must have type AdaptiveConstraintServerCheckpointAndStateModule\"\n        super().__init__(\n            client_manager=client_manager,\n            fl_config=fl_config,\n            strategy=strategy,\n            reporters=reporters,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n</code></pre> <code></code> <code>__init__(client_manager, fl_config, strategy, reporters=None, checkpoint_and_state_module=None, on_init_parameters_config_fn=None, server_name=None, accept_failures=True)</code> \u00b6 <p>This is a very basic wrapper class over the <code>FlServer</code> to ensure that the strategy used for Ditto is of type <code>FedAvgWithAdaptiveConstraint</code>.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fl_config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated training. In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For example, the config used to produce the <code>on_fit_config_fn</code> and <code>on_evaluate_config_fn</code> for the strategy.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>strategy</code> <code>FedAvgWithAdaptiveConstraint</code> <p>The aggregation strategy to be used by the server to handle. client updates and other information potentially sent by the participating clients. For MR-MTL, the strategy must be a derivative of the <code>FedAvgWithAdaptiveConstraint</code> class.</p> required <code>reporters</code> <code>Sequence[BaseReporter]</code> <p>A sequence of FL4Health reporters which the server should send data to before and after each round. Defaults to None.</p> <code>None</code> <code>checkpoint_and_state_module</code> <code>AdaptiveConstraintServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <p>NOTE: For Ditto, the model shared with the server is the GLOBAL MODEL, which isn't the target of FL training for this algorithm. However, one may still want to save this model for other purposes.</p> <code>None</code> <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. If this is none, then a blank config is sent with the parameter request (which is default behavior for flower servers). Defaults to None.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/servers/adaptive_constraint_servers/ditto_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fl_config: Config,\n    strategy: FedAvgWithAdaptiveConstraint,\n    reporters: Sequence[BaseReporter] | None = None,\n    checkpoint_and_state_module: AdaptiveConstraintServerCheckpointAndStateModule | None = None,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n) -&gt; None:\n    \"\"\"\n    This is a very basic wrapper class over the ``FlServer`` to ensure that the strategy used for Ditto is of type\n    ``FedAvgWithAdaptiveConstraint``.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fl_config (Config): This should be the configuration that was used to setup the federated training.\n            In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n            example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n            strategy.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        strategy (FedAvgWithAdaptiveConstraint): The aggregation strategy to be used by the server to handle.\n            client updates and other information potentially sent by the participating clients. For MR-MTL, the\n            strategy must be a derivative of the ``FedAvgWithAdaptiveConstraint`` class.\n        reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the server should\n            send data to before and after each round. Defaults to None.\n        checkpoint_and_state_module (AdaptiveConstraintServerCheckpointAndStateModule | None, optional): This\n            module is used to handle both model checkpointing and state checkpointing. The former is aimed at\n            saving model artifacts to be used or evaluated after training. The latter is used to preserve training\n            state (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n\n            **NOTE**: For Ditto, the model shared with the server is the **GLOBAL MODEL**, which isn't the target\n            of FL training for this algorithm. However, one may still want to save this model for other purposes.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n            configure how one asks a client to provide parameters from which to initialize all other clients by\n            providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n            request (which is default behavior for flower servers). Defaults to None.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n    \"\"\"\n    assert isinstance(strategy, FedAvgWithAdaptiveConstraint), (\n        \"Strategy must be of base type FedAvgWithAdaptiveConstraint\"\n    )\n    if checkpoint_and_state_module is not None:\n        assert isinstance(\n            checkpoint_and_state_module,\n            AdaptiveConstraintServerCheckpointAndStateModule,\n        ), \"checkpoint_and_state_module must have type AdaptiveConstraintServerCheckpointAndStateModule\"\n    super().__init__(\n        client_manager=client_manager,\n        fl_config=fl_config,\n        strategy=strategy,\n        reporters=reporters,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n</code></pre>"},{"location":"api/#fl4health.servers.adaptive_constraint_servers.fedprox_server","title":"<code>fedprox_server</code>","text":""},{"location":"api/#fl4health.servers.adaptive_constraint_servers.fedprox_server.FedProxServer","title":"<code>FedProxServer</code>","text":"<p>               Bases: <code>FlServer</code></p> Source code in <code>fl4health/servers/adaptive_constraint_servers/fedprox_server.py</code> <pre><code>class FedProxServer(FlServer):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fl_config: Config,\n        strategy: FedAvgWithAdaptiveConstraint,\n        reporters: Sequence[BaseReporter] | None = None,\n        checkpoint_and_state_module: AdaptiveConstraintServerCheckpointAndStateModule | None = None,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        This is a wrapper class around ``FlServer`` for using the FedProx method that enforces that the\n        strategy is of type ``FedAvgWithAdaptiveConstraint`` and that any checkpointing is done with the right\n        server-side model and state checkpointers.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fl_config (Config): This should be the configuration that was used to setup the federated training.\n                In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n                example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n                strategy.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            strategy (FedAvgWithAdaptiveConstraint): The aggregation strategy to be used by the server to handle.\n                client updates and other information potentially sent by the participating clients. This is required\n                to be of type ``FedAvgWithAdaptiveConstraint`` to use FedProx\n            reporters (Sequence[BaseReporter] | None, optional): sequence of FL4Health reporters which the server\n                should send data to before and after each round. Defaults to None.\n            checkpoint_and_state_module (AdaptiveConstraintServerCheckpointAndStateModule | None, optional): This\n                module is used to handle both model checkpointing and state checkpointing. The former is aimed at\n                saving model artifacts to be used or evaluated after training. The latter is used to preserve training\n                state (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n                configure how one asks a client to provide parameters from which to initialize all other clients by\n                providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n                request (which is default behavior for flower servers). Defaults to None.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n        \"\"\"\n        assert isinstance(strategy, FedAvgWithAdaptiveConstraint), (\n            \"Strategy must be of base type FedAvgWithAdaptiveConstraint\"\n        )\n        if checkpoint_and_state_module is not None:\n            assert isinstance(\n                checkpoint_and_state_module,\n                AdaptiveConstraintServerCheckpointAndStateModule,\n            ), \"checkpoint_and_state_module must have type AdaptiveConstraintServerCheckpointAndStateModule\"\n        super().__init__(\n            client_manager=client_manager,\n            fl_config=fl_config,\n            strategy=strategy,\n            reporters=reporters,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n</code></pre> <code></code> <code>__init__(client_manager, fl_config, strategy, reporters=None, checkpoint_and_state_module=None, on_init_parameters_config_fn=None, server_name=None, accept_failures=True)</code> \u00b6 <p>This is a wrapper class around <code>FlServer</code> for using the FedProx method that enforces that the strategy is of type <code>FedAvgWithAdaptiveConstraint</code> and that any checkpointing is done with the right server-side model and state checkpointers.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fl_config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated training. In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For example, the config used to produce the <code>on_fit_config_fn</code> and <code>on_evaluate_config_fn</code> for the strategy.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>strategy</code> <code>FedAvgWithAdaptiveConstraint</code> <p>The aggregation strategy to be used by the server to handle. client updates and other information potentially sent by the participating clients. This is required to be of type <code>FedAvgWithAdaptiveConstraint</code> to use FedProx</p> required <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>sequence of FL4Health reporters which the server should send data to before and after each round. Defaults to None.</p> <code>None</code> <code>checkpoint_and_state_module</code> <code>AdaptiveConstraintServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <code>None</code> <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. If this is none, then a blank config is sent with the parameter request (which is default behavior for flower servers). Defaults to None.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/servers/adaptive_constraint_servers/fedprox_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fl_config: Config,\n    strategy: FedAvgWithAdaptiveConstraint,\n    reporters: Sequence[BaseReporter] | None = None,\n    checkpoint_and_state_module: AdaptiveConstraintServerCheckpointAndStateModule | None = None,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n) -&gt; None:\n    \"\"\"\n    This is a wrapper class around ``FlServer`` for using the FedProx method that enforces that the\n    strategy is of type ``FedAvgWithAdaptiveConstraint`` and that any checkpointing is done with the right\n    server-side model and state checkpointers.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fl_config (Config): This should be the configuration that was used to setup the federated training.\n            In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n            example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n            strategy.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        strategy (FedAvgWithAdaptiveConstraint): The aggregation strategy to be used by the server to handle.\n            client updates and other information potentially sent by the participating clients. This is required\n            to be of type ``FedAvgWithAdaptiveConstraint`` to use FedProx\n        reporters (Sequence[BaseReporter] | None, optional): sequence of FL4Health reporters which the server\n            should send data to before and after each round. Defaults to None.\n        checkpoint_and_state_module (AdaptiveConstraintServerCheckpointAndStateModule | None, optional): This\n            module is used to handle both model checkpointing and state checkpointing. The former is aimed at\n            saving model artifacts to be used or evaluated after training. The latter is used to preserve training\n            state (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n            configure how one asks a client to provide parameters from which to initialize all other clients by\n            providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n            request (which is default behavior for flower servers). Defaults to None.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n    \"\"\"\n    assert isinstance(strategy, FedAvgWithAdaptiveConstraint), (\n        \"Strategy must be of base type FedAvgWithAdaptiveConstraint\"\n    )\n    if checkpoint_and_state_module is not None:\n        assert isinstance(\n            checkpoint_and_state_module,\n            AdaptiveConstraintServerCheckpointAndStateModule,\n        ), \"checkpoint_and_state_module must have type AdaptiveConstraintServerCheckpointAndStateModule\"\n    super().__init__(\n        client_manager=client_manager,\n        fl_config=fl_config,\n        strategy=strategy,\n        reporters=reporters,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n</code></pre>"},{"location":"api/#fl4health.servers.adaptive_constraint_servers.mrmtl_server","title":"<code>mrmtl_server</code>","text":""},{"location":"api/#fl4health.servers.adaptive_constraint_servers.mrmtl_server.MrMtlServer","title":"<code>MrMtlServer</code>","text":"<p>               Bases: <code>FlServer</code></p> Source code in <code>fl4health/servers/adaptive_constraint_servers/mrmtl_server.py</code> <pre><code>class MrMtlServer(FlServer):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fl_config: Config,\n        strategy: FedAvgWithAdaptiveConstraint,\n        reporters: Sequence[BaseReporter] | None = None,\n        checkpoint_and_state_module: AdaptiveConstraintServerCheckpointAndStateModule | None = None,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        This is a very basic wrapper class over the ``FlServer`` to ensure that the strategy used for MR-MTL is of type\n        ``FedAvgWithAdaptiveConstraint``.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fl_config (Config): This should be the configuration that was used to setup the federated training.\n                In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n                example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n                strategy.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            strategy (FedAvgWithAdaptiveConstraint): The aggregation strategy to be used by the server to handle.\n                client updates and other information potentially sent by the participating clients. For MR-MTL, the\n                strategy must be a derivative of the ``FedAvgWithAdaptiveConstraint`` class.\n            reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the server should\n                send data to before and after each round. Defaults to None.\n            checkpoint_and_state_module (AdaptiveConstraintServerCheckpointAndStateModule | None, optional): This\n                module is used to handle both model checkpointing and state checkpointing. The former is aimed at\n                saving model artifacts to be used or evaluated after training. The latter is used to preserve training\n                state (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n\n                **NOTE**: For MR-MTL, the server model is an aggregation of the personal models, which isn't the\n                target of FL training for this algorithm. However, one may still want to save this model for other\n                purposes.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n                configure how one asks a client to provide parameters from which to initialize all other clients by\n                providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n                request (which is default behavior for flower servers). Defaults to None.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n\n        \"\"\"\n        assert isinstance(strategy, FedAvgWithAdaptiveConstraint), (\n            \"Strategy must be of base type FedAvgWithAdaptiveConstraint\"\n        )\n        if checkpoint_and_state_module is not None:\n            assert isinstance(\n                checkpoint_and_state_module,\n                AdaptiveConstraintServerCheckpointAndStateModule,\n            ), \"checkpoint_and_state_module must have type AdaptiveConstraintServerCheckpointAndStateModule\"\n        super().__init__(\n            client_manager=client_manager,\n            fl_config=fl_config,\n            strategy=strategy,\n            reporters=reporters,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n</code></pre> <code></code> <code>__init__(client_manager, fl_config, strategy, reporters=None, checkpoint_and_state_module=None, on_init_parameters_config_fn=None, server_name=None, accept_failures=True)</code> \u00b6 <p>This is a very basic wrapper class over the <code>FlServer</code> to ensure that the strategy used for MR-MTL is of type <code>FedAvgWithAdaptiveConstraint</code>.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fl_config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated training. In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For example, the config used to produce the <code>on_fit_config_fn</code> and <code>on_evaluate_config_fn</code> for the strategy.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>strategy</code> <code>FedAvgWithAdaptiveConstraint</code> <p>The aggregation strategy to be used by the server to handle. client updates and other information potentially sent by the participating clients. For MR-MTL, the strategy must be a derivative of the <code>FedAvgWithAdaptiveConstraint</code> class.</p> required <code>reporters</code> <code>Sequence[BaseReporter]</code> <p>A sequence of FL4Health reporters which the server should send data to before and after each round. Defaults to None.</p> <code>None</code> <code>checkpoint_and_state_module</code> <code>AdaptiveConstraintServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <p>NOTE: For MR-MTL, the server model is an aggregation of the personal models, which isn't the target of FL training for this algorithm. However, one may still want to save this model for other purposes.</p> <code>None</code> <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. If this is none, then a blank config is sent with the parameter request (which is default behavior for flower servers). Defaults to None.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/servers/adaptive_constraint_servers/mrmtl_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fl_config: Config,\n    strategy: FedAvgWithAdaptiveConstraint,\n    reporters: Sequence[BaseReporter] | None = None,\n    checkpoint_and_state_module: AdaptiveConstraintServerCheckpointAndStateModule | None = None,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n) -&gt; None:\n    \"\"\"\n    This is a very basic wrapper class over the ``FlServer`` to ensure that the strategy used for MR-MTL is of type\n    ``FedAvgWithAdaptiveConstraint``.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fl_config (Config): This should be the configuration that was used to setup the federated training.\n            In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n            example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n            strategy.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        strategy (FedAvgWithAdaptiveConstraint): The aggregation strategy to be used by the server to handle.\n            client updates and other information potentially sent by the participating clients. For MR-MTL, the\n            strategy must be a derivative of the ``FedAvgWithAdaptiveConstraint`` class.\n        reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the server should\n            send data to before and after each round. Defaults to None.\n        checkpoint_and_state_module (AdaptiveConstraintServerCheckpointAndStateModule | None, optional): This\n            module is used to handle both model checkpointing and state checkpointing. The former is aimed at\n            saving model artifacts to be used or evaluated after training. The latter is used to preserve training\n            state (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n\n            **NOTE**: For MR-MTL, the server model is an aggregation of the personal models, which isn't the\n            target of FL training for this algorithm. However, one may still want to save this model for other\n            purposes.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n            configure how one asks a client to provide parameters from which to initialize all other clients by\n            providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n            request (which is default behavior for flower servers). Defaults to None.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n\n    \"\"\"\n    assert isinstance(strategy, FedAvgWithAdaptiveConstraint), (\n        \"Strategy must be of base type FedAvgWithAdaptiveConstraint\"\n    )\n    if checkpoint_and_state_module is not None:\n        assert isinstance(\n            checkpoint_and_state_module,\n            AdaptiveConstraintServerCheckpointAndStateModule,\n        ), \"checkpoint_and_state_module must have type AdaptiveConstraintServerCheckpointAndStateModule\"\n    super().__init__(\n        client_manager=client_manager,\n        fl_config=fl_config,\n        strategy=strategy,\n        reporters=reporters,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n</code></pre>"},{"location":"api/#fl4health.servers.base_server","title":"<code>base_server</code>","text":""},{"location":"api/#fl4health.servers.base_server.FlServer","title":"<code>FlServer</code>","text":"<p>               Bases: <code>Server</code></p> Source code in <code>fl4health/servers/base_server.py</code> <pre><code>class FlServer(Server):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fl_config: Config,\n        strategy: Strategy | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        checkpoint_and_state_module: BaseServerCheckpointAndStateModule | None = None,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Base Server for the library to facilitate strapping additional/useful machinery to the base flwr server.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fl_config (Config): This should be the configuration that was used to setup the federated training.\n                In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n                example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n                strategy.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            strategy (Strategy | None, optional): The aggregation strategy to be used by the server to handle.\n                client updates and other information potentially sent by the participating clients. If None the\n                strategy is FedAvg as set by the flwr Server. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): sequence of FL4Health reporters which the server\n                should send data to before and after each round. Defaults to None.\n            checkpoint_and_state_module (BaseServerCheckpointAndStateModule | None, optional): This module is used\n                to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n                artifacts to be used or evaluated after training. The latter is used to preserve training state\n                (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n                configure how one asks a client to provide parameters from which to initialize all other clients by\n                providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n                request (which is default behavior for flower servers).\n\n                **NOTE:** If you are using a client defined in this library, passing a blank configuration will ALMOST\n                CERTAINLY fail. This is because asking a client for parameters will almost always require setting up\n                the client, as is done when fitting. In many cases, you can simply pass your ``on_fit_config_fn``\n                function from the strategy to as this argument as well.\n\n                Defaults to None.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n        \"\"\"\n        super().__init__(client_manager=client_manager, strategy=strategy)\n        self.fl_config = fl_config\n        if checkpoint_and_state_module is not None:\n            self.checkpoint_and_state_module = checkpoint_and_state_module\n        else:\n            # Define a default module that does nothing.\n            self.checkpoint_and_state_module = BaseServerCheckpointAndStateModule(\n                model=None,\n                parameter_exchanger=None,\n                model_checkpointers=None,\n                state_checkpointer=None,\n            )\n        self.on_init_parameters_config_fn = on_init_parameters_config_fn\n\n        self.server_name = server_name if server_name is not None else generate_hash()\n        log(INFO, f\"Server Name: {self.server_name}\")\n\n        self.accept_failures = accept_failures\n\n        self.current_round: int\n        self.history: History\n\n        # Initialize reporters with server name information.\n        self.reports_manager = ReportsManager(reporters)\n        self.reports_manager.initialize(id=self.server_name)\n        self._log_fl_config()\n\n    def update_before_fit(self, num_rounds: int, timeout: float | None) -&gt; None:\n        \"\"\"\n        Hook method to allow the server to do some work before starting the fit process. In the base server, it is a\n        no-op function, but it can be overridden in child classes for custom functionality. For example, the\n        ``NnUNetServer`` class uses this method to ask a client to initialize the global nnunet plans if one is not\n        provided in the config. This can only be done after the clients have started up and are ready to train.\n\n        Args:\n            num_rounds (int): The number of server rounds of FL to be performed.\n            timeout (float | None, optional): The server's timeout parameter. Useful if one is requesting\n                information from a client. Defaults to None, which indicates indefinite timeout.\n        \"\"\"\n        pass\n\n    def report_centralized_eval(self, history: History, num_rounds: int) -&gt; None:\n        if len(history.losses_centralized) == 0:\n            return\n\n        # Parse and report history for loss and metrics on centralized validation set.\n        for round in range(num_rounds):\n            self.reports_manager.report(\n                {\"val - loss - centralized\": history.losses_centralized[round][1]},\n                round + 1,\n            )\n            round_metrics = {}\n            for metric, vals in history.metrics_centralized.items():\n                round_metrics.update({metric: vals[round][1]})\n            self.reports_manager.report({\"eval_round_metrics_centralized\": round_metrics}, round + 1)\n\n    def fit_with_per_round_checkpointing(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n        \"\"\"\n        Runs federated learning for a number of rounds. Heavily based on the fit method from the base\n        server provided by flower (``flwr.server.server.Server``) except that it is resilient to preemptions.\n        It accomplishes this by checkpointing the server state each round. In the case of preemption,\n        when the server is restarted it will load from the most recent checkpoint.\n\n        Args:\n            num_rounds (int): The number of rounds to perform federated learning.\n            timeout (float | None): The timeout for clients to return results in a given FL round.\n\n        Returns:\n            (tuple[History, float]): The first element of the tuple is a ``History`` object containing the losses and\n                metrics computed during training and validation. The second element of the tuple is the elapsed time in\n                seconds.\n        \"\"\"\n        log(INFO, \"Initializing server state and global parameters\")\n        self.parameters = self._get_initial_parameters(server_round=0, timeout=timeout)\n        self.history = History()\n        self.current_round = 1\n        # Attempt to load the server state if it exists. If the state checkpoint exists, update the initiated\n        # attributes like history, server round and model accordingly\n        state_load_success = self._load_server_state()\n        if state_load_success:\n            log(INFO, \"Server state checkpoint successfully loaded.\")\n        else:\n            log(INFO, \"No server state checkpoint found. Starting from scratch.\")\n        if self.current_round == 1:\n            log(INFO, \"Evaluating initial parameters\")\n            res = self.strategy.evaluate(0, parameters=self.parameters)\n            if res is not None:\n                log(\n                    INFO,\n                    \"initial parameters (loss, other metrics): %s, %s\",\n                    res[0],\n                    res[1],\n                )\n                self.history.add_loss_centralized(server_round=0, loss=res[0])\n                self.history.add_metrics_centralized(server_round=0, metrics=res[1])\n\n            # Run federated learning for num_rounds\n            log(INFO, \"FL starting\")\n\n        start_time = datetime.datetime.now()\n\n        while self.current_round &lt; (num_rounds + 1):\n            # Train model and replace previous global model\n            res_fit = self.fit_round(server_round=self.current_round, timeout=timeout)\n            if res_fit:\n                parameters_prime, fit_metrics, _ = res_fit  # fit_metrics_aggregated\n                if parameters_prime:\n                    self.parameters = parameters_prime\n                self.history.add_metrics_distributed_fit(server_round=self.current_round, metrics=fit_metrics)\n\n            # Evaluate model using strategy implementation\n            res_cen = self.strategy.evaluate(self.current_round, parameters=self.parameters)\n            if res_cen is not None:\n                loss_cen, metrics_cen = res_cen\n                log(\n                    INFO,\n                    \"fit progress: (%s, %s, %s, %s)\",\n                    self.current_round,\n                    loss_cen,\n                    metrics_cen,\n                    (datetime.datetime.now() - start_time).total_seconds(),\n                )\n                self.history.add_loss_centralized(server_round=self.current_round, loss=loss_cen)\n                self.history.add_metrics_centralized(server_round=self.current_round, metrics=metrics_cen)\n\n            # Evaluate model on a sample of available clients\n            res_fed = self.evaluate_round(server_round=self.current_round, timeout=timeout)\n            if res_fed:\n                loss_fed, evaluate_metrics_fed, _ = res_fed\n                if loss_fed:\n                    self.history.add_loss_distributed(server_round=self.current_round, loss=loss_fed)\n                    self.history.add_metrics_distributed(server_round=self.current_round, metrics=evaluate_metrics_fed)\n\n            self.current_round += 1\n\n            # Save checkpoint after training and testing\n            self._save_server_state()\n\n        # Bookkeeping\n        end_time = datetime.datetime.now()\n        elapsed_time = end_time - start_time\n        log(INFO, \"FL finished in %s\", str(elapsed_time))\n        return self.history, elapsed_time.total_seconds()\n\n    @override\n    def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n        \"\"\"\n        Run federated learning for a number of rounds. This function also allows the server to perform some operations\n        prior to fitting starting. This is useful, for example, if you need to communicate with the clients to\n        initialize anything prior to FL starting (see nnunet server for an example).\n\n        Args:\n            num_rounds (int): Number of server rounds to run.\n            timeout (float | None): The amount of time in seconds that the server will wait for results from the\n                clients selected to participate in federated training.\n\n        Returns:\n            (tuple[History, float]): The first element of the tuple is a ``History`` object containing the full set of\n                FL training results, including things like aggregated loss and metrics. Tuple also contains the elapsed\n                time in seconds for the round.\n        \"\"\"\n        start_time = datetime.datetime.now()\n        self.reports_manager.report(\n            {\n                \"fit_start\": str(start_time),\n                \"host_type\": \"server\",\n            }\n        )\n\n        self.update_before_fit(num_rounds, timeout)\n\n        if self.checkpoint_and_state_module.state_checkpointer is not None:\n            history, elapsed_time = self.fit_with_per_round_checkpointing(num_rounds, timeout)\n        else:\n            history, elapsed_time = super().fit(num_rounds, timeout)\n        end_time = datetime.datetime.now()\n        self.reports_manager.report(\n            {\n                \"fit_elapsed_time\": round((end_time - start_time).total_seconds()),\n                \"fit_end\": str(end_time),\n                \"num_rounds\": num_rounds,\n                \"host_type\": \"server\",\n            }\n        )\n\n        # WARNING: This will not work with wandb. Wandb reporting must be done live.\n        self.report_centralized_eval(history, num_rounds)\n\n        return history, elapsed_time\n\n    @override\n    def fit_round(\n        self,\n        server_round: int,\n        timeout: float | None,\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar], FitResultsAndFailures] | None:\n        \"\"\"\n        This function is called at each round of federated training. The flow is generally the same as a flower\n        server, where clients are sampled and client side training is requested from the clients that are chosen.\n        This function simply adds a bit of logging, post processing of the results.\n\n        Args:\n            server_round (int): Current round number of the FL training. Begins at 1.\n            timeout (float | None): Time that the server should wait (in seconds) for responses from the clients.\n                Defaults to None, which indicates indefinite timeout.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar], FitResultsAndFailures] | None): The results of training\n                on the client sit. The first set of parameters are the **AGGREGATED** parameters from the strategy. The\n                second is a dictionary of **AGGREGATED** metrics. The third component holds the individual\n                (non-aggregated) parameters, loss, and metrics for successful and unsuccessful client-side training.\n        \"\"\"\n        round_start = datetime.datetime.now()\n        fit_round_results = super().fit_round(server_round, timeout)\n        round_end = datetime.datetime.now()\n\n        self.reports_manager.report(\n            {\n                \"fit_round_start\": str(round_start),\n                \"fit_round_end\": str(round_end),\n                \"fit_round_time_elapsed\": round((round_end - round_start).total_seconds()),\n            },\n            server_round,\n        )\n        if fit_round_results is not None:\n            _, metrics, fit_results_and_failures = fit_round_results\n            self.reports_manager.report({\"fit_round_metrics\": metrics}, server_round)\n            failures = fit_results_and_failures[1] if fit_results_and_failures else None\n\n            if failures and not self.accept_failures:\n                self._log_client_failures(failures)\n                self._terminate_after_unacceptable_failures(timeout)\n\n        return fit_round_results\n\n    def shutdown(self) -&gt; None:\n        \"\"\"Currently just records termination of the server process and disconnects and reporters that need to be.\"\"\"\n        self.reports_manager.report({\"shutdown\": str(datetime.datetime.now())})\n        self.reports_manager.shutdown()\n\n    def poll_clients_for_sample_counts(self, timeout: float | None) -&gt; list[int]:\n        \"\"\"\n        Poll clients for sample counts from their training set, if you want to use this functionality your strategy\n        needs to inherit from the ``StrategyWithPolling`` ABC and implement a ``configure_poll`` function.\n\n        Args:\n            timeout (float | None): Timeout for how long the server will wait for clients to report counts. If none\n                then the server waits indefinitely.\n\n        Returns:\n            (list[int]): The number of training samples held by each client in the pool of available clients.\n        \"\"\"\n        # Poll clients for sample counts, if you want to use this functionality your strategy needs to inherit from\n        # the StrategyWithPolling ABC and implement a configure_poll function\n        log(INFO, \"Polling Clients for sample counts\")\n        assert isinstance(self.strategy, StrategyWithPolling)\n        client_instructions = self.strategy.configure_poll(server_round=1, client_manager=self._client_manager)\n        results, _ = poll_clients(\n            client_instructions=client_instructions,\n            max_workers=self.max_workers,\n            timeout=timeout,\n        )\n\n        sample_counts: list[int] = [\n            int(get_properties_res.properties[\"num_train_samples\"]) for (_, get_properties_res) in results\n        ]\n        log(INFO, f\"Polling complete: Retrieved {len(sample_counts)} sample counts\")\n\n        return sample_counts\n\n    @override\n    def evaluate_round(\n        self,\n        server_round: int,\n        timeout: float | None,\n    ) -&gt; tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None:\n        \"\"\"\n        This function runs evaluation after a round of training.\n\n        By default the checkpointing works off of the aggregated evaluation loss from each of the clients\n\n        **NOTE**: parameter aggregation occurs **before** evaluation, so the parameters held by the server have been\n        updated prior to this function being called.\n\n        Args:\n            server_round (int): Server round we're currently on.\n            timeout (float | None): Time that the server should wait (in seconds) for responses from the clients.\n                Defaults to None, which indicates indefinite timeout.\n\n        Returns:\n            (tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None): Tuple of loss value, metrics\n                dictionary and individual client results (client ids and failures).\n        \"\"\"\n        start_time = datetime.datetime.now()\n        eval_round_results = self._evaluate_round(server_round, timeout)\n        end_time = datetime.datetime.now()\n        if eval_round_results:\n            loss_aggregated, metrics_aggregated, (_, failures) = eval_round_results\n\n            if failures and not self.accept_failures:\n                self._log_client_failures(failures)\n                self._terminate_after_unacceptable_failures(timeout)\n\n            if loss_aggregated:\n                self._maybe_checkpoint(loss_aggregated, metrics_aggregated, server_round)\n                # Report evaluation results\n                report_data = {\n                    \"val - loss - aggregated\": loss_aggregated,\n                    \"round\": server_round,\n                    \"eval_round_start\": str(start_time),\n                    \"eval_round_end\": str(end_time),\n                    \"eval_round_time_elapsed\": round((end_time - start_time).total_seconds()),\n                }\n\n                if self.fl_config.get(\"local_epochs\", None) is not None:\n                    report_data[\"fit_epoch\"] = server_round * self.fl_config[\"local_epochs\"]\n                elif self.fl_config.get(\"local_steps\", None) is not None:\n                    report_data[\"fit_step\"] = server_round * self.fl_config[\"local_steps\"]\n                self.reports_manager.report(report_data, server_round)\n                if len(metrics_aggregated) &gt; 0:\n                    self.reports_manager.report(\n                        {\"eval_round_metrics_aggregated\": metrics_aggregated},\n                        server_round,\n                    )\n\n        return eval_round_results\n\n    def _log_fl_config(self) -&gt; None:\n        log(INFO, \"FL Configuration:\") if self.fl_config else log(INFO, \"FL Config is Empty\")\n        for config_key, config_value in self.fl_config.items():\n            if not isinstance(config_value, bytes):\n                log(INFO, f\"Key: {config_key} Value: {config_value!r}\")\n\n    def _save_server_state(self) -&gt; None:\n        \"\"\"\n        Save server checkpoint consisting of model, history, server round, metrics reporter and server name. This\n        method can be overridden to add any necessary state to the checkpoint. The model will be injected into the\n        ckpt state by the checkpoint module.\n        \"\"\"\n        assert self.checkpoint_and_state_module.state_checkpointer is not None\n        self.checkpoint_and_state_module.save_state(self, self.parameters)\n\n    def _load_server_state(self) -&gt; bool:\n        \"\"\"\n        Load server checkpoint consisting of model, history, server name, current round and metrics reporter.\n        The method can be overridden to add any necessary state when loading the checkpoint.\n        \"\"\"\n        assert self.checkpoint_and_state_module.state_checkpointer is not None\n        # Attempt to load the server state if it exists.\n        server_parameters = self.checkpoint_and_state_module.maybe_load_state(self)\n        if server_parameters:\n            self.parameters = server_parameters\n            log(INFO, \"Loaded server state from checkpoint\")\n            return True\n        return False\n\n    def _terminate_after_unacceptable_failures(self, timeout: float | None) -&gt; None:\n        assert not self.accept_failures\n        # First we shutdown all clients involved in the FL training/evaluation if they can be.\n        self.disconnect_all_clients(timeout=timeout)\n        # Throw an exception alerting the user to failures on the client-side causing termination\n        self.shutdown()\n        raise ValueError(\n            f\"The server encountered failures from the clients and accept_failures is set to {self.accept_failures}\"\n        )\n\n    def _log_client_failures(self, failures: FitFailures | EvaluateFailures) -&gt; None:\n        log(\n            ERROR,\n            f\"There were {len(failures)} failures in the fitting process. This will result in termination of \"\n            \"the FL process\",\n        )\n        for failure in failures:\n            if isinstance(failure, BaseException):\n                log(\n                    ERROR,\n                    \"An exception was returned instead of any failed results. As such the client ID is unknown. \"\n                    \"Please check the client logs to determine which failed.\\n\"\n                    f\"The exception thrown was {repr(failure)}\",\n                )\n            else:\n                client_proxy, _ = failure\n                log(\n                    ERROR,\n                    f\"Client {client_proxy.cid} failed but did not return an exception. Partial results were received\",\n                )\n\n    def _maybe_checkpoint(\n        self,\n        loss_aggregated: float,\n        metrics_aggregated: dict[str, Scalar],\n        server_round: int,\n    ) -&gt; None:\n        \"\"\"\n        This function simply runs the ``maybe_checkpoint`` functionality of the ``checkpoint_and_state_module``. If\n        additional functionality is desired, this function may be overridden.\n\n        Args:\n            loss_aggregated (float): aggregated loss value that can be used to determine whether to checkpoint\n            metrics_aggregated (dict[str, Scalar]): aggregated metrics from each of the clients for checkpointing\n            server_round (int): What round of federated training we're on. This is just for logging purposes.\n        \"\"\"\n        self.checkpoint_and_state_module.maybe_checkpoint(self.parameters, loss_aggregated, metrics_aggregated)\n\n    @override\n    def _get_initial_parameters(self, server_round: int, timeout: float | None) -&gt; Parameters:\n        \"\"\"\n        Get initial parameters from one of the available clients. This function is the same as the parent function\n        in the flower server class except that we make use of the ``on_parameter_initialization_config_fn`` to provide\n        a non-empty config to a client when requesting parameters from which to initialize all other clients.\n\n        **NOTE**: The default behavior of flower servers is to simply send over a blank config, but this is\n        insufficient for certain uses, where the client requires additional information from the server. This is\n        needed, for example in nnUnet-based Servers. An issue has been logged with\n        flower: https://github.com/adap/flower/issues/3770.\n        \"\"\"\n        # Server-side parameter initialization\n        parameters: Parameters | None = self.strategy.initialize_parameters(client_manager=self._client_manager)\n        if parameters is not None:\n            log(INFO, \"Using initial global parameters provided by strategy\")\n            return parameters\n\n        # Get initial parameters from one of the clients\n        log(INFO, \"Requesting initial parameters from one random client\")\n        if isinstance(self._client_manager, BaseFractionSamplingManager):\n            random_client = self._client_manager.sample_one()[0]\n        else:\n            random_client = self._client_manager.sample(1)[0]\n\n        if self.on_init_parameters_config_fn is None:\n            log(\n                WARNING,\n                (\n                    \"on_init_parameters_config_fn is None. Please ensure that this is expected behavior. When using \"\n                    \"clients from the FL4Health library this will generally fail. See class documentation of this \"\n                    \"parameter for additional details.\"\n                ),\n            )\n            # An empty configuration is the default for Flower servers\n            ins = GetParametersIns(config={})\n        else:\n            ins = GetParametersIns(config=self.on_init_parameters_config_fn(server_round))\n        get_parameters_res = random_client.get_parameters(ins=ins, timeout=timeout, group_id=server_round)\n        if get_parameters_res.status.code == Code.OK:\n            log(INFO, \"Received initial parameters from one random client\")\n        else:\n            log(\n                WARNING,\n                \"Failed to receive initial parameters from the client. Empty initial parameters will be used.\",\n            )\n\n        initial_parameters = get_parameters_res.parameters\n        if isinstance(self.strategy, BasicFedAvg):\n            # Potentially add auxiliary information if necessary.\n            self.strategy.add_auxiliary_information(initial_parameters)\n\n        return initial_parameters\n\n    def _unpack_metrics(\n        self, results: list[tuple[ClientProxy, EvaluateRes]]\n    ) -&gt; tuple[list[tuple[ClientProxy, EvaluateRes]], list[tuple[ClientProxy, EvaluateRes]]]:\n        val_results = []\n        test_results = []\n\n        for client_proxy, eval_res in results:\n            val_metrics = {\n                k: v for k, v in eval_res.metrics.items() if not k.startswith(MetricPrefix.TEST_PREFIX.value)\n            }\n            test_metrics = {k: v for k, v in eval_res.metrics.items() if k.startswith(MetricPrefix.TEST_PREFIX.value)}\n\n            if len(test_metrics) &gt; 0:\n                assert TEST_LOSS_KEY in test_metrics and TEST_NUM_EXAMPLES_KEY in test_metrics, (\n                    f\"'{TEST_NUM_EXAMPLES_KEY}' and '{TEST_LOSS_KEY}' keys must be present in \"\n                    \"test_metrics dictionary for aggregation\"\n                )\n                # Remove loss and num_examples from test_metrics if they exist\n                test_loss = float(test_metrics.pop(TEST_LOSS_KEY))\n                test_num_examples = int(test_metrics.pop(TEST_NUM_EXAMPLES_KEY))\n                test_eval_res = EvaluateRes(eval_res.status, test_loss, test_num_examples, test_metrics)\n                test_results.append((client_proxy, test_eval_res))\n\n            val_eval_res = EvaluateRes(eval_res.status, eval_res.loss, eval_res.num_examples, val_metrics)\n            val_results.append((client_proxy, val_eval_res))\n\n        return val_results, test_results\n\n    def _handle_result_aggregation(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, EvaluateRes]],\n        failures: list[tuple[ClientProxy, EvaluateRes] | BaseException],\n    ) -&gt; tuple[float | None, dict[str, Scalar]]:\n        val_results, test_results = self._unpack_metrics(results)\n\n        # Aggregate the validation results\n        val_aggregated_result: tuple[\n            float | None,\n            dict[str, Scalar],\n        ] = self.strategy.aggregate_evaluate(server_round, val_results, failures)\n        val_loss_aggregated, val_metrics_aggregated = val_aggregated_result\n\n        # Aggregate the test results if they are present\n        if len(test_results) &gt; 0:\n            test_aggregated_result: tuple[\n                float | None,\n                dict[str, Scalar],\n            ] = self.strategy.aggregate_evaluate(server_round, test_results, failures)\n            test_loss_aggregated, test_metrics_aggregated = test_aggregated_result\n\n            for key, value in test_metrics_aggregated.items():\n                val_metrics_aggregated[key] = value\n            if test_loss_aggregated is not None:\n                val_metrics_aggregated[f\"{MetricPrefix.TEST_PREFIX.value} loss - aggregated\"] = test_loss_aggregated\n\n        return val_loss_aggregated, val_metrics_aggregated\n\n    def _evaluate_round(\n        self,\n        server_round: int,\n        timeout: float | None,\n    ) -&gt; tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None:\n        \"\"\"Validate current global model on a number of clients.\"\"\"\n        # Get clients and their respective instructions from strategy\n        client_instructions = self.strategy.configure_evaluate(\n            server_round=server_round,\n            parameters=self.parameters,\n            client_manager=self._client_manager,\n        )\n        if not client_instructions:\n            log(INFO, \"evaluate_round %s: no clients selected, cancel\", server_round)\n            return None\n        log(\n            DEBUG,\n            \"evaluate_round %s: strategy sampled %s clients (out of %s)\",\n            server_round,\n            len(client_instructions),\n            self._client_manager.num_available(),\n        )\n        # Collect `evaluate` results from all clients participating in this round\n        # flwr sets group_id to server_round by default, so we follow that convention\n        results, failures = evaluate_clients(\n            client_instructions,\n            max_workers=self.max_workers,\n            timeout=timeout,\n            group_id=server_round,\n        )\n        log(\n            DEBUG,\n            \"evaluate_round %s received %s results and %s failures for Validation\",\n            server_round,\n            len(results),\n            len(failures),\n        )\n\n        val_loss_aggregated, val_metrics_aggregated = self._handle_result_aggregation(server_round, results, failures)\n\n        return val_loss_aggregated, val_metrics_aggregated, (results, failures)\n</code></pre>"},{"location":"api/#fl4health.servers.base_server.FlServer.__init__","title":"<code>__init__(client_manager, fl_config, strategy=None, reporters=None, checkpoint_and_state_module=None, on_init_parameters_config_fn=None, server_name=None, accept_failures=True)</code>","text":"<p>Base Server for the library to facilitate strapping additional/useful machinery to the base flwr server.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fl_config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated training. In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For example, the config used to produce the <code>on_fit_config_fn</code> and <code>on_evaluate_config_fn</code> for the strategy.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>strategy</code> <code>Strategy | None</code> <p>The aggregation strategy to be used by the server to handle. client updates and other information potentially sent by the participating clients. If None the strategy is FedAvg as set by the flwr Server. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>sequence of FL4Health reporters which the server should send data to before and after each round. Defaults to None.</p> <code>None</code> <code>checkpoint_and_state_module</code> <code>BaseServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <code>None</code> <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. If this is none, then a blank config is sent with the parameter request (which is default behavior for flower servers).</p> <p>NOTE: If you are using a client defined in this library, passing a blank configuration will ALMOST CERTAINLY fail. This is because asking a client for parameters will almost always require setting up the client, as is done when fitting. In many cases, you can simply pass your <code>on_fit_config_fn</code> function from the strategy to as this argument as well.</p> <p>Defaults to None.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/servers/base_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fl_config: Config,\n    strategy: Strategy | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    checkpoint_and_state_module: BaseServerCheckpointAndStateModule | None = None,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n) -&gt; None:\n    \"\"\"\n    Base Server for the library to facilitate strapping additional/useful machinery to the base flwr server.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fl_config (Config): This should be the configuration that was used to setup the federated training.\n            In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n            example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n            strategy.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        strategy (Strategy | None, optional): The aggregation strategy to be used by the server to handle.\n            client updates and other information potentially sent by the participating clients. If None the\n            strategy is FedAvg as set by the flwr Server. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): sequence of FL4Health reporters which the server\n            should send data to before and after each round. Defaults to None.\n        checkpoint_and_state_module (BaseServerCheckpointAndStateModule | None, optional): This module is used\n            to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n            artifacts to be used or evaluated after training. The latter is used to preserve training state\n            (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n            configure how one asks a client to provide parameters from which to initialize all other clients by\n            providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n            request (which is default behavior for flower servers).\n\n            **NOTE:** If you are using a client defined in this library, passing a blank configuration will ALMOST\n            CERTAINLY fail. This is because asking a client for parameters will almost always require setting up\n            the client, as is done when fitting. In many cases, you can simply pass your ``on_fit_config_fn``\n            function from the strategy to as this argument as well.\n\n            Defaults to None.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n    \"\"\"\n    super().__init__(client_manager=client_manager, strategy=strategy)\n    self.fl_config = fl_config\n    if checkpoint_and_state_module is not None:\n        self.checkpoint_and_state_module = checkpoint_and_state_module\n    else:\n        # Define a default module that does nothing.\n        self.checkpoint_and_state_module = BaseServerCheckpointAndStateModule(\n            model=None,\n            parameter_exchanger=None,\n            model_checkpointers=None,\n            state_checkpointer=None,\n        )\n    self.on_init_parameters_config_fn = on_init_parameters_config_fn\n\n    self.server_name = server_name if server_name is not None else generate_hash()\n    log(INFO, f\"Server Name: {self.server_name}\")\n\n    self.accept_failures = accept_failures\n\n    self.current_round: int\n    self.history: History\n\n    # Initialize reporters with server name information.\n    self.reports_manager = ReportsManager(reporters)\n    self.reports_manager.initialize(id=self.server_name)\n    self._log_fl_config()\n</code></pre>"},{"location":"api/#fl4health.servers.base_server.FlServer.update_before_fit","title":"<code>update_before_fit(num_rounds, timeout)</code>","text":"<p>Hook method to allow the server to do some work before starting the fit process. In the base server, it is a no-op function, but it can be overridden in child classes for custom functionality. For example, the <code>NnUNetServer</code> class uses this method to ask a client to initialize the global nnunet plans if one is not provided in the config. This can only be done after the clients have started up and are ready to train.</p> <p>Parameters:</p> Name Type Description Default <code>num_rounds</code> <code>int</code> <p>The number of server rounds of FL to be performed.</p> required <code>timeout</code> <code>float | None</code> <p>The server's timeout parameter. Useful if one is requesting information from a client. Defaults to None, which indicates indefinite timeout.</p> required Source code in <code>fl4health/servers/base_server.py</code> <pre><code>def update_before_fit(self, num_rounds: int, timeout: float | None) -&gt; None:\n    \"\"\"\n    Hook method to allow the server to do some work before starting the fit process. In the base server, it is a\n    no-op function, but it can be overridden in child classes for custom functionality. For example, the\n    ``NnUNetServer`` class uses this method to ask a client to initialize the global nnunet plans if one is not\n    provided in the config. This can only be done after the clients have started up and are ready to train.\n\n    Args:\n        num_rounds (int): The number of server rounds of FL to be performed.\n        timeout (float | None, optional): The server's timeout parameter. Useful if one is requesting\n            information from a client. Defaults to None, which indicates indefinite timeout.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.servers.base_server.FlServer.fit_with_per_round_checkpointing","title":"<code>fit_with_per_round_checkpointing(num_rounds, timeout)</code>","text":"<p>Runs federated learning for a number of rounds. Heavily based on the fit method from the base server provided by flower (<code>flwr.server.server.Server</code>) except that it is resilient to preemptions. It accomplishes this by checkpointing the server state each round. In the case of preemption, when the server is restarted it will load from the most recent checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>num_rounds</code> <code>int</code> <p>The number of rounds to perform federated learning.</p> required <code>timeout</code> <code>float | None</code> <p>The timeout for clients to return results in a given FL round.</p> required <p>Returns:</p> Type Description <code>tuple[History, float]</code> <p>The first element of the tuple is a <code>History</code> object containing the losses and metrics computed during training and validation. The second element of the tuple is the elapsed time in seconds.</p> Source code in <code>fl4health/servers/base_server.py</code> <pre><code>def fit_with_per_round_checkpointing(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n    \"\"\"\n    Runs federated learning for a number of rounds. Heavily based on the fit method from the base\n    server provided by flower (``flwr.server.server.Server``) except that it is resilient to preemptions.\n    It accomplishes this by checkpointing the server state each round. In the case of preemption,\n    when the server is restarted it will load from the most recent checkpoint.\n\n    Args:\n        num_rounds (int): The number of rounds to perform federated learning.\n        timeout (float | None): The timeout for clients to return results in a given FL round.\n\n    Returns:\n        (tuple[History, float]): The first element of the tuple is a ``History`` object containing the losses and\n            metrics computed during training and validation. The second element of the tuple is the elapsed time in\n            seconds.\n    \"\"\"\n    log(INFO, \"Initializing server state and global parameters\")\n    self.parameters = self._get_initial_parameters(server_round=0, timeout=timeout)\n    self.history = History()\n    self.current_round = 1\n    # Attempt to load the server state if it exists. If the state checkpoint exists, update the initiated\n    # attributes like history, server round and model accordingly\n    state_load_success = self._load_server_state()\n    if state_load_success:\n        log(INFO, \"Server state checkpoint successfully loaded.\")\n    else:\n        log(INFO, \"No server state checkpoint found. Starting from scratch.\")\n    if self.current_round == 1:\n        log(INFO, \"Evaluating initial parameters\")\n        res = self.strategy.evaluate(0, parameters=self.parameters)\n        if res is not None:\n            log(\n                INFO,\n                \"initial parameters (loss, other metrics): %s, %s\",\n                res[0],\n                res[1],\n            )\n            self.history.add_loss_centralized(server_round=0, loss=res[0])\n            self.history.add_metrics_centralized(server_round=0, metrics=res[1])\n\n        # Run federated learning for num_rounds\n        log(INFO, \"FL starting\")\n\n    start_time = datetime.datetime.now()\n\n    while self.current_round &lt; (num_rounds + 1):\n        # Train model and replace previous global model\n        res_fit = self.fit_round(server_round=self.current_round, timeout=timeout)\n        if res_fit:\n            parameters_prime, fit_metrics, _ = res_fit  # fit_metrics_aggregated\n            if parameters_prime:\n                self.parameters = parameters_prime\n            self.history.add_metrics_distributed_fit(server_round=self.current_round, metrics=fit_metrics)\n\n        # Evaluate model using strategy implementation\n        res_cen = self.strategy.evaluate(self.current_round, parameters=self.parameters)\n        if res_cen is not None:\n            loss_cen, metrics_cen = res_cen\n            log(\n                INFO,\n                \"fit progress: (%s, %s, %s, %s)\",\n                self.current_round,\n                loss_cen,\n                metrics_cen,\n                (datetime.datetime.now() - start_time).total_seconds(),\n            )\n            self.history.add_loss_centralized(server_round=self.current_round, loss=loss_cen)\n            self.history.add_metrics_centralized(server_round=self.current_round, metrics=metrics_cen)\n\n        # Evaluate model on a sample of available clients\n        res_fed = self.evaluate_round(server_round=self.current_round, timeout=timeout)\n        if res_fed:\n            loss_fed, evaluate_metrics_fed, _ = res_fed\n            if loss_fed:\n                self.history.add_loss_distributed(server_round=self.current_round, loss=loss_fed)\n                self.history.add_metrics_distributed(server_round=self.current_round, metrics=evaluate_metrics_fed)\n\n        self.current_round += 1\n\n        # Save checkpoint after training and testing\n        self._save_server_state()\n\n    # Bookkeeping\n    end_time = datetime.datetime.now()\n    elapsed_time = end_time - start_time\n    log(INFO, \"FL finished in %s\", str(elapsed_time))\n    return self.history, elapsed_time.total_seconds()\n</code></pre>"},{"location":"api/#fl4health.servers.base_server.FlServer.fit","title":"<code>fit(num_rounds, timeout)</code>","text":"<p>Run federated learning for a number of rounds. This function also allows the server to perform some operations prior to fitting starting. This is useful, for example, if you need to communicate with the clients to initialize anything prior to FL starting (see nnunet server for an example).</p> <p>Parameters:</p> Name Type Description Default <code>num_rounds</code> <code>int</code> <p>Number of server rounds to run.</p> required <code>timeout</code> <code>float | None</code> <p>The amount of time in seconds that the server will wait for results from the clients selected to participate in federated training.</p> required <p>Returns:</p> Type Description <code>tuple[History, float]</code> <p>The first element of the tuple is a <code>History</code> object containing the full set of FL training results, including things like aggregated loss and metrics. Tuple also contains the elapsed time in seconds for the round.</p> Source code in <code>fl4health/servers/base_server.py</code> <pre><code>@override\ndef fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n    \"\"\"\n    Run federated learning for a number of rounds. This function also allows the server to perform some operations\n    prior to fitting starting. This is useful, for example, if you need to communicate with the clients to\n    initialize anything prior to FL starting (see nnunet server for an example).\n\n    Args:\n        num_rounds (int): Number of server rounds to run.\n        timeout (float | None): The amount of time in seconds that the server will wait for results from the\n            clients selected to participate in federated training.\n\n    Returns:\n        (tuple[History, float]): The first element of the tuple is a ``History`` object containing the full set of\n            FL training results, including things like aggregated loss and metrics. Tuple also contains the elapsed\n            time in seconds for the round.\n    \"\"\"\n    start_time = datetime.datetime.now()\n    self.reports_manager.report(\n        {\n            \"fit_start\": str(start_time),\n            \"host_type\": \"server\",\n        }\n    )\n\n    self.update_before_fit(num_rounds, timeout)\n\n    if self.checkpoint_and_state_module.state_checkpointer is not None:\n        history, elapsed_time = self.fit_with_per_round_checkpointing(num_rounds, timeout)\n    else:\n        history, elapsed_time = super().fit(num_rounds, timeout)\n    end_time = datetime.datetime.now()\n    self.reports_manager.report(\n        {\n            \"fit_elapsed_time\": round((end_time - start_time).total_seconds()),\n            \"fit_end\": str(end_time),\n            \"num_rounds\": num_rounds,\n            \"host_type\": \"server\",\n        }\n    )\n\n    # WARNING: This will not work with wandb. Wandb reporting must be done live.\n    self.report_centralized_eval(history, num_rounds)\n\n    return history, elapsed_time\n</code></pre>"},{"location":"api/#fl4health.servers.base_server.FlServer.fit_round","title":"<code>fit_round(server_round, timeout)</code>","text":"<p>This function is called at each round of federated training. The flow is generally the same as a flower server, where clients are sampled and client side training is requested from the clients that are chosen. This function simply adds a bit of logging, post processing of the results.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Current round number of the FL training. Begins at 1.</p> required <code>timeout</code> <code>float | None</code> <p>Time that the server should wait (in seconds) for responses from the clients. Defaults to None, which indicates indefinite timeout.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar], FitResultsAndFailures] | None</code> <p>The results of training on the client sit. The first set of parameters are the AGGREGATED parameters from the strategy. The second is a dictionary of AGGREGATED metrics. The third component holds the individual (non-aggregated) parameters, loss, and metrics for successful and unsuccessful client-side training.</p> Source code in <code>fl4health/servers/base_server.py</code> <pre><code>@override\ndef fit_round(\n    self,\n    server_round: int,\n    timeout: float | None,\n) -&gt; tuple[Parameters | None, dict[str, Scalar], FitResultsAndFailures] | None:\n    \"\"\"\n    This function is called at each round of federated training. The flow is generally the same as a flower\n    server, where clients are sampled and client side training is requested from the clients that are chosen.\n    This function simply adds a bit of logging, post processing of the results.\n\n    Args:\n        server_round (int): Current round number of the FL training. Begins at 1.\n        timeout (float | None): Time that the server should wait (in seconds) for responses from the clients.\n            Defaults to None, which indicates indefinite timeout.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar], FitResultsAndFailures] | None): The results of training\n            on the client sit. The first set of parameters are the **AGGREGATED** parameters from the strategy. The\n            second is a dictionary of **AGGREGATED** metrics. The third component holds the individual\n            (non-aggregated) parameters, loss, and metrics for successful and unsuccessful client-side training.\n    \"\"\"\n    round_start = datetime.datetime.now()\n    fit_round_results = super().fit_round(server_round, timeout)\n    round_end = datetime.datetime.now()\n\n    self.reports_manager.report(\n        {\n            \"fit_round_start\": str(round_start),\n            \"fit_round_end\": str(round_end),\n            \"fit_round_time_elapsed\": round((round_end - round_start).total_seconds()),\n        },\n        server_round,\n    )\n    if fit_round_results is not None:\n        _, metrics, fit_results_and_failures = fit_round_results\n        self.reports_manager.report({\"fit_round_metrics\": metrics}, server_round)\n        failures = fit_results_and_failures[1] if fit_results_and_failures else None\n\n        if failures and not self.accept_failures:\n            self._log_client_failures(failures)\n            self._terminate_after_unacceptable_failures(timeout)\n\n    return fit_round_results\n</code></pre>"},{"location":"api/#fl4health.servers.base_server.FlServer.shutdown","title":"<code>shutdown()</code>","text":"<p>Currently just records termination of the server process and disconnects and reporters that need to be.</p> Source code in <code>fl4health/servers/base_server.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"Currently just records termination of the server process and disconnects and reporters that need to be.\"\"\"\n    self.reports_manager.report({\"shutdown\": str(datetime.datetime.now())})\n    self.reports_manager.shutdown()\n</code></pre>"},{"location":"api/#fl4health.servers.base_server.FlServer.poll_clients_for_sample_counts","title":"<code>poll_clients_for_sample_counts(timeout)</code>","text":"<p>Poll clients for sample counts from their training set, if you want to use this functionality your strategy needs to inherit from the <code>StrategyWithPolling</code> ABC and implement a <code>configure_poll</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float | None</code> <p>Timeout for how long the server will wait for clients to report counts. If none then the server waits indefinitely.</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>The number of training samples held by each client in the pool of available clients.</p> Source code in <code>fl4health/servers/base_server.py</code> <pre><code>def poll_clients_for_sample_counts(self, timeout: float | None) -&gt; list[int]:\n    \"\"\"\n    Poll clients for sample counts from their training set, if you want to use this functionality your strategy\n    needs to inherit from the ``StrategyWithPolling`` ABC and implement a ``configure_poll`` function.\n\n    Args:\n        timeout (float | None): Timeout for how long the server will wait for clients to report counts. If none\n            then the server waits indefinitely.\n\n    Returns:\n        (list[int]): The number of training samples held by each client in the pool of available clients.\n    \"\"\"\n    # Poll clients for sample counts, if you want to use this functionality your strategy needs to inherit from\n    # the StrategyWithPolling ABC and implement a configure_poll function\n    log(INFO, \"Polling Clients for sample counts\")\n    assert isinstance(self.strategy, StrategyWithPolling)\n    client_instructions = self.strategy.configure_poll(server_round=1, client_manager=self._client_manager)\n    results, _ = poll_clients(\n        client_instructions=client_instructions,\n        max_workers=self.max_workers,\n        timeout=timeout,\n    )\n\n    sample_counts: list[int] = [\n        int(get_properties_res.properties[\"num_train_samples\"]) for (_, get_properties_res) in results\n    ]\n    log(INFO, f\"Polling complete: Retrieved {len(sample_counts)} sample counts\")\n\n    return sample_counts\n</code></pre>"},{"location":"api/#fl4health.servers.base_server.FlServer.evaluate_round","title":"<code>evaluate_round(server_round, timeout)</code>","text":"<p>This function runs evaluation after a round of training.</p> <p>By default the checkpointing works off of the aggregated evaluation loss from each of the clients</p> <p>NOTE: parameter aggregation occurs before evaluation, so the parameters held by the server have been updated prior to this function being called.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Server round we're currently on.</p> required <code>timeout</code> <code>float | None</code> <p>Time that the server should wait (in seconds) for responses from the clients. Defaults to None, which indicates indefinite timeout.</p> required <p>Returns:</p> Type Description <code>tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None</code> <p>Tuple of loss value, metrics dictionary and individual client results (client ids and failures).</p> Source code in <code>fl4health/servers/base_server.py</code> <pre><code>@override\ndef evaluate_round(\n    self,\n    server_round: int,\n    timeout: float | None,\n) -&gt; tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None:\n    \"\"\"\n    This function runs evaluation after a round of training.\n\n    By default the checkpointing works off of the aggregated evaluation loss from each of the clients\n\n    **NOTE**: parameter aggregation occurs **before** evaluation, so the parameters held by the server have been\n    updated prior to this function being called.\n\n    Args:\n        server_round (int): Server round we're currently on.\n        timeout (float | None): Time that the server should wait (in seconds) for responses from the clients.\n            Defaults to None, which indicates indefinite timeout.\n\n    Returns:\n        (tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None): Tuple of loss value, metrics\n            dictionary and individual client results (client ids and failures).\n    \"\"\"\n    start_time = datetime.datetime.now()\n    eval_round_results = self._evaluate_round(server_round, timeout)\n    end_time = datetime.datetime.now()\n    if eval_round_results:\n        loss_aggregated, metrics_aggregated, (_, failures) = eval_round_results\n\n        if failures and not self.accept_failures:\n            self._log_client_failures(failures)\n            self._terminate_after_unacceptable_failures(timeout)\n\n        if loss_aggregated:\n            self._maybe_checkpoint(loss_aggregated, metrics_aggregated, server_round)\n            # Report evaluation results\n            report_data = {\n                \"val - loss - aggregated\": loss_aggregated,\n                \"round\": server_round,\n                \"eval_round_start\": str(start_time),\n                \"eval_round_end\": str(end_time),\n                \"eval_round_time_elapsed\": round((end_time - start_time).total_seconds()),\n            }\n\n            if self.fl_config.get(\"local_epochs\", None) is not None:\n                report_data[\"fit_epoch\"] = server_round * self.fl_config[\"local_epochs\"]\n            elif self.fl_config.get(\"local_steps\", None) is not None:\n                report_data[\"fit_step\"] = server_round * self.fl_config[\"local_steps\"]\n            self.reports_manager.report(report_data, server_round)\n            if len(metrics_aggregated) &gt; 0:\n                self.reports_manager.report(\n                    {\"eval_round_metrics_aggregated\": metrics_aggregated},\n                    server_round,\n                )\n\n    return eval_round_results\n</code></pre>"},{"location":"api/#fl4health.servers.client_level_dp_fed_avg_server","title":"<code>client_level_dp_fed_avg_server</code>","text":""},{"location":"api/#fl4health.servers.client_level_dp_fed_avg_server.ClientLevelDPFedAvgServer","title":"<code>ClientLevelDPFedAvgServer</code>","text":"<p>               Bases: <code>FlServer</code></p> Source code in <code>fl4health/servers/client_level_dp_fed_avg_server.py</code> <pre><code>class ClientLevelDPFedAvgServer(FlServer):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fl_config: Config,\n        strategy: ClientLevelDPFedAvgM,\n        server_noise_multiplier: float,\n        num_server_rounds: int,\n        reporters: Sequence[BaseReporter] | None = None,\n        checkpoint_and_state_module: ClippingBitServerCheckpointAndStateModule | None = None,\n        delta: int | None = None,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Server to be used in case of Client Level Differential Privacy with Federated Averaging.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fl_config (Config): This should be the configuration that was used to setup the federated training.\n                In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n                example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n                strategy.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            strategy (ClientLevelDPFedAvgM): The aggregation strategy to be used by the server to handle.\n                client updates and other information potentially sent by the participating clients.\n            server_noise_multiplier (float): Magnitude of noise added to the weights aggregation process by the server.\n            num_server_rounds (int): Number of rounds of FL training carried out by the server.\n            reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the server should\n                send data to before and after each round.\n            checkpoint_and_state_module (BaseServerCheckpointAndStateModule | None, optional): This module is used\n                to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n                artifacts to be used or evaluated after training. The latter is used to preserve training state\n                (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n            delta (float | None, optional): The delta value for epsilon-delta DP accounting. If None it defaults to\n                being ``1/total_samples`` in the FL run. Defaults to None.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n                configure how one asks a client to provide parameters from which to initialize all other clients by\n                providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n                request (which is default behavior for flower servers). Defaults to None.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n        \"\"\"\n        if checkpoint_and_state_module is not None:\n            assert isinstance(\n                checkpoint_and_state_module,\n                ClippingBitServerCheckpointAndStateModule,\n            ), \"checkpoint_and_state_module must have type ClippingBitServerCheckpointAndStateModule\"\n        super().__init__(\n            client_manager=client_manager,\n            fl_config=fl_config,\n            strategy=strategy,\n            reporters=reporters,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n        self.accountant: ClientLevelAccountant\n        self.server_noise_multiplier = server_noise_multiplier\n        self.num_server_rounds = num_server_rounds\n        self.delta = delta\n\n    def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n        \"\"\"\n        Run federated averaging for a number of rounds.\n\n        Args:\n            num_rounds (int): Number of server rounds to run.\n            timeout (float | None): The amount of time in seconds that the server will wait for results from the\n                clients selected to participate in federated training.\n\n        Returns:\n            (tuple[History, float]): The first element of the tuple is a ``History`` object containing the full set of\n                FL training results, including things like aggregated loss and metrics. Tuple also contains the elapsed\n                time in seconds for the round.\n        \"\"\"\n        assert isinstance(self.strategy, ClientLevelDPFedAvgM)\n\n        sample_counts = self.poll_clients_for_sample_counts(timeout)\n\n        # If Weighted FedAvg, set sample counts to compute client weights\n        if self.strategy.weighted_aggregation:\n            self.strategy.sample_counts = sample_counts\n\n        self.setup_privacy_accountant(sample_counts)\n\n        return super().fit(num_rounds=num_rounds, timeout=timeout)\n\n    def setup_privacy_accountant(self, sample_counts: list[int]) -&gt; None:\n        \"\"\"\n        Sets up FL Accountant and computes privacy loss based on class attributes and retrieved sample counts.\n\n        Args:\n            sample_counts (list[int]): These should be the total number of training examples fetched from all clients\n                during the sample polling process.\n        \"\"\"\n        assert isinstance(self.strategy, ClientLevelDPFedAvgM)\n\n        num_clients = len(sample_counts)\n        target_delta = self.delta if self.delta is not None else 1 / num_clients\n\n        if isinstance(self._client_manager, PoissonSamplingClientManager):\n            self.accountant = FlClientLevelAccountantPoissonSampling(\n                client_sampling_rate=self.strategy.fraction_fit,\n                noise_multiplier=self.server_noise_multiplier,\n            )\n        else:\n            assert isinstance(self._client_manager, FixedSamplingByFractionClientManager)\n            num_clients_sampled = ceil(len(sample_counts) * self.strategy.fraction_fit)\n            self.accountant = FlClientLevelAccountantFixedSamplingNoReplacement(\n                n_total_clients=num_clients,\n                n_clients_sampled=num_clients_sampled,\n                noise_multiplier=self.server_noise_multiplier,\n            )\n\n        # Note that this assumes that the FL round has exactly n_clients participating.\n        epsilon = self.accountant.get_epsilon(self.num_server_rounds, target_delta)\n        log(\n            INFO,\n            f\"Model privacy after full training will be ({epsilon}, {target_delta})\",\n        )\n</code></pre>"},{"location":"api/#fl4health.servers.client_level_dp_fed_avg_server.ClientLevelDPFedAvgServer.__init__","title":"<code>__init__(client_manager, fl_config, strategy, server_noise_multiplier, num_server_rounds, reporters=None, checkpoint_and_state_module=None, delta=None, on_init_parameters_config_fn=None, server_name=None, accept_failures=True)</code>","text":"<p>Server to be used in case of Client Level Differential Privacy with Federated Averaging.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fl_config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated training. In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For example, the config used to produce the <code>on_fit_config_fn</code> and <code>on_evaluate_config_fn</code> for the strategy.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>strategy</code> <code>ClientLevelDPFedAvgM</code> <p>The aggregation strategy to be used by the server to handle. client updates and other information potentially sent by the participating clients.</p> required <code>server_noise_multiplier</code> <code>float</code> <p>Magnitude of noise added to the weights aggregation process by the server.</p> required <code>num_server_rounds</code> <code>int</code> <p>Number of rounds of FL training carried out by the server.</p> required <code>reporters</code> <code>Sequence[BaseReporter]</code> <p>A sequence of FL4Health reporters which the server should send data to before and after each round.</p> <code>None</code> <code>checkpoint_and_state_module</code> <code>BaseServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <code>None</code> <code>delta</code> <code>float | None</code> <p>The delta value for epsilon-delta DP accounting. If None it defaults to being <code>1/total_samples</code> in the FL run. Defaults to None.</p> <code>None</code> <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. If this is none, then a blank config is sent with the parameter request (which is default behavior for flower servers). Defaults to None.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/servers/client_level_dp_fed_avg_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fl_config: Config,\n    strategy: ClientLevelDPFedAvgM,\n    server_noise_multiplier: float,\n    num_server_rounds: int,\n    reporters: Sequence[BaseReporter] | None = None,\n    checkpoint_and_state_module: ClippingBitServerCheckpointAndStateModule | None = None,\n    delta: int | None = None,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n) -&gt; None:\n    \"\"\"\n    Server to be used in case of Client Level Differential Privacy with Federated Averaging.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fl_config (Config): This should be the configuration that was used to setup the federated training.\n            In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n            example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n            strategy.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        strategy (ClientLevelDPFedAvgM): The aggregation strategy to be used by the server to handle.\n            client updates and other information potentially sent by the participating clients.\n        server_noise_multiplier (float): Magnitude of noise added to the weights aggregation process by the server.\n        num_server_rounds (int): Number of rounds of FL training carried out by the server.\n        reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the server should\n            send data to before and after each round.\n        checkpoint_and_state_module (BaseServerCheckpointAndStateModule | None, optional): This module is used\n            to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n            artifacts to be used or evaluated after training. The latter is used to preserve training state\n            (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n        delta (float | None, optional): The delta value for epsilon-delta DP accounting. If None it defaults to\n            being ``1/total_samples`` in the FL run. Defaults to None.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n            configure how one asks a client to provide parameters from which to initialize all other clients by\n            providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n            request (which is default behavior for flower servers). Defaults to None.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n    \"\"\"\n    if checkpoint_and_state_module is not None:\n        assert isinstance(\n            checkpoint_and_state_module,\n            ClippingBitServerCheckpointAndStateModule,\n        ), \"checkpoint_and_state_module must have type ClippingBitServerCheckpointAndStateModule\"\n    super().__init__(\n        client_manager=client_manager,\n        fl_config=fl_config,\n        strategy=strategy,\n        reporters=reporters,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n    self.accountant: ClientLevelAccountant\n    self.server_noise_multiplier = server_noise_multiplier\n    self.num_server_rounds = num_server_rounds\n    self.delta = delta\n</code></pre>"},{"location":"api/#fl4health.servers.client_level_dp_fed_avg_server.ClientLevelDPFedAvgServer.fit","title":"<code>fit(num_rounds, timeout)</code>","text":"<p>Run federated averaging for a number of rounds.</p> <p>Parameters:</p> Name Type Description Default <code>num_rounds</code> <code>int</code> <p>Number of server rounds to run.</p> required <code>timeout</code> <code>float | None</code> <p>The amount of time in seconds that the server will wait for results from the clients selected to participate in federated training.</p> required <p>Returns:</p> Type Description <code>tuple[History, float]</code> <p>The first element of the tuple is a <code>History</code> object containing the full set of FL training results, including things like aggregated loss and metrics. Tuple also contains the elapsed time in seconds for the round.</p> Source code in <code>fl4health/servers/client_level_dp_fed_avg_server.py</code> <pre><code>def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n    \"\"\"\n    Run federated averaging for a number of rounds.\n\n    Args:\n        num_rounds (int): Number of server rounds to run.\n        timeout (float | None): The amount of time in seconds that the server will wait for results from the\n            clients selected to participate in federated training.\n\n    Returns:\n        (tuple[History, float]): The first element of the tuple is a ``History`` object containing the full set of\n            FL training results, including things like aggregated loss and metrics. Tuple also contains the elapsed\n            time in seconds for the round.\n    \"\"\"\n    assert isinstance(self.strategy, ClientLevelDPFedAvgM)\n\n    sample_counts = self.poll_clients_for_sample_counts(timeout)\n\n    # If Weighted FedAvg, set sample counts to compute client weights\n    if self.strategy.weighted_aggregation:\n        self.strategy.sample_counts = sample_counts\n\n    self.setup_privacy_accountant(sample_counts)\n\n    return super().fit(num_rounds=num_rounds, timeout=timeout)\n</code></pre>"},{"location":"api/#fl4health.servers.client_level_dp_fed_avg_server.ClientLevelDPFedAvgServer.setup_privacy_accountant","title":"<code>setup_privacy_accountant(sample_counts)</code>","text":"<p>Sets up FL Accountant and computes privacy loss based on class attributes and retrieved sample counts.</p> <p>Parameters:</p> Name Type Description Default <code>sample_counts</code> <code>list[int]</code> <p>These should be the total number of training examples fetched from all clients during the sample polling process.</p> required Source code in <code>fl4health/servers/client_level_dp_fed_avg_server.py</code> <pre><code>def setup_privacy_accountant(self, sample_counts: list[int]) -&gt; None:\n    \"\"\"\n    Sets up FL Accountant and computes privacy loss based on class attributes and retrieved sample counts.\n\n    Args:\n        sample_counts (list[int]): These should be the total number of training examples fetched from all clients\n            during the sample polling process.\n    \"\"\"\n    assert isinstance(self.strategy, ClientLevelDPFedAvgM)\n\n    num_clients = len(sample_counts)\n    target_delta = self.delta if self.delta is not None else 1 / num_clients\n\n    if isinstance(self._client_manager, PoissonSamplingClientManager):\n        self.accountant = FlClientLevelAccountantPoissonSampling(\n            client_sampling_rate=self.strategy.fraction_fit,\n            noise_multiplier=self.server_noise_multiplier,\n        )\n    else:\n        assert isinstance(self._client_manager, FixedSamplingByFractionClientManager)\n        num_clients_sampled = ceil(len(sample_counts) * self.strategy.fraction_fit)\n        self.accountant = FlClientLevelAccountantFixedSamplingNoReplacement(\n            n_total_clients=num_clients,\n            n_clients_sampled=num_clients_sampled,\n            noise_multiplier=self.server_noise_multiplier,\n        )\n\n    # Note that this assumes that the FL round has exactly n_clients participating.\n    epsilon = self.accountant.get_epsilon(self.num_server_rounds, target_delta)\n    log(\n        INFO,\n        f\"Model privacy after full training will be ({epsilon}, {target_delta})\",\n    )\n</code></pre>"},{"location":"api/#fl4health.servers.evaluate_server","title":"<code>evaluate_server</code>","text":""},{"location":"api/#fl4health.servers.evaluate_server.EvaluateServer","title":"<code>EvaluateServer</code>","text":"<p>               Bases: <code>Server</code></p> Source code in <code>fl4health/servers/evaluate_server.py</code> <pre><code>class EvaluateServer(Server):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fraction_evaluate: float,\n        model_checkpoint_path: Path | None = None,\n        evaluate_config: dict[str, Scalar] | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        accept_failures: bool = True,\n        min_available_clients: int = 1,\n        reporters: Sequence[BaseReporter] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Server meant to facilitate federated evaluation only (that is, no training).\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fraction_evaluate (float): Fraction of clients used during evaluation.\n            model_checkpoint_path (Path | None, optional): Server side model checkpoint path to load global model\n                from. Defaults to None.\n            evaluate_config (dict[str, Scalar] | None, optional): Configuration dictionary to configure evaluation\n                on clients. Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional):  Metrics aggregation function.\n                 Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 1.\n            reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the client should\n                send data to.\n        \"\"\"\n        # We aren't aggregating model weights, so setting the strategy to be none.\n        super().__init__(client_manager=client_manager, strategy=None)\n        self.model_checkpoint_path = model_checkpoint_path\n        # Load model parameters if checkpoint provided, otherwise leave as empty params\n        if model_checkpoint_path:\n            self.parameters = self.load_model_checkpoint_to_parameters()\n        self.fraction_evaluate = fraction_evaluate\n        self.evaluate_config = evaluate_config\n        self.min_available_clients = min_available_clients\n        self.accept_failures = accept_failures\n        self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n        if self.fraction_evaluate &lt; 1.0:\n            log(\n                INFO,\n                f\"Fraction Evaluate is {self.fraction_evaluate}. Thus, some clients may not participate in evaluation\",\n            )\n        self.server_name = generate_hash()\n        self.reporters = [] if reporters is None else list(reporters)\n        for r in self.reporters:\n            r.initialize(id=self.server_name)\n\n    def load_model_checkpoint_to_parameters(self) -&gt; Parameters:\n        assert self.model_checkpoint_path\n        log(INFO, f\"Loading model checkpoint at: {self.model_checkpoint_path.__str__()}\")\n        model = torch.load(self.model_checkpoint_path, weights_only=False)\n        # Extracting all parameters from the model to be sent to the clients\n        parameters = ndarrays_to_parameters([val.cpu().numpy() for _, val in model.state_dict().items()])\n        log(INFO, \"Model loaded and state converted to parameters\")\n        return parameters\n\n    def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n        \"\"\"\n        In order to head off training and only run eval, we have to override the fit function as this is\n        essentially the entry point for federated learning from the app.\n\n        Args:\n            num_rounds (int): Not used.\n            timeout (float | None): Timeout in seconds that the server should wait for the clients to respond.\n                If none, then it will wait for the minimum number to respond indefinitely.\n\n        Returns:\n            (tuple[History, float]): The first element of the tuple is a ``History`` object containing the aggregated\n                metrics returned from the clients. Tuple also contains elapsed time in seconds for round.\n        \"\"\"\n        history = History()\n\n        # Run Federated Evaluation\n        log(INFO, \"Federated Evaluation Starting\")\n        start_time = datetime.datetime.now()\n\n        for reporter in self.reporters:\n            reporter.report(\n                {\n                    \"fit_start\": str(start_time),\n                    \"host_type\": \"server\",\n                }\n            )\n        # We're only performing federated evaluation. So we make use of the evaluate round function, but simply\n        # perform such evaluation once.\n        res_fed = self.federated_evaluate(timeout=timeout)\n        end_time = datetime.datetime.now()\n\n        for r in self.reporters:\n            r.report(\n                {\n                    \"fit_elapsed_time\": str(start_time - end_time),\n                    \"fit_end\": str(end_time),\n                    \"num_rounds\": num_rounds,\n                    \"host_type\": \"server\",\n                }\n            )\n        if res_fed:\n            _, evaluate_metrics_fed, _ = res_fed\n            if evaluate_metrics_fed:\n                history.add_metrics_distributed(server_round=0, metrics=evaluate_metrics_fed)\n                if evaluate_metrics_fed:\n                    for r in self.reporters:\n                        r.report({\"fit_metrics\": evaluate_metrics_fed})\n\n        # Bookkeeping\n        elapsed = end_time - start_time\n        log(INFO, \"Federated Evaluation Finished in %s\", str(elapsed))\n        return history, elapsed.total_seconds()\n\n    def federated_evaluate(\n        self,\n        timeout: float | None,\n    ) -&gt; tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None:\n        \"\"\"\n        Validate current global model on a number of clients.\n\n        Args:\n            timeout (float | None): Timeout in seconds that the server should wait for the clients to response.\n                If none, then it will wait for the minimum number to respond indefinitely.\n\n        Returns:\n            (tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None): The first value is the\n                loss, which is ignored since we pack loss from the global and local models into the metrics dictionary\n                The second is the aggregated metrics passed from the clients, the third is the set of raw results and\n                failure objects returned by the clients.\n        \"\"\"\n        # Get clients and their respective instructions from client manager\n        client_instructions = self.configure_evaluate()\n\n        if not client_instructions:\n            log(INFO, \"Federated Evaluation: no clients selected, cancel\")\n            return None\n        log(\n            INFO,\n            f\"Federated Evaluation: Client manager sampled {len(client_instructions)} \"\n            f\"clients (out of {self._client_manager.num_available()})\",\n        )\n\n        # Collect `evaluate` results from all clients participating in this round\n        results, failures = evaluate_clients(\n            client_instructions,\n            max_workers=self.max_workers,\n            timeout=timeout,\n            group_id=0,\n        )\n        log(\n            INFO,\n            f\"Federated Evaluation received {len(results)} results and {len(failures)} failures\",\n        )\n\n        # Aggregate the evaluation results, note that we assume that the losses have been packed and aggregated with\n        # the metrics. A dummy loss is returned by each of the clients. We therefore return none for the aggregated\n        # loss\n        aggregated_result: tuple[\n            float | None,\n            dict[str, Scalar],\n        ] = self.aggregate_evaluate(results, failures)\n\n        _, metrics_aggregated = aggregated_result\n        return None, metrics_aggregated, (results, failures)\n\n    def configure_evaluate(self) -&gt; list[tuple[ClientProxy, EvaluateIns]]:\n        \"\"\"\n        Configure the next round of evaluation. This handles the two different was that a set of clients might be\n        sampled.\n\n        Returns:\n            (list[tuple[ClientProxy, EvaluateIns]]): List of configuration instructions for the clients selected by the\n                client manager for evaluation. These configuration objects are sent to the clients to customize\n                evaluation.\n        \"\"\"\n        # Do not configure federated evaluation if fraction eval is 0.\n        if self.fraction_evaluate == 0.0:\n            return []\n\n        # Parameters and config\n        config = {}\n        if self.evaluate_config is not None:\n            # Custom evaluation config function provided\n            config = self.evaluate_config\n        evaluate_ins = EvaluateIns(self.parameters, config)\n\n        # Sample clients\n        if isinstance(self._client_manager, BaseFractionSamplingManager):\n            clients = self._client_manager.sample_fraction(self.fraction_evaluate, self.min_available_clients)\n        else:\n            sample_size = int(self._client_manager.num_available() * self.fraction_evaluate)\n            clients = self._client_manager.sample(num_clients=sample_size, min_num_clients=self.min_available_clients)\n\n        # Return client/config pairs\n        return [(client, evaluate_ins) for client in clients]\n\n    def aggregate_evaluate(\n        self,\n        results: list[tuple[ClientProxy, EvaluateRes]],\n        failures: list[tuple[ClientProxy, EvaluateRes] | BaseException],\n    ) -&gt; tuple[float | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate evaluation results using the ``evaluate_metrics_aggregation_fn`` provided. Note that a dummy loss is\n        returned as we assume that it was packed into the metrics dictionary for this functionality.\n\n        Args:\n            results (list[tuple[ClientProxy, EvaluateRes]]): List of results objects that have the metrics returned\n                from each client, if successful, along with the number of samples used in the evaluation.\n            failures (list[tuple[ClientProxy, EvaluateRes] | BaseException]): Failures reported by the clients\n                along with the client id, the results that we passed, if any, and the associated exception if one was\n                raised.\n\n        Returns:\n            (tuple[float | None, dict[str, Scalar]]): A dummy float for the \"loss\" (these are packed with the metrics)\n                and the aggregated metrics dictionary.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.evaluate_metrics_aggregation_fn:\n            eval_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.evaluate_metrics_aggregation_fn(eval_metrics)\n        else:\n            log(WARNING, \"No evaluate_metrics_aggregation_fn provided\")\n\n        # Losses contained in results are dummy values for federated evaluation. It is assume that the client losses\n        # are packed, and therefore aggregated, in the metrics dictionary.\n        return None, metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.servers.evaluate_server.EvaluateServer.__init__","title":"<code>__init__(client_manager, fraction_evaluate, model_checkpoint_path=None, evaluate_config=None, evaluate_metrics_aggregation_fn=None, accept_failures=True, min_available_clients=1, reporters=None)</code>","text":"<p>Server meant to facilitate federated evaluation only (that is, no training).</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during evaluation.</p> required <code>model_checkpoint_path</code> <code>Path | None</code> <p>Server side model checkpoint path to load global model from. Defaults to None.</p> <code>None</code> <code>evaluate_config</code> <code>dict[str, Scalar] | None</code> <p>Configuration dictionary to configure evaluation on clients. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function.  Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of total clients in the system. Defaults to 1.</p> <code>1</code> <code>reporters</code> <code>Sequence[BaseReporter]</code> <p>A sequence of FL4Health reporters which the client should send data to.</p> <code>None</code> Source code in <code>fl4health/servers/evaluate_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fraction_evaluate: float,\n    model_checkpoint_path: Path | None = None,\n    evaluate_config: dict[str, Scalar] | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    accept_failures: bool = True,\n    min_available_clients: int = 1,\n    reporters: Sequence[BaseReporter] | None = None,\n) -&gt; None:\n    \"\"\"\n    Server meant to facilitate federated evaluation only (that is, no training).\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fraction_evaluate (float): Fraction of clients used during evaluation.\n        model_checkpoint_path (Path | None, optional): Server side model checkpoint path to load global model\n            from. Defaults to None.\n        evaluate_config (dict[str, Scalar] | None, optional): Configuration dictionary to configure evaluation\n            on clients. Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional):  Metrics aggregation function.\n             Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 1.\n        reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the client should\n            send data to.\n    \"\"\"\n    # We aren't aggregating model weights, so setting the strategy to be none.\n    super().__init__(client_manager=client_manager, strategy=None)\n    self.model_checkpoint_path = model_checkpoint_path\n    # Load model parameters if checkpoint provided, otherwise leave as empty params\n    if model_checkpoint_path:\n        self.parameters = self.load_model_checkpoint_to_parameters()\n    self.fraction_evaluate = fraction_evaluate\n    self.evaluate_config = evaluate_config\n    self.min_available_clients = min_available_clients\n    self.accept_failures = accept_failures\n    self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n    if self.fraction_evaluate &lt; 1.0:\n        log(\n            INFO,\n            f\"Fraction Evaluate is {self.fraction_evaluate}. Thus, some clients may not participate in evaluation\",\n        )\n    self.server_name = generate_hash()\n    self.reporters = [] if reporters is None else list(reporters)\n    for r in self.reporters:\n        r.initialize(id=self.server_name)\n</code></pre>"},{"location":"api/#fl4health.servers.evaluate_server.EvaluateServer.fit","title":"<code>fit(num_rounds, timeout)</code>","text":"<p>In order to head off training and only run eval, we have to override the fit function as this is essentially the entry point for federated learning from the app.</p> <p>Parameters:</p> Name Type Description Default <code>num_rounds</code> <code>int</code> <p>Not used.</p> required <code>timeout</code> <code>float | None</code> <p>Timeout in seconds that the server should wait for the clients to respond. If none, then it will wait for the minimum number to respond indefinitely.</p> required <p>Returns:</p> Type Description <code>tuple[History, float]</code> <p>The first element of the tuple is a <code>History</code> object containing the aggregated metrics returned from the clients. Tuple also contains elapsed time in seconds for round.</p> Source code in <code>fl4health/servers/evaluate_server.py</code> <pre><code>def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n    \"\"\"\n    In order to head off training and only run eval, we have to override the fit function as this is\n    essentially the entry point for federated learning from the app.\n\n    Args:\n        num_rounds (int): Not used.\n        timeout (float | None): Timeout in seconds that the server should wait for the clients to respond.\n            If none, then it will wait for the minimum number to respond indefinitely.\n\n    Returns:\n        (tuple[History, float]): The first element of the tuple is a ``History`` object containing the aggregated\n            metrics returned from the clients. Tuple also contains elapsed time in seconds for round.\n    \"\"\"\n    history = History()\n\n    # Run Federated Evaluation\n    log(INFO, \"Federated Evaluation Starting\")\n    start_time = datetime.datetime.now()\n\n    for reporter in self.reporters:\n        reporter.report(\n            {\n                \"fit_start\": str(start_time),\n                \"host_type\": \"server\",\n            }\n        )\n    # We're only performing federated evaluation. So we make use of the evaluate round function, but simply\n    # perform such evaluation once.\n    res_fed = self.federated_evaluate(timeout=timeout)\n    end_time = datetime.datetime.now()\n\n    for r in self.reporters:\n        r.report(\n            {\n                \"fit_elapsed_time\": str(start_time - end_time),\n                \"fit_end\": str(end_time),\n                \"num_rounds\": num_rounds,\n                \"host_type\": \"server\",\n            }\n        )\n    if res_fed:\n        _, evaluate_metrics_fed, _ = res_fed\n        if evaluate_metrics_fed:\n            history.add_metrics_distributed(server_round=0, metrics=evaluate_metrics_fed)\n            if evaluate_metrics_fed:\n                for r in self.reporters:\n                    r.report({\"fit_metrics\": evaluate_metrics_fed})\n\n    # Bookkeeping\n    elapsed = end_time - start_time\n    log(INFO, \"Federated Evaluation Finished in %s\", str(elapsed))\n    return history, elapsed.total_seconds()\n</code></pre>"},{"location":"api/#fl4health.servers.evaluate_server.EvaluateServer.federated_evaluate","title":"<code>federated_evaluate(timeout)</code>","text":"<p>Validate current global model on a number of clients.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float | None</code> <p>Timeout in seconds that the server should wait for the clients to response. If none, then it will wait for the minimum number to respond indefinitely.</p> required <p>Returns:</p> Type Description <code>tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None</code> <p>The first value is the loss, which is ignored since we pack loss from the global and local models into the metrics dictionary The second is the aggregated metrics passed from the clients, the third is the set of raw results and failure objects returned by the clients.</p> Source code in <code>fl4health/servers/evaluate_server.py</code> <pre><code>def federated_evaluate(\n    self,\n    timeout: float | None,\n) -&gt; tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None:\n    \"\"\"\n    Validate current global model on a number of clients.\n\n    Args:\n        timeout (float | None): Timeout in seconds that the server should wait for the clients to response.\n            If none, then it will wait for the minimum number to respond indefinitely.\n\n    Returns:\n        (tuple[float | None, dict[str, Scalar], EvaluateResultsAndFailures] | None): The first value is the\n            loss, which is ignored since we pack loss from the global and local models into the metrics dictionary\n            The second is the aggregated metrics passed from the clients, the third is the set of raw results and\n            failure objects returned by the clients.\n    \"\"\"\n    # Get clients and their respective instructions from client manager\n    client_instructions = self.configure_evaluate()\n\n    if not client_instructions:\n        log(INFO, \"Federated Evaluation: no clients selected, cancel\")\n        return None\n    log(\n        INFO,\n        f\"Federated Evaluation: Client manager sampled {len(client_instructions)} \"\n        f\"clients (out of {self._client_manager.num_available()})\",\n    )\n\n    # Collect `evaluate` results from all clients participating in this round\n    results, failures = evaluate_clients(\n        client_instructions,\n        max_workers=self.max_workers,\n        timeout=timeout,\n        group_id=0,\n    )\n    log(\n        INFO,\n        f\"Federated Evaluation received {len(results)} results and {len(failures)} failures\",\n    )\n\n    # Aggregate the evaluation results, note that we assume that the losses have been packed and aggregated with\n    # the metrics. A dummy loss is returned by each of the clients. We therefore return none for the aggregated\n    # loss\n    aggregated_result: tuple[\n        float | None,\n        dict[str, Scalar],\n    ] = self.aggregate_evaluate(results, failures)\n\n    _, metrics_aggregated = aggregated_result\n    return None, metrics_aggregated, (results, failures)\n</code></pre>"},{"location":"api/#fl4health.servers.evaluate_server.EvaluateServer.configure_evaluate","title":"<code>configure_evaluate()</code>","text":"<p>Configure the next round of evaluation. This handles the two different was that a set of clients might be sampled.</p> <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, EvaluateIns]]</code> <p>List of configuration instructions for the clients selected by the client manager for evaluation. These configuration objects are sent to the clients to customize evaluation.</p> Source code in <code>fl4health/servers/evaluate_server.py</code> <pre><code>def configure_evaluate(self) -&gt; list[tuple[ClientProxy, EvaluateIns]]:\n    \"\"\"\n    Configure the next round of evaluation. This handles the two different was that a set of clients might be\n    sampled.\n\n    Returns:\n        (list[tuple[ClientProxy, EvaluateIns]]): List of configuration instructions for the clients selected by the\n            client manager for evaluation. These configuration objects are sent to the clients to customize\n            evaluation.\n    \"\"\"\n    # Do not configure federated evaluation if fraction eval is 0.\n    if self.fraction_evaluate == 0.0:\n        return []\n\n    # Parameters and config\n    config = {}\n    if self.evaluate_config is not None:\n        # Custom evaluation config function provided\n        config = self.evaluate_config\n    evaluate_ins = EvaluateIns(self.parameters, config)\n\n    # Sample clients\n    if isinstance(self._client_manager, BaseFractionSamplingManager):\n        clients = self._client_manager.sample_fraction(self.fraction_evaluate, self.min_available_clients)\n    else:\n        sample_size = int(self._client_manager.num_available() * self.fraction_evaluate)\n        clients = self._client_manager.sample(num_clients=sample_size, min_num_clients=self.min_available_clients)\n\n    # Return client/config pairs\n    return [(client, evaluate_ins) for client in clients]\n</code></pre>"},{"location":"api/#fl4health.servers.evaluate_server.EvaluateServer.aggregate_evaluate","title":"<code>aggregate_evaluate(results, failures)</code>","text":"<p>Aggregate evaluation results using the <code>evaluate_metrics_aggregation_fn</code> provided. Note that a dummy loss is returned as we assume that it was packed into the metrics dictionary for this functionality.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[ClientProxy, EvaluateRes]]</code> <p>List of results objects that have the metrics returned from each client, if successful, along with the number of samples used in the evaluation.</p> required <code>failures</code> <code>list[tuple[ClientProxy, EvaluateRes] | BaseException]</code> <p>Failures reported by the clients along with the client id, the results that we passed, if any, and the associated exception if one was raised.</p> required <p>Returns:</p> Type Description <code>tuple[float | None, dict[str, Scalar]]</code> <p>A dummy float for the \"loss\" (these are packed with the metrics) and the aggregated metrics dictionary.</p> Source code in <code>fl4health/servers/evaluate_server.py</code> <pre><code>def aggregate_evaluate(\n    self,\n    results: list[tuple[ClientProxy, EvaluateRes]],\n    failures: list[tuple[ClientProxy, EvaluateRes] | BaseException],\n) -&gt; tuple[float | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate evaluation results using the ``evaluate_metrics_aggregation_fn`` provided. Note that a dummy loss is\n    returned as we assume that it was packed into the metrics dictionary for this functionality.\n\n    Args:\n        results (list[tuple[ClientProxy, EvaluateRes]]): List of results objects that have the metrics returned\n            from each client, if successful, along with the number of samples used in the evaluation.\n        failures (list[tuple[ClientProxy, EvaluateRes] | BaseException]): Failures reported by the clients\n            along with the client id, the results that we passed, if any, and the associated exception if one was\n            raised.\n\n    Returns:\n        (tuple[float | None, dict[str, Scalar]]): A dummy float for the \"loss\" (these are packed with the metrics)\n            and the aggregated metrics dictionary.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.evaluate_metrics_aggregation_fn:\n        eval_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.evaluate_metrics_aggregation_fn(eval_metrics)\n    else:\n        log(WARNING, \"No evaluate_metrics_aggregation_fn provided\")\n\n    # Losses contained in results are dummy values for federated evaluation. It is assume that the client losses\n    # are packed, and therefore aggregated, in the metrics dictionary.\n    return None, metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.servers.fedpm_server","title":"<code>fedpm_server</code>","text":""},{"location":"api/#fl4health.servers.fedpm_server.FedPmServer","title":"<code>FedPmServer</code>","text":"<p>               Bases: <code>FlServer</code></p> Source code in <code>fl4health/servers/fedpm_server.py</code> <pre><code>class FedPmServer(FlServer):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fl_config: Config,\n        strategy: FedPm,\n        reporters: Sequence[BaseReporter] | None = None,\n        checkpoint_and_state_module: LayerNamesServerCheckpointAndStateModule | None = None,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n        reset_frequency: int = 1,\n    ) -&gt; None:\n        \"\"\"\n        Custom FL Server for the FedPM algorithm to allow for resetting the beta priors in Bayesian aggregation,\n        as specified in http://arxiv.org/pdf/2209.15328.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fl_config (Config): This should be the configuration that was used to setup the federated training.\n                In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n                example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n                strategy.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            strategy (FedPm): The aggregation strategy to be used by the server to handle client updates and other\n                information potentially sent by the participating clients. This strategy must be of FedPm type.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the server\n                should send data to before and after each round.\n            checkpoint_and_state_module (LayerNamesServerCheckpointAndStateModule | None, optional): This module is\n                used to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n                artifacts to be used or evaluated after training. The latter is used to preserve training state\n                (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n                configure how one asks a client to provide parameters from which to initialize all other clients by\n                providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n                request (which is default behavior for flower servers). Defaults to None.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n            reset_frequency (int, optional): Determines the frequency with which the beta priors are reset.\n                Defaults to 1.\n        \"\"\"\n        if checkpoint_and_state_module is not None:\n            assert isinstance(\n                checkpoint_and_state_module,\n                LayerNamesServerCheckpointAndStateModule,\n            ), \"checkpoint_and_state_module must have type LayerNamesServerCheckpointAndStateModule\"\n        super().__init__(\n            client_manager=client_manager,\n            fl_config=fl_config,\n            strategy=strategy,\n            reporters=reporters,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n        self.reset_frequency = reset_frequency\n\n    def fit_round(\n        self,\n        server_round: int,\n        timeout: float | None,\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar], FitResultsAndFailures] | None:\n        assert isinstance(self.strategy, FedPm)\n        # If self.reset_frequency == x, then the beta priors are reset every x fitting rounds.\n        # Note that (server_round + 1) % self.reset_frequency == 0 is to ensure that the priors\n        # are not reset in the second round when self.reset_frequency is 2.\n        if server_round &gt; 1 and (server_round + 1) % self.reset_frequency == 0:\n            self.strategy.reset_beta_priors()\n        return super().fit_round(server_round, timeout)\n</code></pre>"},{"location":"api/#fl4health.servers.fedpm_server.FedPmServer.__init__","title":"<code>__init__(client_manager, fl_config, strategy, reporters=None, checkpoint_and_state_module=None, on_init_parameters_config_fn=None, server_name=None, accept_failures=True, reset_frequency=1)</code>","text":"<p>Custom FL Server for the FedPM algorithm to allow for resetting the beta priors in Bayesian aggregation, as specified in http://arxiv.org/pdf/2209.15328.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fl_config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated training. In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For example, the config used to produce the <code>on_fit_config_fn</code> and <code>on_evaluate_config_fn</code> for the strategy.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>strategy</code> <code>FedPm</code> <p>The aggregation strategy to be used by the server to handle client updates and other information potentially sent by the participating clients. This strategy must be of FedPm type.</p> required <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the server should send data to before and after each round.</p> <code>None</code> <code>checkpoint_and_state_module</code> <code>LayerNamesServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <code>None</code> <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. If this is none, then a blank config is sent with the parameter request (which is default behavior for flower servers). Defaults to None.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> <code>reset_frequency</code> <code>int</code> <p>Determines the frequency with which the beta priors are reset. Defaults to 1.</p> <code>1</code> Source code in <code>fl4health/servers/fedpm_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fl_config: Config,\n    strategy: FedPm,\n    reporters: Sequence[BaseReporter] | None = None,\n    checkpoint_and_state_module: LayerNamesServerCheckpointAndStateModule | None = None,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n    reset_frequency: int = 1,\n) -&gt; None:\n    \"\"\"\n    Custom FL Server for the FedPM algorithm to allow for resetting the beta priors in Bayesian aggregation,\n    as specified in http://arxiv.org/pdf/2209.15328.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fl_config (Config): This should be the configuration that was used to setup the federated training.\n            In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n            example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n            strategy.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        strategy (FedPm): The aggregation strategy to be used by the server to handle client updates and other\n            information potentially sent by the participating clients. This strategy must be of FedPm type.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the server\n            should send data to before and after each round.\n        checkpoint_and_state_module (LayerNamesServerCheckpointAndStateModule | None, optional): This module is\n            used to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n            artifacts to be used or evaluated after training. The latter is used to preserve training state\n            (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n            configure how one asks a client to provide parameters from which to initialize all other clients by\n            providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n            request (which is default behavior for flower servers). Defaults to None.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n        reset_frequency (int, optional): Determines the frequency with which the beta priors are reset.\n            Defaults to 1.\n    \"\"\"\n    if checkpoint_and_state_module is not None:\n        assert isinstance(\n            checkpoint_and_state_module,\n            LayerNamesServerCheckpointAndStateModule,\n        ), \"checkpoint_and_state_module must have type LayerNamesServerCheckpointAndStateModule\"\n    super().__init__(\n        client_manager=client_manager,\n        fl_config=fl_config,\n        strategy=strategy,\n        reporters=reporters,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n    self.reset_frequency = reset_frequency\n</code></pre>"},{"location":"api/#fl4health.servers.instance_level_dp_server","title":"<code>instance_level_dp_server</code>","text":""},{"location":"api/#fl4health.servers.instance_level_dp_server.InstanceLevelDpServer","title":"<code>InstanceLevelDpServer</code>","text":"<p>               Bases: <code>FlServer</code></p> Source code in <code>fl4health/servers/instance_level_dp_server.py</code> <pre><code>class InstanceLevelDpServer(FlServer):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fl_config: Config,\n        noise_multiplier: float,\n        batch_size: int,\n        num_server_rounds: int,\n        strategy: BasicFedAvg,\n        local_epochs: int | None = None,\n        local_steps: int | None = None,\n        checkpoint_and_state_module: OpacusServerCheckpointAndStateModule | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        delta: float | None = None,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Server to be used in case of Instance Level Differential Privacy with Federated Averaging.\n        Modified the fit function to poll clients for sample counts prior to the first round of FL.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fl_config (Config): This should be the configuration that was used to setup the federated training.\n                In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n                example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n                strategy.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            noise_multiplier (float): The amount of Gaussian noise to be added to the per sample gradient during\n                DP-SGD.\n            batch_size (int): The batch size to be used in training on the client-side. Used in privacy accounting.\n            num_server_rounds (int): The number of server rounds to be done in FL training. Used in privacy accounting\n            strategy (BasicFedAvg): The aggregation strategy to be used by the server to handle\n                client updates and other information potentially sent by the participating clients. this must be an\n                ``OpacusBasicFedAvg`` strategy to ensure proper treatment of the model in the Opacus framework\n            local_epochs (int | None, optional): Number of local epochs to be performed on the client-side. This is\n                used in privacy accounting. One of ``local_epochs`` or ``local_steps`` should be defined, but not both.\n                Defaults to None.\n            local_steps (int | None, optional): Number of local steps to be performed on the client-side. This is\n                used in privacy accounting. One of ``local_epochs`` or ``local_steps`` should be defined, but not both.\n                Defaults to None.\n            checkpoint_and_state_module (OpacusServerCheckpointAndStateModule | None, optional): This module is used\n                to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n                artifacts to be used or evaluated after training. The latter is used to preserve training state\n                (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to.\n            delta (float | None, optional): The delta value for epsilon-delta DP accounting. If None it defaults to\n                being ``1/total_samples`` in the FL run. Defaults to None.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n                configure how one asks a client to provide parameters from which to initialize all other clients by\n                providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n                request (which is default behavior for flower servers). Defaults to None.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n        \"\"\"\n        if checkpoint_and_state_module is not None:\n            assert isinstance(\n                checkpoint_and_state_module,\n                OpacusServerCheckpointAndStateModule,\n            ), \"checkpoint_and_state_module must have type OpacusServerCheckpointAndStateModule\"\n        super().__init__(\n            client_manager=client_manager,\n            fl_config=fl_config,\n            strategy=strategy,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            reporters=reporters,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n\n        # Ensure that one of local_epochs and local_steps is passed (and not both)\n        assert isinstance(local_epochs, int) ^ isinstance(local_steps, int)\n        self.accountant: FlInstanceLevelAccountant\n        self.local_steps = local_steps\n        self.local_epochs = local_epochs\n\n        # Whether or not we have to convert local_steps to local_epochs\n        self.convert_steps_to_epochs = self.local_epochs is None\n        self.noise_multiplier = noise_multiplier\n        self.batch_size = batch_size\n        self.num_server_rounds = num_server_rounds\n        self.delta = delta\n\n    def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n        \"\"\"\n        Run federated averaging for a number of rounds.\n\n        Args:\n            num_rounds (int): Number of server rounds to run.\n            timeout (float | None): The amount of time in seconds that the server will wait for results from the\n                clients selected to participate in federated training.\n\n        Returns:\n            (tuple[History, float]): The first element of the tuple is a ``History`` object containing the full set of\n                FL training results, including things like aggregated loss and metrics. Tuple also includes elapsed\n                time in seconds for round.\n        \"\"\"\n        assert isinstance(self.strategy, StrategyWithPolling)\n        sample_counts = self.poll_clients_for_sample_counts(timeout)\n        self.setup_privacy_accountant(sample_counts)\n\n        return super().fit(num_rounds=num_rounds, timeout=timeout)\n\n    def setup_privacy_accountant(self, sample_counts: list[int]) -&gt; None:\n        \"\"\"\n        Sets up FL Accountant and computes privacy loss based on class attributes and retrieved sample counts.\n\n        Args:\n            sample_counts (list[int]): These should be the total number of training examples fetched from all clients\n                during the sample polling process.\n        \"\"\"\n        # Ensures that we're using a fraction sampler of the\n        assert isinstance(self._client_manager, PoissonSamplingClientManager)\n\n        total_samples = sum(sample_counts)\n\n        if self.convert_steps_to_epochs:\n            # Compute the ceiling of number of local epochs per clients and take max as local epochs\n            # Ensures we do not underestimate the privacy loss\n            assert isinstance(self.local_steps, int)\n\n            epochs_per_client = [ceil(self.local_steps * self.batch_size / count) for count in sample_counts]\n            self.local_epochs = max(epochs_per_client)\n\n        assert isinstance(self.local_epochs, int)\n        assert isinstance(self.strategy, BasicFedAvg)\n\n        self.accountant = FlInstanceLevelAccountant(\n            client_sampling_rate=self.strategy.fraction_fit,\n            noise_multiplier=self.noise_multiplier,\n            epochs_per_round=self.local_epochs,\n            client_batch_sizes=[self.batch_size] * len(sample_counts),\n            client_dataset_sizes=sample_counts,\n        )\n\n        target_delta = 1.0 / total_samples if self.delta is None else self.delta\n        epsilon = self.accountant.get_epsilon(self.num_server_rounds, target_delta)\n        log(\n            INFO,\n            f\"Model privacy after full training will be ({epsilon}, {target_delta})\",\n        )\n</code></pre>"},{"location":"api/#fl4health.servers.instance_level_dp_server.InstanceLevelDpServer.__init__","title":"<code>__init__(client_manager, fl_config, noise_multiplier, batch_size, num_server_rounds, strategy, local_epochs=None, local_steps=None, checkpoint_and_state_module=None, reporters=None, delta=None, on_init_parameters_config_fn=None, server_name=None, accept_failures=True)</code>","text":"<p>Server to be used in case of Instance Level Differential Privacy with Federated Averaging. Modified the fit function to poll clients for sample counts prior to the first round of FL.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fl_config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated training. In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For example, the config used to produce the <code>on_fit_config_fn</code> and <code>on_evaluate_config_fn</code> for the strategy.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>noise_multiplier</code> <code>float</code> <p>The amount of Gaussian noise to be added to the per sample gradient during DP-SGD.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to be used in training on the client-side. Used in privacy accounting.</p> required <code>num_server_rounds</code> <code>int</code> <p>The number of server rounds to be done in FL training. Used in privacy accounting</p> required <code>strategy</code> <code>BasicFedAvg</code> <p>The aggregation strategy to be used by the server to handle client updates and other information potentially sent by the participating clients. this must be an <code>OpacusBasicFedAvg</code> strategy to ensure proper treatment of the model in the Opacus framework</p> required <code>local_epochs</code> <code>int | None</code> <p>Number of local epochs to be performed on the client-side. This is used in privacy accounting. One of <code>local_epochs</code> or <code>local_steps</code> should be defined, but not both. Defaults to None.</p> <code>None</code> <code>local_steps</code> <code>int | None</code> <p>Number of local steps to be performed on the client-side. This is used in privacy accounting. One of <code>local_epochs</code> or <code>local_steps</code> should be defined, but not both. Defaults to None.</p> <code>None</code> <code>checkpoint_and_state_module</code> <code>OpacusServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to.</p> <code>None</code> <code>delta</code> <code>float | None</code> <p>The delta value for epsilon-delta DP accounting. If None it defaults to being <code>1/total_samples</code> in the FL run. Defaults to None.</p> <code>None</code> <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. If this is none, then a blank config is sent with the parameter request (which is default behavior for flower servers). Defaults to None.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/servers/instance_level_dp_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fl_config: Config,\n    noise_multiplier: float,\n    batch_size: int,\n    num_server_rounds: int,\n    strategy: BasicFedAvg,\n    local_epochs: int | None = None,\n    local_steps: int | None = None,\n    checkpoint_and_state_module: OpacusServerCheckpointAndStateModule | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    delta: float | None = None,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n) -&gt; None:\n    \"\"\"\n    Server to be used in case of Instance Level Differential Privacy with Federated Averaging.\n    Modified the fit function to poll clients for sample counts prior to the first round of FL.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fl_config (Config): This should be the configuration that was used to setup the federated training.\n            In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n            example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n            strategy.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        noise_multiplier (float): The amount of Gaussian noise to be added to the per sample gradient during\n            DP-SGD.\n        batch_size (int): The batch size to be used in training on the client-side. Used in privacy accounting.\n        num_server_rounds (int): The number of server rounds to be done in FL training. Used in privacy accounting\n        strategy (BasicFedAvg): The aggregation strategy to be used by the server to handle\n            client updates and other information potentially sent by the participating clients. this must be an\n            ``OpacusBasicFedAvg`` strategy to ensure proper treatment of the model in the Opacus framework\n        local_epochs (int | None, optional): Number of local epochs to be performed on the client-side. This is\n            used in privacy accounting. One of ``local_epochs`` or ``local_steps`` should be defined, but not both.\n            Defaults to None.\n        local_steps (int | None, optional): Number of local steps to be performed on the client-side. This is\n            used in privacy accounting. One of ``local_epochs`` or ``local_steps`` should be defined, but not both.\n            Defaults to None.\n        checkpoint_and_state_module (OpacusServerCheckpointAndStateModule | None, optional): This module is used\n            to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n            artifacts to be used or evaluated after training. The latter is used to preserve training state\n            (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to.\n        delta (float | None, optional): The delta value for epsilon-delta DP accounting. If None it defaults to\n            being ``1/total_samples`` in the FL run. Defaults to None.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n            configure how one asks a client to provide parameters from which to initialize all other clients by\n            providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n            request (which is default behavior for flower servers). Defaults to None.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n    \"\"\"\n    if checkpoint_and_state_module is not None:\n        assert isinstance(\n            checkpoint_and_state_module,\n            OpacusServerCheckpointAndStateModule,\n        ), \"checkpoint_and_state_module must have type OpacusServerCheckpointAndStateModule\"\n    super().__init__(\n        client_manager=client_manager,\n        fl_config=fl_config,\n        strategy=strategy,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        reporters=reporters,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n\n    # Ensure that one of local_epochs and local_steps is passed (and not both)\n    assert isinstance(local_epochs, int) ^ isinstance(local_steps, int)\n    self.accountant: FlInstanceLevelAccountant\n    self.local_steps = local_steps\n    self.local_epochs = local_epochs\n\n    # Whether or not we have to convert local_steps to local_epochs\n    self.convert_steps_to_epochs = self.local_epochs is None\n    self.noise_multiplier = noise_multiplier\n    self.batch_size = batch_size\n    self.num_server_rounds = num_server_rounds\n    self.delta = delta\n</code></pre>"},{"location":"api/#fl4health.servers.instance_level_dp_server.InstanceLevelDpServer.fit","title":"<code>fit(num_rounds, timeout)</code>","text":"<p>Run federated averaging for a number of rounds.</p> <p>Parameters:</p> Name Type Description Default <code>num_rounds</code> <code>int</code> <p>Number of server rounds to run.</p> required <code>timeout</code> <code>float | None</code> <p>The amount of time in seconds that the server will wait for results from the clients selected to participate in federated training.</p> required <p>Returns:</p> Type Description <code>tuple[History, float]</code> <p>The first element of the tuple is a <code>History</code> object containing the full set of FL training results, including things like aggregated loss and metrics. Tuple also includes elapsed time in seconds for round.</p> Source code in <code>fl4health/servers/instance_level_dp_server.py</code> <pre><code>def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n    \"\"\"\n    Run federated averaging for a number of rounds.\n\n    Args:\n        num_rounds (int): Number of server rounds to run.\n        timeout (float | None): The amount of time in seconds that the server will wait for results from the\n            clients selected to participate in federated training.\n\n    Returns:\n        (tuple[History, float]): The first element of the tuple is a ``History`` object containing the full set of\n            FL training results, including things like aggregated loss and metrics. Tuple also includes elapsed\n            time in seconds for round.\n    \"\"\"\n    assert isinstance(self.strategy, StrategyWithPolling)\n    sample_counts = self.poll_clients_for_sample_counts(timeout)\n    self.setup_privacy_accountant(sample_counts)\n\n    return super().fit(num_rounds=num_rounds, timeout=timeout)\n</code></pre>"},{"location":"api/#fl4health.servers.instance_level_dp_server.InstanceLevelDpServer.setup_privacy_accountant","title":"<code>setup_privacy_accountant(sample_counts)</code>","text":"<p>Sets up FL Accountant and computes privacy loss based on class attributes and retrieved sample counts.</p> <p>Parameters:</p> Name Type Description Default <code>sample_counts</code> <code>list[int]</code> <p>These should be the total number of training examples fetched from all clients during the sample polling process.</p> required Source code in <code>fl4health/servers/instance_level_dp_server.py</code> <pre><code>def setup_privacy_accountant(self, sample_counts: list[int]) -&gt; None:\n    \"\"\"\n    Sets up FL Accountant and computes privacy loss based on class attributes and retrieved sample counts.\n\n    Args:\n        sample_counts (list[int]): These should be the total number of training examples fetched from all clients\n            during the sample polling process.\n    \"\"\"\n    # Ensures that we're using a fraction sampler of the\n    assert isinstance(self._client_manager, PoissonSamplingClientManager)\n\n    total_samples = sum(sample_counts)\n\n    if self.convert_steps_to_epochs:\n        # Compute the ceiling of number of local epochs per clients and take max as local epochs\n        # Ensures we do not underestimate the privacy loss\n        assert isinstance(self.local_steps, int)\n\n        epochs_per_client = [ceil(self.local_steps * self.batch_size / count) for count in sample_counts]\n        self.local_epochs = max(epochs_per_client)\n\n    assert isinstance(self.local_epochs, int)\n    assert isinstance(self.strategy, BasicFedAvg)\n\n    self.accountant = FlInstanceLevelAccountant(\n        client_sampling_rate=self.strategy.fraction_fit,\n        noise_multiplier=self.noise_multiplier,\n        epochs_per_round=self.local_epochs,\n        client_batch_sizes=[self.batch_size] * len(sample_counts),\n        client_dataset_sizes=sample_counts,\n    )\n\n    target_delta = 1.0 / total_samples if self.delta is None else self.delta\n    epsilon = self.accountant.get_epsilon(self.num_server_rounds, target_delta)\n    log(\n        INFO,\n        f\"Model privacy after full training will be ({epsilon}, {target_delta})\",\n    )\n</code></pre>"},{"location":"api/#fl4health.servers.model_merge_server","title":"<code>model_merge_server</code>","text":""},{"location":"api/#fl4health.servers.model_merge_server.ModelMergeServer","title":"<code>ModelMergeServer</code>","text":"<p>               Bases: <code>Server</code></p> Source code in <code>fl4health/servers/model_merge_server.py</code> <pre><code>class ModelMergeServer(Server):\n    def __init__(\n        self,\n        *,\n        client_manager: ClientManager,\n        strategy: Strategy | None = None,\n        server_model: nn.Module | None = None,\n        checkpointer: LatestTorchModuleCheckpointer | None = None,\n        parameter_exchanger: ParameterExchanger | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        server_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"\n        ``ModelMergeServer`` provides functionality to fetch client weights, perform a simple average, redistribute to\n        clients for evaluation. Optionally can perform server side evaluation as well.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            strategy (Strategy | None, optional): The aggregation strategy to be used by the server to handle\n                client updates sent by the participating clients. Must be ``ModelMergeStrategy``.\n            checkpointer (LatestTorchCheckpointer | None, optional): To be provided if the server should perform\n                server side checkpointing on the merged model. If none, then no server-side checkpointing is\n                performed. Defaults to None.\n            server_model (nn.Module | None): Optional model to be hydrated with parameters from model merge if doing\n                server side checkpointing. Must only be provided if checkpointer is also provided. Defaults to None.\n            parameter_exchanger (ExchangerType | None, optional): A parameter exchanger used to facilitate\n                server-side model checkpointing if a checkpointer has been defined. If not provided then checkpointing\n                will not be done unless the ``_hydrate_model_for_checkpointing`` function is overridden. Because the\n                server only sees numpy arrays, the parameter exchanger is used to insert the numpy arrays into a\n                provided model. Defaults to None.\n            reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the server should\n                send data to before and after each round.\n            server_name (str | None): An optional string name to uniquely identify server.\n        \"\"\"\n        assert isinstance(strategy, ModelMergeStrategy)\n        assert (server_model is None and checkpointer is None and parameter_exchanger is None) or (\n            server_model is not None and checkpointer is not None and parameter_exchanger is not None\n        )\n        super().__init__(client_manager=client_manager, strategy=strategy)\n\n        self.checkpointer = checkpointer\n        self.server_model = server_model\n        self.parameter_exchanger = parameter_exchanger\n        self.server_name = server_name if server_name is not None else generate_hash()\n\n        # Initialize reporters with server name information.\n        self.reports_manager = ReportsManager(reporters)\n        self.reports_manager.initialize(id=self.server_name)\n\n    def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n        \"\"\"\n        Performs a fit round in which the local client weights are evaluated on their test set, uploaded to the server\n        and averaged, then redistributed to clients for evaluation. Optionally, can perform evaluation of the merged\n        model on the server side as well.\n\n        Args:\n            num_rounds (int): Not used.\n            timeout (float | None): Timeout in seconds that the server should wait for the clients to respond.\n                If none, then it will wait for the minimum number to respond indefinitely.\n\n        Returns:\n            (tuple[History, float]): The first element of the tuple is a ``History`` object containing the aggregated\n                metrics returned from the clients. Tuple also contains elapsed time in seconds for round.\n        \"\"\"\n        self.reports_manager.report({\"host_type\": \"server\", \"fit_start\": datetime.datetime.now()})\n\n        history = History()\n\n        # Run Federated Model Merging\n        log(INFO, \"Federated Model Merging Starting\")\n        start_time = timeit.default_timer()\n\n        res_fit = self.fit_round(\n            server_round=1,\n            timeout=timeout,\n        )\n\n        if res_fit is not None:\n            parameters_prime, fit_metrics, _ = res_fit  # fit_metrics_aggregated\n            if parameters_prime:\n                self.parameters = parameters_prime\n            history.add_metrics_distributed_fit(server_round=1, metrics=fit_metrics)\n        else:\n            log(WARNING, \"Federated Model Merging Failed\")\n\n        res_fed = self.evaluate_round(server_round=1, timeout=timeout)\n        if res_fed is not None:\n            # ignore loss as one is not defined in model merging\n            _, evaluate_metrics_fed, _ = res_fed\n            if evaluate_metrics_fed is not None:\n                history.add_metrics_distributed(server_round=1, metrics=evaluate_metrics_fed)\n\n        # Evaluate model using strategy implementation\n        res_cen = self.strategy.evaluate(1, parameters=self.parameters)\n        if res_cen is not None:\n            # ignore loss as one is not defined in model merging\n            _, metrics_cen = res_cen\n            history.add_metrics_centralized(server_round=1, metrics=metrics_cen)\n\n        # Checkpoint based on dummy loss aggregated and metrics aggregated since\n        # we are using LatestTorchCheckpointer and will always checkpoint if\n        # server_model, parameter_exchanger and checkpointer are not None\n        self._maybe_checkpoint(loss_aggregated=0.0, metrics_aggregated={}, server_round=1)\n\n        self.reports_manager.report(\n            data={\n                \"fit_end\": datetime.datetime.now(),\n                \"metrics_centralized\": history.metrics_centralized,\n                \"losses_centralized\": history.losses_centralized,\n                \"host_type\": \"server\",\n            }\n        )\n\n        # Bookkeeping\n        end_time = timeit.default_timer()\n        elapsed = end_time - start_time\n        log(INFO, \"Federated Model Merging Finished in %s\", elapsed)\n        return history, elapsed\n\n    def _hydrate_model_for_checkpointing(self) -&gt; nn.Module:\n        \"\"\"\n        Method used for converting server parameters into a torch model that can be checkpointed.\n\n        Returns:\n            (nn.Module): Torch model to be checkpointed by a torch checkpointer.\n        \"\"\"\n        assert self.server_model is not None, (\n            \"Model hydration has been called but no server_model is defined to hydrate. The functionality of \"\n            \"_hydrate_model_for_checkpointing can be overridden if checkpointing without a torch architecture is \"\n            \"possible and desired\"\n        )\n        assert self.parameter_exchanger is not None, (\n            \"Model hydration has been called but no parameter_exchanger is defined to hydrate. The functionality of \"\n            \"_hydrate_model_for_checkpointing can be overridden if checkpointing without a parameter exchanger is \"\n            \"possible and desired\"\n        )\n        model_ndarrays = parameters_to_ndarrays(self.parameters)\n        self.parameter_exchanger.pull_parameters(model_ndarrays, self.server_model)\n        return self.server_model\n\n    def _maybe_checkpoint(\n        self, loss_aggregated: float, metrics_aggregated: dict[str, Scalar], server_round: int\n    ) -&gt; None:\n        \"\"\"\n        Method to checkpoint merged model on server side if the checkpointer, ``server_model`` and\n        ``parameter_exchanger`` provided at initialization are all not None.\n\n        Args:\n            loss_aggregated (float): Not used.\n            metrics_aggregated (dict[str, Scalar]): Not used.\n            server_round (int): Not used.\n        \"\"\"\n        if self.checkpointer and self.server_model and self.parameter_exchanger:\n            model = self._hydrate_model_for_checkpointing()\n            self.checkpointer.maybe_checkpoint(model, loss_aggregated, metrics_aggregated)\n        else:\n            attribute_dict = {\n                \"checkpointer\": self.checkpointer,\n                \"server_model\": self.server_model,\n                \"parameter_exchanger\": self.parameter_exchanger,\n            }\n\n            error_str = \" and \".join([key for key, val in attribute_dict.items() if val is None])\n            log(\n                WARNING,\n                f\"\"\"All of checkpointer, server_model and parameter_exchanger must be None to\n                perform server-side checkpointing. {error_str} is None\"\"\",\n            )\n</code></pre>"},{"location":"api/#fl4health.servers.model_merge_server.ModelMergeServer.__init__","title":"<code>__init__(*, client_manager, strategy=None, server_model=None, checkpointer=None, parameter_exchanger=None, reporters=None, server_name=None)</code>","text":"<p><code>ModelMergeServer</code> provides functionality to fetch client weights, perform a simple average, redistribute to clients for evaluation. Optionally can perform server side evaluation as well.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>strategy</code> <code>Strategy | None</code> <p>The aggregation strategy to be used by the server to handle client updates sent by the participating clients. Must be <code>ModelMergeStrategy</code>.</p> <code>None</code> <code>checkpointer</code> <code>LatestTorchCheckpointer | None</code> <p>To be provided if the server should perform server side checkpointing on the merged model. If none, then no server-side checkpointing is performed. Defaults to None.</p> <code>None</code> <code>server_model</code> <code>Module | None</code> <p>Optional model to be hydrated with parameters from model merge if doing server side checkpointing. Must only be provided if checkpointer is also provided. Defaults to None.</p> <code>None</code> <code>parameter_exchanger</code> <code>ExchangerType | None</code> <p>A parameter exchanger used to facilitate server-side model checkpointing if a checkpointer has been defined. If not provided then checkpointing will not be done unless the <code>_hydrate_model_for_checkpointing</code> function is overridden. Because the server only sees numpy arrays, the parameter exchanger is used to insert the numpy arrays into a provided model. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter]</code> <p>A sequence of FL4Health reporters which the server should send data to before and after each round.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server.</p> <code>None</code> Source code in <code>fl4health/servers/model_merge_server.py</code> <pre><code>def __init__(\n    self,\n    *,\n    client_manager: ClientManager,\n    strategy: Strategy | None = None,\n    server_model: nn.Module | None = None,\n    checkpointer: LatestTorchModuleCheckpointer | None = None,\n    parameter_exchanger: ParameterExchanger | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    server_name: str | None = None,\n) -&gt; None:\n    \"\"\"\n    ``ModelMergeServer`` provides functionality to fetch client weights, perform a simple average, redistribute to\n    clients for evaluation. Optionally can perform server side evaluation as well.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        strategy (Strategy | None, optional): The aggregation strategy to be used by the server to handle\n            client updates sent by the participating clients. Must be ``ModelMergeStrategy``.\n        checkpointer (LatestTorchCheckpointer | None, optional): To be provided if the server should perform\n            server side checkpointing on the merged model. If none, then no server-side checkpointing is\n            performed. Defaults to None.\n        server_model (nn.Module | None): Optional model to be hydrated with parameters from model merge if doing\n            server side checkpointing. Must only be provided if checkpointer is also provided. Defaults to None.\n        parameter_exchanger (ExchangerType | None, optional): A parameter exchanger used to facilitate\n            server-side model checkpointing if a checkpointer has been defined. If not provided then checkpointing\n            will not be done unless the ``_hydrate_model_for_checkpointing`` function is overridden. Because the\n            server only sees numpy arrays, the parameter exchanger is used to insert the numpy arrays into a\n            provided model. Defaults to None.\n        reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the server should\n            send data to before and after each round.\n        server_name (str | None): An optional string name to uniquely identify server.\n    \"\"\"\n    assert isinstance(strategy, ModelMergeStrategy)\n    assert (server_model is None and checkpointer is None and parameter_exchanger is None) or (\n        server_model is not None and checkpointer is not None and parameter_exchanger is not None\n    )\n    super().__init__(client_manager=client_manager, strategy=strategy)\n\n    self.checkpointer = checkpointer\n    self.server_model = server_model\n    self.parameter_exchanger = parameter_exchanger\n    self.server_name = server_name if server_name is not None else generate_hash()\n\n    # Initialize reporters with server name information.\n    self.reports_manager = ReportsManager(reporters)\n    self.reports_manager.initialize(id=self.server_name)\n</code></pre>"},{"location":"api/#fl4health.servers.model_merge_server.ModelMergeServer.fit","title":"<code>fit(num_rounds, timeout)</code>","text":"<p>Performs a fit round in which the local client weights are evaluated on their test set, uploaded to the server and averaged, then redistributed to clients for evaluation. Optionally, can perform evaluation of the merged model on the server side as well.</p> <p>Parameters:</p> Name Type Description Default <code>num_rounds</code> <code>int</code> <p>Not used.</p> required <code>timeout</code> <code>float | None</code> <p>Timeout in seconds that the server should wait for the clients to respond. If none, then it will wait for the minimum number to respond indefinitely.</p> required <p>Returns:</p> Type Description <code>tuple[History, float]</code> <p>The first element of the tuple is a <code>History</code> object containing the aggregated metrics returned from the clients. Tuple also contains elapsed time in seconds for round.</p> Source code in <code>fl4health/servers/model_merge_server.py</code> <pre><code>def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n    \"\"\"\n    Performs a fit round in which the local client weights are evaluated on their test set, uploaded to the server\n    and averaged, then redistributed to clients for evaluation. Optionally, can perform evaluation of the merged\n    model on the server side as well.\n\n    Args:\n        num_rounds (int): Not used.\n        timeout (float | None): Timeout in seconds that the server should wait for the clients to respond.\n            If none, then it will wait for the minimum number to respond indefinitely.\n\n    Returns:\n        (tuple[History, float]): The first element of the tuple is a ``History`` object containing the aggregated\n            metrics returned from the clients. Tuple also contains elapsed time in seconds for round.\n    \"\"\"\n    self.reports_manager.report({\"host_type\": \"server\", \"fit_start\": datetime.datetime.now()})\n\n    history = History()\n\n    # Run Federated Model Merging\n    log(INFO, \"Federated Model Merging Starting\")\n    start_time = timeit.default_timer()\n\n    res_fit = self.fit_round(\n        server_round=1,\n        timeout=timeout,\n    )\n\n    if res_fit is not None:\n        parameters_prime, fit_metrics, _ = res_fit  # fit_metrics_aggregated\n        if parameters_prime:\n            self.parameters = parameters_prime\n        history.add_metrics_distributed_fit(server_round=1, metrics=fit_metrics)\n    else:\n        log(WARNING, \"Federated Model Merging Failed\")\n\n    res_fed = self.evaluate_round(server_round=1, timeout=timeout)\n    if res_fed is not None:\n        # ignore loss as one is not defined in model merging\n        _, evaluate_metrics_fed, _ = res_fed\n        if evaluate_metrics_fed is not None:\n            history.add_metrics_distributed(server_round=1, metrics=evaluate_metrics_fed)\n\n    # Evaluate model using strategy implementation\n    res_cen = self.strategy.evaluate(1, parameters=self.parameters)\n    if res_cen is not None:\n        # ignore loss as one is not defined in model merging\n        _, metrics_cen = res_cen\n        history.add_metrics_centralized(server_round=1, metrics=metrics_cen)\n\n    # Checkpoint based on dummy loss aggregated and metrics aggregated since\n    # we are using LatestTorchCheckpointer and will always checkpoint if\n    # server_model, parameter_exchanger and checkpointer are not None\n    self._maybe_checkpoint(loss_aggregated=0.0, metrics_aggregated={}, server_round=1)\n\n    self.reports_manager.report(\n        data={\n            \"fit_end\": datetime.datetime.now(),\n            \"metrics_centralized\": history.metrics_centralized,\n            \"losses_centralized\": history.losses_centralized,\n            \"host_type\": \"server\",\n        }\n    )\n\n    # Bookkeeping\n    end_time = timeit.default_timer()\n    elapsed = end_time - start_time\n    log(INFO, \"Federated Model Merging Finished in %s\", elapsed)\n    return history, elapsed\n</code></pre>"},{"location":"api/#fl4health.servers.nnunet_server","title":"<code>nnunet_server</code>","text":""},{"location":"api/#fl4health.servers.nnunet_server.NnunetServer","title":"<code>NnunetServer</code>","text":"<p>               Bases: <code>FlServer</code></p> Source code in <code>fl4health/servers/nnunet_server.py</code> <pre><code>class NnunetServer(FlServer):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fl_config: Config,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]],\n        strategy: Strategy | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        checkpoint_and_state_module: NnUnetServerCheckpointAndStateModule | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n        nnunet_trainer_class: type[nnUNetTrainer] = nnUNetTrainer,\n        global_deep_supervision: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        A Basic ``FlServer`` with added functionality to ask a client to initialize the global nnunet plans if one was\n        not provided in the config. Intended for use with ``NnUNetClient``.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fl_config (Config): This should be the configuration that was used to setup the federated training.\n                In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n                example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n                strategy.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]]): Function used to configure how one\n                asks a client to provide parameters from which to initialize all other clients by providing a\n                ``Config`` dictionary. For ``NnunetServers`` this is a required function to provide the additional\n                information necessary to a client for parameter initialization\n            strategy (Strategy | None, optional): The aggregation strategy to be used by the server to handle\n                client updates and other information potentially sent by the participating clients. If None the\n                strategy is FedAvg as set by the flwr Server. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n                should send data to. Defaults to None.\n            checkpoint_and_state_module (NnUnetServerCheckpointAndStateModule | None, optional): This module is used\n                to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n                artifacts to be used or evaluated after training. The latter is used to preserve training state\n                (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n\n                **NOTE**: For NnUnet, this module is allowed to have all components defined other than the model, as it\n                may be set later when the server asks the clients to provide the architecture.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n            nnunet_trainer_class (type[nnUNetTrainer]): ``nnUNetTrainer`` class.\n                Useful for passing custom ``nnUNetTrainer``. Defaults to the standard ``nnUNetTrainer`` class.\n                Must match the ``nnunet_trainer_class`` passed to the ``NnunetClient``.\n            global_deep_supervision (bool): Whether or not the global model should use deep supervision. Does\n                not affect the model architecture just the output during inference. This argument applies only to the\n                global model, not local client models. Defaults to False.\n        \"\"\"\n        if checkpoint_and_state_module is not None:\n            assert isinstance(\n                checkpoint_and_state_module,\n                NnUnetServerCheckpointAndStateModule,\n            ), \"checkpoint_and_state_module must have type NnUnetServerCheckpointAndStateModule\"\n        super().__init__(\n            client_manager=client_manager,\n            fl_config=fl_config,\n            strategy=strategy,\n            reporters=reporters,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n        self.nnunet_trainer_class = nnunet_trainer_class\n        self.global_deep_supervision = global_deep_supervision\n        self.nnunet_config = NnunetConfig(self.fl_config[\"nnunet_config\"])\n\n        self.nnunet_plans_bytes: bytes\n        self.num_input_channels: int\n        self.num_segmentation_heads: int\n\n    def initialize_server_model(self) -&gt; None:\n        \"\"\"Initializes the global server model so that it can be checkpointed.\"\"\"\n        # Ensure required attributes are set\n        assert (\n            self.nnunet_plans_bytes is not None\n            and self.num_input_channels is not None\n            and self.num_segmentation_heads is not None\n            and self.nnunet_config is not None\n        )\n\n        plans = pickle.loads(self.nnunet_plans_bytes)\n        plans_manager = PlansManager(plans)\n        configuration_manager = plans_manager.get_configuration(self.nnunet_config.value)\n        model = self.nnunet_trainer_class.build_network_architecture(\n            configuration_manager.network_arch_class_name,\n            configuration_manager.network_arch_init_kwargs,\n            configuration_manager.network_arch_init_kwargs_req_import,\n            self.num_input_channels,\n            self.num_segmentation_heads,\n            self.global_deep_supervision,\n        )\n        self.checkpoint_and_state_module.model = model\n\n    def update_before_fit(self, num_rounds: int, timeout: float | None) -&gt; None:\n        \"\"\"\n        Hook method to allow the server to do some additional initialization prior to fitting.\n\n        ``NnunetServer`` uses this method to sample a client for properties for one of two reasons\n\n        1. If a global ``nnunet_plans`` file is not provided in the config, this method will request that a random\n           client which generate a plans file from it local dataset and return it to the server through the\n           ``get_properties`` RPC. The server then distributes the ``nnunet_plans`` to the other clients by including\n           it in the config for subsequent FL rounds.\n\n           AND/OR\n\n        2. If server side state or model checkpointing is being used, then server will  poll a client in order to have\n           the required properties to instantiate the model architecture on the server side. These properties include\n           ``num_segmentation_heads`` and ``num_input_channels``, essentially the number of input and output channels\n           (which are not specified in nnunet plans for some reason).\n\n        Args:\n            num_rounds (int): The number of server rounds of FL to be performed.\n            timeout (float | None, optional): The server's timeout parameter. Useful if one is requesting\n                information from a client. Defaults to None, which indicates indefinite timeout.\n        \"\"\"\n        # Check if nnunet_plans specified config returned by configure_fit\n        dummy_params = Parameters([], \"None\")\n        config = self.strategy.configure_fit(0, dummy_params, self._client_manager)[0][1].config\n        plans_bytes = config.get(\"nnunet_plans\")\n\n        # Check for checkpointers\n        checkpointer_exists = (\n            self.checkpoint_and_state_module.state_checkpointer is not None\n            or self.checkpoint_and_state_module.model_checkpointers is not None\n        )\n\n        if checkpointer_exists or plans_bytes is None:\n            log(INFO, \"\")\n            log(INFO, \"[PRE-INIT]\")\n            log(INFO, \"Requesting properties from one random client via get_properties\")\n\n            # 1) If nnUnet plans are unspecified, we ask a client to generate the global plans using its local dataset\n            if plans_bytes is None:\n                log(INFO, \"\\tThis client will be asked to initialize the global nnunet plans\")\n\n            # 2) If the checkpointer is not None, then we want to do checkpointing. Therefore we need to\n            #   be able to construct the model and for that we need the number of input and output channels.\n            if checkpointer_exists:\n                log(\n                    INFO,\n                    \"\\tThis client's local dataset will be used to determine the number of input and output channels\",\n                )\n\n            # Sample a random client and request properties\n            random_client = self._client_manager.sample(1)[0]\n            ins = GetPropertiesIns(config=config)\n            properties_res = random_client.get_properties(ins=ins, timeout=timeout, group_id=0)\n\n            if properties_res.status.code == Code.OK:\n                log(INFO, \"Received properties from one random client\")\n            else:\n                raise Exception(\"Failed to successfully receive properties from client\")\n            properties = properties_res.properties\n\n            # Set self.nnunet_plans_bytes\n            if plans_bytes is None:\n                self.nnunet_plans_bytes = narrow_dict_type(properties, \"nnunet_plans\", bytes)\n            else:\n                assert isinstance(plans_bytes, bytes)\n                self.nnunet_plans_bytes = plans_bytes\n\n            # Save number of input and output channels as attributes\n            self.num_segmentation_heads = narrow_dict_type(properties, \"num_segmentation_heads\", int)\n            self.num_input_channels = narrow_dict_type(properties, \"num_input_channels\", int)\n\n            # Initialize global model\n            if checkpointer_exists:\n                self.initialize_server_model()\n\n            # If the state_checkpointer has been specified and a state checkpoint exists, the state\n            # will be loaded when executing ``fit_with_per_round_checkpointing`` of the base_server.\n            # NOTE: Inherent assumption that if checkpoint exists for server that it also will exist for client.\n\n            # Wrap config functions so that we are sure the nnunet_plans are included\n            new_fit_cfg_fn = add_items_to_config_fn(\n                self.strategy.configure_fit, {\"nnunet_plans\": self.nnunet_plans_bytes}\n            )\n            new_eval_cfg_fn = add_items_to_config_fn(\n                self.strategy.configure_evaluate, {\"nnunet_plans\": self.nnunet_plans_bytes}\n            )\n            self.strategy.configure_fit = new_fit_cfg_fn  # type: ignore\n            self.strategy.configure_evaluate = new_eval_cfg_fn  # type: ignore\n\n        # Finish\n        log(INFO, \"\")\n\n    def _save_server_state(self) -&gt; None:\n        \"\"\"\n        Save server checkpoint consisting of model, history, server round, metrics reporter and server name. This\n        method overrides parent to also `checkpoint` ``nnunet_plans``, ``num_input_channels``,\n        ``num_segmentation_heads`` and ``global_deep_supervision``.\n        \"\"\"\n        assert (\n            self.nnunet_plans_bytes is not None\n            and self.num_input_channels is not None\n            and self.num_segmentation_heads is not None\n            and self.global_deep_supervision is not None\n            and self.nnunet_config is not None\n        )\n\n        super()._save_server_state()\n</code></pre>"},{"location":"api/#fl4health.servers.nnunet_server.NnunetServer.__init__","title":"<code>__init__(client_manager, fl_config, on_init_parameters_config_fn, strategy=None, reporters=None, checkpoint_and_state_module=None, server_name=None, accept_failures=True, nnunet_trainer_class=nnUNetTrainer, global_deep_supervision=False)</code>","text":"<p>A Basic <code>FlServer</code> with added functionality to ask a client to initialize the global nnunet plans if one was not provided in the config. Intended for use with <code>NnUNetClient</code>.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fl_config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated training. In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For example, the config used to produce the <code>on_fit_config_fn</code> and <code>on_evaluate_config_fn</code> for the strategy.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]]</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. For <code>NnunetServers</code> this is a required function to provide the additional information necessary to a client for parameter initialization</p> required <code>strategy</code> <code>Strategy | None</code> <p>The aggregation strategy to be used by the server to handle client updates and other information potentially sent by the participating clients. If None the strategy is FedAvg as set by the flwr Server. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>A sequence of FL4Health reporters which the client should send data to. Defaults to None.</p> <code>None</code> <code>checkpoint_and_state_module</code> <code>NnUnetServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <p>NOTE: For NnUnet, this module is allowed to have all components defined other than the model, as it may be set later when the server asks the clients to provide the architecture.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> <code>nnunet_trainer_class</code> <code>type[nnUNetTrainer]</code> <p><code>nnUNetTrainer</code> class. Useful for passing custom <code>nnUNetTrainer</code>. Defaults to the standard <code>nnUNetTrainer</code> class. Must match the <code>nnunet_trainer_class</code> passed to the <code>NnunetClient</code>.</p> <code>nnUNetTrainer</code> <code>global_deep_supervision</code> <code>bool</code> <p>Whether or not the global model should use deep supervision. Does not affect the model architecture just the output during inference. This argument applies only to the global model, not local client models. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/servers/nnunet_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fl_config: Config,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]],\n    strategy: Strategy | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    checkpoint_and_state_module: NnUnetServerCheckpointAndStateModule | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n    nnunet_trainer_class: type[nnUNetTrainer] = nnUNetTrainer,\n    global_deep_supervision: bool = False,\n) -&gt; None:\n    \"\"\"\n    A Basic ``FlServer`` with added functionality to ask a client to initialize the global nnunet plans if one was\n    not provided in the config. Intended for use with ``NnUNetClient``.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fl_config (Config): This should be the configuration that was used to setup the federated training.\n            In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n            example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n            strategy.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]]): Function used to configure how one\n            asks a client to provide parameters from which to initialize all other clients by providing a\n            ``Config`` dictionary. For ``NnunetServers`` this is a required function to provide the additional\n            information necessary to a client for parameter initialization\n        strategy (Strategy | None, optional): The aggregation strategy to be used by the server to handle\n            client updates and other information potentially sent by the participating clients. If None the\n            strategy is FedAvg as set by the flwr Server. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): A sequence of FL4Health reporters which the client\n            should send data to. Defaults to None.\n        checkpoint_and_state_module (NnUnetServerCheckpointAndStateModule | None, optional): This module is used\n            to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n            artifacts to be used or evaluated after training. The latter is used to preserve training state\n            (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n\n            **NOTE**: For NnUnet, this module is allowed to have all components defined other than the model, as it\n            may be set later when the server asks the clients to provide the architecture.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n        nnunet_trainer_class (type[nnUNetTrainer]): ``nnUNetTrainer`` class.\n            Useful for passing custom ``nnUNetTrainer``. Defaults to the standard ``nnUNetTrainer`` class.\n            Must match the ``nnunet_trainer_class`` passed to the ``NnunetClient``.\n        global_deep_supervision (bool): Whether or not the global model should use deep supervision. Does\n            not affect the model architecture just the output during inference. This argument applies only to the\n            global model, not local client models. Defaults to False.\n    \"\"\"\n    if checkpoint_and_state_module is not None:\n        assert isinstance(\n            checkpoint_and_state_module,\n            NnUnetServerCheckpointAndStateModule,\n        ), \"checkpoint_and_state_module must have type NnUnetServerCheckpointAndStateModule\"\n    super().__init__(\n        client_manager=client_manager,\n        fl_config=fl_config,\n        strategy=strategy,\n        reporters=reporters,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n    self.nnunet_trainer_class = nnunet_trainer_class\n    self.global_deep_supervision = global_deep_supervision\n    self.nnunet_config = NnunetConfig(self.fl_config[\"nnunet_config\"])\n\n    self.nnunet_plans_bytes: bytes\n    self.num_input_channels: int\n    self.num_segmentation_heads: int\n</code></pre>"},{"location":"api/#fl4health.servers.nnunet_server.NnunetServer.initialize_server_model","title":"<code>initialize_server_model()</code>","text":"<p>Initializes the global server model so that it can be checkpointed.</p> Source code in <code>fl4health/servers/nnunet_server.py</code> <pre><code>def initialize_server_model(self) -&gt; None:\n    \"\"\"Initializes the global server model so that it can be checkpointed.\"\"\"\n    # Ensure required attributes are set\n    assert (\n        self.nnunet_plans_bytes is not None\n        and self.num_input_channels is not None\n        and self.num_segmentation_heads is not None\n        and self.nnunet_config is not None\n    )\n\n    plans = pickle.loads(self.nnunet_plans_bytes)\n    plans_manager = PlansManager(plans)\n    configuration_manager = plans_manager.get_configuration(self.nnunet_config.value)\n    model = self.nnunet_trainer_class.build_network_architecture(\n        configuration_manager.network_arch_class_name,\n        configuration_manager.network_arch_init_kwargs,\n        configuration_manager.network_arch_init_kwargs_req_import,\n        self.num_input_channels,\n        self.num_segmentation_heads,\n        self.global_deep_supervision,\n    )\n    self.checkpoint_and_state_module.model = model\n</code></pre>"},{"location":"api/#fl4health.servers.nnunet_server.NnunetServer.update_before_fit","title":"<code>update_before_fit(num_rounds, timeout)</code>","text":"<p>Hook method to allow the server to do some additional initialization prior to fitting.</p> <p><code>NnunetServer</code> uses this method to sample a client for properties for one of two reasons</p> <ol> <li>If a global <code>nnunet_plans</code> file is not provided in the config, this method will request that a random    client which generate a plans file from it local dataset and return it to the server through the    <code>get_properties</code> RPC. The server then distributes the <code>nnunet_plans</code> to the other clients by including    it in the config for subsequent FL rounds.</li> </ol> <p>AND/OR</p> <ol> <li>If server side state or model checkpointing is being used, then server will  poll a client in order to have    the required properties to instantiate the model architecture on the server side. These properties include    <code>num_segmentation_heads</code> and <code>num_input_channels</code>, essentially the number of input and output channels    (which are not specified in nnunet plans for some reason).</li> </ol> <p>Parameters:</p> Name Type Description Default <code>num_rounds</code> <code>int</code> <p>The number of server rounds of FL to be performed.</p> required <code>timeout</code> <code>float | None</code> <p>The server's timeout parameter. Useful if one is requesting information from a client. Defaults to None, which indicates indefinite timeout.</p> required Source code in <code>fl4health/servers/nnunet_server.py</code> <pre><code>def update_before_fit(self, num_rounds: int, timeout: float | None) -&gt; None:\n    \"\"\"\n    Hook method to allow the server to do some additional initialization prior to fitting.\n\n    ``NnunetServer`` uses this method to sample a client for properties for one of two reasons\n\n    1. If a global ``nnunet_plans`` file is not provided in the config, this method will request that a random\n       client which generate a plans file from it local dataset and return it to the server through the\n       ``get_properties`` RPC. The server then distributes the ``nnunet_plans`` to the other clients by including\n       it in the config for subsequent FL rounds.\n\n       AND/OR\n\n    2. If server side state or model checkpointing is being used, then server will  poll a client in order to have\n       the required properties to instantiate the model architecture on the server side. These properties include\n       ``num_segmentation_heads`` and ``num_input_channels``, essentially the number of input and output channels\n       (which are not specified in nnunet plans for some reason).\n\n    Args:\n        num_rounds (int): The number of server rounds of FL to be performed.\n        timeout (float | None, optional): The server's timeout parameter. Useful if one is requesting\n            information from a client. Defaults to None, which indicates indefinite timeout.\n    \"\"\"\n    # Check if nnunet_plans specified config returned by configure_fit\n    dummy_params = Parameters([], \"None\")\n    config = self.strategy.configure_fit(0, dummy_params, self._client_manager)[0][1].config\n    plans_bytes = config.get(\"nnunet_plans\")\n\n    # Check for checkpointers\n    checkpointer_exists = (\n        self.checkpoint_and_state_module.state_checkpointer is not None\n        or self.checkpoint_and_state_module.model_checkpointers is not None\n    )\n\n    if checkpointer_exists or plans_bytes is None:\n        log(INFO, \"\")\n        log(INFO, \"[PRE-INIT]\")\n        log(INFO, \"Requesting properties from one random client via get_properties\")\n\n        # 1) If nnUnet plans are unspecified, we ask a client to generate the global plans using its local dataset\n        if plans_bytes is None:\n            log(INFO, \"\\tThis client will be asked to initialize the global nnunet plans\")\n\n        # 2) If the checkpointer is not None, then we want to do checkpointing. Therefore we need to\n        #   be able to construct the model and for that we need the number of input and output channels.\n        if checkpointer_exists:\n            log(\n                INFO,\n                \"\\tThis client's local dataset will be used to determine the number of input and output channels\",\n            )\n\n        # Sample a random client and request properties\n        random_client = self._client_manager.sample(1)[0]\n        ins = GetPropertiesIns(config=config)\n        properties_res = random_client.get_properties(ins=ins, timeout=timeout, group_id=0)\n\n        if properties_res.status.code == Code.OK:\n            log(INFO, \"Received properties from one random client\")\n        else:\n            raise Exception(\"Failed to successfully receive properties from client\")\n        properties = properties_res.properties\n\n        # Set self.nnunet_plans_bytes\n        if plans_bytes is None:\n            self.nnunet_plans_bytes = narrow_dict_type(properties, \"nnunet_plans\", bytes)\n        else:\n            assert isinstance(plans_bytes, bytes)\n            self.nnunet_plans_bytes = plans_bytes\n\n        # Save number of input and output channels as attributes\n        self.num_segmentation_heads = narrow_dict_type(properties, \"num_segmentation_heads\", int)\n        self.num_input_channels = narrow_dict_type(properties, \"num_input_channels\", int)\n\n        # Initialize global model\n        if checkpointer_exists:\n            self.initialize_server_model()\n\n        # If the state_checkpointer has been specified and a state checkpoint exists, the state\n        # will be loaded when executing ``fit_with_per_round_checkpointing`` of the base_server.\n        # NOTE: Inherent assumption that if checkpoint exists for server that it also will exist for client.\n\n        # Wrap config functions so that we are sure the nnunet_plans are included\n        new_fit_cfg_fn = add_items_to_config_fn(\n            self.strategy.configure_fit, {\"nnunet_plans\": self.nnunet_plans_bytes}\n        )\n        new_eval_cfg_fn = add_items_to_config_fn(\n            self.strategy.configure_evaluate, {\"nnunet_plans\": self.nnunet_plans_bytes}\n        )\n        self.strategy.configure_fit = new_fit_cfg_fn  # type: ignore\n        self.strategy.configure_evaluate = new_eval_cfg_fn  # type: ignore\n\n    # Finish\n    log(INFO, \"\")\n</code></pre>"},{"location":"api/#fl4health.servers.nnunet_server.add_items_to_config_fn","title":"<code>add_items_to_config_fn(fn, items)</code>","text":"<p>Accepts a flwr Strategy configure function (either <code>configure_fit</code> or <code>configure_evaluate</code>) and returns a new function  that returns the same thing except the dictionary items in the items argument have been added to the config that  is returned by the original function.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>CFG_FN</code> <p>The Strategy configure function to wrap</p> required <code>items</code> <code>Config</code> <p>A <code>Config</code> containing additional items to update the original config with</p> required <p>Returns:</p> Type Description <code>CFG_FN</code> <p>The wrapped function. Argument and return type is the same</p> Source code in <code>fl4health/servers/nnunet_server.py</code> <pre><code>def add_items_to_config_fn(fn: CFG_FN, items: Config) -&gt; CFG_FN:\n    \"\"\"\n    Accepts a flwr Strategy configure function (either ``configure_fit`` or ``configure_evaluate``) and returns a new\n    function  that returns the same thing except the dictionary items in the items argument have been added to the\n    config that  is returned by the original function.\n\n    Args:\n        fn (CFG_FN): The Strategy configure function to wrap\n        items (Config): A ``Config`` containing additional items to update the original config with\n\n    Returns:\n        (CFG_FN): The wrapped function. Argument and return type is the same\n    \"\"\"\n\n    def new_fn(*args: Any, **kwargs: Any) -&gt; Any:\n        cfg_ins = fn(*args, **kwargs)\n        for _, ins in cfg_ins:\n            ins.config.update(items)\n        return cfg_ins\n\n    return new_fn\n</code></pre>"},{"location":"api/#fl4health.servers.polling","title":"<code>polling</code>","text":""},{"location":"api/#fl4health.servers.polling.poll_client","title":"<code>poll_client(client, ins)</code>","text":"<p>Get Properties of client. This is run for each client to extract the properties from the target client.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>ClientProxy</code> <p>Client proxy representing one of the clients managed by the server.</p> required <code>ins</code> <code>GetPropertiesIns</code> <p>ins provides any configurations required to help the client retrieve the correct properties.</p> required <p>Returns:</p> Type Description <code>tuple[ClientProxy, GetPropertiesRes]</code> <p>Returns the resulting properties from the client response.</p> Source code in <code>fl4health/servers/polling.py</code> <pre><code>def poll_client(client: ClientProxy, ins: GetPropertiesIns) -&gt; tuple[ClientProxy, GetPropertiesRes]:\n    \"\"\"\n    Get Properties of client. This is run for each client to extract the properties from the target client.\n\n    Args:\n        client (ClientProxy): Client proxy representing one of the clients managed by the server.\n        ins (GetPropertiesIns): ins provides any configurations required to help the client retrieve the correct\n            properties.\n\n    Returns:\n        (tuple[ClientProxy, GetPropertiesRes]): Returns the resulting properties from the client response.\n    \"\"\"\n    property_res: GetPropertiesRes = client.get_properties(ins=ins, timeout=None, group_id=None)\n    return client, property_res\n</code></pre>"},{"location":"api/#fl4health.servers.polling.poll_clients","title":"<code>poll_clients(client_instructions, max_workers, timeout)</code>","text":"<p>Poll clients concurrently on all selected clients.</p> <p>Parameters:</p> Name Type Description Default <code>client_instructions</code> <code>list[tuple[ClientProxy, GetPropertiesIns]]</code> <p>This is the set of instructions for the polling to be passed to each client. Each client is represented by a single <code>ClientProxy</code> in the list.</p> required <code>max_workers</code> <code>int | None</code> <p>This is the maximum number of concurrent workers to be used by the server to poll the clients. This should be set if pooling an extremely large number, if none a maximum of 32 workers are used.</p> required <code>timeout</code> <code>float | None</code> <p>How long the executor should wait to receive a response before moving on.</p> required <p>Returns:</p> Type Description <code>PollResultsAndFailures</code> <p>Object holding the results and failures associate with the concurrent polling.</p> Source code in <code>fl4health/servers/polling.py</code> <pre><code>def poll_clients(\n    client_instructions: list[tuple[ClientProxy, GetPropertiesIns]],\n    max_workers: int | None,\n    timeout: float | None,\n) -&gt; PollResultsAndFailures:\n    \"\"\"\n    Poll clients concurrently on all selected clients.\n\n    Args:\n        client_instructions (list[tuple[ClientProxy, GetPropertiesIns]]): This is the set of instructions for the\n            polling to be passed to each client. Each client is represented by a single ``ClientProxy`` in the list.\n        max_workers (int | None): This is the maximum number of concurrent workers to be used by the server to\n            poll the clients. This should be set if pooling an extremely large number, if none a maximum of 32 workers\n            are used.\n        timeout (float | None): How long the executor should wait to receive a response before moving on.\n\n    Returns:\n        (PollResultsAndFailures): Object holding the results and failures associate with the concurrent polling.\n    \"\"\"\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        submitted_fs = {\n            executor.submit(poll_client, client_proxy, property_ins)\n            for client_proxy, property_ins in client_instructions\n        }\n        finished_fs, _ = concurrent.futures.wait(\n            fs=submitted_fs,\n            timeout=None,  # Handled in the respective communication stack\n        )\n\n    # Gather results\n    results: list[tuple[ClientProxy, GetPropertiesRes]] = []\n    failures: list[tuple[ClientProxy, GetPropertiesRes] | BaseException] = []\n    for future in finished_fs:\n        _handle_finished_future_after_poll(future=future, results=results, failures=failures)\n\n    return results, failures\n</code></pre>"},{"location":"api/#fl4health.servers.scaffold_server","title":"<code>scaffold_server</code>","text":""},{"location":"api/#fl4health.servers.scaffold_server.ScaffoldServer","title":"<code>ScaffoldServer</code>","text":"<p>               Bases: <code>FlServer</code></p> Source code in <code>fl4health/servers/scaffold_server.py</code> <pre><code>class ScaffoldServer(FlServer):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fl_config: Config,\n        strategy: Scaffold,\n        reporters: Sequence[BaseReporter] | None = None,\n        checkpoint_and_state_module: ScaffoldServerCheckpointAndStateModule | None = None,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n        warm_start: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Custom FL Server for scaffold algorithm to handle warm initialization of control variates as specified in\n        https://arxiv.org/abs/1910.06378.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fl_config (Config): This should be the configuration that was used to setup the federated training.\n                In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n                example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n                strategy.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            strategy (Scaffold): The aggregation strategy to be used by the server to handle client updates and\n                other information potentially sent by the participating clients. This strategy must be of SCAFFOLD\n                type.\n            reporters (Sequence[BaseReporter] | None, optional): sequence of FL4Health reporters which the server\n                should send data to before and after each round. Defaults to None.\n            checkpoint_and_state_module (ScaffoldServerCheckpointAndStateModule | None, optional): This module is used\n                to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n                artifacts to be used or evaluated after training. The latter is used to preserve training state\n                (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n                configure how one asks a client to provide parameters from which to initialize all other clients by\n                providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n                request (which is default behavior for flower servers). Defaults to None.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n            warm_start (bool, optional):  Whether or not to initialize control variates of each client as local\n                gradients. The clients will perform a training pass (without updating the weights) in order to provide\n                a \"warm\" estimate of the SCAFFOLD control variates. If false, variates are initialized to 0.\n                Defaults to False.\n        \"\"\"\n        if checkpoint_and_state_module is not None:\n            assert isinstance(\n                checkpoint_and_state_module,\n                ScaffoldServerCheckpointAndStateModule,\n            ), \"checkpoint_and_state_module must have type ScaffoldServerCheckpointAndStateModule\"\n        FlServer.__init__(\n            self,\n            client_manager=client_manager,\n            fl_config=fl_config,\n            strategy=strategy,\n            reporters=reporters,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n        self.warm_start = warm_start\n\n    def _get_initial_parameters(self, server_round: int, timeout: float | None) -&gt; Parameters:\n        \"\"\"\n        Overrides the ``_get_initial_parameters`` in the flwr server base class to strap on the possibility of a\n        ``warm_start`` for SCAFFOLD. Initializes parameters (models weights and control variates) of the server.\n        If warm_start is True, control variates are initialized as the the average local client-side gradients\n        (while model weights remain unchanged). That is, all of the clients run a training pass, but the trained\n        weights are discarded.\n\n        Args:\n            server_round (int): The current server round.\n            timeout (float | None): If the server strategy object does not have a server-side initial parameters\n                function defined, then one of the clients is polled and their model parameters are returned in order to\n                initialize the models of all clients. Timeout defines how long to wait for a response.\n\n        Returns:\n            (Parameters): Initial parameters (model weights and control variates).\n        \"\"\"\n        assert isinstance(self.strategy, Scaffold)\n        # First run basic parameter initialization from the parent server\n        initial_parameters = super()._get_initial_parameters(server_round, timeout=timeout)\n\n        # If warm_start, run routine to initialize control variates without updating global model\n        # control variates are initialized as average local gradient over training steps\n        # while the model weights remain unchanged (until the FL rounds start)\n        if self.warm_start:\n            log(\n                INFO,\n                \"Using Warm Start Strategy. Waiting for clients to be available for polling\",\n            )\n            client_instructions = self.strategy.configure_fit_all(\n                server_round=0,\n                parameters=initial_parameters,\n                client_manager=self._client_manager,\n            )\n            if not client_instructions:\n                log(ERROR, \"Warm Start initialization failed: no clients selected\", 1)\n            else:\n                log(\n                    DEBUG,\n                    f\"Warm start: strategy sampled {len(client_instructions)} \\\n                    clients (out of {self._client_manager.num_available()})\",\n                )\n\n                results, failures = fit_clients(\n                    client_instructions,\n                    self.max_workers,\n                    timeout,\n                    group_id=server_round,\n                )\n\n                log(\n                    DEBUG,\n                    f\"Warm Start: received {len(results)} results and {len(failures)} failures\",\n                )\n\n                updated_params = [parameters_to_ndarrays(fit_res.parameters) for _, fit_res in results]\n                aggregated_params = self.strategy.aggregate(updated_params)\n\n                # drop the updated weights as the warm start strictly updates control variates\n                # and leaves model weights unchanged\n                _, control_variates_update = self.strategy.parameter_packer.unpack_parameters(aggregated_params)\n                server_control_variates = self.strategy.compute_updated_control_variates(control_variates_update)\n\n                # Get initial weights from original parameters\n                initial_weights, _ = self.strategy.parameter_packer.unpack_parameters(\n                    parameters_to_ndarrays(initial_parameters)\n                )\n\n                # Get new parameters by combining original weights with server control variates from warm start\n                initial_parameters = ndarrays_to_parameters(\n                    self.strategy.parameter_packer.pack_parameters(initial_weights, server_control_variates)\n                )\n\n        return initial_parameters\n\n    def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n        \"\"\"\n        Run the SCAFFOLD FL algorithm for a fixed number of rounds. This overrides the base server fit class just to\n        ensure that the provided strategy is a Scaffold strategy object before proceeding.\n\n        Args:\n            num_rounds (int): Number of rounds of FL to perform (i.e. server rounds).\n            timeout (float | None): Timeout associated with queries to the clients in seconds. The server waits for\n                timeout seconds before moving on without any unresponsive clients. If None, there is no timeout and the\n                server waits for the minimum number of clients to be available set in the strategy.\n\n        Returns:\n            (tuple[History, float]): The first element of the tuple is a ``History`` object containing the full set of\n                FL training results, including things like aggregated loss and metrics. Tuple also includes elapsed\n                time in seconds for round.\n        \"\"\"\n        assert isinstance(self.strategy, Scaffold)\n        return super().fit(num_rounds=num_rounds, timeout=timeout)\n</code></pre>"},{"location":"api/#fl4health.servers.scaffold_server.ScaffoldServer.__init__","title":"<code>__init__(client_manager, fl_config, strategy, reporters=None, checkpoint_and_state_module=None, on_init_parameters_config_fn=None, server_name=None, accept_failures=True, warm_start=False)</code>","text":"<p>Custom FL Server for scaffold algorithm to handle warm initialization of control variates as specified in https://arxiv.org/abs/1910.06378.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fl_config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated training. In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For example, the config used to produce the <code>on_fit_config_fn</code> and <code>on_evaluate_config_fn</code> for the strategy.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>strategy</code> <code>Scaffold</code> <p>The aggregation strategy to be used by the server to handle client updates and other information potentially sent by the participating clients. This strategy must be of SCAFFOLD type.</p> required <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>sequence of FL4Health reporters which the server should send data to before and after each round. Defaults to None.</p> <code>None</code> <code>checkpoint_and_state_module</code> <code>ScaffoldServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <code>None</code> <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. If this is none, then a blank config is sent with the parameter request (which is default behavior for flower servers). Defaults to None.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> <code>warm_start</code> <code>bool</code> <p>Whether or not to initialize control variates of each client as local gradients. The clients will perform a training pass (without updating the weights) in order to provide a \"warm\" estimate of the SCAFFOLD control variates. If false, variates are initialized to 0. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/servers/scaffold_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fl_config: Config,\n    strategy: Scaffold,\n    reporters: Sequence[BaseReporter] | None = None,\n    checkpoint_and_state_module: ScaffoldServerCheckpointAndStateModule | None = None,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n    warm_start: bool = False,\n) -&gt; None:\n    \"\"\"\n    Custom FL Server for scaffold algorithm to handle warm initialization of control variates as specified in\n    https://arxiv.org/abs/1910.06378.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fl_config (Config): This should be the configuration that was used to setup the federated training.\n            In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n            example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n            strategy.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        strategy (Scaffold): The aggregation strategy to be used by the server to handle client updates and\n            other information potentially sent by the participating clients. This strategy must be of SCAFFOLD\n            type.\n        reporters (Sequence[BaseReporter] | None, optional): sequence of FL4Health reporters which the server\n            should send data to before and after each round. Defaults to None.\n        checkpoint_and_state_module (ScaffoldServerCheckpointAndStateModule | None, optional): This module is used\n            to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n            artifacts to be used or evaluated after training. The latter is used to preserve training state\n            (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n            configure how one asks a client to provide parameters from which to initialize all other clients by\n            providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n            request (which is default behavior for flower servers). Defaults to None.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n        warm_start (bool, optional):  Whether or not to initialize control variates of each client as local\n            gradients. The clients will perform a training pass (without updating the weights) in order to provide\n            a \"warm\" estimate of the SCAFFOLD control variates. If false, variates are initialized to 0.\n            Defaults to False.\n    \"\"\"\n    if checkpoint_and_state_module is not None:\n        assert isinstance(\n            checkpoint_and_state_module,\n            ScaffoldServerCheckpointAndStateModule,\n        ), \"checkpoint_and_state_module must have type ScaffoldServerCheckpointAndStateModule\"\n    FlServer.__init__(\n        self,\n        client_manager=client_manager,\n        fl_config=fl_config,\n        strategy=strategy,\n        reporters=reporters,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n    self.warm_start = warm_start\n</code></pre>"},{"location":"api/#fl4health.servers.scaffold_server.ScaffoldServer.fit","title":"<code>fit(num_rounds, timeout)</code>","text":"<p>Run the SCAFFOLD FL algorithm for a fixed number of rounds. This overrides the base server fit class just to ensure that the provided strategy is a Scaffold strategy object before proceeding.</p> <p>Parameters:</p> Name Type Description Default <code>num_rounds</code> <code>int</code> <p>Number of rounds of FL to perform (i.e. server rounds).</p> required <code>timeout</code> <code>float | None</code> <p>Timeout associated with queries to the clients in seconds. The server waits for timeout seconds before moving on without any unresponsive clients. If None, there is no timeout and the server waits for the minimum number of clients to be available set in the strategy.</p> required <p>Returns:</p> Type Description <code>tuple[History, float]</code> <p>The first element of the tuple is a <code>History</code> object containing the full set of FL training results, including things like aggregated loss and metrics. Tuple also includes elapsed time in seconds for round.</p> Source code in <code>fl4health/servers/scaffold_server.py</code> <pre><code>def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n    \"\"\"\n    Run the SCAFFOLD FL algorithm for a fixed number of rounds. This overrides the base server fit class just to\n    ensure that the provided strategy is a Scaffold strategy object before proceeding.\n\n    Args:\n        num_rounds (int): Number of rounds of FL to perform (i.e. server rounds).\n        timeout (float | None): Timeout associated with queries to the clients in seconds. The server waits for\n            timeout seconds before moving on without any unresponsive clients. If None, there is no timeout and the\n            server waits for the minimum number of clients to be available set in the strategy.\n\n    Returns:\n        (tuple[History, float]): The first element of the tuple is a ``History`` object containing the full set of\n            FL training results, including things like aggregated loss and metrics. Tuple also includes elapsed\n            time in seconds for round.\n    \"\"\"\n    assert isinstance(self.strategy, Scaffold)\n    return super().fit(num_rounds=num_rounds, timeout=timeout)\n</code></pre>"},{"location":"api/#fl4health.servers.scaffold_server.DPScaffoldServer","title":"<code>DPScaffoldServer</code>","text":"<p>               Bases: <code>ScaffoldServer</code>, <code>InstanceLevelDpServer</code></p> Source code in <code>fl4health/servers/scaffold_server.py</code> <pre><code>class DPScaffoldServer(ScaffoldServer, InstanceLevelDpServer):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        fl_config: Config,\n        noise_multiplier: int,\n        batch_size: int,\n        num_server_rounds: int,\n        strategy: OpacusScaffold,\n        local_epochs: int | None = None,\n        local_steps: int | None = None,\n        delta: float | None = None,\n        checkpoint_and_state_module: DpScaffoldServerCheckpointAndStateModule | None = None,\n        warm_start: bool = False,\n        reporters: Sequence[BaseReporter] | None = None,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Custom FL Server for Instance Level Differentially Private Scaffold algorithm as specified in\n        https://arxiv.org/abs/2111.09278.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            fl_config (Config): This should be the configuration that was used to setup the federated training.\n                In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n                example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n                strategy.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            noise_multiplier (int): The amount of Gaussian noise to be added to the per sample gradient during\n                DP-SGD.\n            batch_size (int): The batch size to be used in training on the client-side. Used in privacy accounting.\n            num_server_rounds (int): The number of server rounds to be done in FL training. Used in privacy accounting\n            local_epochs (int | None, optional): Number of local epochs to be performed on the client-side. This is\n                used in privacy accounting. One of ``local_epochs`` or ``local_steps`` should be defined, but not both.\n                Defaults to None.\n            local_steps (int | None, optional): Number of local steps to be performed on the client-side. This is\n                used in privacy accounting. One of ``local_epochs`` or ``local_steps`` should be defined, but not both.\n                Defaults to None.\n            strategy (Scaffold): The aggregation strategy to be used by the server to handle client updates and\n                other information potentially sent by the participating clients. This strategy must be of SCAFFOLD\n                type.\n            checkpoint_and_state_module (DpScaffoldServerCheckpointAndStateModule | None, optional): This module is\n                used to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n                artifacts to be used or evaluated after training. The latter is used to preserve training state\n                (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n            warm_start (bool, optional): Whether or not to initialize control variates of each client as local\n                gradients. The clients will perform a training pass (without updating the weights) in order to provide\n                a \"warm\" estimate of the SCAFFOLD control variates. If false, variates are initialized to 0.\n                Defaults to False.\n            delta (float | None, optional): The delta value for epsilon-delta DP accounting. If None it defaults to\n                being ``1/total_samples`` in the FL run. Defaults to None.\n            reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the client should\n                send data to.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n                configure how one asks a client to provide parameters from which to initialize all other clients by\n                providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n                request (which is default behavior for flower servers). Defaults to None.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n        \"\"\"\n        # Require the strategy to be an  OpacusStrategy to handle the Opacus model conversion etc.\n        assert isinstance(strategy, OpacusScaffold), (\n            f\"Strategy much be of type OpacusScaffold to handle Opacus models but is of type {type(strategy)}\"\n        )\n        ScaffoldServer.__init__(\n            self,\n            client_manager=client_manager,\n            fl_config=fl_config,\n            strategy=strategy,\n            warm_start=warm_start,\n            reporters=reporters,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n        InstanceLevelDpServer.__init__(\n            self,\n            client_manager=client_manager,\n            fl_config=fl_config,\n            noise_multiplier=noise_multiplier,\n            num_server_rounds=num_server_rounds,\n            batch_size=batch_size,\n            strategy=strategy,\n            local_epochs=local_epochs,\n            local_steps=local_steps,\n            delta=delta,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n\n    def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n        \"\"\"\n        Run DP Scaffold algorithm for the specified number of rounds.\n\n        Args:\n            num_rounds (int): Number of rounds of FL to perform (i.e. server rounds).\n            timeout (float | None): Timeout associated with queries to the clients in seconds. The server waits for\n                timeout seconds before moving on without any unresponsive clients. If None, there is no timeout and the\n                server waits for the minimum number of clients to be available set in the strategy.\n\n        Returns:\n            (tuple[History, float]): First element of tuple is history object containing the full set of FL training\n                results, including aggregated loss and metrics. Tuple also includes the elapsed time in seconds for\n                round.\n        \"\"\"\n        assert isinstance(self.strategy, Scaffold)\n        # Now that we initialized the parameters for scaffold, call instance level privacy fit\n        return InstanceLevelDpServer.fit(self, num_rounds=num_rounds, timeout=timeout)\n</code></pre>"},{"location":"api/#fl4health.servers.scaffold_server.DPScaffoldServer.__init__","title":"<code>__init__(client_manager, fl_config, noise_multiplier, batch_size, num_server_rounds, strategy, local_epochs=None, local_steps=None, delta=None, checkpoint_and_state_module=None, warm_start=False, reporters=None, on_init_parameters_config_fn=None, server_name=None, accept_failures=True)</code>","text":"<p>Custom FL Server for Instance Level Differentially Private Scaffold algorithm as specified in https://arxiv.org/abs/2111.09278.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>fl_config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated training. In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For example, the config used to produce the <code>on_fit_config_fn</code> and <code>on_evaluate_config_fn</code> for the strategy.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>noise_multiplier</code> <code>int</code> <p>The amount of Gaussian noise to be added to the per sample gradient during DP-SGD.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to be used in training on the client-side. Used in privacy accounting.</p> required <code>num_server_rounds</code> <code>int</code> <p>The number of server rounds to be done in FL training. Used in privacy accounting</p> required <code>local_epochs</code> <code>int | None</code> <p>Number of local epochs to be performed on the client-side. This is used in privacy accounting. One of <code>local_epochs</code> or <code>local_steps</code> should be defined, but not both. Defaults to None.</p> <code>None</code> <code>local_steps</code> <code>int | None</code> <p>Number of local steps to be performed on the client-side. This is used in privacy accounting. One of <code>local_epochs</code> or <code>local_steps</code> should be defined, but not both. Defaults to None.</p> <code>None</code> <code>strategy</code> <code>Scaffold</code> <p>The aggregation strategy to be used by the server to handle client updates and other information potentially sent by the participating clients. This strategy must be of SCAFFOLD type.</p> required <code>checkpoint_and_state_module</code> <code>DpScaffoldServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <code>None</code> <code>warm_start</code> <code>bool</code> <p>Whether or not to initialize control variates of each client as local gradients. The clients will perform a training pass (without updating the weights) in order to provide a \"warm\" estimate of the SCAFFOLD control variates. If false, variates are initialized to 0. Defaults to False.</p> <code>False</code> <code>delta</code> <code>float | None</code> <p>The delta value for epsilon-delta DP accounting. If None it defaults to being <code>1/total_samples</code> in the FL run. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter]</code> <p>A sequence of FL4Health reporters which the client should send data to.</p> <code>None</code> <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. If this is none, then a blank config is sent with the parameter request (which is default behavior for flower servers). Defaults to None.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/servers/scaffold_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    fl_config: Config,\n    noise_multiplier: int,\n    batch_size: int,\n    num_server_rounds: int,\n    strategy: OpacusScaffold,\n    local_epochs: int | None = None,\n    local_steps: int | None = None,\n    delta: float | None = None,\n    checkpoint_and_state_module: DpScaffoldServerCheckpointAndStateModule | None = None,\n    warm_start: bool = False,\n    reporters: Sequence[BaseReporter] | None = None,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n) -&gt; None:\n    \"\"\"\n    Custom FL Server for Instance Level Differentially Private Scaffold algorithm as specified in\n    https://arxiv.org/abs/2111.09278.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        fl_config (Config): This should be the configuration that was used to setup the federated training.\n            In most cases it should be the \"source of truth\" for how FL training/evaluation should proceed. For\n            example, the config used to produce the ``on_fit_config_fn`` and ``on_evaluate_config_fn`` for the\n            strategy.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        noise_multiplier (int): The amount of Gaussian noise to be added to the per sample gradient during\n            DP-SGD.\n        batch_size (int): The batch size to be used in training on the client-side. Used in privacy accounting.\n        num_server_rounds (int): The number of server rounds to be done in FL training. Used in privacy accounting\n        local_epochs (int | None, optional): Number of local epochs to be performed on the client-side. This is\n            used in privacy accounting. One of ``local_epochs`` or ``local_steps`` should be defined, but not both.\n            Defaults to None.\n        local_steps (int | None, optional): Number of local steps to be performed on the client-side. This is\n            used in privacy accounting. One of ``local_epochs`` or ``local_steps`` should be defined, but not both.\n            Defaults to None.\n        strategy (Scaffold): The aggregation strategy to be used by the server to handle client updates and\n            other information potentially sent by the participating clients. This strategy must be of SCAFFOLD\n            type.\n        checkpoint_and_state_module (DpScaffoldServerCheckpointAndStateModule | None, optional): This module is\n            used to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n            artifacts to be used or evaluated after training. The latter is used to preserve training state\n            (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n        warm_start (bool, optional): Whether or not to initialize control variates of each client as local\n            gradients. The clients will perform a training pass (without updating the weights) in order to provide\n            a \"warm\" estimate of the SCAFFOLD control variates. If false, variates are initialized to 0.\n            Defaults to False.\n        delta (float | None, optional): The delta value for epsilon-delta DP accounting. If None it defaults to\n            being ``1/total_samples`` in the FL run. Defaults to None.\n        reporters (Sequence[BaseReporter], optional): A sequence of FL4Health reporters which the client should\n            send data to.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n            configure how one asks a client to provide parameters from which to initialize all other clients by\n            providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n            request (which is default behavior for flower servers). Defaults to None.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n    \"\"\"\n    # Require the strategy to be an  OpacusStrategy to handle the Opacus model conversion etc.\n    assert isinstance(strategy, OpacusScaffold), (\n        f\"Strategy much be of type OpacusScaffold to handle Opacus models but is of type {type(strategy)}\"\n    )\n    ScaffoldServer.__init__(\n        self,\n        client_manager=client_manager,\n        fl_config=fl_config,\n        strategy=strategy,\n        warm_start=warm_start,\n        reporters=reporters,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n    InstanceLevelDpServer.__init__(\n        self,\n        client_manager=client_manager,\n        fl_config=fl_config,\n        noise_multiplier=noise_multiplier,\n        num_server_rounds=num_server_rounds,\n        batch_size=batch_size,\n        strategy=strategy,\n        local_epochs=local_epochs,\n        local_steps=local_steps,\n        delta=delta,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n</code></pre>"},{"location":"api/#fl4health.servers.scaffold_server.DPScaffoldServer.fit","title":"<code>fit(num_rounds, timeout)</code>","text":"<p>Run DP Scaffold algorithm for the specified number of rounds.</p> <p>Parameters:</p> Name Type Description Default <code>num_rounds</code> <code>int</code> <p>Number of rounds of FL to perform (i.e. server rounds).</p> required <code>timeout</code> <code>float | None</code> <p>Timeout associated with queries to the clients in seconds. The server waits for timeout seconds before moving on without any unresponsive clients. If None, there is no timeout and the server waits for the minimum number of clients to be available set in the strategy.</p> required <p>Returns:</p> Type Description <code>tuple[History, float]</code> <p>First element of tuple is history object containing the full set of FL training results, including aggregated loss and metrics. Tuple also includes the elapsed time in seconds for round.</p> Source code in <code>fl4health/servers/scaffold_server.py</code> <pre><code>def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n    \"\"\"\n    Run DP Scaffold algorithm for the specified number of rounds.\n\n    Args:\n        num_rounds (int): Number of rounds of FL to perform (i.e. server rounds).\n        timeout (float | None): Timeout associated with queries to the clients in seconds. The server waits for\n            timeout seconds before moving on without any unresponsive clients. If None, there is no timeout and the\n            server waits for the minimum number of clients to be available set in the strategy.\n\n    Returns:\n        (tuple[History, float]): First element of tuple is history object containing the full set of FL training\n            results, including aggregated loss and metrics. Tuple also includes the elapsed time in seconds for\n            round.\n    \"\"\"\n    assert isinstance(self.strategy, Scaffold)\n    # Now that we initialized the parameters for scaffold, call instance level privacy fit\n    return InstanceLevelDpServer.fit(self, num_rounds=num_rounds, timeout=timeout)\n</code></pre>"},{"location":"api/#fl4health.servers.tabular_feature_alignment_server","title":"<code>tabular_feature_alignment_server</code>","text":""},{"location":"api/#fl4health.servers.tabular_feature_alignment_server.TabularFeatureAlignmentServer","title":"<code>TabularFeatureAlignmentServer</code>","text":"<p>               Bases: <code>FlServer</code></p> Source code in <code>fl4health/servers/tabular_feature_alignment_server.py</code> <pre><code>class TabularFeatureAlignmentServer(FlServer):\n    def __init__(\n        self,\n        client_manager: ClientManager,\n        config: Config,\n        initialize_parameters: Callable[[int, int], Parameters],\n        strategy: BasicFedAvg,\n        tabular_features_source_of_truth: TabularFeaturesInfoEncoder | None = None,\n        reporters: Sequence[BaseReporter] | None = None,\n        checkpoint_and_state_module: BaseServerCheckpointAndStateModule | None = None,\n        on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        server_name: str | None = None,\n        accept_failures: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        This server is used when the clients all have tabular data that needs to be aligned.\n\n        Args:\n            client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n                they are to be sampled at all.\n            config (Config): This should be the configuration that was used to setup the federated alignment.\n                In most cases it should be the \"source of truth\" for how FL alignment should proceed.\n\n                **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n            initialize_parameters (Callable[[int, int], Parameters]): Function used to initialize the model to be\n                trained and used for the tabular task.\n\n                **NOTE**: The model architecture is not finalized until we are able to determine the dimensionality of\n                the input and output space during feature alignment.\n            strategy (BasicFedAvg): The aggregation strategy to be used by the server to handle.\n                client updates and other information potentially sent by the participating clients. If None the\n                strategy is FedAvg as set by the flwr Server.\n            tabular_features_source_of_truth (TabularFeaturesInfoEncoder | None, optional): The information that is\n                required for aligning client features. If it is not specified, then the server will randomly poll a\n                client and gather this information from its data source. Defaults to None.\n            reporters (Sequence[BaseReporter] | None, optional): Sequence of FL4Health reporters which the server\n                should send data to before and after each round. Defaults to None\n            checkpoint_and_state_module (BaseServerCheckpointAndStateModule | None, optional): This module is used\n                to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n                artifacts to be used or evaluated after training. The latter is used to preserve training state\n                (including models) such that if FL training is interrupted, the process may be restarted. If no\n                module is provided, no checkpointing or state preservation will happen. Defaults to None.\n            on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n                configure how one asks a client to provide parameters from which to initialize all other clients by\n                providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n                request (which is default behavior for flower servers). Defaults to None.\n            server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n                used as part of any state checkpointing done by the server. Defaults to None.\n            accept_failures (bool, optional): Determines whether the server should accept failures during training or\n                evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n                and throw an exception. Defaults to True.\n        \"\"\"\n        if strategy.on_fit_config_fn is not None:\n            log(WARNING, \"strategy.on_fit_config_fn will be overwritten.\")\n        if strategy.initial_parameters is not None:\n            log(WARNING, \"strategy.initial_parameters will be overwritten.\")\n\n        super().__init__(\n            client_manager=client_manager,\n            fl_config=config,\n            strategy=strategy,\n            reporters=reporters,\n            checkpoint_and_state_module=checkpoint_and_state_module,\n            on_init_parameters_config_fn=on_init_parameters_config_fn,\n            server_name=server_name,\n            accept_failures=accept_failures,\n        )\n        # The server performs one or two rounds of polls before the normal federated training.\n        # The first one gathers feature information if the server does not already have it,\n        # and the second one gathers the input/output dimensions of the model.\n        self.initial_polls_complete = False\n        self.tab_features_info = tabular_features_source_of_truth\n        self.initialize_parameters = initialize_parameters\n        self.source_info_gathered = False\n        self.dimension_info: dict[str, int] = {}\n        # ensure that self.strategy has type BasicFedAvg so its on_fit_config_fn can be specified.\n        assert isinstance(self.strategy, BasicFedAvg), \"This server is only compatible with BasicFedAvg at this time\"\n        self.strategy.on_fit_config_fn = partial(fit_config, self.fl_config, self.source_info_gathered)\n\n    def _set_dimension_info(self, input_dimension: int, output_dimension: int) -&gt; None:\n        self.dimension_info[INPUT_DIMENSION] = input_dimension\n        self.dimension_info[OUTPUT_DIMENSION] = output_dimension\n\n    def _get_initial_parameters(self, server_round: int, timeout: float | None) -&gt; Parameters:\n        assert INPUT_DIMENSION in self.dimension_info and OUTPUT_DIMENSION in self.dimension_info\n        input_dimension = self.dimension_info[INPUT_DIMENSION]\n        output_dimension = self.dimension_info[OUTPUT_DIMENSION]\n        return self.initialize_parameters(input_dimension, output_dimension)\n\n    def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n        \"\"\"Run federated averaging for a number of rounds.\"\"\"\n        assert isinstance(self.strategy, BasicFedAvg)\n\n        # Before the normal fitting round begins, the server provides all clients\n        # the feature information needed to perform feature alignment. Then the server\n        # gathers information from the clients that is necessary for initializing the global model.\n        if not self.initial_polls_complete:\n            # If the server does not have the needed feature info a priori,\n            # then it requests such information from the clients before the\n            # very first fitting round.\n            if self.tab_features_info is None:\n                # A random client's feature information is selected as the source of truth for feature alignment.\n                feature_info_source = self.poll_clients_for_feature_info(timeout)\n            # If the server already has the feature info, then it simply sends it to the clients.\n            else:\n                log(\n                    INFO,\n                    \"Features information source already specified. Sending to clients to perform feature alignment.\",\n                )\n                feature_info_source = self.tab_features_info.to_json()\n\n            # the feature information is sent to clients through the config parameter.\n            self.fl_config[FEATURE_INFO] = feature_info_source\n            self.source_info_gathered = True\n\n            self.strategy.on_fit_config_fn = partial(fit_config, self.fl_config, self.source_info_gathered)\n\n            # Now the server waits until feature alignment is performed on the clients' side\n            # and subsequently requests the input and output dimensions, which are needed for initializing\n            # the global model.\n            input_dimension, output_dimension = self.poll_clients_for_dimension_info(timeout)\n            log(DEBUG, f\"input dimension: {input_dimension}, output dimension: {output_dimension}\")\n            self._set_dimension_info(input_dimension, output_dimension)\n            self.initial_polls_complete = True\n\n        # Normal federated learning rounds commence after all clients' features\n        # are aligned and global model is initialized.\n        return super().fit(num_rounds=num_rounds, timeout=timeout)\n\n    def poll_clients_for_feature_info(self, timeout: float | None) -&gt; str:\n        log(INFO, \"Feature information source unspecified. Polling clients for feature information.\")\n        assert isinstance(self.strategy, BasicFedAvg)\n        client_instructions = self.strategy.configure_poll(server_round=1, client_manager=self._client_manager)\n        # Randomly select one client to obtain its feature information.\n        client_instructions_rand_sample = random.sample(population=client_instructions, k=1)\n        results, _ = poll_clients(\n            client_instructions=client_instructions_rand_sample, max_workers=self.max_workers, timeout=timeout\n        )\n\n        assert len(results) == 1\n        _, get_properties_res = results[0]\n        return str(get_properties_res.properties[FEATURE_INFO])\n\n    def poll_clients_for_dimension_info(self, timeout: float | None) -&gt; tuple[int, int]:\n        log(INFO, \"Waiting for Clients to align features and then polling for dimension information.\")\n        assert isinstance(self.strategy, BasicFedAvg)\n        client_instructions = self.strategy.configure_poll(server_round=1, client_manager=self._client_manager)\n\n        # Since the features of all clients are aligned, we just select one client\n        # to obtain the input/output dimensions.\n        results, _ = poll_clients(\n            client_instructions=client_instructions[:1], max_workers=self.max_workers, timeout=timeout\n        )\n        assert len(results) == 1\n        input_dimension = int(results[0][1].properties[INPUT_DIMENSION])\n        output_dimension = int(results[0][1].properties[OUTPUT_DIMENSION])\n\n        return input_dimension, output_dimension\n</code></pre>"},{"location":"api/#fl4health.servers.tabular_feature_alignment_server.TabularFeatureAlignmentServer.__init__","title":"<code>__init__(client_manager, config, initialize_parameters, strategy, tabular_features_source_of_truth=None, reporters=None, checkpoint_and_state_module=None, on_init_parameters_config_fn=None, server_name=None, accept_failures=True)</code>","text":"<p>This server is used when the clients all have tabular data that needs to be aligned.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Determines the mechanism by which clients are sampled by the server, if they are to be sampled at all.</p> required <code>config</code> <code>Config</code> <p>This should be the configuration that was used to setup the federated alignment. In most cases it should be the \"source of truth\" for how FL alignment should proceed.</p> <p>NOTE: This config is DISTINCT from the Flwr server config, which is extremely minimal.</p> required <code>initialize_parameters</code> <code>Callable[[int, int], Parameters]</code> <p>Function used to initialize the model to be trained and used for the tabular task.</p> <p>NOTE: The model architecture is not finalized until we are able to determine the dimensionality of the input and output space during feature alignment.</p> required <code>strategy</code> <code>BasicFedAvg</code> <p>The aggregation strategy to be used by the server to handle. client updates and other information potentially sent by the participating clients. If None the strategy is FedAvg as set by the flwr Server.</p> required <code>tabular_features_source_of_truth</code> <code>TabularFeaturesInfoEncoder | None</code> <p>The information that is required for aligning client features. If it is not specified, then the server will randomly poll a client and gather this information from its data source. Defaults to None.</p> <code>None</code> <code>reporters</code> <code>Sequence[BaseReporter] | None</code> <p>Sequence of FL4Health reporters which the server should send data to before and after each round. Defaults to None</p> <code>None</code> <code>checkpoint_and_state_module</code> <code>BaseServerCheckpointAndStateModule | None</code> <p>This module is used to handle both model checkpointing and state checkpointing. The former is aimed at saving model artifacts to be used or evaluated after training. The latter is used to preserve training state (including models) such that if FL training is interrupted, the process may be restarted. If no module is provided, no checkpointing or state preservation will happen. Defaults to None.</p> <code>None</code> <code>on_init_parameters_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure how one asks a client to provide parameters from which to initialize all other clients by providing a <code>Config</code> dictionary. If this is none, then a blank config is sent with the parameter request (which is default behavior for flower servers). Defaults to None.</p> <code>None</code> <code>server_name</code> <code>str | None</code> <p>An optional string name to uniquely identify server. This name is also used as part of any state checkpointing done by the server. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Determines whether the server should accept failures during training or evaluation from clients or not. If set to False, this will cause the server to shutdown all clients and throw an exception. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/servers/tabular_feature_alignment_server.py</code> <pre><code>def __init__(\n    self,\n    client_manager: ClientManager,\n    config: Config,\n    initialize_parameters: Callable[[int, int], Parameters],\n    strategy: BasicFedAvg,\n    tabular_features_source_of_truth: TabularFeaturesInfoEncoder | None = None,\n    reporters: Sequence[BaseReporter] | None = None,\n    checkpoint_and_state_module: BaseServerCheckpointAndStateModule | None = None,\n    on_init_parameters_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    server_name: str | None = None,\n    accept_failures: bool = True,\n) -&gt; None:\n    \"\"\"\n    This server is used when the clients all have tabular data that needs to be aligned.\n\n    Args:\n        client_manager (ClientManager): Determines the mechanism by which clients are sampled by the server, if\n            they are to be sampled at all.\n        config (Config): This should be the configuration that was used to setup the federated alignment.\n            In most cases it should be the \"source of truth\" for how FL alignment should proceed.\n\n            **NOTE**: This config is **DISTINCT** from the Flwr server config, which is extremely minimal.\n        initialize_parameters (Callable[[int, int], Parameters]): Function used to initialize the model to be\n            trained and used for the tabular task.\n\n            **NOTE**: The model architecture is not finalized until we are able to determine the dimensionality of\n            the input and output space during feature alignment.\n        strategy (BasicFedAvg): The aggregation strategy to be used by the server to handle.\n            client updates and other information potentially sent by the participating clients. If None the\n            strategy is FedAvg as set by the flwr Server.\n        tabular_features_source_of_truth (TabularFeaturesInfoEncoder | None, optional): The information that is\n            required for aligning client features. If it is not specified, then the server will randomly poll a\n            client and gather this information from its data source. Defaults to None.\n        reporters (Sequence[BaseReporter] | None, optional): Sequence of FL4Health reporters which the server\n            should send data to before and after each round. Defaults to None\n        checkpoint_and_state_module (BaseServerCheckpointAndStateModule | None, optional): This module is used\n            to handle both model checkpointing and state checkpointing. The former is aimed at saving model\n            artifacts to be used or evaluated after training. The latter is used to preserve training state\n            (including models) such that if FL training is interrupted, the process may be restarted. If no\n            module is provided, no checkpointing or state preservation will happen. Defaults to None.\n        on_init_parameters_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to\n            configure how one asks a client to provide parameters from which to initialize all other clients by\n            providing a ``Config`` dictionary. If this is none, then a blank config is sent with the parameter\n            request (which is default behavior for flower servers). Defaults to None.\n        server_name (str | None, optional): An optional string name to uniquely identify server. This name is also\n            used as part of any state checkpointing done by the server. Defaults to None.\n        accept_failures (bool, optional): Determines whether the server should accept failures during training or\n            evaluation from clients or not. If set to False, this will cause the server to shutdown all clients\n            and throw an exception. Defaults to True.\n    \"\"\"\n    if strategy.on_fit_config_fn is not None:\n        log(WARNING, \"strategy.on_fit_config_fn will be overwritten.\")\n    if strategy.initial_parameters is not None:\n        log(WARNING, \"strategy.initial_parameters will be overwritten.\")\n\n    super().__init__(\n        client_manager=client_manager,\n        fl_config=config,\n        strategy=strategy,\n        reporters=reporters,\n        checkpoint_and_state_module=checkpoint_and_state_module,\n        on_init_parameters_config_fn=on_init_parameters_config_fn,\n        server_name=server_name,\n        accept_failures=accept_failures,\n    )\n    # The server performs one or two rounds of polls before the normal federated training.\n    # The first one gathers feature information if the server does not already have it,\n    # and the second one gathers the input/output dimensions of the model.\n    self.initial_polls_complete = False\n    self.tab_features_info = tabular_features_source_of_truth\n    self.initialize_parameters = initialize_parameters\n    self.source_info_gathered = False\n    self.dimension_info: dict[str, int] = {}\n    # ensure that self.strategy has type BasicFedAvg so its on_fit_config_fn can be specified.\n    assert isinstance(self.strategy, BasicFedAvg), \"This server is only compatible with BasicFedAvg at this time\"\n    self.strategy.on_fit_config_fn = partial(fit_config, self.fl_config, self.source_info_gathered)\n</code></pre>"},{"location":"api/#fl4health.servers.tabular_feature_alignment_server.TabularFeatureAlignmentServer.fit","title":"<code>fit(num_rounds, timeout)</code>","text":"<p>Run federated averaging for a number of rounds.</p> Source code in <code>fl4health/servers/tabular_feature_alignment_server.py</code> <pre><code>def fit(self, num_rounds: int, timeout: float | None) -&gt; tuple[History, float]:\n    \"\"\"Run federated averaging for a number of rounds.\"\"\"\n    assert isinstance(self.strategy, BasicFedAvg)\n\n    # Before the normal fitting round begins, the server provides all clients\n    # the feature information needed to perform feature alignment. Then the server\n    # gathers information from the clients that is necessary for initializing the global model.\n    if not self.initial_polls_complete:\n        # If the server does not have the needed feature info a priori,\n        # then it requests such information from the clients before the\n        # very first fitting round.\n        if self.tab_features_info is None:\n            # A random client's feature information is selected as the source of truth for feature alignment.\n            feature_info_source = self.poll_clients_for_feature_info(timeout)\n        # If the server already has the feature info, then it simply sends it to the clients.\n        else:\n            log(\n                INFO,\n                \"Features information source already specified. Sending to clients to perform feature alignment.\",\n            )\n            feature_info_source = self.tab_features_info.to_json()\n\n        # the feature information is sent to clients through the config parameter.\n        self.fl_config[FEATURE_INFO] = feature_info_source\n        self.source_info_gathered = True\n\n        self.strategy.on_fit_config_fn = partial(fit_config, self.fl_config, self.source_info_gathered)\n\n        # Now the server waits until feature alignment is performed on the clients' side\n        # and subsequently requests the input and output dimensions, which are needed for initializing\n        # the global model.\n        input_dimension, output_dimension = self.poll_clients_for_dimension_info(timeout)\n        log(DEBUG, f\"input dimension: {input_dimension}, output dimension: {output_dimension}\")\n        self._set_dimension_info(input_dimension, output_dimension)\n        self.initial_polls_complete = True\n\n    # Normal federated learning rounds commence after all clients' features\n    # are aligned and global model is initialized.\n    return super().fit(num_rounds=num_rounds, timeout=timeout)\n</code></pre>"},{"location":"api/#fl4health.strategies","title":"<code>strategies</code>","text":""},{"location":"api/#fl4health.strategies.aggregate_utils","title":"<code>aggregate_utils</code>","text":""},{"location":"api/#fl4health.strategies.aggregate_utils.aggregate_results","title":"<code>aggregate_results(results, weighted=True)</code>","text":"<p>Compute weighted or unweighted average.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[NDArrays, int]]</code> <p>This is a set of <code>NDArrays</code> (list of numpy arrays) and the number of relevant samples from each client (training or validation samples where appropriate). These are to be aggregated together in a weighted or unweighted average. The <code>NDArrays</code> most often represent model states.</p> required <code>weighted</code> <code>bool</code> <p>Whether or not the aggregation is a weighted average (by the sample counts provided in the tuple) or a uniform average. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>NDArrays</code> <p>Aggregated numpy arrays by the desired averaging.</p> Source code in <code>fl4health/strategies/aggregate_utils.py</code> <pre><code>def aggregate_results(results: list[tuple[NDArrays, int]], weighted: bool = True) -&gt; NDArrays:\n    \"\"\"\n    Compute weighted or unweighted average.\n\n    Args:\n        results (list[tuple[NDArrays, int]]): This is a set of ``NDArrays`` (list of numpy arrays) and the number of\n            relevant samples from each client (training or validation samples where appropriate). These are to be\n            aggregated together in a weighted or unweighted average. The ``NDArrays`` most often represent model\n            states.\n        weighted (bool, optional): Whether or not the aggregation is a weighted average (by the sample counts\n            provided in the tuple) or a uniform average. Defaults to True.\n\n    Returns:\n        (NDArrays): Aggregated numpy arrays by the desired averaging.\n    \"\"\"\n    if weighted:\n        # Uses the underlying flwr aggregation scheme\n        return aggregate(results)\n    # Number of client weights to average\n    num_clients = len(results)\n    # Create a list of weights, each multiplied by 1/num_clients\n    weighted_weights = [[layer * (1.0 / num_clients) for layer in weights] for weights, _ in results]\n\n    # Compute unweighted average by summing up across clients for each layer.\n    return [reduce(np.add, layer_updates) for layer_updates in zip(*weighted_weights)]\n</code></pre>"},{"location":"api/#fl4health.strategies.aggregate_utils.aggregate_losses","title":"<code>aggregate_losses(results, weighted=True)</code>","text":"<p>Aggregate evaluation results obtained from multiple clients.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[int, float]]</code> <p>A list of sample counts and loss values (in that order). The sample counts from each client (training or validation samples where appropriate) are used if weighted averaging is requested.</p> required <code>weighted</code> <code>bool</code> <p>Whether or not the aggregation is a weighted average (by the sample counts provided in the tuple) or a uniform average. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>The weighted or unweighted average of the loss values in the results list.</p> Source code in <code>fl4health/strategies/aggregate_utils.py</code> <pre><code>def aggregate_losses(results: list[tuple[int, float]], weighted: bool = True) -&gt; float:\n    \"\"\"\n    Aggregate evaluation results obtained from multiple clients.\n\n    Args:\n        results (list[tuple[int, float]]): A list of sample counts and loss values (in that order). The sample counts\n            from each client (training or validation samples where appropriate) are used if weighted averaging is\n            requested.\n        weighted (bool, optional): Whether or not the aggregation is a weighted average (by the sample counts\n            provided in the tuple) or a uniform average. Defaults to True.\n\n    Returns:\n        (float): The weighted or unweighted average of the loss values in the results list.\n    \"\"\"\n    # Sorting the results by the loss values for numerical fluctuation determinism of the sum\n    results = sorted(results, key=lambda x: x[1])\n    if weighted:\n        # uses flwr implementation of weighted loss averaging\n        return weighted_loss_avg(results)\n    # standard averaging\n    return sum([loss for _, loss in results]) / len(results)\n</code></pre>"},{"location":"api/#fl4health.strategies.basic_fedavg","title":"<code>basic_fedavg</code>","text":""},{"location":"api/#fl4health.strategies.basic_fedavg.BasicFedAvg","title":"<code>BasicFedAvg</code>","text":"<p>               Bases: <code>FedAvg</code>, <code>StrategyWithPolling</code></p> <p>Configurable FedAvg strategy implementation.</p> Source code in <code>fl4health/strategies/basic_fedavg.py</code> <pre><code>class BasicFedAvg(FedAvg, StrategyWithPolling):\n    \"\"\"Configurable FedAvg strategy implementation.\"\"\"\n\n    # pylint: disable=too-many-arguments,too-many-instance-attributes\n    def __init__(\n        self,\n        *,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters | None = None,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        weighted_aggregation: bool = True,\n        weighted_eval_losses: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Federated Averaging with Flexible Sampling. This implementation extends that of Flower in two ways. The first\n        is that it provides an option for unweighted averaging, where Flower only offers weighted averaging based on\n        client sample counts. The second is that it allows users to Flower's standard sampling or use a custom\n        sampling approach implemented in by a custom client manager.\n\n        FedAvg Paper: https://arxiv.org/abs/1602.05629.\n\n        Args:\n            fraction_fit (float, optional): Fraction of clients used during training. In case ``min_fit_clients`` is\n                larger than ``fraction_fit * available_clients``, ``min_fit_clients`` will still be sampled.\n                Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. In case\n                ``min_evaluate_clients`` is larger than ``fraction_evaluate * available_clients``,\n                ``min_evaluate_clients`` will still be sampled. Defaults to 1.0.\n            min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for central server-side evaluation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                client-side validation by providing a ``Config`` dictionary. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n                average or a uniform average. FedAvg default is weighted average by client dataset counts.\n                Defaults to True.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n                counts. Defaults to True.\n        \"\"\"\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_fit_clients=min_fit_clients,\n            min_evaluate_clients=min_evaluate_clients,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        )\n        self.weighted_aggregation = weighted_aggregation\n        self.weighted_eval_losses = weighted_eval_losses\n\n    def add_auxiliary_information(self, original_parameters: Parameters) -&gt; None:\n        \"\"\"\n        Identity function for the BasicFedAvg strategy. This function is made available for override in more complex\n        FL strategies to allow for the strategies to add auxiliary information to sets of parameters. This function\n        is specifically designed to allow addition to parameters initialized by the server calling out to a client for\n        weight initialization.\n\n        Here we need not add anything. So no modifications are made.\n\n        Args:\n            original_parameters (Parameters): Original set of parameters\n        \"\"\"\n        pass\n\n    def configure_fit(\n        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n    ) -&gt; list[tuple[ClientProxy, FitIns]]:\n        \"\"\"\n        This function configures a sample of clients for a training round. It handles the case where the client\n        manager has a sample fraction vs. a sample function (to allow for more flexible sampling).\n        The function follows the standard configuration flow where the ``on_fit_config_fn`` function is used to produce\n        configurations to be sent to all clients. These are packaged with the provided parameters and set over to the\n        clients.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            parameters (Parameters): The parameters to be used to initialize the clients for the fit round.\n            client_manager (ClientManager): The manager used to sample from the available clients.\n\n        Returns:\n            (list[tuple[ClientProxy, FitIns]]): List of sampled client identifiers and the configuration/parameters to\n                be sent to each client (packaged as ``FitIns``).\n        \"\"\"\n        if isinstance(client_manager, BaseFractionSamplingManager):\n            # Using one of the custom FractionSamplingManager classes, sampling fraction is based on fraction_fit\n            config = {}\n            if self.on_fit_config_fn is not None:\n                # Custom fit config function provided\n                config = self.on_fit_config_fn(server_round)\n            else:\n                config = {\"current_server_round\": server_round}\n            fit_ins = FitIns(parameters, config)\n\n            # Sample clients\n            clients = client_manager.sample_fraction(self.fraction_fit, self.min_available_clients)\n\n            # Return client/config pairs\n            return [(client, fit_ins) for client in clients]\n        log(INFO, f\"Using the standard Flower ClientManager: {type(client_manager)}\")\n        return super().configure_fit(server_round, parameters, client_manager)\n\n    def configure_evaluate(\n        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n    ) -&gt; list[tuple[ClientProxy, EvaluateIns]]:\n        \"\"\"\n        This function configures a sample of clients for a evaluation round. It handles the case where the client\n        manager has a sample fraction vs. a sample function (to allow for more flexible sampling).\n        The function follows the standard configuration flow where the ``on_evaluate_config_fn`` function is used to\n        produce configurations to be sent to all clients. These are packaged with the provided parameters and set over\n        to the clients.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            parameters (Parameters): The parameters to be used to initialize the clients for the eval round.\n            client_manager (ClientManager): The manager used to sample from the available clients.\n\n        Returns:\n            (list[tuple[ClientProxy, EvaluateIns]]): List of sampled client identifiers and the\n                configuration/parameters to be sent to each client (packaged as ``EvaluateIns``).\n        \"\"\"\n        # Do not configure federated evaluation if fraction eval is 0.\n        if self.fraction_evaluate == 0.0:\n            return []\n\n        if isinstance(client_manager, BaseFractionSamplingManager):\n            # Using one of the custom FractionSamplingManager classes, sampling fraction is based on fraction_evaluate\n            # Parameters and config\n            config = {}\n            if self.on_evaluate_config_fn is not None:\n                # Custom evaluation config function provided\n                config = self.on_evaluate_config_fn(server_round)\n            else:\n                config = {\"current_server_round\": server_round}\n            evaluate_ins = EvaluateIns(parameters, config)\n\n            # Sample clients\n            clients = client_manager.sample_fraction(self.fraction_evaluate, self.min_available_clients)\n\n            # Return client/config pairs\n            return [(client, evaluate_ins) for client in clients]\n        log(INFO, f\"Using the standard Flower ClientManager: {type(client_manager)}\")\n        return super().configure_evaluate(server_round, parameters, client_manager)\n\n    def configure_poll(\n        self, server_round: int, client_manager: ClientManager\n    ) -&gt; list[tuple[ClientProxy, GetPropertiesIns]]:\n        \"\"\"\n        This function configures everything required to request properties from **ALL** of the clients. The client\n        manger, regardless of type, is instructed to grab all available clients to perform the polling process.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            client_manager (ClientManager): The manager used to sample all available clients.\n\n        Returns:\n            (list[tuple[ClientProxy, GetPropertiesIns]]): List of sampled client identifiers and the configuration\n                to be sent to each client (packaged as ``GetPropertiesIns``).\n        \"\"\"\n        config = {}\n        if self.on_fit_config_fn is not None:\n            # Custom fit config function provided\n            config = self.on_fit_config_fn(server_round)\n\n        property_ins = GetPropertiesIns(config)\n\n        if isinstance(client_manager, BaseFractionSamplingManager):\n            clients = client_manager.sample_all(min_num_clients=self.min_available_clients)\n        else:\n            # Grab all available clients using the basic Flower client manager\n            num_available_clients = client_manager.num_available()\n            clients = client_manager.sample(num_available_clients, min_num_clients=self.min_available_clients)\n\n        # Return client/config pairs\n        return [(client, property_ins) for client in clients]\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate the results from the federated fit round. This is done with either weighted or unweighted FedAvg,\n        depending on the settings used for the strategy.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n                that need to be aggregated on the server-side.\n            failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n                from clients that experienced an issue during training, such as timeouts or exceptions.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n        # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n        # reducing numerical fluctuation.\n        decoded_and_sorted_results = [\n            (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n        ]\n\n        # Aggregate them in a weighted or unweighted fashion based on settings.\n        aggregated_arrays = aggregate_results(decoded_and_sorted_results, self.weighted_aggregation)\n        # Convert back to parameters\n        parameters_aggregated = ndarrays_to_parameters(aggregated_arrays)\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.fit_metrics_aggregation_fn:\n            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n        return parameters_aggregated, metrics_aggregated\n\n    def aggregate_evaluate(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, EvaluateRes]],\n        failures: list[tuple[ClientProxy, EvaluateRes] | BaseException],\n    ) -&gt; tuple[float | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate the metrics and losses returned from the clients as a result of the evaluation round.\n\n        Args:\n            server_round (int): Current FL server Round.\n            results (list[tuple[ClientProxy, EvaluateRes]]): The client identifiers and the results of their local\n                evaluation that need to be aggregated on the server-side. These results are loss values and the\n                metrics dictionary.\n            failures (list[tuple[ClientProxy, EvaluateRes]  |  BaseException]): These are the results and\n                exceptions from clients that experienced an issue during evaluation, such as timeouts or exceptions.\n\n        Returns:\n            (tuple[float | None, dict[str, Scalar]]): Aggregated loss values and the aggregated metrics. The metrics\n                are aggregated according to ``evaluate_metrics_aggregation_fn``.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Get losses and number of examples from the evaluation results.\n        loss_results = [(evaluate_res.num_examples, evaluate_res.loss) for _, evaluate_res in results]\n        # Then aggregate the losses\n        loss_aggregated = aggregate_losses(loss_results, self.weighted_eval_losses)\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.evaluate_metrics_aggregation_fn:\n            eval_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.evaluate_metrics_aggregation_fn(eval_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No evaluate_metrics_aggregation_fn provided\")\n\n        return loss_aggregated, metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.basic_fedavg.BasicFedAvg.__init__","title":"<code>__init__(*, fraction_fit=1.0, fraction_evaluate=1.0, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters=None, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, weighted_aggregation=True, weighted_eval_losses=True)</code>","text":"<p>Federated Averaging with Flexible Sampling. This implementation extends that of Flower in two ways. The first is that it provides an option for unweighted averaging, where Flower only offers weighted averaging based on client sample counts. The second is that it allows users to Flower's standard sampling or use a custom sampling approach implemented in by a custom client manager.</p> <p>FedAvg Paper: https://arxiv.org/abs/1602.05629.</p> <p>Parameters:</p> Name Type Description Default <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. In case <code>min_fit_clients</code> is larger than <code>fraction_fit * available_clients</code>, <code>min_fit_clients</code> will still be sampled. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. In case <code>min_evaluate_clients</code> is larger than <code>fraction_evaluate * available_clients</code>, <code>min_evaluate_clients</code> will still be sampled. Defaults to 1.0.</p> <code>1.0</code> <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during training. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of total clients in the system. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>initial_parameters</code> <code>Parameters | None</code> <p>Initial global model parameters. Defaults to None.</p> <code>None</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>weighted_aggregation</code> <code>bool</code> <p>Determines whether parameter aggregation is a linearly weighted average or a uniform average. FedAvg default is weighted average by client dataset counts. Defaults to True.</p> <code>True</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. FedAvg default is weighted average of the losses by client dataset counts. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/strategies/basic_fedavg.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters | None = None,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    weighted_aggregation: bool = True,\n    weighted_eval_losses: bool = True,\n) -&gt; None:\n    \"\"\"\n    Federated Averaging with Flexible Sampling. This implementation extends that of Flower in two ways. The first\n    is that it provides an option for unweighted averaging, where Flower only offers weighted averaging based on\n    client sample counts. The second is that it allows users to Flower's standard sampling or use a custom\n    sampling approach implemented in by a custom client manager.\n\n    FedAvg Paper: https://arxiv.org/abs/1602.05629.\n\n    Args:\n        fraction_fit (float, optional): Fraction of clients used during training. In case ``min_fit_clients`` is\n            larger than ``fraction_fit * available_clients``, ``min_fit_clients`` will still be sampled.\n            Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. In case\n            ``min_evaluate_clients`` is larger than ``fraction_evaluate * available_clients``,\n            ``min_evaluate_clients`` will still be sampled. Defaults to 1.0.\n        min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for central server-side evaluation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            client-side validation by providing a ``Config`` dictionary. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n            average or a uniform average. FedAvg default is weighted average by client dataset counts.\n            Defaults to True.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n            counts. Defaults to True.\n    \"\"\"\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_fit_clients=min_fit_clients,\n        min_evaluate_clients=min_evaluate_clients,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n    )\n    self.weighted_aggregation = weighted_aggregation\n    self.weighted_eval_losses = weighted_eval_losses\n</code></pre>"},{"location":"api/#fl4health.strategies.basic_fedavg.BasicFedAvg.add_auxiliary_information","title":"<code>add_auxiliary_information(original_parameters)</code>","text":"<p>Identity function for the BasicFedAvg strategy. This function is made available for override in more complex FL strategies to allow for the strategies to add auxiliary information to sets of parameters. This function is specifically designed to allow addition to parameters initialized by the server calling out to a client for weight initialization.</p> <p>Here we need not add anything. So no modifications are made.</p> <p>Parameters:</p> Name Type Description Default <code>original_parameters</code> <code>Parameters</code> <p>Original set of parameters</p> required Source code in <code>fl4health/strategies/basic_fedavg.py</code> <pre><code>def add_auxiliary_information(self, original_parameters: Parameters) -&gt; None:\n    \"\"\"\n    Identity function for the BasicFedAvg strategy. This function is made available for override in more complex\n    FL strategies to allow for the strategies to add auxiliary information to sets of parameters. This function\n    is specifically designed to allow addition to parameters initialized by the server calling out to a client for\n    weight initialization.\n\n    Here we need not add anything. So no modifications are made.\n\n    Args:\n        original_parameters (Parameters): Original set of parameters\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.strategies.basic_fedavg.BasicFedAvg.configure_fit","title":"<code>configure_fit(server_round, parameters, client_manager)</code>","text":"<p>This function configures a sample of clients for a training round. It handles the case where the client manager has a sample fraction vs. a sample function (to allow for more flexible sampling). The function follows the standard configuration flow where the <code>on_fit_config_fn</code> function is used to produce configurations to be sent to all clients. These are packaged with the provided parameters and set over to the clients.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>parameters</code> <code>Parameters</code> <p>The parameters to be used to initialize the clients for the fit round.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The manager used to sample from the available clients.</p> required <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, FitIns]]</code> <p>List of sampled client identifiers and the configuration/parameters to be sent to each client (packaged as <code>FitIns</code>).</p> Source code in <code>fl4health/strategies/basic_fedavg.py</code> <pre><code>def configure_fit(\n    self, server_round: int, parameters: Parameters, client_manager: ClientManager\n) -&gt; list[tuple[ClientProxy, FitIns]]:\n    \"\"\"\n    This function configures a sample of clients for a training round. It handles the case where the client\n    manager has a sample fraction vs. a sample function (to allow for more flexible sampling).\n    The function follows the standard configuration flow where the ``on_fit_config_fn`` function is used to produce\n    configurations to be sent to all clients. These are packaged with the provided parameters and set over to the\n    clients.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        parameters (Parameters): The parameters to be used to initialize the clients for the fit round.\n        client_manager (ClientManager): The manager used to sample from the available clients.\n\n    Returns:\n        (list[tuple[ClientProxy, FitIns]]): List of sampled client identifiers and the configuration/parameters to\n            be sent to each client (packaged as ``FitIns``).\n    \"\"\"\n    if isinstance(client_manager, BaseFractionSamplingManager):\n        # Using one of the custom FractionSamplingManager classes, sampling fraction is based on fraction_fit\n        config = {}\n        if self.on_fit_config_fn is not None:\n            # Custom fit config function provided\n            config = self.on_fit_config_fn(server_round)\n        else:\n            config = {\"current_server_round\": server_round}\n        fit_ins = FitIns(parameters, config)\n\n        # Sample clients\n        clients = client_manager.sample_fraction(self.fraction_fit, self.min_available_clients)\n\n        # Return client/config pairs\n        return [(client, fit_ins) for client in clients]\n    log(INFO, f\"Using the standard Flower ClientManager: {type(client_manager)}\")\n    return super().configure_fit(server_round, parameters, client_manager)\n</code></pre>"},{"location":"api/#fl4health.strategies.basic_fedavg.BasicFedAvg.configure_evaluate","title":"<code>configure_evaluate(server_round, parameters, client_manager)</code>","text":"<p>This function configures a sample of clients for a evaluation round. It handles the case where the client manager has a sample fraction vs. a sample function (to allow for more flexible sampling). The function follows the standard configuration flow where the <code>on_evaluate_config_fn</code> function is used to produce configurations to be sent to all clients. These are packaged with the provided parameters and set over to the clients.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>parameters</code> <code>Parameters</code> <p>The parameters to be used to initialize the clients for the eval round.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The manager used to sample from the available clients.</p> required <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, EvaluateIns]]</code> <p>List of sampled client identifiers and the configuration/parameters to be sent to each client (packaged as <code>EvaluateIns</code>).</p> Source code in <code>fl4health/strategies/basic_fedavg.py</code> <pre><code>def configure_evaluate(\n    self, server_round: int, parameters: Parameters, client_manager: ClientManager\n) -&gt; list[tuple[ClientProxy, EvaluateIns]]:\n    \"\"\"\n    This function configures a sample of clients for a evaluation round. It handles the case where the client\n    manager has a sample fraction vs. a sample function (to allow for more flexible sampling).\n    The function follows the standard configuration flow where the ``on_evaluate_config_fn`` function is used to\n    produce configurations to be sent to all clients. These are packaged with the provided parameters and set over\n    to the clients.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        parameters (Parameters): The parameters to be used to initialize the clients for the eval round.\n        client_manager (ClientManager): The manager used to sample from the available clients.\n\n    Returns:\n        (list[tuple[ClientProxy, EvaluateIns]]): List of sampled client identifiers and the\n            configuration/parameters to be sent to each client (packaged as ``EvaluateIns``).\n    \"\"\"\n    # Do not configure federated evaluation if fraction eval is 0.\n    if self.fraction_evaluate == 0.0:\n        return []\n\n    if isinstance(client_manager, BaseFractionSamplingManager):\n        # Using one of the custom FractionSamplingManager classes, sampling fraction is based on fraction_evaluate\n        # Parameters and config\n        config = {}\n        if self.on_evaluate_config_fn is not None:\n            # Custom evaluation config function provided\n            config = self.on_evaluate_config_fn(server_round)\n        else:\n            config = {\"current_server_round\": server_round}\n        evaluate_ins = EvaluateIns(parameters, config)\n\n        # Sample clients\n        clients = client_manager.sample_fraction(self.fraction_evaluate, self.min_available_clients)\n\n        # Return client/config pairs\n        return [(client, evaluate_ins) for client in clients]\n    log(INFO, f\"Using the standard Flower ClientManager: {type(client_manager)}\")\n    return super().configure_evaluate(server_round, parameters, client_manager)\n</code></pre>"},{"location":"api/#fl4health.strategies.basic_fedavg.BasicFedAvg.configure_poll","title":"<code>configure_poll(server_round, client_manager)</code>","text":"<p>This function configures everything required to request properties from ALL of the clients. The client manger, regardless of type, is instructed to grab all available clients to perform the polling process.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The manager used to sample all available clients.</p> required <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, GetPropertiesIns]]</code> <p>List of sampled client identifiers and the configuration to be sent to each client (packaged as <code>GetPropertiesIns</code>).</p> Source code in <code>fl4health/strategies/basic_fedavg.py</code> <pre><code>def configure_poll(\n    self, server_round: int, client_manager: ClientManager\n) -&gt; list[tuple[ClientProxy, GetPropertiesIns]]:\n    \"\"\"\n    This function configures everything required to request properties from **ALL** of the clients. The client\n    manger, regardless of type, is instructed to grab all available clients to perform the polling process.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        client_manager (ClientManager): The manager used to sample all available clients.\n\n    Returns:\n        (list[tuple[ClientProxy, GetPropertiesIns]]): List of sampled client identifiers and the configuration\n            to be sent to each client (packaged as ``GetPropertiesIns``).\n    \"\"\"\n    config = {}\n    if self.on_fit_config_fn is not None:\n        # Custom fit config function provided\n        config = self.on_fit_config_fn(server_round)\n\n    property_ins = GetPropertiesIns(config)\n\n    if isinstance(client_manager, BaseFractionSamplingManager):\n        clients = client_manager.sample_all(min_num_clients=self.min_available_clients)\n    else:\n        # Grab all available clients using the basic Flower client manager\n        num_available_clients = client_manager.num_available()\n        clients = client_manager.sample(num_available_clients, min_num_clients=self.min_available_clients)\n\n    # Return client/config pairs\n    return [(client, property_ins) for client in clients]\n</code></pre>"},{"location":"api/#fl4health.strategies.basic_fedavg.BasicFedAvg.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Aggregate the results from the federated fit round. This is done with either weighted or unweighted FedAvg, depending on the settings used for the strategy.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The client identifiers and the results of their local training that need to be aggregated on the server-side.</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>These are the results and exceptions from clients that experienced an issue during training, such as timeouts or exceptions.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar]]</code> <p>The aggregated model weights and the metrics dictionary.</p> Source code in <code>fl4health/strategies/basic_fedavg.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate the results from the federated fit round. This is done with either weighted or unweighted FedAvg,\n    depending on the settings used for the strategy.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n            that need to be aggregated on the server-side.\n        failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n            from clients that experienced an issue during training, such as timeouts or exceptions.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n    # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n    # reducing numerical fluctuation.\n    decoded_and_sorted_results = [\n        (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n    ]\n\n    # Aggregate them in a weighted or unweighted fashion based on settings.\n    aggregated_arrays = aggregate_results(decoded_and_sorted_results, self.weighted_aggregation)\n    # Convert back to parameters\n    parameters_aggregated = ndarrays_to_parameters(aggregated_arrays)\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.fit_metrics_aggregation_fn:\n        fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n    return parameters_aggregated, metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.basic_fedavg.BasicFedAvg.aggregate_evaluate","title":"<code>aggregate_evaluate(server_round, results, failures)</code>","text":"<p>Aggregate the metrics and losses returned from the clients as a result of the evaluation round.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Current FL server Round.</p> required <code>results</code> <code>list[tuple[ClientProxy, EvaluateRes]]</code> <p>The client identifiers and the results of their local evaluation that need to be aggregated on the server-side. These results are loss values and the metrics dictionary.</p> required <code>failures</code> <code>list[tuple[ClientProxy, EvaluateRes] | BaseException]</code> <p>These are the results and exceptions from clients that experienced an issue during evaluation, such as timeouts or exceptions.</p> required <p>Returns:</p> Type Description <code>tuple[float | None, dict[str, Scalar]]</code> <p>Aggregated loss values and the aggregated metrics. The metrics are aggregated according to <code>evaluate_metrics_aggregation_fn</code>.</p> Source code in <code>fl4health/strategies/basic_fedavg.py</code> <pre><code>def aggregate_evaluate(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, EvaluateRes]],\n    failures: list[tuple[ClientProxy, EvaluateRes] | BaseException],\n) -&gt; tuple[float | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate the metrics and losses returned from the clients as a result of the evaluation round.\n\n    Args:\n        server_round (int): Current FL server Round.\n        results (list[tuple[ClientProxy, EvaluateRes]]): The client identifiers and the results of their local\n            evaluation that need to be aggregated on the server-side. These results are loss values and the\n            metrics dictionary.\n        failures (list[tuple[ClientProxy, EvaluateRes]  |  BaseException]): These are the results and\n            exceptions from clients that experienced an issue during evaluation, such as timeouts or exceptions.\n\n    Returns:\n        (tuple[float | None, dict[str, Scalar]]): Aggregated loss values and the aggregated metrics. The metrics\n            are aggregated according to ``evaluate_metrics_aggregation_fn``.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Get losses and number of examples from the evaluation results.\n    loss_results = [(evaluate_res.num_examples, evaluate_res.loss) for _, evaluate_res in results]\n    # Then aggregate the losses\n    loss_aggregated = aggregate_losses(loss_results, self.weighted_eval_losses)\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.evaluate_metrics_aggregation_fn:\n        eval_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.evaluate_metrics_aggregation_fn(eval_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No evaluate_metrics_aggregation_fn provided\")\n\n    return loss_aggregated, metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.basic_fedavg.OpacusBasicFedAvg","title":"<code>OpacusBasicFedAvg</code>","text":"<p>               Bases: <code>BasicFedAvg</code></p> <p>Configurable FedAvg strategy implementation.</p> Source code in <code>fl4health/strategies/basic_fedavg.py</code> <pre><code>class OpacusBasicFedAvg(BasicFedAvg):\n    \"\"\"Configurable FedAvg strategy implementation.\"\"\"\n\n    # pylint: disable=too-many-arguments,too-many-instance-attributes\n    def __init__(\n        self,\n        *,\n        model: GradSampleModule,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        weighted_aggregation: bool = True,\n        weighted_eval_losses: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        This strategy is a simple extension of the BasicFedAvg strategy to force the model being federally trained to\n        be an valid Opacus ``GradSampleModule`` and, thereby, ensure that associated the parameters are aligned with\n        those of Opacus based models used by the ``InstanceLevelDpClient``.\n\n        Args:\n            model (GradSampleModule): The model architecture to be federally trained. When using this strategy,\n                the model must be of type Opacus ``GradSampleModule``. This model will then be used to set\n                ``initialize_parameters`` as the initial parameters to be used by all clients.\n            fraction_fit (float, optional): Fraction of clients used during training. In case ``min_fit_clients`` is\n                larger than ``fraction_fit * available_clients``, ``min_fit_clients`` will still be sampled.\n                Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. In case\n                ``min_evaluate_clients`` is larger than ``fraction_evaluate * available_clients``,\n                ``min_evaluate_clients`` will still be sampled. Defaults to 1.0.\n            min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for central server-side evaluation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                client-side validation by providing a ``Config`` dictionary. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n                average or a uniform average. FedAvg default is weighted average by client dataset counts.\n                Defaults to True.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n                counts. Defaults to True.\n        \"\"\"\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_fit_clients=min_fit_clients,\n            min_evaluate_clients=min_evaluate_clients,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            weighted_aggregation=weighted_aggregation,\n            weighted_eval_losses=weighted_eval_losses,\n        )\n        assert isinstance(model, GradSampleModule), \"Provided model must be Opacus type GradSampleModule\"\n        # Setting the initial parameters to correspond with those of the provided model\n        self.initial_parameters = get_all_model_parameters(model)\n</code></pre>"},{"location":"api/#fl4health.strategies.basic_fedavg.OpacusBasicFedAvg.__init__","title":"<code>__init__(*, model, fraction_fit=1.0, fraction_evaluate=1.0, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, weighted_aggregation=True, weighted_eval_losses=True)</code>","text":"<p>This strategy is a simple extension of the BasicFedAvg strategy to force the model being federally trained to be an valid Opacus <code>GradSampleModule</code> and, thereby, ensure that associated the parameters are aligned with those of Opacus based models used by the <code>InstanceLevelDpClient</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>GradSampleModule</code> <p>The model architecture to be federally trained. When using this strategy, the model must be of type Opacus <code>GradSampleModule</code>. This model will then be used to set <code>initialize_parameters</code> as the initial parameters to be used by all clients.</p> required <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. In case <code>min_fit_clients</code> is larger than <code>fraction_fit * available_clients</code>, <code>min_fit_clients</code> will still be sampled. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. In case <code>min_evaluate_clients</code> is larger than <code>fraction_evaluate * available_clients</code>, <code>min_evaluate_clients</code> will still be sampled. Defaults to 1.0.</p> <code>1.0</code> <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during training. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of total clients in the system. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>weighted_aggregation</code> <code>bool</code> <p>Determines whether parameter aggregation is a linearly weighted average or a uniform average. FedAvg default is weighted average by client dataset counts. Defaults to True.</p> <code>True</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. FedAvg default is weighted average of the losses by client dataset counts. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/strategies/basic_fedavg.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model: GradSampleModule,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    weighted_aggregation: bool = True,\n    weighted_eval_losses: bool = True,\n) -&gt; None:\n    \"\"\"\n    This strategy is a simple extension of the BasicFedAvg strategy to force the model being federally trained to\n    be an valid Opacus ``GradSampleModule`` and, thereby, ensure that associated the parameters are aligned with\n    those of Opacus based models used by the ``InstanceLevelDpClient``.\n\n    Args:\n        model (GradSampleModule): The model architecture to be federally trained. When using this strategy,\n            the model must be of type Opacus ``GradSampleModule``. This model will then be used to set\n            ``initialize_parameters`` as the initial parameters to be used by all clients.\n        fraction_fit (float, optional): Fraction of clients used during training. In case ``min_fit_clients`` is\n            larger than ``fraction_fit * available_clients``, ``min_fit_clients`` will still be sampled.\n            Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. In case\n            ``min_evaluate_clients`` is larger than ``fraction_evaluate * available_clients``,\n            ``min_evaluate_clients`` will still be sampled. Defaults to 1.0.\n        min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for central server-side evaluation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            client-side validation by providing a ``Config`` dictionary. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n            average or a uniform average. FedAvg default is weighted average by client dataset counts.\n            Defaults to True.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n            counts. Defaults to True.\n    \"\"\"\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_fit_clients=min_fit_clients,\n        min_evaluate_clients=min_evaluate_clients,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        weighted_aggregation=weighted_aggregation,\n        weighted_eval_losses=weighted_eval_losses,\n    )\n    assert isinstance(model, GradSampleModule), \"Provided model must be Opacus type GradSampleModule\"\n    # Setting the initial parameters to correspond with those of the provided model\n    self.initial_parameters = get_all_model_parameters(model)\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm","title":"<code>client_dp_fedavgm</code>","text":""},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM","title":"<code>ClientLevelDPFedAvgM</code>","text":"<p>               Bases: <code>BasicFedAvg</code></p> Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>class ClientLevelDPFedAvgM(BasicFedAvg):\n    # pylint: disable=too-many-arguments,too-many-instance-attributes\n    def __init__(\n        self,\n        *,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters | None = None,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        weighted_aggregation: bool = False,\n        weighted_eval_losses: bool = True,\n        per_client_example_cap: float | None = None,\n        adaptive_clipping: bool = False,\n        server_learning_rate: float = 1.0,\n        clipping_learning_rate: float = 1.0,\n        clipping_quantile: float = 0.5,\n        initial_clipping_bound: float = 0.1,\n        weight_noise_multiplier: float = 1.0,\n        clipping_noise_multiplier: float = 1.0,\n        beta: float = 0.9,\n    ) -&gt; None:\n        \"\"\"\n        This strategy implements the Federated Learning with client-level DP approach discussed in\n        Differentially Private Learning with Adaptive Clipping. This function provides a noised version of unweighted\n        ``FedAvgM``.\n\n        Paper: https://arxiv.org/abs/1905.03871\n\n        **NOTE**: It assumes that the models are packaging clipping bits along with the model parameters. If adaptive\n        clipping is false, these bits will simply be 0.\n\n        If enabled, it performs adaptive clipping rather than fixed threshold clipping.\n\n        Args:\n            fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n            min_available_clients (int, optional): Minimum number of clients used during validation.\n                Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for central server-side evaluation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional):\n                Function used to configure client-side validation by providing a ``Config`` dictionary.\n                Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            initial_parameters (Parameters | None, optional): Initial global model parameters.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            weighted_aggregation (bool, optional): Determines whether the FedAvg update is weighted by client dataset\n                size or unweighted. Defaults to False.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n                counts. Defaults to True.\n            per_client_example_cap (float | None, optional): The maximum number samples per client. \\\\(\\\\hat{w}\\\\) in\n                https://arxiv.org/pdf/1710.06963.pdf. Defaults to None.\n            adaptive_clipping (bool, optional): If enabled, the model expects the last entry of the parameter list to\n                be a binary value indicating whether or not the batch gradient was clipped. Defaults to False.\n            server_learning_rate (float, optional): Learning rate for the server side updates. Defaults to 1.0.\n            clipping_learning_rate (float, optional): Learning rate for the clipping bound. Only used if adaptive\n                clipping is turned on. Defaults to 1.0.\n            clipping_quantile (float, optional): Quantile we are trying to estimate in adaptive clipping.\n                i.e. \\\\(P(\\\\Vert g \\\\Vert &lt; C_t) \\\\approx\\\\) ``clipping_quantile``. Only used if adaptive clipping\n                is turned on. Defaults to 0.5.\n            initial_clipping_bound (float, optional):  Initial guess for the clipping bound corresponding to the\n                clipping quantile described above.\n\n                **NOTE**: If Adaptive clipping is turned off, this is the clipping bound through out FL training.\n\n                Defaults to 0.1.\n            weight_noise_multiplier (float, optional): Noise multiplier for the noising of gradients. Defaults to 1.0.\n            clipping_noise_multiplier (float, optional): Noise multiplier for the noising of clipping bits.\n                Defaults to 1.0.\n            beta (float, optional): Momentum weight for previous weight updates. If it is 0, there is no momentum.\n                Defaults to 0.9.\n        \"\"\"\n        assert 0.0 &lt;= clipping_quantile &lt;= 1.0\n        self.clipping_bound = initial_clipping_bound\n\n        if initial_parameters:\n            self.add_auxiliary_information(initial_parameters)\n\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            weighted_aggregation=weighted_aggregation,\n            weighted_eval_losses=weighted_eval_losses,\n        )\n        # If per_client_example_cap is None, it will be set as the total samples across clients\n        self.per_client_example_cap = per_client_example_cap\n        self.adaptive_clipping = adaptive_clipping\n        self.server_learning_rate = server_learning_rate\n        self.clipping_learning_rate = clipping_learning_rate\n        self.clipping_quantile = clipping_quantile\n        self.weight_noise_multiplier = weight_noise_multiplier\n        self.clipping_noise_multiplier = clipping_noise_multiplier\n        self.beta = beta\n\n        # Parameter Packer to handle packing and unpacking parameters with clipping bit\n        self.parameter_packer = ParameterPackerWithClippingBit()\n\n        # Weighted averaging requires list of sample counts\n        # to compute client weights. Set by server after polling clients.\n        self.sample_counts: list[int] | None = None\n        self.m_t: NDArrays | None = None\n\n    def __repr__(self) -&gt; str:\n        \"\"\"\n        Printable representation of the object.\n\n        Returns:\n            (str): Printable representation of the object.\n        \"\"\"\n        return f\"ClientLevelDPFedAvgM(accept_failures={self.accept_failures})\"\n\n    def add_auxiliary_information(self, original_parameters: Parameters) -&gt; None:\n        \"\"\"\n        Function for adding in the ``clipping_bound`` to the provided set of parameters. This function is meant to be\n        called after a server requests model weight initialization from a client, allowing the proper information to\n        be included with the model parameters when sent to all clients for model initialization etc.\n\n        Args:\n            original_parameters (Parameters): Original set of parameters provided by a client for model weight\n                initialization\n        \"\"\"\n        # Copy the model parameters into NDArrays for storage\n        self.current_weights = parameters_to_ndarrays(original_parameters)\n        # Add the clipping bound to the original parameters\n        original_parameters.tensors.append(ndarray_to_bytes(np.array([self.clipping_bound])))\n\n    def modify_noise_multiplier(self) -&gt; float:\n        \"\"\"\n        Modifying the noise multiplier as in Algorithm 1 of Differentially Private Learning with Adaptive Clipping.\n        This is done to ensure the privacy accountant computes the correct privacy values.\n\n        Raises:\n            ValueError: If the noise multiplier and the clipping noise multiplier are not well related then we'll end\n                up with a sqrt of a negative number. If this happens a value error is raised.\n\n        Returns:\n            (float): The modified noise multiplier when performing adaptive clipping.\n        \"\"\"\n        # Modifying the noise multiplier as in Algorithm 1 of Differentially Private Learning with Adaptive Clipping\n        sqrt_argument = pow(self.weight_noise_multiplier, -2.0) - pow(2.0 * self.clipping_noise_multiplier, -2.0)\n        if sqrt_argument &lt; 0.0:\n            raise ValueError(\n                \"Noise Multiplier modification will fail. The relationship of the weight and clipping noise \"\n                f\"multipliers leads to negative sqrt argument {sqrt_argument}\"\n            )\n        return pow(sqrt_argument, -0.5)\n\n    def split_model_weights_and_clipping_bits(\n        self, results: list[tuple[ClientProxy, FitRes]]\n    ) -&gt; tuple[list[tuple[NDArrays, int]], NDArrays]:\n        \"\"\"\n        Given results from an FL round of training, this function splits the result into sets of (weights,\n        training counts) and clipping bits. The split is required because the clipping bits are packed with the\n        weights in order to communicate them back to the server. The parameter packer facilitates this splitting.\n\n        Args:\n            results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n                that need to be aggregated on the server-side. In this strategy, the clients pack the weights to be\n                aggregated along with a clipping bit calculated during training.\n\n        Returns:\n            (tuple[list[tuple[NDArrays, int]], NDArrays]): The first tuple is the set of (weights, training counts) per\n                client. The second is a set of clipping bits, one for each client.\n        \"\"\"\n        # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n        # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n        # reducing numerical fluctuation.\n        decoded_and_sorted_results = [\n            (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n        ]\n\n        weights_and_counts: list[tuple[NDArrays, int]] = []\n        clipping_bits: NDArrays = []\n        for weights, sample_count in decoded_and_sorted_results:\n            updated_weights, clipping_bit = self.parameter_packer.unpack_parameters(weights)\n            weights_and_counts.append((updated_weights, sample_count))\n            clipping_bits.append(np.array(clipping_bit))\n\n        return weights_and_counts, clipping_bits\n\n    def calculate_update_with_momentum(self, weights_update: NDArrays) -&gt; None:\n        \"\"\"\n        Performs a weight update with momentum. That is, combining some weighted value of the previous update with\n        the current update.\n\n        Args:\n            weights_update (NDArrays): The current update after the weights have been aggregated from the training\n                round.\n        \"\"\"\n        if not self.m_t:\n            self.m_t = weights_update\n        else:\n            self.m_t = [\n                # NOTE: This is not normalized (beta vs. 1-beta) as used in the original implementation\n                self.beta * prev_layer_update + noised_layer_update\n                for prev_layer_update, noised_layer_update in zip(self.m_t, weights_update)\n            ]\n\n    def update_current_weights(self) -&gt; None:\n        r\"\"\"\n        This function updates each of the layer weights using the server learning rate and the \\(m_t\\) values\n        (computed with or without momentum).\n\n        **NOTE**: It assumes that the values in \\(m_t\\) are **UPDATES** rather than raw weights.\n        \"\"\"\n        assert self.m_t is not None\n        self.current_weights = [\n            current_layer_weight + self.server_learning_rate * layer_mt\n            for current_layer_weight, layer_mt in zip(self.current_weights, self.m_t)\n        ]\n\n    def _update_clipping_bound_with_noised_bits(\n        self,\n        noised_clipping_bits: float,\n    ) -&gt; None:\n        \"\"\"\n        Update the clipping bound help by the server given the noised aggregated clipping bits returned by the clients.\n\n        **NOTE**: The update formula may be found in the original paper.\n\n        Args:\n            noised_clipping_bits (float): This is the aggregated noised clipping bits derived from the clients.\n        \"\"\"\n        self.clipping_bound = self.clipping_bound * math.exp(\n            -self.clipping_learning_rate * (noised_clipping_bits - self.clipping_quantile)\n        )\n\n    def update_clipping_bound(self, clipping_bits: NDArrays) -&gt; None:\n        \"\"\"\n        This adds noise to the clipping bits returned by the clients and then updates the server-side clipping bound\n        using this information.\n\n        Args:\n            clipping_bits (NDArrays): Bits associated with each of the clients. These are to be noised and aggregated\n                in order to update the clipping bound on the server side.\n        \"\"\"\n        noised_clipping_bits_sum = gaussian_noisy_aggregate_clipping_bits(\n            clipping_bits, self.clipping_noise_multiplier\n        )\n        self._update_clipping_bound_with_noised_bits(noised_clipping_bits_sum)\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate fit using averaging of weights (can be unweighted or weighted) and inject noise and optionally\n        perform adaptive clipping updates.\n\n        **NOTE**: This assumes that the model weights sent back by the clients are **UPDATES** rather than raw weights.\n        That is they are ``theta_client - theta_server`` rather than just ``theta_client``.\n\n        **NOTE**: this function packs the clipping bound for clients as the last member of the parameters list.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n                that need to be aggregated on the server-side. In this strategy, the clients pack the weights to be\n                aggregated along with a clipping bit calculated during their local training cycle.\n            failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n                from clients that experienced an issue during training, such as timeouts or exceptions.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n                For this strategy, the server also packs a clipping bound to be sent to the clients. This is sent even\n                if adaptive clipping is turned off and the value simply remains constant.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # If we're doing weighted aggregation, we need to compute a per_client_example_cap. If it has been provided\n        # manually, we skip this step. Otherwise, we perform the necessary calculations.\n        if self.weighted_aggregation and self.per_client_example_cap is None:\n            assert self.sample_counts is not None\n\n            total_samples = sum(self.sample_counts)\n\n            self.per_client_example_cap = (\n                total_samples if self.per_client_example_cap is None else self.per_client_example_cap\n            )\n\n            self.total_client_weight: float = sum(\n                [sample_count / self.per_client_example_cap for sample_count in self.sample_counts]\n            )\n\n        # Convert results with packed params of model weights and clipping bits\n        weights_and_counts, clipping_bits = self.split_model_weights_and_clipping_bits(results)\n\n        noise_multiplier = self.weight_noise_multiplier\n        if self.adaptive_clipping:\n            # The noise multiplier need only be modified in the event of using adaptive clipping to account for the\n            # extra gradient information used to adapt the clipping threshold.\n            noise_multiplier = self.modify_noise_multiplier()\n            self.update_clipping_bound(clipping_bits)\n            log(INFO, f\"New Clipping Bound is: {self.clipping_bound}\")\n\n        if self.weighted_aggregation:\n            assert self.per_client_example_cap is not None\n            noised_aggregated_update = gaussian_noisy_weighted_aggregate(\n                weights_and_counts,\n                noise_multiplier,\n                self.clipping_bound,\n                self.fraction_fit,\n                self.per_client_example_cap,\n                self.total_client_weight,\n            )\n        else:\n            noised_aggregated_update = gaussian_noisy_unweighted_aggregate(\n                weights_and_counts,\n                noise_multiplier,\n                self.clipping_bound,\n            )\n\n        # momentum calculation\n        self.calculate_update_with_momentum(noised_aggregated_update)\n        self.update_current_weights()\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.fit_metrics_aggregation_fn:\n            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n        # Weights plus the clipping bound to be used by the clients\n        packed_ndarrays = self.parameter_packer.pack_parameters(self.current_weights, self.clipping_bound)\n        return ndarrays_to_parameters(packed_ndarrays), metrics_aggregated\n\n    def configure_fit(\n        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n    ) -&gt; list[tuple[ClientProxy, FitIns]]:\n        \"\"\"\n        This function configures a sample of clients for a training round. Due to the privacy accounting, this strategy\n        requires that the sampling manager be of type ``BaseFractionSamplingManager``.\n\n        The function follows the standard configuration flow where the ``on_fit_config_fn`` function is used to produce\n        configurations to be sent to all clients. These are packaged with the provided parameters and set over to the\n        clients.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            parameters (Parameters): The parameters to be used to initialize the clients for the fit round.\n            client_manager (ClientManager): The manager used to sample the clients. Currently we restrict this to\n                be ``BaseFractionSamplingManager``, which has a ``sample_fraction`` function built in.\n\n        Returns:\n            (list[tuple[ClientProxy, FitIns]]): List of sampled client identifiers and the configuration/parameters to\n                be sent to each client (packaged as ``FitIns``).\n        \"\"\"\n        # This strategy requires the client manager to be of type at least BaseFractionSamplingManager\n        assert isinstance(client_manager, BaseFractionSamplingManager)\n        config = {}\n        if self.on_fit_config_fn is not None:\n            # Custom fit config function provided\n            config = self.on_fit_config_fn(server_round)\n        else:\n            config = {\"current_server_round\": server_round}\n\n        fit_ins = FitIns(parameters, config)\n\n        clients = client_manager.sample_fraction(self.fraction_fit, self.min_available_clients)\n\n        # Return client/config pairs\n        return [(client, fit_ins) for client in clients]\n\n    def configure_evaluate(\n        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n    ) -&gt; list[tuple[ClientProxy, EvaluateIns]]:\n        \"\"\"\n        This function configures a sample of clients for an eval round. Due to the privacy accounting, this strategy\n        requires that the sampling manager be of type ``BaseFractionSamplingManager``.\n\n        The function follows the standard configuration flow where the ``on_evaluate_config_fn`` function is used to\n        produce configurations to be sent to all clients. These are packaged with the provided parameters and set over\n        to the clients.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            parameters (Parameters): The parameters to be used to initialize the clients for the eval round.\n            client_manager (ClientManager): The manager used to grab all of the clients. Currently we restrict this to\n                be ``BaseFractionSamplingManager``, which has a ``sample_fraction`` function built in.\n\n        Returns:\n            (list[tuple[ClientProxy, EvaluateIns]]): List of sampled client identifiers and the\n                configuration/parameters to be sent to each client (packaged as ``EvaluateIns``).\n        \"\"\"\n        # This strategy requires the client manager to be of type at least BaseFractionSamplingManager\n        assert isinstance(client_manager, BaseFractionSamplingManager)\n\n        # Do not configure federated evaluation if fraction eval is 0 or server is not initialized\n        if self.fraction_evaluate == 0.0:\n            return []\n\n        # Parameters and config\n        config = {}\n        if self.on_evaluate_config_fn is not None:\n            # Custom evaluation config function provided\n            config = self.on_evaluate_config_fn(server_round)\n        else:\n            config = {\"current_server_round\": server_round}\n        evaluate_ins = EvaluateIns(parameters, config)\n\n        # Sample clients\n        clients = client_manager.sample_fraction(self.fraction_evaluate, self.min_available_clients)\n\n        # Return client/config pairs\n        return [(client, evaluate_ins) for client in clients]\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.__init__","title":"<code>__init__(*, fraction_fit=1.0, fraction_evaluate=1.0, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters=None, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, weighted_aggregation=False, weighted_eval_losses=True, per_client_example_cap=None, adaptive_clipping=False, server_learning_rate=1.0, clipping_learning_rate=1.0, clipping_quantile=0.5, initial_clipping_bound=0.1, weight_noise_multiplier=1.0, clipping_noise_multiplier=1.0, beta=0.9)</code>","text":"<p>This strategy implements the Federated Learning with client-level DP approach discussed in Differentially Private Learning with Adaptive Clipping. This function provides a noised version of unweighted <code>FedAvgM</code>.</p> <p>Paper: https://arxiv.org/abs/1905.03871</p> <p>NOTE: It assumes that the models are packaging clipping bits along with the model parameters. If adaptive clipping is false, these bits will simply be 0.</p> <p>If enabled, it performs adaptive clipping rather than fixed threshold clipping.</p> <p>Parameters:</p> Name Type Description Default <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. Defaults to 1.0.</p> <code>1.0</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>initial_parameters</code> <code>Parameters | None</code> <p>Initial global model parameters.</p> <code>None</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>weighted_aggregation</code> <code>bool</code> <p>Determines whether the FedAvg update is weighted by client dataset size or unweighted. Defaults to False.</p> <code>False</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. FedAvg default is weighted average of the losses by client dataset counts. Defaults to True.</p> <code>True</code> <code>per_client_example_cap</code> <code>float | None</code> <p>The maximum number samples per client. \\(\\hat{w}\\) in https://arxiv.org/pdf/1710.06963.pdf. Defaults to None.</p> <code>None</code> <code>adaptive_clipping</code> <code>bool</code> <p>If enabled, the model expects the last entry of the parameter list to be a binary value indicating whether or not the batch gradient was clipped. Defaults to False.</p> <code>False</code> <code>server_learning_rate</code> <code>float</code> <p>Learning rate for the server side updates. Defaults to 1.0.</p> <code>1.0</code> <code>clipping_learning_rate</code> <code>float</code> <p>Learning rate for the clipping bound. Only used if adaptive clipping is turned on. Defaults to 1.0.</p> <code>1.0</code> <code>clipping_quantile</code> <code>float</code> <p>Quantile we are trying to estimate in adaptive clipping. i.e. \\(P(\\Vert g \\Vert &lt; C_t) \\approx\\) <code>clipping_quantile</code>. Only used if adaptive clipping is turned on. Defaults to 0.5.</p> <code>0.5</code> <code>initial_clipping_bound</code> <code>float</code> <p>Initial guess for the clipping bound corresponding to the clipping quantile described above.</p> <p>NOTE: If Adaptive clipping is turned off, this is the clipping bound through out FL training.</p> <p>Defaults to 0.1.</p> <code>0.1</code> <code>weight_noise_multiplier</code> <code>float</code> <p>Noise multiplier for the noising of gradients. Defaults to 1.0.</p> <code>1.0</code> <code>clipping_noise_multiplier</code> <code>float</code> <p>Noise multiplier for the noising of clipping bits. Defaults to 1.0.</p> <code>1.0</code> <code>beta</code> <code>float</code> <p>Momentum weight for previous weight updates. If it is 0, there is no momentum. Defaults to 0.9.</p> <code>0.9</code> Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters | None = None,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    weighted_aggregation: bool = False,\n    weighted_eval_losses: bool = True,\n    per_client_example_cap: float | None = None,\n    adaptive_clipping: bool = False,\n    server_learning_rate: float = 1.0,\n    clipping_learning_rate: float = 1.0,\n    clipping_quantile: float = 0.5,\n    initial_clipping_bound: float = 0.1,\n    weight_noise_multiplier: float = 1.0,\n    clipping_noise_multiplier: float = 1.0,\n    beta: float = 0.9,\n) -&gt; None:\n    \"\"\"\n    This strategy implements the Federated Learning with client-level DP approach discussed in\n    Differentially Private Learning with Adaptive Clipping. This function provides a noised version of unweighted\n    ``FedAvgM``.\n\n    Paper: https://arxiv.org/abs/1905.03871\n\n    **NOTE**: It assumes that the models are packaging clipping bits along with the model parameters. If adaptive\n    clipping is false, these bits will simply be 0.\n\n    If enabled, it performs adaptive clipping rather than fixed threshold clipping.\n\n    Args:\n        fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n        min_available_clients (int, optional): Minimum number of clients used during validation.\n            Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for central server-side evaluation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional):\n            Function used to configure client-side validation by providing a ``Config`` dictionary.\n            Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        initial_parameters (Parameters | None, optional): Initial global model parameters.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        weighted_aggregation (bool, optional): Determines whether the FedAvg update is weighted by client dataset\n            size or unweighted. Defaults to False.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n            counts. Defaults to True.\n        per_client_example_cap (float | None, optional): The maximum number samples per client. \\\\(\\\\hat{w}\\\\) in\n            https://arxiv.org/pdf/1710.06963.pdf. Defaults to None.\n        adaptive_clipping (bool, optional): If enabled, the model expects the last entry of the parameter list to\n            be a binary value indicating whether or not the batch gradient was clipped. Defaults to False.\n        server_learning_rate (float, optional): Learning rate for the server side updates. Defaults to 1.0.\n        clipping_learning_rate (float, optional): Learning rate for the clipping bound. Only used if adaptive\n            clipping is turned on. Defaults to 1.0.\n        clipping_quantile (float, optional): Quantile we are trying to estimate in adaptive clipping.\n            i.e. \\\\(P(\\\\Vert g \\\\Vert &lt; C_t) \\\\approx\\\\) ``clipping_quantile``. Only used if adaptive clipping\n            is turned on. Defaults to 0.5.\n        initial_clipping_bound (float, optional):  Initial guess for the clipping bound corresponding to the\n            clipping quantile described above.\n\n            **NOTE**: If Adaptive clipping is turned off, this is the clipping bound through out FL training.\n\n            Defaults to 0.1.\n        weight_noise_multiplier (float, optional): Noise multiplier for the noising of gradients. Defaults to 1.0.\n        clipping_noise_multiplier (float, optional): Noise multiplier for the noising of clipping bits.\n            Defaults to 1.0.\n        beta (float, optional): Momentum weight for previous weight updates. If it is 0, there is no momentum.\n            Defaults to 0.9.\n    \"\"\"\n    assert 0.0 &lt;= clipping_quantile &lt;= 1.0\n    self.clipping_bound = initial_clipping_bound\n\n    if initial_parameters:\n        self.add_auxiliary_information(initial_parameters)\n\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        weighted_aggregation=weighted_aggregation,\n        weighted_eval_losses=weighted_eval_losses,\n    )\n    # If per_client_example_cap is None, it will be set as the total samples across clients\n    self.per_client_example_cap = per_client_example_cap\n    self.adaptive_clipping = adaptive_clipping\n    self.server_learning_rate = server_learning_rate\n    self.clipping_learning_rate = clipping_learning_rate\n    self.clipping_quantile = clipping_quantile\n    self.weight_noise_multiplier = weight_noise_multiplier\n    self.clipping_noise_multiplier = clipping_noise_multiplier\n    self.beta = beta\n\n    # Parameter Packer to handle packing and unpacking parameters with clipping bit\n    self.parameter_packer = ParameterPackerWithClippingBit()\n\n    # Weighted averaging requires list of sample counts\n    # to compute client weights. Set by server after polling clients.\n    self.sample_counts: list[int] | None = None\n    self.m_t: NDArrays | None = None\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.__repr__","title":"<code>__repr__()</code>","text":"<p>Printable representation of the object.</p> <p>Returns:</p> Type Description <code>str</code> <p>Printable representation of the object.</p> Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"\n    Printable representation of the object.\n\n    Returns:\n        (str): Printable representation of the object.\n    \"\"\"\n    return f\"ClientLevelDPFedAvgM(accept_failures={self.accept_failures})\"\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.add_auxiliary_information","title":"<code>add_auxiliary_information(original_parameters)</code>","text":"<p>Function for adding in the <code>clipping_bound</code> to the provided set of parameters. This function is meant to be called after a server requests model weight initialization from a client, allowing the proper information to be included with the model parameters when sent to all clients for model initialization etc.</p> <p>Parameters:</p> Name Type Description Default <code>original_parameters</code> <code>Parameters</code> <p>Original set of parameters provided by a client for model weight initialization</p> required Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def add_auxiliary_information(self, original_parameters: Parameters) -&gt; None:\n    \"\"\"\n    Function for adding in the ``clipping_bound`` to the provided set of parameters. This function is meant to be\n    called after a server requests model weight initialization from a client, allowing the proper information to\n    be included with the model parameters when sent to all clients for model initialization etc.\n\n    Args:\n        original_parameters (Parameters): Original set of parameters provided by a client for model weight\n            initialization\n    \"\"\"\n    # Copy the model parameters into NDArrays for storage\n    self.current_weights = parameters_to_ndarrays(original_parameters)\n    # Add the clipping bound to the original parameters\n    original_parameters.tensors.append(ndarray_to_bytes(np.array([self.clipping_bound])))\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.modify_noise_multiplier","title":"<code>modify_noise_multiplier()</code>","text":"<p>Modifying the noise multiplier as in Algorithm 1 of Differentially Private Learning with Adaptive Clipping. This is done to ensure the privacy accountant computes the correct privacy values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the noise multiplier and the clipping noise multiplier are not well related then we'll end up with a sqrt of a negative number. If this happens a value error is raised.</p> <p>Returns:</p> Type Description <code>float</code> <p>The modified noise multiplier when performing adaptive clipping.</p> Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def modify_noise_multiplier(self) -&gt; float:\n    \"\"\"\n    Modifying the noise multiplier as in Algorithm 1 of Differentially Private Learning with Adaptive Clipping.\n    This is done to ensure the privacy accountant computes the correct privacy values.\n\n    Raises:\n        ValueError: If the noise multiplier and the clipping noise multiplier are not well related then we'll end\n            up with a sqrt of a negative number. If this happens a value error is raised.\n\n    Returns:\n        (float): The modified noise multiplier when performing adaptive clipping.\n    \"\"\"\n    # Modifying the noise multiplier as in Algorithm 1 of Differentially Private Learning with Adaptive Clipping\n    sqrt_argument = pow(self.weight_noise_multiplier, -2.0) - pow(2.0 * self.clipping_noise_multiplier, -2.0)\n    if sqrt_argument &lt; 0.0:\n        raise ValueError(\n            \"Noise Multiplier modification will fail. The relationship of the weight and clipping noise \"\n            f\"multipliers leads to negative sqrt argument {sqrt_argument}\"\n        )\n    return pow(sqrt_argument, -0.5)\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.split_model_weights_and_clipping_bits","title":"<code>split_model_weights_and_clipping_bits(results)</code>","text":"<p>Given results from an FL round of training, this function splits the result into sets of (weights, training counts) and clipping bits. The split is required because the clipping bits are packed with the weights in order to communicate them back to the server. The parameter packer facilitates this splitting.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The client identifiers and the results of their local training that need to be aggregated on the server-side. In this strategy, the clients pack the weights to be aggregated along with a clipping bit calculated during training.</p> required <p>Returns:</p> Type Description <code>tuple[list[tuple[NDArrays, int]], NDArrays]</code> <p>The first tuple is the set of (weights, training counts) per client. The second is a set of clipping bits, one for each client.</p> Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def split_model_weights_and_clipping_bits(\n    self, results: list[tuple[ClientProxy, FitRes]]\n) -&gt; tuple[list[tuple[NDArrays, int]], NDArrays]:\n    \"\"\"\n    Given results from an FL round of training, this function splits the result into sets of (weights,\n    training counts) and clipping bits. The split is required because the clipping bits are packed with the\n    weights in order to communicate them back to the server. The parameter packer facilitates this splitting.\n\n    Args:\n        results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n            that need to be aggregated on the server-side. In this strategy, the clients pack the weights to be\n            aggregated along with a clipping bit calculated during training.\n\n    Returns:\n        (tuple[list[tuple[NDArrays, int]], NDArrays]): The first tuple is the set of (weights, training counts) per\n            client. The second is a set of clipping bits, one for each client.\n    \"\"\"\n    # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n    # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n    # reducing numerical fluctuation.\n    decoded_and_sorted_results = [\n        (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n    ]\n\n    weights_and_counts: list[tuple[NDArrays, int]] = []\n    clipping_bits: NDArrays = []\n    for weights, sample_count in decoded_and_sorted_results:\n        updated_weights, clipping_bit = self.parameter_packer.unpack_parameters(weights)\n        weights_and_counts.append((updated_weights, sample_count))\n        clipping_bits.append(np.array(clipping_bit))\n\n    return weights_and_counts, clipping_bits\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.calculate_update_with_momentum","title":"<code>calculate_update_with_momentum(weights_update)</code>","text":"<p>Performs a weight update with momentum. That is, combining some weighted value of the previous update with the current update.</p> <p>Parameters:</p> Name Type Description Default <code>weights_update</code> <code>NDArrays</code> <p>The current update after the weights have been aggregated from the training round.</p> required Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def calculate_update_with_momentum(self, weights_update: NDArrays) -&gt; None:\n    \"\"\"\n    Performs a weight update with momentum. That is, combining some weighted value of the previous update with\n    the current update.\n\n    Args:\n        weights_update (NDArrays): The current update after the weights have been aggregated from the training\n            round.\n    \"\"\"\n    if not self.m_t:\n        self.m_t = weights_update\n    else:\n        self.m_t = [\n            # NOTE: This is not normalized (beta vs. 1-beta) as used in the original implementation\n            self.beta * prev_layer_update + noised_layer_update\n            for prev_layer_update, noised_layer_update in zip(self.m_t, weights_update)\n        ]\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.update_current_weights","title":"<code>update_current_weights()</code>","text":"<p>This function updates each of the layer weights using the server learning rate and the \\(m_t\\) values (computed with or without momentum).</p> <p>NOTE: It assumes that the values in \\(m_t\\) are UPDATES rather than raw weights.</p> Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def update_current_weights(self) -&gt; None:\n    r\"\"\"\n    This function updates each of the layer weights using the server learning rate and the \\(m_t\\) values\n    (computed with or without momentum).\n\n    **NOTE**: It assumes that the values in \\(m_t\\) are **UPDATES** rather than raw weights.\n    \"\"\"\n    assert self.m_t is not None\n    self.current_weights = [\n        current_layer_weight + self.server_learning_rate * layer_mt\n        for current_layer_weight, layer_mt in zip(self.current_weights, self.m_t)\n    ]\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.update_clipping_bound","title":"<code>update_clipping_bound(clipping_bits)</code>","text":"<p>This adds noise to the clipping bits returned by the clients and then updates the server-side clipping bound using this information.</p> <p>Parameters:</p> Name Type Description Default <code>clipping_bits</code> <code>NDArrays</code> <p>Bits associated with each of the clients. These are to be noised and aggregated in order to update the clipping bound on the server side.</p> required Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def update_clipping_bound(self, clipping_bits: NDArrays) -&gt; None:\n    \"\"\"\n    This adds noise to the clipping bits returned by the clients and then updates the server-side clipping bound\n    using this information.\n\n    Args:\n        clipping_bits (NDArrays): Bits associated with each of the clients. These are to be noised and aggregated\n            in order to update the clipping bound on the server side.\n    \"\"\"\n    noised_clipping_bits_sum = gaussian_noisy_aggregate_clipping_bits(\n        clipping_bits, self.clipping_noise_multiplier\n    )\n    self._update_clipping_bound_with_noised_bits(noised_clipping_bits_sum)\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Aggregate fit using averaging of weights (can be unweighted or weighted) and inject noise and optionally perform adaptive clipping updates.</p> <p>NOTE: This assumes that the model weights sent back by the clients are UPDATES rather than raw weights. That is they are <code>theta_client - theta_server</code> rather than just <code>theta_client</code>.</p> <p>NOTE: this function packs the clipping bound for clients as the last member of the parameters list.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The client identifiers and the results of their local training that need to be aggregated on the server-side. In this strategy, the clients pack the weights to be aggregated along with a clipping bit calculated during their local training cycle.</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>These are the results and exceptions from clients that experienced an issue during training, such as timeouts or exceptions.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar]]</code> <p>The aggregated model weights and the metrics dictionary. For this strategy, the server also packs a clipping bound to be sent to the clients. This is sent even if adaptive clipping is turned off and the value simply remains constant.</p> Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate fit using averaging of weights (can be unweighted or weighted) and inject noise and optionally\n    perform adaptive clipping updates.\n\n    **NOTE**: This assumes that the model weights sent back by the clients are **UPDATES** rather than raw weights.\n    That is they are ``theta_client - theta_server`` rather than just ``theta_client``.\n\n    **NOTE**: this function packs the clipping bound for clients as the last member of the parameters list.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n            that need to be aggregated on the server-side. In this strategy, the clients pack the weights to be\n            aggregated along with a clipping bit calculated during their local training cycle.\n        failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n            from clients that experienced an issue during training, such as timeouts or exceptions.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n            For this strategy, the server also packs a clipping bound to be sent to the clients. This is sent even\n            if adaptive clipping is turned off and the value simply remains constant.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # If we're doing weighted aggregation, we need to compute a per_client_example_cap. If it has been provided\n    # manually, we skip this step. Otherwise, we perform the necessary calculations.\n    if self.weighted_aggregation and self.per_client_example_cap is None:\n        assert self.sample_counts is not None\n\n        total_samples = sum(self.sample_counts)\n\n        self.per_client_example_cap = (\n            total_samples if self.per_client_example_cap is None else self.per_client_example_cap\n        )\n\n        self.total_client_weight: float = sum(\n            [sample_count / self.per_client_example_cap for sample_count in self.sample_counts]\n        )\n\n    # Convert results with packed params of model weights and clipping bits\n    weights_and_counts, clipping_bits = self.split_model_weights_and_clipping_bits(results)\n\n    noise_multiplier = self.weight_noise_multiplier\n    if self.adaptive_clipping:\n        # The noise multiplier need only be modified in the event of using adaptive clipping to account for the\n        # extra gradient information used to adapt the clipping threshold.\n        noise_multiplier = self.modify_noise_multiplier()\n        self.update_clipping_bound(clipping_bits)\n        log(INFO, f\"New Clipping Bound is: {self.clipping_bound}\")\n\n    if self.weighted_aggregation:\n        assert self.per_client_example_cap is not None\n        noised_aggregated_update = gaussian_noisy_weighted_aggregate(\n            weights_and_counts,\n            noise_multiplier,\n            self.clipping_bound,\n            self.fraction_fit,\n            self.per_client_example_cap,\n            self.total_client_weight,\n        )\n    else:\n        noised_aggregated_update = gaussian_noisy_unweighted_aggregate(\n            weights_and_counts,\n            noise_multiplier,\n            self.clipping_bound,\n        )\n\n    # momentum calculation\n    self.calculate_update_with_momentum(noised_aggregated_update)\n    self.update_current_weights()\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.fit_metrics_aggregation_fn:\n        fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n    # Weights plus the clipping bound to be used by the clients\n    packed_ndarrays = self.parameter_packer.pack_parameters(self.current_weights, self.clipping_bound)\n    return ndarrays_to_parameters(packed_ndarrays), metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.configure_fit","title":"<code>configure_fit(server_round, parameters, client_manager)</code>","text":"<p>This function configures a sample of clients for a training round. Due to the privacy accounting, this strategy requires that the sampling manager be of type <code>BaseFractionSamplingManager</code>.</p> <p>The function follows the standard configuration flow where the <code>on_fit_config_fn</code> function is used to produce configurations to be sent to all clients. These are packaged with the provided parameters and set over to the clients.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>parameters</code> <code>Parameters</code> <p>The parameters to be used to initialize the clients for the fit round.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The manager used to sample the clients. Currently we restrict this to be <code>BaseFractionSamplingManager</code>, which has a <code>sample_fraction</code> function built in.</p> required <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, FitIns]]</code> <p>List of sampled client identifiers and the configuration/parameters to be sent to each client (packaged as <code>FitIns</code>).</p> Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def configure_fit(\n    self, server_round: int, parameters: Parameters, client_manager: ClientManager\n) -&gt; list[tuple[ClientProxy, FitIns]]:\n    \"\"\"\n    This function configures a sample of clients for a training round. Due to the privacy accounting, this strategy\n    requires that the sampling manager be of type ``BaseFractionSamplingManager``.\n\n    The function follows the standard configuration flow where the ``on_fit_config_fn`` function is used to produce\n    configurations to be sent to all clients. These are packaged with the provided parameters and set over to the\n    clients.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        parameters (Parameters): The parameters to be used to initialize the clients for the fit round.\n        client_manager (ClientManager): The manager used to sample the clients. Currently we restrict this to\n            be ``BaseFractionSamplingManager``, which has a ``sample_fraction`` function built in.\n\n    Returns:\n        (list[tuple[ClientProxy, FitIns]]): List of sampled client identifiers and the configuration/parameters to\n            be sent to each client (packaged as ``FitIns``).\n    \"\"\"\n    # This strategy requires the client manager to be of type at least BaseFractionSamplingManager\n    assert isinstance(client_manager, BaseFractionSamplingManager)\n    config = {}\n    if self.on_fit_config_fn is not None:\n        # Custom fit config function provided\n        config = self.on_fit_config_fn(server_round)\n    else:\n        config = {\"current_server_round\": server_round}\n\n    fit_ins = FitIns(parameters, config)\n\n    clients = client_manager.sample_fraction(self.fraction_fit, self.min_available_clients)\n\n    # Return client/config pairs\n    return [(client, fit_ins) for client in clients]\n</code></pre>"},{"location":"api/#fl4health.strategies.client_dp_fedavgm.ClientLevelDPFedAvgM.configure_evaluate","title":"<code>configure_evaluate(server_round, parameters, client_manager)</code>","text":"<p>This function configures a sample of clients for an eval round. Due to the privacy accounting, this strategy requires that the sampling manager be of type <code>BaseFractionSamplingManager</code>.</p> <p>The function follows the standard configuration flow where the <code>on_evaluate_config_fn</code> function is used to produce configurations to be sent to all clients. These are packaged with the provided parameters and set over to the clients.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>parameters</code> <code>Parameters</code> <p>The parameters to be used to initialize the clients for the eval round.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The manager used to grab all of the clients. Currently we restrict this to be <code>BaseFractionSamplingManager</code>, which has a <code>sample_fraction</code> function built in.</p> required <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, EvaluateIns]]</code> <p>List of sampled client identifiers and the configuration/parameters to be sent to each client (packaged as <code>EvaluateIns</code>).</p> Source code in <code>fl4health/strategies/client_dp_fedavgm.py</code> <pre><code>def configure_evaluate(\n    self, server_round: int, parameters: Parameters, client_manager: ClientManager\n) -&gt; list[tuple[ClientProxy, EvaluateIns]]:\n    \"\"\"\n    This function configures a sample of clients for an eval round. Due to the privacy accounting, this strategy\n    requires that the sampling manager be of type ``BaseFractionSamplingManager``.\n\n    The function follows the standard configuration flow where the ``on_evaluate_config_fn`` function is used to\n    produce configurations to be sent to all clients. These are packaged with the provided parameters and set over\n    to the clients.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        parameters (Parameters): The parameters to be used to initialize the clients for the eval round.\n        client_manager (ClientManager): The manager used to grab all of the clients. Currently we restrict this to\n            be ``BaseFractionSamplingManager``, which has a ``sample_fraction`` function built in.\n\n    Returns:\n        (list[tuple[ClientProxy, EvaluateIns]]): List of sampled client identifiers and the\n            configuration/parameters to be sent to each client (packaged as ``EvaluateIns``).\n    \"\"\"\n    # This strategy requires the client manager to be of type at least BaseFractionSamplingManager\n    assert isinstance(client_manager, BaseFractionSamplingManager)\n\n    # Do not configure federated evaluation if fraction eval is 0 or server is not initialized\n    if self.fraction_evaluate == 0.0:\n        return []\n\n    # Parameters and config\n    config = {}\n    if self.on_evaluate_config_fn is not None:\n        # Custom evaluation config function provided\n        config = self.on_evaluate_config_fn(server_round)\n    else:\n        config = {\"current_server_round\": server_round}\n    evaluate_ins = EvaluateIns(parameters, config)\n\n    # Sample clients\n    clients = client_manager.sample_fraction(self.fraction_evaluate, self.min_available_clients)\n\n    # Return client/config pairs\n    return [(client, evaluate_ins) for client in clients]\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_dynamic_layer","title":"<code>fedavg_dynamic_layer</code>","text":""},{"location":"api/#fl4health.strategies.fedavg_dynamic_layer.FedAvgDynamicLayer","title":"<code>FedAvgDynamicLayer</code>","text":"<p>               Bases: <code>BasicFedAvg</code></p> Source code in <code>fl4health/strategies/fedavg_dynamic_layer.py</code> <pre><code>class FedAvgDynamicLayer(BasicFedAvg):\n    def __init__(\n        self,\n        *,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters | None = None,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        weighted_aggregation: bool = True,\n        weighted_eval_losses: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        A generalization of the FedAvg strategy where the server can receive any arbitrary subset of the layers from\n        any arbitrary subset of the clients, and weighted average for each received layer is performed independently.\n\n        Args:\n            fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0. Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n            min_fit_clients (int, optional): Minimum number of clients used during fitting. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for central server-side evaluation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                client-side validation by providing a ``Config`` dictionary. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n                average or a uniform average. FedAvg default is weighted average by client dataset counts.\n                Defaults to True.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n                counts. Defaults to True.\n        \"\"\"\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_fit_clients=min_fit_clients,\n            min_evaluate_clients=min_evaluate_clients,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            weighted_aggregation=weighted_aggregation,\n            weighted_eval_losses=weighted_eval_losses,\n        )\n        self.parameter_packer = ParameterPackerWithLayerNames()\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate the results from the federated fit round. The aggregation requires some special treatment, as the\n        participating clients are allowed to exchange an arbitrary set of weights. So before aggregation takes place\n        alignment must be done using the layer names packed in along with the weights in the client results.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n                that need to be aggregated on the server-side. In this scheme, the clients pack the layer weights into\n                the results object along with the weight values to allow for alignment during aggregation.\n            failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n                from clients that experienced an issue during training, such as timeouts or exceptions.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n                For dynamic layer exchange we also pack in the names of all of the layers that were aggregated in this\n                phase to allow client's to insert the values into the proper areas of their models.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n        # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n        # reducing numerical fluctuation.\n\n        # Convert client layer weights and names into ndarrays\n        decoded_and_sorted_results = [\n            (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n        ]\n\n        # For each layer of the model, perform weighted average of all received weights from clients\n        aggregated_params = self.aggregate(decoded_and_sorted_results)\n\n        weights_names = []\n        weights = []\n        for name in aggregated_params:\n            weights_names.append(name)\n            weights.append(aggregated_params[name])\n\n        parameters = self.parameter_packer.pack_parameters(weights, weights_names)\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.fit_metrics_aggregation_fn:\n            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n        return ndarrays_to_parameters(parameters), metrics_aggregated\n\n    def aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, NDArray]:\n        \"\"\"\n        Aggregate the different layers across clients that have contributed to a layer. This aggregation may be\n        weighted or unweighted. The called functions handle layer alignment.\n\n        Args:\n            results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to be\n                aggregated on the server-side and the number of training samples held on each client. In this scheme,\n                the clients pack the layer weights into the results object along with the weight values to allow for\n                alignment during aggregation.\n\n        Returns:\n            (dict[str, NDArray]): A dictionary mapping the name of the layer that was aggregated to the aggregated\n                weights.\n        \"\"\"\n        if self.weighted_aggregation:\n            return self.weighted_aggregate(results)\n        return self.unweighted_aggregate(results)\n\n    def weighted_aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, NDArray]:\n        \"\"\"\n        Results consists of the layer weights (and their names) sent by clients who participated in this round of\n        training. Since each client can send an arbitrary subset of layers, the aggregate performs weighted averaging\n        for each layer separately.\n\n        Args:\n            results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to be\n                aggregated on the server-side and the number of training samples held on each client. In this scheme,\n                the clients pack the layer weights into the results object along with the weight values to allow for\n                alignment during aggregation.\n\n        Returns:\n            (dict[str, NDArray]): A dictionary mapping the name of the layer that was aggregated to the aggregated\n                weights.\n        \"\"\"\n        names_to_layers: defaultdict[str, list[NDArray]] = defaultdict(list)\n        total_num_examples: defaultdict[str, int] = defaultdict(int)\n\n        for packed_layers, num_examples in results:\n            layers, names = self.parameter_packer.unpack_parameters(packed_layers)\n            for layer, name in zip(layers, names):\n                names_to_layers[name].append(layer * num_examples)\n                total_num_examples[name] += num_examples\n\n        return {\n            name_key: reduce(np.add, names_to_layers[name_key]) / total_num_examples[name_key]\n            for name_key in names_to_layers\n        }\n\n    def unweighted_aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, NDArray]:\n        \"\"\"\n        Results consists of the layer weights (and their names) sent by clients who participated in this round of\n        training. Since each client can send an arbitrary subset of layers, the aggregate performs uniform averaging\n        for each layer separately.\n\n        Args:\n            results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to be\n                aggregated on the server-side and the number of training samples held on each client. In this scheme,\n                the clients pack the layer weights into the results object along with the weight values to allow for\n                alignment during aggregation.\n\n        Returns:\n            (dict[str, NDArray]): A dictionary mapping the name of the layer that was aggregated to the aggregated\n                weights.\n        \"\"\"\n        names_to_layers: defaultdict[str, list[NDArray]] = defaultdict(list)\n        total_num_clients: defaultdict[str, int] = defaultdict(int)\n\n        for packed_layers, _ in results:\n            layers, names = self.parameter_packer.unpack_parameters(packed_layers)\n            for layer, name in zip(layers, names):\n                names_to_layers[name].append(layer)\n                total_num_clients[name] += 1\n\n        return {\n            name_key: reduce(np.add, names_to_layers[name_key]) / total_num_clients[name_key]\n            for name_key in names_to_layers\n        }\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_dynamic_layer.FedAvgDynamicLayer.__init__","title":"<code>__init__(*, fraction_fit=1.0, fraction_evaluate=1.0, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters=None, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, weighted_aggregation=True, weighted_eval_losses=True)</code>","text":"<p>A generalization of the FedAvg strategy where the server can receive any arbitrary subset of the layers from any arbitrary subset of the clients, and weighted average for each received layer is performed independently.</p> <p>Parameters:</p> Name Type Description Default <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. Defaults to 1.0. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. Defaults to 1.0.</p> <code>1.0</code> <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during fitting. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>initial_parameters</code> <code>Parameters | None</code> <p>Initial global model parameters. Defaults to None.</p> <code>None</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>weighted_aggregation</code> <code>bool</code> <p>Determines whether parameter aggregation is a linearly weighted average or a uniform average. FedAvg default is weighted average by client dataset counts. Defaults to True.</p> <code>True</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. FedAvg default is weighted average of the losses by client dataset counts. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/strategies/fedavg_dynamic_layer.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters | None = None,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    weighted_aggregation: bool = True,\n    weighted_eval_losses: bool = True,\n) -&gt; None:\n    \"\"\"\n    A generalization of the FedAvg strategy where the server can receive any arbitrary subset of the layers from\n    any arbitrary subset of the clients, and weighted average for each received layer is performed independently.\n\n    Args:\n        fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0. Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n        min_fit_clients (int, optional): Minimum number of clients used during fitting. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for central server-side evaluation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            client-side validation by providing a ``Config`` dictionary. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n            average or a uniform average. FedAvg default is weighted average by client dataset counts.\n            Defaults to True.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n            counts. Defaults to True.\n    \"\"\"\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_fit_clients=min_fit_clients,\n        min_evaluate_clients=min_evaluate_clients,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        weighted_aggregation=weighted_aggregation,\n        weighted_eval_losses=weighted_eval_losses,\n    )\n    self.parameter_packer = ParameterPackerWithLayerNames()\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_dynamic_layer.FedAvgDynamicLayer.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Aggregate the results from the federated fit round. The aggregation requires some special treatment, as the participating clients are allowed to exchange an arbitrary set of weights. So before aggregation takes place alignment must be done using the layer names packed in along with the weights in the client results.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The client identifiers and the results of their local training that need to be aggregated on the server-side. In this scheme, the clients pack the layer weights into the results object along with the weight values to allow for alignment during aggregation.</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>These are the results and exceptions from clients that experienced an issue during training, such as timeouts or exceptions.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar]]</code> <p>The aggregated model weights and the metrics dictionary. For dynamic layer exchange we also pack in the names of all of the layers that were aggregated in this phase to allow client's to insert the values into the proper areas of their models.</p> Source code in <code>fl4health/strategies/fedavg_dynamic_layer.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate the results from the federated fit round. The aggregation requires some special treatment, as the\n    participating clients are allowed to exchange an arbitrary set of weights. So before aggregation takes place\n    alignment must be done using the layer names packed in along with the weights in the client results.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n            that need to be aggregated on the server-side. In this scheme, the clients pack the layer weights into\n            the results object along with the weight values to allow for alignment during aggregation.\n        failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n            from clients that experienced an issue during training, such as timeouts or exceptions.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n            For dynamic layer exchange we also pack in the names of all of the layers that were aggregated in this\n            phase to allow client's to insert the values into the proper areas of their models.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n    # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n    # reducing numerical fluctuation.\n\n    # Convert client layer weights and names into ndarrays\n    decoded_and_sorted_results = [\n        (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n    ]\n\n    # For each layer of the model, perform weighted average of all received weights from clients\n    aggregated_params = self.aggregate(decoded_and_sorted_results)\n\n    weights_names = []\n    weights = []\n    for name in aggregated_params:\n        weights_names.append(name)\n        weights.append(aggregated_params[name])\n\n    parameters = self.parameter_packer.pack_parameters(weights, weights_names)\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.fit_metrics_aggregation_fn:\n        fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n    return ndarrays_to_parameters(parameters), metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_dynamic_layer.FedAvgDynamicLayer.aggregate","title":"<code>aggregate(results)</code>","text":"<p>Aggregate the different layers across clients that have contributed to a layer. This aggregation may be weighted or unweighted. The called functions handle layer alignment.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[NDArrays, int]]</code> <p>The weight results from each client's local training that need to be aggregated on the server-side and the number of training samples held on each client. In this scheme, the clients pack the layer weights into the results object along with the weight values to allow for alignment during aggregation.</p> required <p>Returns:</p> Type Description <code>dict[str, NDArray]</code> <p>A dictionary mapping the name of the layer that was aggregated to the aggregated weights.</p> Source code in <code>fl4health/strategies/fedavg_dynamic_layer.py</code> <pre><code>def aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, NDArray]:\n    \"\"\"\n    Aggregate the different layers across clients that have contributed to a layer. This aggregation may be\n    weighted or unweighted. The called functions handle layer alignment.\n\n    Args:\n        results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to be\n            aggregated on the server-side and the number of training samples held on each client. In this scheme,\n            the clients pack the layer weights into the results object along with the weight values to allow for\n            alignment during aggregation.\n\n    Returns:\n        (dict[str, NDArray]): A dictionary mapping the name of the layer that was aggregated to the aggregated\n            weights.\n    \"\"\"\n    if self.weighted_aggregation:\n        return self.weighted_aggregate(results)\n    return self.unweighted_aggregate(results)\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_dynamic_layer.FedAvgDynamicLayer.weighted_aggregate","title":"<code>weighted_aggregate(results)</code>","text":"<p>Results consists of the layer weights (and their names) sent by clients who participated in this round of training. Since each client can send an arbitrary subset of layers, the aggregate performs weighted averaging for each layer separately.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[NDArrays, int]]</code> <p>The weight results from each client's local training that need to be aggregated on the server-side and the number of training samples held on each client. In this scheme, the clients pack the layer weights into the results object along with the weight values to allow for alignment during aggregation.</p> required <p>Returns:</p> Type Description <code>dict[str, NDArray]</code> <p>A dictionary mapping the name of the layer that was aggregated to the aggregated weights.</p> Source code in <code>fl4health/strategies/fedavg_dynamic_layer.py</code> <pre><code>def weighted_aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, NDArray]:\n    \"\"\"\n    Results consists of the layer weights (and their names) sent by clients who participated in this round of\n    training. Since each client can send an arbitrary subset of layers, the aggregate performs weighted averaging\n    for each layer separately.\n\n    Args:\n        results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to be\n            aggregated on the server-side and the number of training samples held on each client. In this scheme,\n            the clients pack the layer weights into the results object along with the weight values to allow for\n            alignment during aggregation.\n\n    Returns:\n        (dict[str, NDArray]): A dictionary mapping the name of the layer that was aggregated to the aggregated\n            weights.\n    \"\"\"\n    names_to_layers: defaultdict[str, list[NDArray]] = defaultdict(list)\n    total_num_examples: defaultdict[str, int] = defaultdict(int)\n\n    for packed_layers, num_examples in results:\n        layers, names = self.parameter_packer.unpack_parameters(packed_layers)\n        for layer, name in zip(layers, names):\n            names_to_layers[name].append(layer * num_examples)\n            total_num_examples[name] += num_examples\n\n    return {\n        name_key: reduce(np.add, names_to_layers[name_key]) / total_num_examples[name_key]\n        for name_key in names_to_layers\n    }\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_dynamic_layer.FedAvgDynamicLayer.unweighted_aggregate","title":"<code>unweighted_aggregate(results)</code>","text":"<p>Results consists of the layer weights (and their names) sent by clients who participated in this round of training. Since each client can send an arbitrary subset of layers, the aggregate performs uniform averaging for each layer separately.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[NDArrays, int]]</code> <p>The weight results from each client's local training that need to be aggregated on the server-side and the number of training samples held on each client. In this scheme, the clients pack the layer weights into the results object along with the weight values to allow for alignment during aggregation.</p> required <p>Returns:</p> Type Description <code>dict[str, NDArray]</code> <p>A dictionary mapping the name of the layer that was aggregated to the aggregated weights.</p> Source code in <code>fl4health/strategies/fedavg_dynamic_layer.py</code> <pre><code>def unweighted_aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, NDArray]:\n    \"\"\"\n    Results consists of the layer weights (and their names) sent by clients who participated in this round of\n    training. Since each client can send an arbitrary subset of layers, the aggregate performs uniform averaging\n    for each layer separately.\n\n    Args:\n        results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to be\n            aggregated on the server-side and the number of training samples held on each client. In this scheme,\n            the clients pack the layer weights into the results object along with the weight values to allow for\n            alignment during aggregation.\n\n    Returns:\n        (dict[str, NDArray]): A dictionary mapping the name of the layer that was aggregated to the aggregated\n            weights.\n    \"\"\"\n    names_to_layers: defaultdict[str, list[NDArray]] = defaultdict(list)\n    total_num_clients: defaultdict[str, int] = defaultdict(int)\n\n    for packed_layers, _ in results:\n        layers, names = self.parameter_packer.unpack_parameters(packed_layers)\n        for layer, name in zip(layers, names):\n            names_to_layers[name].append(layer)\n            total_num_clients[name] += 1\n\n    return {\n        name_key: reduce(np.add, names_to_layers[name_key]) / total_num_clients[name_key]\n        for name_key in names_to_layers\n    }\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_sparse_coo_tensor","title":"<code>fedavg_sparse_coo_tensor</code>","text":""},{"location":"api/#fl4health.strategies.fedavg_sparse_coo_tensor.FedAvgSparseCooTensor","title":"<code>FedAvgSparseCooTensor</code>","text":"<p>               Bases: <code>BasicFedAvg</code></p> Source code in <code>fl4health/strategies/fedavg_sparse_coo_tensor.py</code> <pre><code>class FedAvgSparseCooTensor(BasicFedAvg):\n    def __init__(\n        self,\n        *,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters | None = None,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        weighted_aggregation: bool = True,\n        weighted_eval_losses: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        A generalization of the FedAvg strategy where the server can receive any arbitrary subset of parameters from\n        any arbitrary subset of the clients. Weighted average for parameters belonging to each received tensor is\n        performed independently.\n\n        Note that this strategy differs from ``FedAvgDynamicLayer`` in that it does not require clients to send entire\n        layers (tensors).\n\n        A client can send an arbitrary set of parameters within a certain tensor, and these parameters are packed\n        according to the sparse COO format.\n\n        For more information on the sparse COO format and sparse tensors in PyTorch, please see the following\n        two pages:\n\n        1. https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html\n        2. https://pytorch.org/docs/stable/sparse.html\n\n        Args:\n            fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0. Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n            min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for central server-side evaluation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                client-side validation by providing a ``Config`` dictionary. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n                average or a uniform average. FedAvg default is weighted average by client dataset counts.\n                Defaults to True.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n                counts. Defaults to True.\n        \"\"\"\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_fit_clients=min_fit_clients,\n            min_evaluate_clients=min_evaluate_clients,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            weighted_aggregation=weighted_aggregation,\n            weighted_eval_losses=weighted_eval_losses,\n        )\n        self.parameter_packer = SparseCooParameterPacker()\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate the results from the federated fit round. The aggregation requires some special treatment, as the\n        participating clients are allowed to exchange an arbitrary set of parameters. So before aggregation takes place\n        alignment must be done using the tensor names packed in along with the weights in the client results.\n\n        More precisely, this method performs the following steps:\n\n        1. Align all tensors according to their names.\n        2. For tensors that have the same name, construct the sparse COO tensors and convert them to dense tensors.\n        3. Perform averaging on the dense tensors (can be weighted or unweighted).\n        4. For every aggregated dense tensor, discard the zero values and retain all information needed\n           to represent it in the sparse COO format.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n                that need to be aggregated on the server-side. In this scheme, the clients pack the tensor names into\n                the results object along with the weight values to allow for alignment during aggregation.\n            failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n                from clients that experienced an issue during training, such as timeouts or exceptions.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n                For sparse tensor exchange we also pack in the names of all of the tensors that were aggregated in this\n                phase to allow clients to insert the values into the proper areas of their models.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n        # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n        # reducing numerical fluctuation.\n\n        # Convert client tensor weights and names into ndarrays\n        decoded_and_sorted_results = [\n            (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n        ]\n\n        # For each tensor of the model, perform weighted average of all received weights from clients\n        aggregated_tensors = self.aggregate(decoded_and_sorted_results)\n\n        tensor_names = []\n        selected_parameters_all_tensors = []\n        selected_indices_all_tensors = []\n        tensor_shapes = []\n\n        for tensor_name, aggregated_tensor in aggregated_tensors.items():\n            selected_parameters, selected_indices, tensor_shape = self.parameter_packer.extract_coo_info_from_dense(\n                aggregated_tensor\n            )\n            tensor_names.append(tensor_name)\n            selected_parameters_all_tensors.append(selected_parameters)\n            selected_indices_all_tensors.append(selected_indices)\n            tensor_shapes.append(tensor_shape)\n\n        packed_parameters = self.parameter_packer.pack_parameters(\n            selected_parameters_all_tensors, (selected_indices_all_tensors, tensor_shapes, tensor_names)\n        )\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.fit_metrics_aggregation_fn:\n            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n        return ndarrays_to_parameters(packed_parameters), metrics_aggregated\n\n    def aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, Tensor]:\n        \"\"\"\n        Aggregate the different tensors across clients that have contributed to a certain tensor. This aggregation may\n        be weighted or unweighted. The called functions handle tensor alignment.\n\n        Args:\n            results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to\n                be aggregated on the server-side and the number of training samples held on each client.\n\n                In this scheme, the clients pack the tensor names into the results object along with the weight values\n                to allow for alignment during aggregation.\n\n        Returns:\n            (dict[str, Tensor]): A dictionary mapping the name of the tensor that was aggregated to the aggregated\n                weights.\n        \"\"\"\n        if self.weighted_aggregation:\n            return self.weighted_aggregate(results)\n        return self.unweighted_aggregate(results)\n\n    def weighted_aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, Tensor]:\n        \"\"\"\n        \"results\" consist of four parts: the exchanged (nonzero) parameter values, their coordinates within the tensor\n        to which they belong, the shape of that tensor, and finally the name of that tensor.\n\n        The first three items constitute the information that is needed to construct the tensor in the sparse COO\n        format and convert it to a regular dense tensor.\n\n        The tensor name is used to align tensors to ensure that averaging is performed only among tensors with the\n        same name.\n\n        This method performs the following steps:\n\n        1. Align all tensors according to their names.\n        2. For tensors that have the same name, construct the sparse COO tensors and convert them to dense tensors.\n        3. Perform weighted averaging on the dense tensors according to the number of training examples each client\n           has.\n\n        **NOTE**: This method performs weighted averaging.\n\n        Args:\n            results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to be\n                aggregated on the server-side and the number of training samples held on each client.\n\n                The weight results consist of four parts, as detailed above. In this scheme, the clients pack the\n                layer names into the results object along with the weight values to allow for alignment during\n                aggregation.\n\n        Returns:\n            (dict[str, Tensor]): A dictionary mapping the name of the tensor that was aggregated to the aggregated\n                weights.\n        \"\"\"\n        names_to_dense_tensors: defaultdict[str, list[Tensor]] = defaultdict(list)\n        total_num_examples: defaultdict[str, int] = defaultdict(int)\n\n        for packed_parameters, num_examples in results:\n            nonzero_parameter_values, additional_info = self.parameter_packer.unpack_parameters(packed_parameters)\n            parameter_indices, tensor_shapes, tensor_names = additional_info\n\n            # Sanity check to ensure that they all have the same length and the length is &gt; 0.\n            assert (\n                len(nonzero_parameter_values) == len(parameter_indices) == len(tensor_shapes) == len(tensor_names)\n                and len(tensor_names) &gt; 0\n            )\n            for tensor_params, tensor_param_indices, tensor_shape, tensor_name in zip(\n                nonzero_parameter_values, parameter_indices, tensor_shapes, tensor_names\n            ):\n                coo_tensor = torch.sparse_coo_tensor(\n                    indices=torch.tensor(tensor_param_indices.T),\n                    values=torch.tensor(tensor_params),\n                    size=torch.Size(tensor_shape),\n                )\n                dense_tensor = coo_tensor.to_dense()\n                names_to_dense_tensors[tensor_name].append(dense_tensor * num_examples)\n                total_num_examples[tensor_name] += num_examples\n\n        return {\n            name_key: (reduce(torch.add, names_to_dense_tensors[name_key]) / total_num_examples[name_key])\n            for name_key in names_to_dense_tensors\n        }\n\n    def unweighted_aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, Tensor]:\n        \"\"\"\n        \"results\" consist of four parts: the exchanged (nonzero) parameter values, their coordinates within the tensor\n        to which they belong, the shape of that tensor, and finally the name of that tensor.\n\n        The first three items constitute the information that is needed to construct the tensor in the sparse COO\n        format and convert it to a regular dense tensor.\n\n        The tensor name is used to align tensors to ensure that averaging is performed only among tensors with the\n        same name.\n\n        This method performs the following steps:\n\n        1. Align all tensors according to their names.\n        2. For tensors that have the same name, construct the sparse COO tensors and convert them to dense tensors.\n        3. Perform uniform averaging on the dense tensors across all clients.\n\n        **NOTE**: This method performs uniform averaging.\n\n        Args:\n            results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to be\n                aggregated on the server-side and the number of training samples held on each client.\n\n                The weight results consist of four parts, as detailed above. In this scheme, the clients pack the\n                layer names into the results object along with the weight values to allow for alignment during\n                aggregation.\n\n        Returns:\n            (dict[str, Tensor]): A dictionary mapping the name of the tensor that was aggregated to the aggregated\n                weights.\n        \"\"\"\n        names_to_dense_tensors: defaultdict[str, list[Tensor]] = defaultdict(list)\n        total_num_clients: defaultdict[str, int] = defaultdict(int)\n\n        for packed_parameters, _ in results:\n            nonzero_parameter_values, additional_info = self.parameter_packer.unpack_parameters(packed_parameters)\n            parameter_indices, tensor_shapes, tensor_names = additional_info\n\n            # Sanity check.\n            assert (\n                len(nonzero_parameter_values) == len(parameter_indices) == len(tensor_shapes) == len(tensor_names)\n                and len(tensor_names) &gt; 0\n            )\n            for tensor_params, tensor_param_indices, tensor_shape, tensor_name in zip(\n                nonzero_parameter_values, parameter_indices, tensor_shapes, tensor_names\n            ):\n                coo_tensor = torch.sparse_coo_tensor(\n                    indices=torch.tensor(tensor_param_indices.T),\n                    values=torch.tensor(tensor_params),\n                    size=torch.Size(tensor_shape),\n                )\n                dense_tensor = coo_tensor.to_dense()\n                names_to_dense_tensors[tensor_name].append(dense_tensor)\n                total_num_clients[tensor_name] += 1\n\n        return {\n            name_key: (reduce(torch.add, names_to_dense_tensors[name_key]) / total_num_clients[name_key])\n            for name_key in names_to_dense_tensors\n        }\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_sparse_coo_tensor.FedAvgSparseCooTensor.__init__","title":"<code>__init__(*, fraction_fit=1.0, fraction_evaluate=1.0, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters=None, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, weighted_aggregation=True, weighted_eval_losses=True)</code>","text":"<p>A generalization of the FedAvg strategy where the server can receive any arbitrary subset of parameters from any arbitrary subset of the clients. Weighted average for parameters belonging to each received tensor is performed independently.</p> <p>Note that this strategy differs from <code>FedAvgDynamicLayer</code> in that it does not require clients to send entire layers (tensors).</p> <p>A client can send an arbitrary set of parameters within a certain tensor, and these parameters are packed according to the sparse COO format.</p> <p>For more information on the sparse COO format and sparse tensors in PyTorch, please see the following two pages:</p> <ol> <li>https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html</li> <li>https://pytorch.org/docs/stable/sparse.html</li> </ol> <p>Parameters:</p> Name Type Description Default <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. Defaults to 1.0. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. Defaults to 1.0.</p> <code>1.0</code> <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during training. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>initial_parameters</code> <code>Parameters | None</code> <p>Initial global model parameters. Defaults to None.</p> <code>None</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>weighted_aggregation</code> <code>bool</code> <p>Determines whether parameter aggregation is a linearly weighted average or a uniform average. FedAvg default is weighted average by client dataset counts. Defaults to True.</p> <code>True</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. FedAvg default is weighted average of the losses by client dataset counts. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/strategies/fedavg_sparse_coo_tensor.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters | None = None,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    weighted_aggregation: bool = True,\n    weighted_eval_losses: bool = True,\n) -&gt; None:\n    \"\"\"\n    A generalization of the FedAvg strategy where the server can receive any arbitrary subset of parameters from\n    any arbitrary subset of the clients. Weighted average for parameters belonging to each received tensor is\n    performed independently.\n\n    Note that this strategy differs from ``FedAvgDynamicLayer`` in that it does not require clients to send entire\n    layers (tensors).\n\n    A client can send an arbitrary set of parameters within a certain tensor, and these parameters are packed\n    according to the sparse COO format.\n\n    For more information on the sparse COO format and sparse tensors in PyTorch, please see the following\n    two pages:\n\n    1. https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html\n    2. https://pytorch.org/docs/stable/sparse.html\n\n    Args:\n        fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0. Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n        min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for central server-side evaluation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            client-side validation by providing a ``Config`` dictionary. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n            average or a uniform average. FedAvg default is weighted average by client dataset counts.\n            Defaults to True.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n            counts. Defaults to True.\n    \"\"\"\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_fit_clients=min_fit_clients,\n        min_evaluate_clients=min_evaluate_clients,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        weighted_aggregation=weighted_aggregation,\n        weighted_eval_losses=weighted_eval_losses,\n    )\n    self.parameter_packer = SparseCooParameterPacker()\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_sparse_coo_tensor.FedAvgSparseCooTensor.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Aggregate the results from the federated fit round. The aggregation requires some special treatment, as the participating clients are allowed to exchange an arbitrary set of parameters. So before aggregation takes place alignment must be done using the tensor names packed in along with the weights in the client results.</p> <p>More precisely, this method performs the following steps:</p> <ol> <li>Align all tensors according to their names.</li> <li>For tensors that have the same name, construct the sparse COO tensors and convert them to dense tensors.</li> <li>Perform averaging on the dense tensors (can be weighted or unweighted).</li> <li>For every aggregated dense tensor, discard the zero values and retain all information needed    to represent it in the sparse COO format.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The client identifiers and the results of their local training that need to be aggregated on the server-side. In this scheme, the clients pack the tensor names into the results object along with the weight values to allow for alignment during aggregation.</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>These are the results and exceptions from clients that experienced an issue during training, such as timeouts or exceptions.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar]]</code> <p>The aggregated model weights and the metrics dictionary. For sparse tensor exchange we also pack in the names of all of the tensors that were aggregated in this phase to allow clients to insert the values into the proper areas of their models.</p> Source code in <code>fl4health/strategies/fedavg_sparse_coo_tensor.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate the results from the federated fit round. The aggregation requires some special treatment, as the\n    participating clients are allowed to exchange an arbitrary set of parameters. So before aggregation takes place\n    alignment must be done using the tensor names packed in along with the weights in the client results.\n\n    More precisely, this method performs the following steps:\n\n    1. Align all tensors according to their names.\n    2. For tensors that have the same name, construct the sparse COO tensors and convert them to dense tensors.\n    3. Perform averaging on the dense tensors (can be weighted or unweighted).\n    4. For every aggregated dense tensor, discard the zero values and retain all information needed\n       to represent it in the sparse COO format.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n            that need to be aggregated on the server-side. In this scheme, the clients pack the tensor names into\n            the results object along with the weight values to allow for alignment during aggregation.\n        failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n            from clients that experienced an issue during training, such as timeouts or exceptions.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n            For sparse tensor exchange we also pack in the names of all of the tensors that were aggregated in this\n            phase to allow clients to insert the values into the proper areas of their models.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n    # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n    # reducing numerical fluctuation.\n\n    # Convert client tensor weights and names into ndarrays\n    decoded_and_sorted_results = [\n        (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n    ]\n\n    # For each tensor of the model, perform weighted average of all received weights from clients\n    aggregated_tensors = self.aggregate(decoded_and_sorted_results)\n\n    tensor_names = []\n    selected_parameters_all_tensors = []\n    selected_indices_all_tensors = []\n    tensor_shapes = []\n\n    for tensor_name, aggregated_tensor in aggregated_tensors.items():\n        selected_parameters, selected_indices, tensor_shape = self.parameter_packer.extract_coo_info_from_dense(\n            aggregated_tensor\n        )\n        tensor_names.append(tensor_name)\n        selected_parameters_all_tensors.append(selected_parameters)\n        selected_indices_all_tensors.append(selected_indices)\n        tensor_shapes.append(tensor_shape)\n\n    packed_parameters = self.parameter_packer.pack_parameters(\n        selected_parameters_all_tensors, (selected_indices_all_tensors, tensor_shapes, tensor_names)\n    )\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.fit_metrics_aggregation_fn:\n        fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n    return ndarrays_to_parameters(packed_parameters), metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_sparse_coo_tensor.FedAvgSparseCooTensor.aggregate","title":"<code>aggregate(results)</code>","text":"<p>Aggregate the different tensors across clients that have contributed to a certain tensor. This aggregation may be weighted or unweighted. The called functions handle tensor alignment.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[NDArrays, int]]</code> <p>The weight results from each client's local training that need to be aggregated on the server-side and the number of training samples held on each client.</p> <p>In this scheme, the clients pack the tensor names into the results object along with the weight values to allow for alignment during aggregation.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>A dictionary mapping the name of the tensor that was aggregated to the aggregated weights.</p> Source code in <code>fl4health/strategies/fedavg_sparse_coo_tensor.py</code> <pre><code>def aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, Tensor]:\n    \"\"\"\n    Aggregate the different tensors across clients that have contributed to a certain tensor. This aggregation may\n    be weighted or unweighted. The called functions handle tensor alignment.\n\n    Args:\n        results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to\n            be aggregated on the server-side and the number of training samples held on each client.\n\n            In this scheme, the clients pack the tensor names into the results object along with the weight values\n            to allow for alignment during aggregation.\n\n    Returns:\n        (dict[str, Tensor]): A dictionary mapping the name of the tensor that was aggregated to the aggregated\n            weights.\n    \"\"\"\n    if self.weighted_aggregation:\n        return self.weighted_aggregate(results)\n    return self.unweighted_aggregate(results)\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_sparse_coo_tensor.FedAvgSparseCooTensor.weighted_aggregate","title":"<code>weighted_aggregate(results)</code>","text":"<p>\"results\" consist of four parts: the exchanged (nonzero) parameter values, their coordinates within the tensor to which they belong, the shape of that tensor, and finally the name of that tensor.</p> <p>The first three items constitute the information that is needed to construct the tensor in the sparse COO format and convert it to a regular dense tensor.</p> <p>The tensor name is used to align tensors to ensure that averaging is performed only among tensors with the same name.</p> <p>This method performs the following steps:</p> <ol> <li>Align all tensors according to their names.</li> <li>For tensors that have the same name, construct the sparse COO tensors and convert them to dense tensors.</li> <li>Perform weighted averaging on the dense tensors according to the number of training examples each client    has.</li> </ol> <p>NOTE: This method performs weighted averaging.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[NDArrays, int]]</code> <p>The weight results from each client's local training that need to be aggregated on the server-side and the number of training samples held on each client.</p> <p>The weight results consist of four parts, as detailed above. In this scheme, the clients pack the layer names into the results object along with the weight values to allow for alignment during aggregation.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>A dictionary mapping the name of the tensor that was aggregated to the aggregated weights.</p> Source code in <code>fl4health/strategies/fedavg_sparse_coo_tensor.py</code> <pre><code>def weighted_aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, Tensor]:\n    \"\"\"\n    \"results\" consist of four parts: the exchanged (nonzero) parameter values, their coordinates within the tensor\n    to which they belong, the shape of that tensor, and finally the name of that tensor.\n\n    The first three items constitute the information that is needed to construct the tensor in the sparse COO\n    format and convert it to a regular dense tensor.\n\n    The tensor name is used to align tensors to ensure that averaging is performed only among tensors with the\n    same name.\n\n    This method performs the following steps:\n\n    1. Align all tensors according to their names.\n    2. For tensors that have the same name, construct the sparse COO tensors and convert them to dense tensors.\n    3. Perform weighted averaging on the dense tensors according to the number of training examples each client\n       has.\n\n    **NOTE**: This method performs weighted averaging.\n\n    Args:\n        results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to be\n            aggregated on the server-side and the number of training samples held on each client.\n\n            The weight results consist of four parts, as detailed above. In this scheme, the clients pack the\n            layer names into the results object along with the weight values to allow for alignment during\n            aggregation.\n\n    Returns:\n        (dict[str, Tensor]): A dictionary mapping the name of the tensor that was aggregated to the aggregated\n            weights.\n    \"\"\"\n    names_to_dense_tensors: defaultdict[str, list[Tensor]] = defaultdict(list)\n    total_num_examples: defaultdict[str, int] = defaultdict(int)\n\n    for packed_parameters, num_examples in results:\n        nonzero_parameter_values, additional_info = self.parameter_packer.unpack_parameters(packed_parameters)\n        parameter_indices, tensor_shapes, tensor_names = additional_info\n\n        # Sanity check to ensure that they all have the same length and the length is &gt; 0.\n        assert (\n            len(nonzero_parameter_values) == len(parameter_indices) == len(tensor_shapes) == len(tensor_names)\n            and len(tensor_names) &gt; 0\n        )\n        for tensor_params, tensor_param_indices, tensor_shape, tensor_name in zip(\n            nonzero_parameter_values, parameter_indices, tensor_shapes, tensor_names\n        ):\n            coo_tensor = torch.sparse_coo_tensor(\n                indices=torch.tensor(tensor_param_indices.T),\n                values=torch.tensor(tensor_params),\n                size=torch.Size(tensor_shape),\n            )\n            dense_tensor = coo_tensor.to_dense()\n            names_to_dense_tensors[tensor_name].append(dense_tensor * num_examples)\n            total_num_examples[tensor_name] += num_examples\n\n    return {\n        name_key: (reduce(torch.add, names_to_dense_tensors[name_key]) / total_num_examples[name_key])\n        for name_key in names_to_dense_tensors\n    }\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_sparse_coo_tensor.FedAvgSparseCooTensor.unweighted_aggregate","title":"<code>unweighted_aggregate(results)</code>","text":"<p>\"results\" consist of four parts: the exchanged (nonzero) parameter values, their coordinates within the tensor to which they belong, the shape of that tensor, and finally the name of that tensor.</p> <p>The first three items constitute the information that is needed to construct the tensor in the sparse COO format and convert it to a regular dense tensor.</p> <p>The tensor name is used to align tensors to ensure that averaging is performed only among tensors with the same name.</p> <p>This method performs the following steps:</p> <ol> <li>Align all tensors according to their names.</li> <li>For tensors that have the same name, construct the sparse COO tensors and convert them to dense tensors.</li> <li>Perform uniform averaging on the dense tensors across all clients.</li> </ol> <p>NOTE: This method performs uniform averaging.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[NDArrays, int]]</code> <p>The weight results from each client's local training that need to be aggregated on the server-side and the number of training samples held on each client.</p> <p>The weight results consist of four parts, as detailed above. In this scheme, the clients pack the layer names into the results object along with the weight values to allow for alignment during aggregation.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>A dictionary mapping the name of the tensor that was aggregated to the aggregated weights.</p> Source code in <code>fl4health/strategies/fedavg_sparse_coo_tensor.py</code> <pre><code>def unweighted_aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, Tensor]:\n    \"\"\"\n    \"results\" consist of four parts: the exchanged (nonzero) parameter values, their coordinates within the tensor\n    to which they belong, the shape of that tensor, and finally the name of that tensor.\n\n    The first three items constitute the information that is needed to construct the tensor in the sparse COO\n    format and convert it to a regular dense tensor.\n\n    The tensor name is used to align tensors to ensure that averaging is performed only among tensors with the\n    same name.\n\n    This method performs the following steps:\n\n    1. Align all tensors according to their names.\n    2. For tensors that have the same name, construct the sparse COO tensors and convert them to dense tensors.\n    3. Perform uniform averaging on the dense tensors across all clients.\n\n    **NOTE**: This method performs uniform averaging.\n\n    Args:\n        results (list[tuple[NDArrays, int]]): The weight results from each client's local training that need to be\n            aggregated on the server-side and the number of training samples held on each client.\n\n            The weight results consist of four parts, as detailed above. In this scheme, the clients pack the\n            layer names into the results object along with the weight values to allow for alignment during\n            aggregation.\n\n    Returns:\n        (dict[str, Tensor]): A dictionary mapping the name of the tensor that was aggregated to the aggregated\n            weights.\n    \"\"\"\n    names_to_dense_tensors: defaultdict[str, list[Tensor]] = defaultdict(list)\n    total_num_clients: defaultdict[str, int] = defaultdict(int)\n\n    for packed_parameters, _ in results:\n        nonzero_parameter_values, additional_info = self.parameter_packer.unpack_parameters(packed_parameters)\n        parameter_indices, tensor_shapes, tensor_names = additional_info\n\n        # Sanity check.\n        assert (\n            len(nonzero_parameter_values) == len(parameter_indices) == len(tensor_shapes) == len(tensor_names)\n            and len(tensor_names) &gt; 0\n        )\n        for tensor_params, tensor_param_indices, tensor_shape, tensor_name in zip(\n            nonzero_parameter_values, parameter_indices, tensor_shapes, tensor_names\n        ):\n            coo_tensor = torch.sparse_coo_tensor(\n                indices=torch.tensor(tensor_param_indices.T),\n                values=torch.tensor(tensor_params),\n                size=torch.Size(tensor_shape),\n            )\n            dense_tensor = coo_tensor.to_dense()\n            names_to_dense_tensors[tensor_name].append(dense_tensor)\n            total_num_clients[tensor_name] += 1\n\n    return {\n        name_key: (reduce(torch.add, names_to_dense_tensors[name_key]) / total_num_clients[name_key])\n        for name_key in names_to_dense_tensors\n    }\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_with_adaptive_constraint","title":"<code>fedavg_with_adaptive_constraint</code>","text":""},{"location":"api/#fl4health.strategies.fedavg_with_adaptive_constraint.FedAvgWithAdaptiveConstraint","title":"<code>FedAvgWithAdaptiveConstraint</code>","text":"<p>               Bases: <code>BasicFedAvg</code></p> Source code in <code>fl4health/strategies/fedavg_with_adaptive_constraint.py</code> <pre><code>class FedAvgWithAdaptiveConstraint(BasicFedAvg):\n    def __init__(\n        self,\n        *,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters | None,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        initial_loss_weight: float = 1.0,\n        adapt_loss_weight: bool = False,\n        loss_weight_delta: float = 0.1,\n        loss_weight_patience: int = 5,\n        weighted_aggregation: bool = True,\n        weighted_eval_losses: bool = True,\n        weighted_train_losses: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        A generalization of the fedavg strategy for approaches that use a penalty constraint that we might want to\n        adapt based on the loss trajectory. A quintessential example is FedProx, which uses an \\\\(\\\\ell^2\\\\): penalty\n        on model weight drift and potentially adapts the coefficient based on the aggregated loss. In addition to the\n        model weights, the server also receives the training loss from the clients. If adaptation is enabled, these\n        losses are used to update the loss weight parameter according to the FedProx paper recommendations.\n\n        **NOTE**: Initial parameters are **NOT** optional. They must be passed for this strategy.\n\n        The aggregation strategy for weights is the same as in FedAvg.\n\n        Implementation based on https://arxiv.org/abs/1602.05629.\n\n        Args:\n            fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n            min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of total clients in the system.\n                Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for central server-side evaluation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional):\n                Function used to configure client-side validation by providing a ``Config`` dictionary.\n                Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            initial_parameters (Parameters | None, optional): Initial global model parameters.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            initial_loss_weight (float): Initial loss weight (mu in FedProx). If adaptivity is false, then this is the\n                constant weight used for all clients.\n            adapt_loss_weight (bool, optional): Determines whether the value of mu is adaptively modified by\n                the server based on aggregated train loss. Defaults to False.\n            loss_weight_delta (float, optional): This is the amount by which the server changes the value of mu\n                based on the modification criteria. Only applicable if adaptivity is on. Defaults to 0.1.\n            loss_weight_patience (int, optional): This is the number of rounds a server must see decreasing\n                aggregated train loss before reducing the value of mu. Only applicable if adaptivity is on.\n                Defaults to 5.\n            weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n                average or a uniform average. FedAvg default is weighted average by client dataset counts.\n                Defaults to True.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n                counts. Defaults to True.\n            weighted_train_losses (bool, optional): Determines whether the training losses from the clients should be\n                aggregated using a weighted or unweighted average. These aggregated losses are used to adjust the\n                proximal weight in the adaptive setting. Defaults to False.\n        \"\"\"\n        self.loss_weight = initial_loss_weight\n        self.adapt_loss_weight = adapt_loss_weight\n\n        if self.adapt_loss_weight:\n            self.loss_weight_delta = loss_weight_delta\n            self.loss_weight_patience = loss_weight_patience\n            self.loss_weight_patience_counter: int = 0\n\n        self.previous_loss = float(\"inf\")\n\n        if initial_parameters:\n            self.add_auxiliary_information(initial_parameters)\n\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_fit_clients=min_fit_clients,\n            min_evaluate_clients=min_evaluate_clients,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            weighted_aggregation=weighted_aggregation,\n            weighted_eval_losses=weighted_eval_losses,\n        )\n        self.parameter_packer = ParameterPackerAdaptiveConstraint()\n        self.weighted_train_losses = weighted_train_losses\n\n    def add_auxiliary_information(self, original_parameters: Parameters) -&gt; None:\n        \"\"\"\n        Function for adding in the ``loss_weight`` to the provided set of parameters. This function is meant to be\n        called after a server requests model weight initialization from a client, allowing the proper information to\n        be included with the model parameters when sent to all clients for model initialization etc.\n\n        Args:\n            original_parameters (Parameters): Original set of parameters provided by a client for model weight\n                initialization\n        \"\"\"\n        original_parameters.tensors.extend(ndarrays_to_parameters([np.array(self.loss_weight)]).tensors)\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate the results from the federated fit round and, if applicable, determine whether the constraint weight\n        should be updated based on the aggregated loss seen on the clients.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n                that need to be aggregated on the server-side. For adaptive constraints, the clients pack the weights\n                to be aggregated along with the training loss seen during their local training cycle.\n            failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n                from clients that experienced an issue during training, such as timeouts or exceptions.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n                For adaptive constraints, the server also packs a constraint weight to be sent to the clients. This is\n                sent even if adaptive constraint weights are turned off and the value simply remains constant.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n        # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n        # reducing numerical fluctuation.\n        decoded_and_sorted_results = [\n            (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n        ]\n\n        # Convert results with packed params of model weights and training loss\n        weights_and_counts: list[tuple[NDArrays, int]] = []\n        train_losses_and_counts: list[tuple[int, float]] = []\n        for weights, sample_count in decoded_and_sorted_results:\n            updated_weights, train_loss = self.parameter_packer.unpack_parameters(weights)\n            weights_and_counts.append((updated_weights, sample_count))\n            train_losses_and_counts.append((sample_count, train_loss))\n\n        # Aggregate them in a weighted or unweighted fashion based on settings.\n        weights_aggregated = aggregate_results(weights_and_counts, self.weighted_aggregation)\n\n        # Aggregate train loss\n        train_losses_aggregated = aggregate_losses(train_losses_and_counts, self.weighted_train_losses)\n\n        self._maybe_update_constraint_weight_param(train_losses_aggregated)\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.fit_metrics_aggregation_fn:\n            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n        parameters = self.parameter_packer.pack_parameters(weights_aggregated, self.loss_weight)\n        return ndarrays_to_parameters(parameters), metrics_aggregated\n\n    def _maybe_update_constraint_weight_param(self, loss: float) -&gt; None:\n        \"\"\"\n        Update constraint weight parameter if ``adaptive_loss_weight`` is set to True. Regardless of whether adaptivity\n        is turned on at this time, the previous loss seen by the server is updated.\n\n        **NOTE**: For adaptive constraint losses, including FedProx, this loss is exchanged (along with the\n        weights) by each client and is the **VANILLA** loss that does not include the additional penalty losses.\n\n        Args:\n            loss (float): This is the loss to which we compare the previous loss seen by the server. For Adaptive\n            Constraint clients this should be the aggregated training loss seen by each client participating in\n            training.\n        \"\"\"\n        if self.adapt_loss_weight:\n            if loss &lt;= self.previous_loss:\n                self.loss_weight_patience_counter += 1\n                if self.loss_weight_patience_counter == self.loss_weight_patience:\n                    self.loss_weight -= self.loss_weight_delta\n                    self.loss_weight = max(0.0, self.loss_weight)\n                    self.loss_weight_patience_counter = 0\n                    log(INFO, f\"Aggregate training loss has dropped {self.loss_weight_patience} rounds in a row\")\n                    log(INFO, f\"Constraint weight is decreased to {self.loss_weight}\")\n            else:\n                self.loss_weight += self.loss_weight_delta\n                self.loss_weight_patience_counter = 0\n                log(\n                    INFO,\n                    f\"Aggregate training loss increased this round: Current loss {loss}, \"\n                    f\"Previous loss: {self.previous_loss}\",\n                )\n                log(INFO, f\"Constraint weight is increased by {self.loss_weight_delta} to {self.loss_weight}\")\n        self.previous_loss = loss\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_with_adaptive_constraint.FedAvgWithAdaptiveConstraint.__init__","title":"<code>__init__(*, fraction_fit=1.0, fraction_evaluate=1.0, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, initial_loss_weight=1.0, adapt_loss_weight=False, loss_weight_delta=0.1, loss_weight_patience=5, weighted_aggregation=True, weighted_eval_losses=True, weighted_train_losses=False)</code>","text":"<p>A generalization of the fedavg strategy for approaches that use a penalty constraint that we might want to adapt based on the loss trajectory. A quintessential example is FedProx, which uses an \\(\\ell^2\\): penalty on model weight drift and potentially adapts the coefficient based on the aggregated loss. In addition to the model weights, the server also receives the training loss from the clients. If adaptation is enabled, these losses are used to update the loss weight parameter according to the FedProx paper recommendations.</p> <p>NOTE: Initial parameters are NOT optional. They must be passed for this strategy.</p> <p>The aggregation strategy for weights is the same as in FedAvg.</p> <p>Implementation based on https://arxiv.org/abs/1602.05629.</p> <p>Parameters:</p> Name Type Description Default <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. Defaults to 1.0.</p> <code>1.0</code> <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during training. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of total clients in the system. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>initial_parameters</code> <code>Parameters | None</code> <p>Initial global model parameters.</p> required <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>initial_loss_weight</code> <code>float</code> <p>Initial loss weight (mu in FedProx). If adaptivity is false, then this is the constant weight used for all clients.</p> <code>1.0</code> <code>adapt_loss_weight</code> <code>bool</code> <p>Determines whether the value of mu is adaptively modified by the server based on aggregated train loss. Defaults to False.</p> <code>False</code> <code>loss_weight_delta</code> <code>float</code> <p>This is the amount by which the server changes the value of mu based on the modification criteria. Only applicable if adaptivity is on. Defaults to 0.1.</p> <code>0.1</code> <code>loss_weight_patience</code> <code>int</code> <p>This is the number of rounds a server must see decreasing aggregated train loss before reducing the value of mu. Only applicable if adaptivity is on. Defaults to 5.</p> <code>5</code> <code>weighted_aggregation</code> <code>bool</code> <p>Determines whether parameter aggregation is a linearly weighted average or a uniform average. FedAvg default is weighted average by client dataset counts. Defaults to True.</p> <code>True</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. FedAvg default is weighted average of the losses by client dataset counts. Defaults to True.</p> <code>True</code> <code>weighted_train_losses</code> <code>bool</code> <p>Determines whether the training losses from the clients should be aggregated using a weighted or unweighted average. These aggregated losses are used to adjust the proximal weight in the adaptive setting. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/strategies/fedavg_with_adaptive_constraint.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters | None,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    initial_loss_weight: float = 1.0,\n    adapt_loss_weight: bool = False,\n    loss_weight_delta: float = 0.1,\n    loss_weight_patience: int = 5,\n    weighted_aggregation: bool = True,\n    weighted_eval_losses: bool = True,\n    weighted_train_losses: bool = False,\n) -&gt; None:\n    \"\"\"\n    A generalization of the fedavg strategy for approaches that use a penalty constraint that we might want to\n    adapt based on the loss trajectory. A quintessential example is FedProx, which uses an \\\\(\\\\ell^2\\\\): penalty\n    on model weight drift and potentially adapts the coefficient based on the aggregated loss. In addition to the\n    model weights, the server also receives the training loss from the clients. If adaptation is enabled, these\n    losses are used to update the loss weight parameter according to the FedProx paper recommendations.\n\n    **NOTE**: Initial parameters are **NOT** optional. They must be passed for this strategy.\n\n    The aggregation strategy for weights is the same as in FedAvg.\n\n    Implementation based on https://arxiv.org/abs/1602.05629.\n\n    Args:\n        fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n        min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of total clients in the system.\n            Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for central server-side evaluation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional):\n            Function used to configure client-side validation by providing a ``Config`` dictionary.\n            Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        initial_parameters (Parameters | None, optional): Initial global model parameters.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        initial_loss_weight (float): Initial loss weight (mu in FedProx). If adaptivity is false, then this is the\n            constant weight used for all clients.\n        adapt_loss_weight (bool, optional): Determines whether the value of mu is adaptively modified by\n            the server based on aggregated train loss. Defaults to False.\n        loss_weight_delta (float, optional): This is the amount by which the server changes the value of mu\n            based on the modification criteria. Only applicable if adaptivity is on. Defaults to 0.1.\n        loss_weight_patience (int, optional): This is the number of rounds a server must see decreasing\n            aggregated train loss before reducing the value of mu. Only applicable if adaptivity is on.\n            Defaults to 5.\n        weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n            average or a uniform average. FedAvg default is weighted average by client dataset counts.\n            Defaults to True.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n            counts. Defaults to True.\n        weighted_train_losses (bool, optional): Determines whether the training losses from the clients should be\n            aggregated using a weighted or unweighted average. These aggregated losses are used to adjust the\n            proximal weight in the adaptive setting. Defaults to False.\n    \"\"\"\n    self.loss_weight = initial_loss_weight\n    self.adapt_loss_weight = adapt_loss_weight\n\n    if self.adapt_loss_weight:\n        self.loss_weight_delta = loss_weight_delta\n        self.loss_weight_patience = loss_weight_patience\n        self.loss_weight_patience_counter: int = 0\n\n    self.previous_loss = float(\"inf\")\n\n    if initial_parameters:\n        self.add_auxiliary_information(initial_parameters)\n\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_fit_clients=min_fit_clients,\n        min_evaluate_clients=min_evaluate_clients,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        weighted_aggregation=weighted_aggregation,\n        weighted_eval_losses=weighted_eval_losses,\n    )\n    self.parameter_packer = ParameterPackerAdaptiveConstraint()\n    self.weighted_train_losses = weighted_train_losses\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_with_adaptive_constraint.FedAvgWithAdaptiveConstraint.add_auxiliary_information","title":"<code>add_auxiliary_information(original_parameters)</code>","text":"<p>Function for adding in the <code>loss_weight</code> to the provided set of parameters. This function is meant to be called after a server requests model weight initialization from a client, allowing the proper information to be included with the model parameters when sent to all clients for model initialization etc.</p> <p>Parameters:</p> Name Type Description Default <code>original_parameters</code> <code>Parameters</code> <p>Original set of parameters provided by a client for model weight initialization</p> required Source code in <code>fl4health/strategies/fedavg_with_adaptive_constraint.py</code> <pre><code>def add_auxiliary_information(self, original_parameters: Parameters) -&gt; None:\n    \"\"\"\n    Function for adding in the ``loss_weight`` to the provided set of parameters. This function is meant to be\n    called after a server requests model weight initialization from a client, allowing the proper information to\n    be included with the model parameters when sent to all clients for model initialization etc.\n\n    Args:\n        original_parameters (Parameters): Original set of parameters provided by a client for model weight\n            initialization\n    \"\"\"\n    original_parameters.tensors.extend(ndarrays_to_parameters([np.array(self.loss_weight)]).tensors)\n</code></pre>"},{"location":"api/#fl4health.strategies.fedavg_with_adaptive_constraint.FedAvgWithAdaptiveConstraint.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Aggregate the results from the federated fit round and, if applicable, determine whether the constraint weight should be updated based on the aggregated loss seen on the clients.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The client identifiers and the results of their local training that need to be aggregated on the server-side. For adaptive constraints, the clients pack the weights to be aggregated along with the training loss seen during their local training cycle.</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>These are the results and exceptions from clients that experienced an issue during training, such as timeouts or exceptions.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar]]</code> <p>The aggregated model weights and the metrics dictionary. For adaptive constraints, the server also packs a constraint weight to be sent to the clients. This is sent even if adaptive constraint weights are turned off and the value simply remains constant.</p> Source code in <code>fl4health/strategies/fedavg_with_adaptive_constraint.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate the results from the federated fit round and, if applicable, determine whether the constraint weight\n    should be updated based on the aggregated loss seen on the clients.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n            that need to be aggregated on the server-side. For adaptive constraints, the clients pack the weights\n            to be aggregated along with the training loss seen during their local training cycle.\n        failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n            from clients that experienced an issue during training, such as timeouts or exceptions.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n            For adaptive constraints, the server also packs a constraint weight to be sent to the clients. This is\n            sent even if adaptive constraint weights are turned off and the value simply remains constant.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n    # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n    # reducing numerical fluctuation.\n    decoded_and_sorted_results = [\n        (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n    ]\n\n    # Convert results with packed params of model weights and training loss\n    weights_and_counts: list[tuple[NDArrays, int]] = []\n    train_losses_and_counts: list[tuple[int, float]] = []\n    for weights, sample_count in decoded_and_sorted_results:\n        updated_weights, train_loss = self.parameter_packer.unpack_parameters(weights)\n        weights_and_counts.append((updated_weights, sample_count))\n        train_losses_and_counts.append((sample_count, train_loss))\n\n    # Aggregate them in a weighted or unweighted fashion based on settings.\n    weights_aggregated = aggregate_results(weights_and_counts, self.weighted_aggregation)\n\n    # Aggregate train loss\n    train_losses_aggregated = aggregate_losses(train_losses_and_counts, self.weighted_train_losses)\n\n    self._maybe_update_constraint_weight_param(train_losses_aggregated)\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.fit_metrics_aggregation_fn:\n        fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n    parameters = self.parameter_packer.pack_parameters(weights_aggregated, self.loss_weight)\n    return ndarrays_to_parameters(parameters), metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga","title":"<code>feddg_ga</code>","text":""},{"location":"api/#fl4health.strategies.feddg_ga.SignalForTypeError","title":"<code>SignalForTypeError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Thrown when there is an error in <code>signal_for_type</code> function.</p> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>class SignalForTypeError(Exception):\n    \"\"\"Thrown when there is an error in ``signal_for_type`` function.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FairnessMetricType","title":"<code>FairnessMetricType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Defines the basic types for fairness metrics, their default names and their default signals.</p> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>class FairnessMetricType(Enum):\n    \"\"\"Defines the basic types for fairness metrics, their default names and their default signals.\"\"\"\n\n    ACCURACY = \"val - prediction - accuracy\"\n    LOSS = \"val - checkpoint\"\n    CUSTOM = \"custom\"\n\n    @classmethod\n    def signal_for_type(cls, fairness_metric_type: \"FairnessMetricType\") -&gt; float:\n        \"\"\"\n        Return the default signal for the given metric type.\n\n        Args:\n            fairness_metric_type (FairnessMetricType): the fairness metric type.\n\n        Raises:\n            SignalForTypeException: if type is ``CUSTOM`` as the signal has to be defined by the user.\n\n        Returns:\n            (float): -1.0 if ``FairnessMetricType.ACCURACY`` or 1.0 if ``FairnessMetricType.LOSS``.\n        \"\"\"\n        # For loss values, large and **positive** gaps imply worse generalization of global\n        # weights to local models. Therefore, we want to **increase** weight for these model\n        # parameters to improve generalization. So signal is positive. For accuracy, large\n        # **negative** gaps imply worse generalization. So the signal is -1.0, to increase\n        # weights for the associated model parameters.\n        if fairness_metric_type == FairnessMetricType.ACCURACY:\n            return -1.0\n        if fairness_metric_type == FairnessMetricType.LOSS:\n            return 1.0\n        raise SignalForTypeError(\"This function should not be called with CUSTOM type.\")\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FairnessMetricType.signal_for_type","title":"<code>signal_for_type(fairness_metric_type)</code>  <code>classmethod</code>","text":"<p>Return the default signal for the given metric type.</p> <p>Parameters:</p> Name Type Description Default <code>fairness_metric_type</code> <code>FairnessMetricType</code> <p>the fairness metric type.</p> required <p>Raises:</p> Type Description <code>SignalForTypeException</code> <p>if type is <code>CUSTOM</code> as the signal has to be defined by the user.</p> <p>Returns:</p> Type Description <code>float</code> <p>-1.0 if <code>FairnessMetricType.ACCURACY</code> or 1.0 if <code>FairnessMetricType.LOSS</code>.</p> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>@classmethod\ndef signal_for_type(cls, fairness_metric_type: \"FairnessMetricType\") -&gt; float:\n    \"\"\"\n    Return the default signal for the given metric type.\n\n    Args:\n        fairness_metric_type (FairnessMetricType): the fairness metric type.\n\n    Raises:\n        SignalForTypeException: if type is ``CUSTOM`` as the signal has to be defined by the user.\n\n    Returns:\n        (float): -1.0 if ``FairnessMetricType.ACCURACY`` or 1.0 if ``FairnessMetricType.LOSS``.\n    \"\"\"\n    # For loss values, large and **positive** gaps imply worse generalization of global\n    # weights to local models. Therefore, we want to **increase** weight for these model\n    # parameters to improve generalization. So signal is positive. For accuracy, large\n    # **negative** gaps imply worse generalization. So the signal is -1.0, to increase\n    # weights for the associated model parameters.\n    if fairness_metric_type == FairnessMetricType.ACCURACY:\n        return -1.0\n    if fairness_metric_type == FairnessMetricType.LOSS:\n        return 1.0\n    raise SignalForTypeError(\"This function should not be called with CUSTOM type.\")\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FairnessMetric","title":"<code>FairnessMetric</code>","text":"Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>class FairnessMetric:\n    def __init__(\n        self,\n        metric_type: FairnessMetricType,\n        metric_name: str | None = None,\n        signal: float | None = None,\n    ):\n        \"\"\"\n        Defines a fairness metric with attributes that can be overridden if needed.\n\n        Instantiates a fairness metric with a type and optional metric name and signal if one wants to override them.\n\n        Args:\n            metric_type (FairnessMetricType): the fairness metric type. If ``CUSTOM``, the ``metric_name`` and\n                signal should be provided.\n            metric_name (str | None, optional): the name of the metric to be used as fairness metric.\n                Mandatory if ``metric_type`` is ``CUSTOM``. Defaults to None.\n            signal (float | None, optional): the signal of the fairness metric.\n                Mandatory if ``metric_type`` is ``CUSTOM``. Defaults to None.\n        \"\"\"\n        self.metric_type = metric_type\n        self.metric_name = metric_name\n        self.signal = signal\n\n        if metric_type is FairnessMetricType.CUSTOM:\n            assert metric_name is not None and signal is not None\n        else:\n            if metric_name is None:\n                self.metric_name = metric_type.value\n            if signal is None:\n                self.signal = FairnessMetricType.signal_for_type(metric_type)\n\n    def __str__(self) -&gt; str:\n        \"\"\"\n        String produced when calling str(...) on a Fairness metric object.\n\n        Returns:\n            (str): Custom string describing the object attributes.\n        \"\"\"\n        return f\"Metric Type: {self.metric_type}, Metric Name: '{self.metric_name}', Signal: {self.signal}\"\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FairnessMetric.__init__","title":"<code>__init__(metric_type, metric_name=None, signal=None)</code>","text":"<p>Defines a fairness metric with attributes that can be overridden if needed.</p> <p>Instantiates a fairness metric with a type and optional metric name and signal if one wants to override them.</p> <p>Parameters:</p> Name Type Description Default <code>metric_type</code> <code>FairnessMetricType</code> <p>the fairness metric type. If <code>CUSTOM</code>, the <code>metric_name</code> and signal should be provided.</p> required <code>metric_name</code> <code>str | None</code> <p>the name of the metric to be used as fairness metric. Mandatory if <code>metric_type</code> is <code>CUSTOM</code>. Defaults to None.</p> <code>None</code> <code>signal</code> <code>float | None</code> <p>the signal of the fairness metric. Mandatory if <code>metric_type</code> is <code>CUSTOM</code>. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>def __init__(\n    self,\n    metric_type: FairnessMetricType,\n    metric_name: str | None = None,\n    signal: float | None = None,\n):\n    \"\"\"\n    Defines a fairness metric with attributes that can be overridden if needed.\n\n    Instantiates a fairness metric with a type and optional metric name and signal if one wants to override them.\n\n    Args:\n        metric_type (FairnessMetricType): the fairness metric type. If ``CUSTOM``, the ``metric_name`` and\n            signal should be provided.\n        metric_name (str | None, optional): the name of the metric to be used as fairness metric.\n            Mandatory if ``metric_type`` is ``CUSTOM``. Defaults to None.\n        signal (float | None, optional): the signal of the fairness metric.\n            Mandatory if ``metric_type`` is ``CUSTOM``. Defaults to None.\n    \"\"\"\n    self.metric_type = metric_type\n    self.metric_name = metric_name\n    self.signal = signal\n\n    if metric_type is FairnessMetricType.CUSTOM:\n        assert metric_name is not None and signal is not None\n    else:\n        if metric_name is None:\n            self.metric_name = metric_type.value\n        if signal is None:\n            self.signal = FairnessMetricType.signal_for_type(metric_type)\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FairnessMetric.__str__","title":"<code>__str__()</code>","text":"<p>String produced when calling str(...) on a Fairness metric object.</p> <p>Returns:</p> Type Description <code>str</code> <p>Custom string describing the object attributes.</p> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    String produced when calling str(...) on a Fairness metric object.\n\n    Returns:\n        (str): Custom string describing the object attributes.\n    \"\"\"\n    return f\"Metric Type: {self.metric_type}, Metric Name: '{self.metric_name}', Signal: {self.signal}\"\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FedDgGa","title":"<code>FedDgGa</code>","text":"<p>               Bases: <code>FedAvg</code></p> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>class FedDgGa(FedAvg):\n    def __init__(\n        self,\n        *,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters | None = None,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        fairness_metric: FairnessMetric | None = None,\n        adjustment_weight_step_size: float = 0.2,\n    ):\n        \"\"\"\n        Strategy for the FedDG-GA algorithm (Federated Domain Generalization with Generalization Adjustment, Zhang et\n        al. 2023). This strategy assumes (and checks) that the configuration sent by the server to the clients has the\n        key \"evaluate_after_fit\" and it is set to True. It also ensures that the key \"pack_losses_with_val_metrics\" is\n        present and its value is set to True. These are to facilitate the exchange of evaluation information needed\n        for the strategy to work correctly.\n\n        **NOTE**: For FedDG-GA, we require that ``fraction_fit`` and ``fraction_evaluate`` are 1.0, as behavior of the\n        FedDG-GA algorithm is not well-defined when participation in each round of training and evaluation is partial.\n        Thus, we force these values to be 1.0 in super and do not allow them to be set by the user.\n\n        Args:\n            min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for validation.. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training. Must be specified for this strategy.. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                validation. Must be specified for this strategy.. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            fairness_metric (FairnessMetric | None, optional): The metric to evaluate the local model of each client\n                against the global model in order to determine their adjustment weight for aggregation. Can be set to\n                any default metric in ``FairnessMetricType`` or set to use a custom metric. Defaults to None.\n            adjustment_weight_step_size (float, optional): The step size to determine the magnitude of change for the\n                generalization adjustment weights. It has to be ``0 &lt; adjustment_weight_step_size &lt; 1``.\n                Defaults to 0.2.\n        \"\"\"\n        super().__init__(\n            fraction_fit=1.0,\n            fraction_evaluate=1.0,\n            min_fit_clients=min_fit_clients,\n            min_evaluate_clients=min_evaluate_clients,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        )\n\n        if fairness_metric is None:\n            self.fairness_metric = FairnessMetric(FairnessMetricType.LOSS)\n        else:\n            self.fairness_metric = fairness_metric\n\n        self.adjustment_weight_step_size = adjustment_weight_step_size\n        assert 0 &lt; self.adjustment_weight_step_size &lt; 1, (\n            f\"adjustment_weight_step_size has to be between 0 and 1 ({self.adjustment_weight_step_size})\"\n        )\n\n        log(INFO, f\"FedDG-GA Strategy initialized with weight_step_size of {self.adjustment_weight_step_size}\")\n        log(INFO, f\"FedDG-GA Strategy initialized with FairnessMetric {self.fairness_metric}\")\n\n        self.train_metrics: dict[str, dict[str, Scalar]] = {}\n        self.evaluation_metrics: dict[str, dict[str, Scalar]] = {}\n        self.num_rounds: int | None = None\n        self.initial_adjustment_weight: float | None = None\n        self.adjustment_weights: dict[str, float] = {}\n\n    def configure_fit(\n        self,\n        server_round: int,\n        parameters: Parameters,\n        client_manager: ClientManager,\n    ) -&gt; list[tuple[ClientProxy, FitIns]]:\n        \"\"\"\n        Configure the next round of training. Will also collect the number of rounds the training will run for in order\n        to calculate the adjustment weight step size. Fails if ``n_server_rounds`` is not set in the config or if it's\n        not an integer.\n\n        Args:\n            server_round (int): The current server round.\n            parameters (Parameters): The model parameters.\n            client_manager (ClientManager): The client manager which holds all currently connected clients. It must\n                be an instance of ``FixedSamplingClientManager``.\n\n        Returns:\n            (list[tuple[ClientProxy, FitIns]]): the input for the clients' fit function.\n        \"\"\"\n        assert isinstance(client_manager, FixedSamplingClientManager), (\n            f\"Client manager is not of type FixedSamplingClientManager: {type(client_manager)}\"\n        )\n\n        client_manager.reset_sample()\n\n        client_fit_ins = super().configure_fit(server_round, parameters, client_manager)\n\n        self.initial_adjustment_weight = 1.0 / len(client_fit_ins)\n\n        # Setting self.num_rounds once and doing some sanity checks\n        assert self.on_fit_config_fn is not None, \"on_fit_config_fn must be specified\"\n        config = self.on_fit_config_fn(server_round)\n        assert \"evaluate_after_fit\" in config, \"evaluate_after_fit must be present in config\"\n        assert config[\"evaluate_after_fit\"] is True, \"evaluate_after_fit must be set to True\"\n\n        assert \"pack_losses_with_val_metrics\" in config, \"pack_losses_with_val_metrics must be present in config\"\n        assert config[\"pack_losses_with_val_metrics\"] is True, \"pack_losses_with_val_metrics must be set to True\"\n\n        assert \"n_server_rounds\" in config, \"n_server_rounds must be specified\"\n        assert isinstance(config[\"n_server_rounds\"], int), \"n_server_rounds is not an integer\"\n        n_server_rounds = config[\"n_server_rounds\"]\n\n        if self.num_rounds is None:\n            self.num_rounds = n_server_rounds\n        else:\n            assert n_server_rounds == self.num_rounds, (\n                f\"n_server_rounds has changed from the original value of {self.num_rounds} \"\n                f\"and is now {n_server_rounds}\"\n            )\n\n        return client_fit_ins\n\n    def configure_evaluate(\n        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n    ) -&gt; list[tuple[ClientProxy, EvaluateIns]]:\n        assert isinstance(client_manager, FixedSamplingClientManager), (\n            f\"Client manager is not of type FixedSamplingClientManager: {type(client_manager)}\"\n        )\n\n        client_evaluate_ins = super().configure_evaluate(server_round, parameters, client_manager)\n\n        assert self.on_evaluate_config_fn is not None, \"on_fit_config_fn must be specified\"\n        config = self.on_evaluate_config_fn(server_round)\n        assert \"pack_losses_with_val_metrics\" in config, \"pack_losses_with_val_metrics must be present in config\"\n        assert config[\"pack_losses_with_val_metrics\"] is True, \"pack_losses_with_val_metrics must be set to True\"\n\n        return client_evaluate_ins\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate fit results by weighing them against the adjustment weights and then summing them.\n\n        Collects the fit metrics that will be used to change the adjustment weights for the next round.\n\n        Args:\n            server_round (int): The current server round.\n            results (list[tuple[ClientProxy, FitRes]]): The clients' fit results.\n            failures (list[tuple[ClientProxy, FitRes] | BaseException]): The clients' fit failures.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar]]): A tuple containing the aggregated parameters and the\n                aggregated fit metrics.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.fit_metrics_aggregation_fn:\n            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n        self.train_metrics = {}\n        for client_proxy, fit_res in results:\n            self.train_metrics[client_proxy.cid] = fit_res.metrics\n\n        parameters_aggregated = ndarrays_to_parameters(self.weight_and_aggregate_results(results))\n\n        return parameters_aggregated, metrics_aggregated\n\n    def aggregate_evaluate(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, EvaluateRes]],\n        failures: list[tuple[ClientProxy, EvaluateRes] | BaseException],\n    ) -&gt; tuple[float | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate evaluation losses using weighted average.\n\n        Collects the evaluation metrics and updates the adjustment weights, which will be used when aggregating the\n        results for the next round.\n\n        Args:\n            server_round (int): The current server round.\n            results (list[tuple[ClientProxy, FitRes]]): The clients' evaluate results.\n            failures (list[tuple[ClientProxy, FitRes] | BaseException]): the clients' evaluate failures.\n\n        Returns:\n            (tuple[float | None, dict[str, Scalar]]): A tuple containing the aggregated evaluation loss and the\n                aggregated evaluation metrics.\n        \"\"\"\n        loss_aggregated, metrics_aggregated = super().aggregate_evaluate(server_round, results, failures)\n\n        self.evaluation_metrics = {}\n        for client_proxy, eval_res in results:\n            cid = client_proxy.cid\n            # make sure that the metrics has the desired loss key\n            assert FairnessMetricType.LOSS.value in eval_res.metrics\n            self.evaluation_metrics[cid] = eval_res.metrics\n\n        # Updating the weights at the end of the training round\n        cids = [client_proxy.cid for client_proxy, _ in results]\n        log(INFO, \"Updating the Generalization Adjustment Weights\")\n        self.update_weights_by_ga(server_round, cids)\n\n        return loss_aggregated, metrics_aggregated\n\n    def weight_and_aggregate_results(self, results: list[tuple[ClientProxy, FitRes]]) -&gt; NDArrays:\n        \"\"\"\n        Aggregate results by weighing them against the adjustment weights and then summing them.\n\n        Args:\n            results (list[tuple[ClientProxy, FitRes]]): The clients' fit results.\n\n        Returns:\n            (NDArrays): The weighted and aggregated results.\n        \"\"\"\n        if self.adjustment_weights:\n            log(INFO, f\"Current adjustment weights by Client ID (CID) are {self.adjustment_weights}\")\n        else:\n            # If the adjustment weights dictionary doesn't exist, it means that it hasn't been initialized\n            # and will be below.\n            log(INFO, f\"Current adjustment weights are all initialized to {self.initial_adjustment_weight}\")\n\n        # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n        # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n        # reducing numerical fluctuation.\n        decoded_and_sorted_results = decode_and_pseudo_sort_results(results)\n\n        aggregated_results: NDArrays | None = None\n        for client_proxy, weights, _ in decoded_and_sorted_results:\n            cid = client_proxy.cid\n\n            # initializing adjustment weights for this client if they don't exist yet\n            if cid not in self.adjustment_weights:\n                assert self.initial_adjustment_weight is not None\n                self.adjustment_weights[cid] = self.initial_adjustment_weight\n\n            # apply adjustment weights\n            weighted_client_parameters = weights\n            for i in range(len(weighted_client_parameters)):\n                weighted_client_parameters[i] = weighted_client_parameters[i] * self.adjustment_weights[cid]\n\n            # sum weighted parameters\n            if aggregated_results is None:\n                # If this is the first client we're applying adjustment to, we set the results to those parameters.\n                # Remaining client parameters will be subsequently added to these.\n                aggregated_results = weighted_client_parameters\n            else:\n                assert len(weighted_client_parameters) == len(aggregated_results)\n                for i in range(len(weighted_client_parameters)):\n                    aggregated_results[i] = aggregated_results[i] + weighted_client_parameters[i]\n\n        assert aggregated_results is not None\n        return aggregated_results\n\n    def update_weights_by_ga(self, server_round: int, cids: list[str]) -&gt; None:\n        \"\"\"\n        Update the ``self.adjustment_weights`` dictionary by calculating the new weights based on the current server\n        round, fit and evaluation metrics.\n\n        Args:\n            server_round (int): The current server round.\n            cids (list[str]): The list of client ids that participated in this round.\n        \"\"\"\n        generalization_gaps = []\n        # calculating local vs global metric difference (generalization gaps)\n        for cid in cids:\n            assert cid in self.train_metrics and cid in self.evaluation_metrics, (\n                f\"{cid} not in {self.train_metrics.keys()} or {self.evaluation_metrics.keys()}\"\n            )\n\n            assert self.fairness_metric.metric_name is not None\n\n            global_model_metric_value = self.evaluation_metrics[cid][self.fairness_metric.metric_name]\n            local_model_metric_value = self.train_metrics[cid][self.fairness_metric.metric_name]\n            assert isinstance(global_model_metric_value, float) and isinstance(local_model_metric_value, float)\n\n            generalization_gaps.append(global_model_metric_value - local_model_metric_value)\n\n        log(\n            INFO,\n            \"Client ID (CID) and Generalization Gaps (G_{{hat{{D_i}}}}(theta^r)): \"\n            f\"{list(zip(cids, generalization_gaps))}\",\n        )\n\n        # Calculating the normalized generalization gaps\n        generalization_gaps_ndarray = np.array(generalization_gaps)\n        mean_generalization_gap = np.mean(generalization_gaps_ndarray)\n        var_generalization_gaps = generalization_gaps_ndarray - mean_generalization_gap\n        max_var_generalization_gap = np.max(np.abs(var_generalization_gaps))\n        log(INFO, f\"Mean Generalization Gap (mu): {mean_generalization_gap}\")\n        log(INFO, f\"Max Absolute Deviation of Generalization Gaps: {max_var_generalization_gap}\")\n\n        if max_var_generalization_gap == 0:\n            log(\n                WARNING,\n                \"Max variance in generalization gap is 0. Adjustment weights will remain the same. \"\n                + f\"Generalization gaps: {generalization_gaps}\",\n            )\n            normalized_generalization_gaps = np.zeros_like(generalization_gaps)\n        else:\n            step_size = self.get_current_weight_step_size(server_round)\n            normalized_generalization_gaps = (var_generalization_gaps * step_size) / max_var_generalization_gap\n\n        # updating weights\n        new_total_weight = 0.0\n        for i in range(len(cids)):\n            cid = cids[i]\n            # For loss values, large and **positive** gaps imply worse generalization of global\n            # weights to local models. Therefore, we want to **increase** weight for these model\n            # parameters to improve generalization. So signal is positive. For accuracy, large\n            # **negative** gaps imply worse generalization. So the signal is -1.0, to increase\n            # weights for the associated model parameters.\n            self.adjustment_weights[cid] += self.fairness_metric.signal * normalized_generalization_gaps[i]\n\n            # Weight clip\n            # The paper states the clipping only happens for values below 0 but the reference\n            # implementation also clips values larger than 1, probably as an extra assurance.\n            clipped_weight = np.clip(self.adjustment_weights[cid], 0.0, 1.0)\n            self.adjustment_weights[cid] = clipped_weight\n            new_total_weight += clipped_weight\n\n        for cid in cids:\n            self.adjustment_weights[cid] /= new_total_weight\n        log(INFO, f\"New Generalization Adjustment Weights by Client ID (CID) are {self.adjustment_weights}\")\n\n    def get_current_weight_step_size(self, server_round: int) -&gt; float:\n        \"\"\"\n        Calculates the current weight step size based on the current server round,  weight step size and total number\n        of rounds.\n\n        Args:\n            server_round (int): the current server round\n\n        Returns (float):\n            the current value for the weight step size.\n        \"\"\"\n        # The implementation of d^r here differs from the definition in the paper\n        # because our server round starts at 1 instead of 0.\n        assert self.num_rounds is not None\n        weight_step_size_decay = self.adjustment_weight_step_size / self.num_rounds\n        weight_step_size_for_round = self.adjustment_weight_step_size - ((server_round - 1) * weight_step_size_decay)\n        log(\n            INFO, f\"Step size for round: {weight_step_size_for_round}, original was {self.adjustment_weight_step_size}\"\n        )\n\n        # Omitting an additional scaler here that is present in the reference\n        # implementation but not in the paper:\n        # weight_step_size_for_round *= self.initial_adjustment_weight\n\n        return weight_step_size_for_round\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FedDgGa.__init__","title":"<code>__init__(*, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters=None, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, fairness_metric=None, adjustment_weight_step_size=0.2)</code>","text":"<p>Strategy for the FedDG-GA algorithm (Federated Domain Generalization with Generalization Adjustment, Zhang et al. 2023). This strategy assumes (and checks) that the configuration sent by the server to the clients has the key \"evaluate_after_fit\" and it is set to True. It also ensures that the key \"pack_losses_with_val_metrics\" is present and its value is set to True. These are to facilitate the exchange of evaluation information needed for the strategy to work correctly.</p> <p>NOTE: For FedDG-GA, we require that <code>fraction_fit</code> and <code>fraction_evaluate</code> are 1.0, as behavior of the FedDG-GA algorithm is not well-defined when participation in each round of training and evaluation is partial. Thus, we force these values to be 1.0 in super and do not allow them to be set by the user.</p> <p>Parameters:</p> Name Type Description Default <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during training. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of total clients in the system. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for validation.. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training. Must be specified for this strategy.. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure validation. Must be specified for this strategy.. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>initial_parameters</code> <code>Parameters | None</code> <p>Initial global model parameters. Defaults to None.</p> <code>None</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>fairness_metric</code> <code>FairnessMetric | None</code> <p>The metric to evaluate the local model of each client against the global model in order to determine their adjustment weight for aggregation. Can be set to any default metric in <code>FairnessMetricType</code> or set to use a custom metric. Defaults to None.</p> <code>None</code> <code>adjustment_weight_step_size</code> <code>float</code> <p>The step size to determine the magnitude of change for the generalization adjustment weights. It has to be <code>0 &lt; adjustment_weight_step_size &lt; 1</code>. Defaults to 0.2.</p> <code>0.2</code> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>def __init__(\n    self,\n    *,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters | None = None,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    fairness_metric: FairnessMetric | None = None,\n    adjustment_weight_step_size: float = 0.2,\n):\n    \"\"\"\n    Strategy for the FedDG-GA algorithm (Federated Domain Generalization with Generalization Adjustment, Zhang et\n    al. 2023). This strategy assumes (and checks) that the configuration sent by the server to the clients has the\n    key \"evaluate_after_fit\" and it is set to True. It also ensures that the key \"pack_losses_with_val_metrics\" is\n    present and its value is set to True. These are to facilitate the exchange of evaluation information needed\n    for the strategy to work correctly.\n\n    **NOTE**: For FedDG-GA, we require that ``fraction_fit`` and ``fraction_evaluate`` are 1.0, as behavior of the\n    FedDG-GA algorithm is not well-defined when participation in each round of training and evaluation is partial.\n    Thus, we force these values to be 1.0 in super and do not allow them to be set by the user.\n\n    Args:\n        min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for validation.. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training. Must be specified for this strategy.. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            validation. Must be specified for this strategy.. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        fairness_metric (FairnessMetric | None, optional): The metric to evaluate the local model of each client\n            against the global model in order to determine their adjustment weight for aggregation. Can be set to\n            any default metric in ``FairnessMetricType`` or set to use a custom metric. Defaults to None.\n        adjustment_weight_step_size (float, optional): The step size to determine the magnitude of change for the\n            generalization adjustment weights. It has to be ``0 &lt; adjustment_weight_step_size &lt; 1``.\n            Defaults to 0.2.\n    \"\"\"\n    super().__init__(\n        fraction_fit=1.0,\n        fraction_evaluate=1.0,\n        min_fit_clients=min_fit_clients,\n        min_evaluate_clients=min_evaluate_clients,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n    )\n\n    if fairness_metric is None:\n        self.fairness_metric = FairnessMetric(FairnessMetricType.LOSS)\n    else:\n        self.fairness_metric = fairness_metric\n\n    self.adjustment_weight_step_size = adjustment_weight_step_size\n    assert 0 &lt; self.adjustment_weight_step_size &lt; 1, (\n        f\"adjustment_weight_step_size has to be between 0 and 1 ({self.adjustment_weight_step_size})\"\n    )\n\n    log(INFO, f\"FedDG-GA Strategy initialized with weight_step_size of {self.adjustment_weight_step_size}\")\n    log(INFO, f\"FedDG-GA Strategy initialized with FairnessMetric {self.fairness_metric}\")\n\n    self.train_metrics: dict[str, dict[str, Scalar]] = {}\n    self.evaluation_metrics: dict[str, dict[str, Scalar]] = {}\n    self.num_rounds: int | None = None\n    self.initial_adjustment_weight: float | None = None\n    self.adjustment_weights: dict[str, float] = {}\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FedDgGa.configure_fit","title":"<code>configure_fit(server_round, parameters, client_manager)</code>","text":"<p>Configure the next round of training. Will also collect the number of rounds the training will run for in order to calculate the adjustment weight step size. Fails if <code>n_server_rounds</code> is not set in the config or if it's not an integer.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>The current server round.</p> required <code>parameters</code> <code>Parameters</code> <p>The model parameters.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The client manager which holds all currently connected clients. It must be an instance of <code>FixedSamplingClientManager</code>.</p> required <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, FitIns]]</code> <p>the input for the clients' fit function.</p> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>def configure_fit(\n    self,\n    server_round: int,\n    parameters: Parameters,\n    client_manager: ClientManager,\n) -&gt; list[tuple[ClientProxy, FitIns]]:\n    \"\"\"\n    Configure the next round of training. Will also collect the number of rounds the training will run for in order\n    to calculate the adjustment weight step size. Fails if ``n_server_rounds`` is not set in the config or if it's\n    not an integer.\n\n    Args:\n        server_round (int): The current server round.\n        parameters (Parameters): The model parameters.\n        client_manager (ClientManager): The client manager which holds all currently connected clients. It must\n            be an instance of ``FixedSamplingClientManager``.\n\n    Returns:\n        (list[tuple[ClientProxy, FitIns]]): the input for the clients' fit function.\n    \"\"\"\n    assert isinstance(client_manager, FixedSamplingClientManager), (\n        f\"Client manager is not of type FixedSamplingClientManager: {type(client_manager)}\"\n    )\n\n    client_manager.reset_sample()\n\n    client_fit_ins = super().configure_fit(server_round, parameters, client_manager)\n\n    self.initial_adjustment_weight = 1.0 / len(client_fit_ins)\n\n    # Setting self.num_rounds once and doing some sanity checks\n    assert self.on_fit_config_fn is not None, \"on_fit_config_fn must be specified\"\n    config = self.on_fit_config_fn(server_round)\n    assert \"evaluate_after_fit\" in config, \"evaluate_after_fit must be present in config\"\n    assert config[\"evaluate_after_fit\"] is True, \"evaluate_after_fit must be set to True\"\n\n    assert \"pack_losses_with_val_metrics\" in config, \"pack_losses_with_val_metrics must be present in config\"\n    assert config[\"pack_losses_with_val_metrics\"] is True, \"pack_losses_with_val_metrics must be set to True\"\n\n    assert \"n_server_rounds\" in config, \"n_server_rounds must be specified\"\n    assert isinstance(config[\"n_server_rounds\"], int), \"n_server_rounds is not an integer\"\n    n_server_rounds = config[\"n_server_rounds\"]\n\n    if self.num_rounds is None:\n        self.num_rounds = n_server_rounds\n    else:\n        assert n_server_rounds == self.num_rounds, (\n            f\"n_server_rounds has changed from the original value of {self.num_rounds} \"\n            f\"and is now {n_server_rounds}\"\n        )\n\n    return client_fit_ins\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FedDgGa.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Aggregate fit results by weighing them against the adjustment weights and then summing them.</p> <p>Collects the fit metrics that will be used to change the adjustment weights for the next round.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>The current server round.</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The clients' fit results.</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>The clients' fit failures.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar]]</code> <p>A tuple containing the aggregated parameters and the aggregated fit metrics.</p> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate fit results by weighing them against the adjustment weights and then summing them.\n\n    Collects the fit metrics that will be used to change the adjustment weights for the next round.\n\n    Args:\n        server_round (int): The current server round.\n        results (list[tuple[ClientProxy, FitRes]]): The clients' fit results.\n        failures (list[tuple[ClientProxy, FitRes] | BaseException]): The clients' fit failures.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar]]): A tuple containing the aggregated parameters and the\n            aggregated fit metrics.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.fit_metrics_aggregation_fn:\n        fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n    self.train_metrics = {}\n    for client_proxy, fit_res in results:\n        self.train_metrics[client_proxy.cid] = fit_res.metrics\n\n    parameters_aggregated = ndarrays_to_parameters(self.weight_and_aggregate_results(results))\n\n    return parameters_aggregated, metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FedDgGa.aggregate_evaluate","title":"<code>aggregate_evaluate(server_round, results, failures)</code>","text":"<p>Aggregate evaluation losses using weighted average.</p> <p>Collects the evaluation metrics and updates the adjustment weights, which will be used when aggregating the results for the next round.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>The current server round.</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The clients' evaluate results.</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>the clients' evaluate failures.</p> required <p>Returns:</p> Type Description <code>tuple[float | None, dict[str, Scalar]]</code> <p>A tuple containing the aggregated evaluation loss and the aggregated evaluation metrics.</p> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>def aggregate_evaluate(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, EvaluateRes]],\n    failures: list[tuple[ClientProxy, EvaluateRes] | BaseException],\n) -&gt; tuple[float | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate evaluation losses using weighted average.\n\n    Collects the evaluation metrics and updates the adjustment weights, which will be used when aggregating the\n    results for the next round.\n\n    Args:\n        server_round (int): The current server round.\n        results (list[tuple[ClientProxy, FitRes]]): The clients' evaluate results.\n        failures (list[tuple[ClientProxy, FitRes] | BaseException]): the clients' evaluate failures.\n\n    Returns:\n        (tuple[float | None, dict[str, Scalar]]): A tuple containing the aggregated evaluation loss and the\n            aggregated evaluation metrics.\n    \"\"\"\n    loss_aggregated, metrics_aggregated = super().aggregate_evaluate(server_round, results, failures)\n\n    self.evaluation_metrics = {}\n    for client_proxy, eval_res in results:\n        cid = client_proxy.cid\n        # make sure that the metrics has the desired loss key\n        assert FairnessMetricType.LOSS.value in eval_res.metrics\n        self.evaluation_metrics[cid] = eval_res.metrics\n\n    # Updating the weights at the end of the training round\n    cids = [client_proxy.cid for client_proxy, _ in results]\n    log(INFO, \"Updating the Generalization Adjustment Weights\")\n    self.update_weights_by_ga(server_round, cids)\n\n    return loss_aggregated, metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FedDgGa.weight_and_aggregate_results","title":"<code>weight_and_aggregate_results(results)</code>","text":"<p>Aggregate results by weighing them against the adjustment weights and then summing them.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The clients' fit results.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>The weighted and aggregated results.</p> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>def weight_and_aggregate_results(self, results: list[tuple[ClientProxy, FitRes]]) -&gt; NDArrays:\n    \"\"\"\n    Aggregate results by weighing them against the adjustment weights and then summing them.\n\n    Args:\n        results (list[tuple[ClientProxy, FitRes]]): The clients' fit results.\n\n    Returns:\n        (NDArrays): The weighted and aggregated results.\n    \"\"\"\n    if self.adjustment_weights:\n        log(INFO, f\"Current adjustment weights by Client ID (CID) are {self.adjustment_weights}\")\n    else:\n        # If the adjustment weights dictionary doesn't exist, it means that it hasn't been initialized\n        # and will be below.\n        log(INFO, f\"Current adjustment weights are all initialized to {self.initial_adjustment_weight}\")\n\n    # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n    # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n    # reducing numerical fluctuation.\n    decoded_and_sorted_results = decode_and_pseudo_sort_results(results)\n\n    aggregated_results: NDArrays | None = None\n    for client_proxy, weights, _ in decoded_and_sorted_results:\n        cid = client_proxy.cid\n\n        # initializing adjustment weights for this client if they don't exist yet\n        if cid not in self.adjustment_weights:\n            assert self.initial_adjustment_weight is not None\n            self.adjustment_weights[cid] = self.initial_adjustment_weight\n\n        # apply adjustment weights\n        weighted_client_parameters = weights\n        for i in range(len(weighted_client_parameters)):\n            weighted_client_parameters[i] = weighted_client_parameters[i] * self.adjustment_weights[cid]\n\n        # sum weighted parameters\n        if aggregated_results is None:\n            # If this is the first client we're applying adjustment to, we set the results to those parameters.\n            # Remaining client parameters will be subsequently added to these.\n            aggregated_results = weighted_client_parameters\n        else:\n            assert len(weighted_client_parameters) == len(aggregated_results)\n            for i in range(len(weighted_client_parameters)):\n                aggregated_results[i] = aggregated_results[i] + weighted_client_parameters[i]\n\n    assert aggregated_results is not None\n    return aggregated_results\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FedDgGa.update_weights_by_ga","title":"<code>update_weights_by_ga(server_round, cids)</code>","text":"<p>Update the <code>self.adjustment_weights</code> dictionary by calculating the new weights based on the current server round, fit and evaluation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>The current server round.</p> required <code>cids</code> <code>list[str]</code> <p>The list of client ids that participated in this round.</p> required Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>def update_weights_by_ga(self, server_round: int, cids: list[str]) -&gt; None:\n    \"\"\"\n    Update the ``self.adjustment_weights`` dictionary by calculating the new weights based on the current server\n    round, fit and evaluation metrics.\n\n    Args:\n        server_round (int): The current server round.\n        cids (list[str]): The list of client ids that participated in this round.\n    \"\"\"\n    generalization_gaps = []\n    # calculating local vs global metric difference (generalization gaps)\n    for cid in cids:\n        assert cid in self.train_metrics and cid in self.evaluation_metrics, (\n            f\"{cid} not in {self.train_metrics.keys()} or {self.evaluation_metrics.keys()}\"\n        )\n\n        assert self.fairness_metric.metric_name is not None\n\n        global_model_metric_value = self.evaluation_metrics[cid][self.fairness_metric.metric_name]\n        local_model_metric_value = self.train_metrics[cid][self.fairness_metric.metric_name]\n        assert isinstance(global_model_metric_value, float) and isinstance(local_model_metric_value, float)\n\n        generalization_gaps.append(global_model_metric_value - local_model_metric_value)\n\n    log(\n        INFO,\n        \"Client ID (CID) and Generalization Gaps (G_{{hat{{D_i}}}}(theta^r)): \"\n        f\"{list(zip(cids, generalization_gaps))}\",\n    )\n\n    # Calculating the normalized generalization gaps\n    generalization_gaps_ndarray = np.array(generalization_gaps)\n    mean_generalization_gap = np.mean(generalization_gaps_ndarray)\n    var_generalization_gaps = generalization_gaps_ndarray - mean_generalization_gap\n    max_var_generalization_gap = np.max(np.abs(var_generalization_gaps))\n    log(INFO, f\"Mean Generalization Gap (mu): {mean_generalization_gap}\")\n    log(INFO, f\"Max Absolute Deviation of Generalization Gaps: {max_var_generalization_gap}\")\n\n    if max_var_generalization_gap == 0:\n        log(\n            WARNING,\n            \"Max variance in generalization gap is 0. Adjustment weights will remain the same. \"\n            + f\"Generalization gaps: {generalization_gaps}\",\n        )\n        normalized_generalization_gaps = np.zeros_like(generalization_gaps)\n    else:\n        step_size = self.get_current_weight_step_size(server_round)\n        normalized_generalization_gaps = (var_generalization_gaps * step_size) / max_var_generalization_gap\n\n    # updating weights\n    new_total_weight = 0.0\n    for i in range(len(cids)):\n        cid = cids[i]\n        # For loss values, large and **positive** gaps imply worse generalization of global\n        # weights to local models. Therefore, we want to **increase** weight for these model\n        # parameters to improve generalization. So signal is positive. For accuracy, large\n        # **negative** gaps imply worse generalization. So the signal is -1.0, to increase\n        # weights for the associated model parameters.\n        self.adjustment_weights[cid] += self.fairness_metric.signal * normalized_generalization_gaps[i]\n\n        # Weight clip\n        # The paper states the clipping only happens for values below 0 but the reference\n        # implementation also clips values larger than 1, probably as an extra assurance.\n        clipped_weight = np.clip(self.adjustment_weights[cid], 0.0, 1.0)\n        self.adjustment_weights[cid] = clipped_weight\n        new_total_weight += clipped_weight\n\n    for cid in cids:\n        self.adjustment_weights[cid] /= new_total_weight\n    log(INFO, f\"New Generalization Adjustment Weights by Client ID (CID) are {self.adjustment_weights}\")\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga.FedDgGa.get_current_weight_step_size","title":"<code>get_current_weight_step_size(server_round)</code>","text":"<p>Calculates the current weight step size based on the current server round,  weight step size and total number of rounds.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>the current server round</p> required <p>Returns (float):     the current value for the weight step size.</p> Source code in <code>fl4health/strategies/feddg_ga.py</code> <pre><code>def get_current_weight_step_size(self, server_round: int) -&gt; float:\n    \"\"\"\n    Calculates the current weight step size based on the current server round,  weight step size and total number\n    of rounds.\n\n    Args:\n        server_round (int): the current server round\n\n    Returns (float):\n        the current value for the weight step size.\n    \"\"\"\n    # The implementation of d^r here differs from the definition in the paper\n    # because our server round starts at 1 instead of 0.\n    assert self.num_rounds is not None\n    weight_step_size_decay = self.adjustment_weight_step_size / self.num_rounds\n    weight_step_size_for_round = self.adjustment_weight_step_size - ((server_round - 1) * weight_step_size_decay)\n    log(\n        INFO, f\"Step size for round: {weight_step_size_for_round}, original was {self.adjustment_weight_step_size}\"\n    )\n\n    # Omitting an additional scaler here that is present in the reference\n    # implementation but not in the paper:\n    # weight_step_size_for_round *= self.initial_adjustment_weight\n\n    return weight_step_size_for_round\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga_with_adaptive_constraint","title":"<code>feddg_ga_with_adaptive_constraint</code>","text":""},{"location":"api/#fl4health.strategies.feddg_ga_with_adaptive_constraint.FedDgGaAdaptiveConstraint","title":"<code>FedDgGaAdaptiveConstraint</code>","text":"<p>               Bases: <code>FedDgGa</code></p> Source code in <code>fl4health/strategies/feddg_ga_with_adaptive_constraint.py</code> <pre><code>class FedDgGaAdaptiveConstraint(FedDgGa):\n    def __init__(\n        self,\n        *,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        initial_loss_weight: float = 1.0,\n        adapt_loss_weight: bool = False,\n        loss_weight_delta: float = 0.1,\n        loss_weight_patience: int = 5,\n        weighted_train_losses: bool = False,\n        fairness_metric: FairnessMetric | None = None,\n        adjustment_weight_step_size: float = 0.2,\n    ):\n        \"\"\"\n        Strategy for the FedDG-GA algorithm (Federated Domain Generalization with Generalization Adjustment,\n        Zhang et al. 2023) combined with the Adaptive Strategy for Auxiliary constraints like FedProx. See\n        documentation on ``FedAvgWithAdaptiveConstraint`` for more information.\n\n        **NOTE**: Initial parameters are **NOT** optional. They must be passed for this strategy.\n\n        Args:\n            min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for validation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                validation. Defaults to None\n            initial_parameters (Parameters): Initial global model parameters.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional):\n                Metrics aggregation function, Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional):\n                Metrics aggregation function. Defaults to None.\n            initial_loss_weight (float, optional): Initial penalty loss weight (mu in FedProx). If adaptivity is false,\n                then this is the constant weight used for all clients. Defaults to 1.0.\n            adapt_loss_weight (bool, optional): Determines whether the value of the penalty loss weight is adaptively\n                modified by the server based on aggregated train loss. Defaults to False.\n            loss_weight_delta (float, optional): This is the amount by which the server changes the value of the\n                penalty loss weight based on the modification criteria. Only applicable if adaptivity is on.\n                Defaults to 0.1.\n            loss_weight_patience (int, optional): This is the number of rounds a server must see decreasing\n                aggregated train loss before reducing the value of the penalty loss weight. Only applicable if\n                adaptivity is on. Defaults to 5.\n            weighted_train_losses (bool, optional): Determines whether the training losses from the clients should be\n                aggregated using a weighted or unweighted average. These aggregated losses are used to adjust the\n                proximal weight in the adaptive setting. Defaults to False.\n            fairness_metric (FairnessMetric | None, optional): he metric to evaluate the local model of each\n                client against the global model in order to determine their adjustment weight for aggregation.\n                Can be set to any default metric in ``FairnessMetricType`` or set to use a custom metric.\n                Optional, default is ``FairnessMetric(FairnessMetricType.LOSS)`` when specified as None.\n            adjustment_weight_step_size (float, optional): The step size to determine the magnitude of change for\n                the generalization adjustment weight. It has to be ``0 &lt; adjustment_weight_step_size &lt; 1.``\n                Optional, default is 0.2.\n        \"\"\"\n        self.loss_weight = initial_loss_weight\n        self.adapt_loss_weight = adapt_loss_weight\n\n        if self.adapt_loss_weight:\n            self.loss_weight_delta = loss_weight_delta\n            self.loss_weight_patience = loss_weight_patience\n            self.loss_weight_patience_counter: int = 0\n\n        self.previous_loss = float(\"inf\")\n\n        if initial_parameters:\n            self.add_auxiliary_information(initial_parameters)\n\n        super().__init__(\n            min_fit_clients=min_fit_clients,\n            min_evaluate_clients=min_evaluate_clients,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            fairness_metric=fairness_metric,\n            adjustment_weight_step_size=adjustment_weight_step_size,\n        )\n\n        self.parameter_packer = ParameterPackerAdaptiveConstraint()\n        self.weighted_train_losses = weighted_train_losses\n\n    def add_auxiliary_information(self, original_parameters: Parameters) -&gt; None:\n        \"\"\"\n        Function for adding in the ``loss_weight`` to the provided set of parameters. This function is meant to be\n        called after a server requests model weight initialization from a client, allowing the proper information to\n        be included with the model parameters when sent to all clients for model initialization etc.\n\n        Args:\n            original_parameters (Parameters): Original set of parameters provided by a client for model weight\n                initialization\n        \"\"\"\n        original_parameters.tensors.extend(ndarrays_to_parameters([np.array(self.loss_weight)]).tensors)\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate fit results by weighing them against the adjustment weights and then summing them.\n\n        Collects the fit metrics that will be used to change the adjustment weights for the next round.\n\n        If applicable, determine whether the constraint weight should be updated based on the aggregated loss\n        seen on the clients.\n\n        Args:\n            server_round: (int) The current server round.\n            results: (list[tuple[ClientProxy, FitRes]]) The clients' fit results.\n            failures: (list[tuple[ClientProxy, FitRes] | BaseException]) The clients' fit failures.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar]]) A tuple containing the aggregated parameters\n                and the aggregated fit metrics. For adaptive constraints, the server also packs a constraint weight\n                to be sent to the clients. This is sent even if adaptive constraint weights are turned off and\n                the value simply remains constant.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Convert results with packed params of model weights and training loss. The results list is modified in-place\n        # to only contain model parameters for use in the Fed-DGGA calculations and aggregation\n        train_losses_and_counts = self._unpack_weights_and_losses(results)\n\n        # Aggregate train loss\n        train_losses_aggregated = aggregate_losses(train_losses_and_counts, self.weighted_train_losses)\n        self._maybe_update_constraint_weight_param(train_losses_aggregated)\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.fit_metrics_aggregation_fn:\n            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n        self.train_metrics = {}\n        for client_proxy, fit_res in results:\n            self.train_metrics[client_proxy.cid] = fit_res.metrics\n\n        weights_aggregated = self.weight_and_aggregate_results(results)\n\n        parameters = self.parameter_packer.pack_parameters(weights_aggregated, self.loss_weight)\n        return ndarrays_to_parameters(parameters), metrics_aggregated\n\n    def _unpack_weights_and_losses(self, results: list[tuple[ClientProxy, FitRes]]) -&gt; list[tuple[int, float]]:\n        \"\"\"\n        This function takes results returned from a fit round from each of the participating clients and unpacks the\n        information into the appropriate objects. The parameters contained in the FitRes object are unpacked to\n        separate the model weights from the training losses. The model weights are reinserted into the parameters\n        of the FitRes objects and the losses (along with sample counts) are placed in a list and returned.\n\n        **NOTE**: The results that are passed to this function are **MODIFIED IN-PLACE**.\n\n        Args:\n            results (list[tuple[ClientProxy, FitRes]]): The results produced in a fitting round by each of the clients\n                these the FitRes object contains both model weights and training losses which need to be processed.\n\n        Returns:\n            (list[tuple[int, float]]): A list of the training losses produced by client training\n        \"\"\"\n        train_losses_and_counts: list[tuple[int, float]] = []\n        for _, fit_res in results:\n            sample_count = fit_res.num_examples\n            updated_weights, train_loss = self.parameter_packer.unpack_parameters(\n                parameters_to_ndarrays(fit_res.parameters)\n            )\n            # Modify the parameters in-place to just be the model weights.\n            fit_res.parameters = ndarrays_to_parameters(updated_weights)\n            train_losses_and_counts.append((sample_count, train_loss))\n\n        return train_losses_and_counts\n\n    def _maybe_update_constraint_weight_param(self, loss: float) -&gt; None:\n        \"\"\"\n        Update constraint weight parameter if ``adaptive_loss_weight`` is set to True. Regardless of whether adaptivity\n        is turned on at this time, the previous loss seen by the server is updated.\n\n        **NOTE**: For adaptive constraint losses, including FedProx, this loss is exchanged (along with the\n        weights) by each client and is the VANILLA loss that does not include the additional penalty losses.\n\n        Args:\n            loss (float): This is the loss to which we compare the previous loss seen by the server. For Adaptive\n                Constraint clients this should be the aggregated training loss seen by each client participating in\n                training.\n        \"\"\"\n        if self.adapt_loss_weight:\n            if loss &lt;= self.previous_loss:\n                self.loss_weight_patience_counter += 1\n                if self.loss_weight_patience_counter == self.loss_weight_patience:\n                    self.loss_weight -= self.loss_weight_delta\n                    self.loss_weight = max(0.0, self.loss_weight)\n                    self.loss_weight_patience_counter = 0\n                    log(INFO, f\"Aggregate training loss has dropped {self.loss_weight_patience} rounds in a row\")\n                    log(INFO, f\"Constraint weight is decreased to {self.loss_weight}\")\n            else:\n                self.loss_weight += self.loss_weight_delta\n                self.loss_weight_patience_counter = 0\n                log(\n                    INFO,\n                    f\"Aggregate training loss increased this round: Current loss {loss}, \"\n                    f\"Previous loss: {self.previous_loss}\",\n                )\n                log(INFO, f\"Constraint weight is increased by {self.loss_weight_delta} to {self.loss_weight}\")\n        self.previous_loss = loss\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga_with_adaptive_constraint.FedDgGaAdaptiveConstraint.__init__","title":"<code>__init__(*, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, initial_loss_weight=1.0, adapt_loss_weight=False, loss_weight_delta=0.1, loss_weight_patience=5, weighted_train_losses=False, fairness_metric=None, adjustment_weight_step_size=0.2)</code>","text":"<p>Strategy for the FedDG-GA algorithm (Federated Domain Generalization with Generalization Adjustment, Zhang et al. 2023) combined with the Adaptive Strategy for Auxiliary constraints like FedProx. See documentation on <code>FedAvgWithAdaptiveConstraint</code> for more information.</p> <p>NOTE: Initial parameters are NOT optional. They must be passed for this strategy.</p> <p>Parameters:</p> Name Type Description Default <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during training. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of total clients in the system. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for validation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure validation. Defaults to None</p> <code>None</code> <code>initial_parameters</code> <code>Parameters</code> <p>Initial global model parameters.</p> required <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function, Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>initial_loss_weight</code> <code>float</code> <p>Initial penalty loss weight (mu in FedProx). If adaptivity is false, then this is the constant weight used for all clients. Defaults to 1.0.</p> <code>1.0</code> <code>adapt_loss_weight</code> <code>bool</code> <p>Determines whether the value of the penalty loss weight is adaptively modified by the server based on aggregated train loss. Defaults to False.</p> <code>False</code> <code>loss_weight_delta</code> <code>float</code> <p>This is the amount by which the server changes the value of the penalty loss weight based on the modification criteria. Only applicable if adaptivity is on. Defaults to 0.1.</p> <code>0.1</code> <code>loss_weight_patience</code> <code>int</code> <p>This is the number of rounds a server must see decreasing aggregated train loss before reducing the value of the penalty loss weight. Only applicable if adaptivity is on. Defaults to 5.</p> <code>5</code> <code>weighted_train_losses</code> <code>bool</code> <p>Determines whether the training losses from the clients should be aggregated using a weighted or unweighted average. These aggregated losses are used to adjust the proximal weight in the adaptive setting. Defaults to False.</p> <code>False</code> <code>fairness_metric</code> <code>FairnessMetric | None</code> <p>he metric to evaluate the local model of each client against the global model in order to determine their adjustment weight for aggregation. Can be set to any default metric in <code>FairnessMetricType</code> or set to use a custom metric. Optional, default is <code>FairnessMetric(FairnessMetricType.LOSS)</code> when specified as None.</p> <code>None</code> <code>adjustment_weight_step_size</code> <code>float</code> <p>The step size to determine the magnitude of change for the generalization adjustment weight. It has to be <code>0 &lt; adjustment_weight_step_size &lt; 1.</code> Optional, default is 0.2.</p> <code>0.2</code> Source code in <code>fl4health/strategies/feddg_ga_with_adaptive_constraint.py</code> <pre><code>def __init__(\n    self,\n    *,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    initial_loss_weight: float = 1.0,\n    adapt_loss_weight: bool = False,\n    loss_weight_delta: float = 0.1,\n    loss_weight_patience: int = 5,\n    weighted_train_losses: bool = False,\n    fairness_metric: FairnessMetric | None = None,\n    adjustment_weight_step_size: float = 0.2,\n):\n    \"\"\"\n    Strategy for the FedDG-GA algorithm (Federated Domain Generalization with Generalization Adjustment,\n    Zhang et al. 2023) combined with the Adaptive Strategy for Auxiliary constraints like FedProx. See\n    documentation on ``FedAvgWithAdaptiveConstraint`` for more information.\n\n    **NOTE**: Initial parameters are **NOT** optional. They must be passed for this strategy.\n\n    Args:\n        min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for validation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            validation. Defaults to None\n        initial_parameters (Parameters): Initial global model parameters.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional):\n            Metrics aggregation function, Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional):\n            Metrics aggregation function. Defaults to None.\n        initial_loss_weight (float, optional): Initial penalty loss weight (mu in FedProx). If adaptivity is false,\n            then this is the constant weight used for all clients. Defaults to 1.0.\n        adapt_loss_weight (bool, optional): Determines whether the value of the penalty loss weight is adaptively\n            modified by the server based on aggregated train loss. Defaults to False.\n        loss_weight_delta (float, optional): This is the amount by which the server changes the value of the\n            penalty loss weight based on the modification criteria. Only applicable if adaptivity is on.\n            Defaults to 0.1.\n        loss_weight_patience (int, optional): This is the number of rounds a server must see decreasing\n            aggregated train loss before reducing the value of the penalty loss weight. Only applicable if\n            adaptivity is on. Defaults to 5.\n        weighted_train_losses (bool, optional): Determines whether the training losses from the clients should be\n            aggregated using a weighted or unweighted average. These aggregated losses are used to adjust the\n            proximal weight in the adaptive setting. Defaults to False.\n        fairness_metric (FairnessMetric | None, optional): he metric to evaluate the local model of each\n            client against the global model in order to determine their adjustment weight for aggregation.\n            Can be set to any default metric in ``FairnessMetricType`` or set to use a custom metric.\n            Optional, default is ``FairnessMetric(FairnessMetricType.LOSS)`` when specified as None.\n        adjustment_weight_step_size (float, optional): The step size to determine the magnitude of change for\n            the generalization adjustment weight. It has to be ``0 &lt; adjustment_weight_step_size &lt; 1.``\n            Optional, default is 0.2.\n    \"\"\"\n    self.loss_weight = initial_loss_weight\n    self.adapt_loss_weight = adapt_loss_weight\n\n    if self.adapt_loss_weight:\n        self.loss_weight_delta = loss_weight_delta\n        self.loss_weight_patience = loss_weight_patience\n        self.loss_weight_patience_counter: int = 0\n\n    self.previous_loss = float(\"inf\")\n\n    if initial_parameters:\n        self.add_auxiliary_information(initial_parameters)\n\n    super().__init__(\n        min_fit_clients=min_fit_clients,\n        min_evaluate_clients=min_evaluate_clients,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        fairness_metric=fairness_metric,\n        adjustment_weight_step_size=adjustment_weight_step_size,\n    )\n\n    self.parameter_packer = ParameterPackerAdaptiveConstraint()\n    self.weighted_train_losses = weighted_train_losses\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga_with_adaptive_constraint.FedDgGaAdaptiveConstraint.add_auxiliary_information","title":"<code>add_auxiliary_information(original_parameters)</code>","text":"<p>Function for adding in the <code>loss_weight</code> to the provided set of parameters. This function is meant to be called after a server requests model weight initialization from a client, allowing the proper information to be included with the model parameters when sent to all clients for model initialization etc.</p> <p>Parameters:</p> Name Type Description Default <code>original_parameters</code> <code>Parameters</code> <p>Original set of parameters provided by a client for model weight initialization</p> required Source code in <code>fl4health/strategies/feddg_ga_with_adaptive_constraint.py</code> <pre><code>def add_auxiliary_information(self, original_parameters: Parameters) -&gt; None:\n    \"\"\"\n    Function for adding in the ``loss_weight`` to the provided set of parameters. This function is meant to be\n    called after a server requests model weight initialization from a client, allowing the proper information to\n    be included with the model parameters when sent to all clients for model initialization etc.\n\n    Args:\n        original_parameters (Parameters): Original set of parameters provided by a client for model weight\n            initialization\n    \"\"\"\n    original_parameters.tensors.extend(ndarrays_to_parameters([np.array(self.loss_weight)]).tensors)\n</code></pre>"},{"location":"api/#fl4health.strategies.feddg_ga_with_adaptive_constraint.FedDgGaAdaptiveConstraint.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Aggregate fit results by weighing them against the adjustment weights and then summing them.</p> <p>Collects the fit metrics that will be used to change the adjustment weights for the next round.</p> <p>If applicable, determine whether the constraint weight should be updated based on the aggregated loss seen on the clients.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>(int) The current server round.</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>(list[tuple[ClientProxy, FitRes]]) The clients' fit results.</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>(list[tuple[ClientProxy, FitRes] | BaseException]) The clients' fit failures.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar]]</code> <p>(tuple[Parameters | None, dict[str, Scalar]]) A tuple containing the aggregated parameters and the aggregated fit metrics. For adaptive constraints, the server also packs a constraint weight to be sent to the clients. This is sent even if adaptive constraint weights are turned off and the value simply remains constant.</p> Source code in <code>fl4health/strategies/feddg_ga_with_adaptive_constraint.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate fit results by weighing them against the adjustment weights and then summing them.\n\n    Collects the fit metrics that will be used to change the adjustment weights for the next round.\n\n    If applicable, determine whether the constraint weight should be updated based on the aggregated loss\n    seen on the clients.\n\n    Args:\n        server_round: (int) The current server round.\n        results: (list[tuple[ClientProxy, FitRes]]) The clients' fit results.\n        failures: (list[tuple[ClientProxy, FitRes] | BaseException]) The clients' fit failures.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar]]) A tuple containing the aggregated parameters\n            and the aggregated fit metrics. For adaptive constraints, the server also packs a constraint weight\n            to be sent to the clients. This is sent even if adaptive constraint weights are turned off and\n            the value simply remains constant.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Convert results with packed params of model weights and training loss. The results list is modified in-place\n    # to only contain model parameters for use in the Fed-DGGA calculations and aggregation\n    train_losses_and_counts = self._unpack_weights_and_losses(results)\n\n    # Aggregate train loss\n    train_losses_aggregated = aggregate_losses(train_losses_and_counts, self.weighted_train_losses)\n    self._maybe_update_constraint_weight_param(train_losses_aggregated)\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.fit_metrics_aggregation_fn:\n        fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n    self.train_metrics = {}\n    for client_proxy, fit_res in results:\n        self.train_metrics[client_proxy.cid] = fit_res.metrics\n\n    weights_aggregated = self.weight_and_aggregate_results(results)\n\n    parameters = self.parameter_packer.pack_parameters(weights_aggregated, self.loss_weight)\n    return ndarrays_to_parameters(parameters), metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.fedpca","title":"<code>fedpca</code>","text":""},{"location":"api/#fl4health.strategies.fedpca.FedPCA","title":"<code>FedPCA</code>","text":"<p>               Bases: <code>BasicFedAvg</code></p> Source code in <code>fl4health/strategies/fedpca.py</code> <pre><code>class FedPCA(BasicFedAvg):\n    def __init__(\n        self,\n        *,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: EVALUATE_FN_TYPE = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters | None = None,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        weighted_aggregation: bool = True,\n        weighted_eval_losses: bool = True,\n        svd_merging: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Strategy responsible for performing federated Principal Component Analysis. More specifically, this strategy\n        merges client-computed local principal components to obtain the principal components for all data.\n\n        Args:\n            fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n            min_fit_clients (int, optional): Minimum number of clients used during fit. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of clients before starting FL. Defaults to 2.\n            evaluate_fn (EVALUATE_FN_TYPE, optional): Optional function used for central server-side evaluation.\n                Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                client-side validation by providing a ``Config`` dictionary. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function. Defaults\n                to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n                average or a uniform average. FedAvg default is weighted average by client dataset counts. Defaults to\n                True.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n                counts. Defaults to True.\n            svd_merging (bool, optional): Indicates whether merging of client principal components is done by directly\n                performing SVD or using a procedure based on QR decomposition. Defaults to True.\n        \"\"\"\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_fit_clients=min_fit_clients,\n            min_evaluate_clients=min_evaluate_clients,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            weighted_aggregation=weighted_aggregation,\n            weighted_eval_losses=weighted_eval_losses,\n        )\n        # Since federated PCA does not use initial parameters, we fix it here.\n        self.initial_parameters = Parameters(tensors=[], tensor_type=\"numpy.ndarray\")\n        self.svd_merging = svd_merging\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate client parameters. In this case, merge all clients' local principal components.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n                that need to be aggregated on the server-side. In this scheme, the clients pack the layer weights into\n                the results object along with the weight values to allow for alignment during aggregation.\n            failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n                from clients that experienced an issue during training, such as timeouts or exceptions.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar]]): The aggregated parameters and the metrics dictionary.\n                In this case, the parameters are the new singular vectors and their corresponding singular values.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n        # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n        # reducing numerical fluctuation.\n        decoded_and_sorted_results = [weights for _, weights, _ in decode_and_pseudo_sort_results(results)]\n\n        client_singular_values = []\n        client_singular_vectors = []\n        for a in decoded_and_sorted_results:\n            singular_vectors, singular_values = a[0], a[1]\n            client_singular_vectors.append(singular_vectors)\n            client_singular_values.append(singular_values)\n\n        if self.svd_merging:\n            log(INFO, \"Performing SVD-based merging.\")\n            merged_singular_vectors, merged_singular_values = self.merge_subspaces_svd(\n                client_singular_vectors, client_singular_values\n            )\n        else:\n            # use qr merging instead\n            log(INFO, \"Performing QR-based merging.\")\n            merged_singular_vectors, merged_singular_values = self.merge_subspaces_qr(\n                client_singular_vectors, client_singular_values\n            )\n        parameters = ndarrays_to_parameters([merged_singular_vectors, merged_singular_values])\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.fit_metrics_aggregation_fn:\n            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n        return parameters, metrics_aggregated\n\n    def merge_subspaces_svd(\n        self, client_singular_vectors: NDArrays, client_singular_values: NDArrays\n    ) -&gt; tuple[NDArray, NDArray]:\n        \"\"\"\n        Produce the principal components for all the data distributed across clients by merging the principal\n        components belonging to each local dataset.\n\n        Each clients sends a matrix whose columns are its local principal components to the server. The corresponding\n        singular values are also shared.\n\n        The server arranges the local principal components into a block matrix, then performs SVD.\n\n        More precisely, if ``U_i`` denotes the matrix of the principal components of client i, and ``S_i`` denotes\n        the corresponding diagonal matrix of singular values, and there are n clients, then merging is done by\n        performing SVD on the matrix\n\n        ``B = [U_1 @ S_1 | U_2 @ S_2 | ... | U_n @ S_n],``\n\n        where the new left singular vectors are returned as the merging result.\n\n        Notes:\n        1. If ``U_i @ S_i`` is of size ``d`` by ``N_i``, then ``B`` has size ``d`` by ``N``, where\n           ``N = N_1 + N_2 + ... + N_n.``\n        2. If ``U @ S @ V.T = B`` is the SVD of ``B``, then it turns out that ``U = A @ U'``, where the columns\n           of ``U'`` are the true principal components of the aggregated data, and ``A`` is some block unitary matrix.\n\n        For the theoretical justification behind this procedure, see the paper\n        \"A Distributed and Incremental SVD Algorithm for Agglomerative Data Analysis on Large Networks\".\n\n        **NOTE**: This method assumes that the *columns* of ``U_i``'s are the local principal components. Thus, after\n        performing SVD on the matrix ``B`` (defined above), the merging result is the **left** singular vectors.\n\n        This is in contrast with the client-side implementation of PCA (contained in class ``PcaModule``), which\n        assumes that the **rows** of the input data matrix are the data points. Hence, in ``PcaModule``, the **right**\n        singular vectors of the SVD of each client's data matrix are the principal components. (In a nutshell, the\n        input data matrices in these two cases are \"transposes\" of each other.)\n\n        Args:\n            client_singular_vectors (NDArrays): Local PCs.\n            client_singular_values (NDArrays): Singular values corresponding to local PCs.\n\n        Returns:\n            (tuple[NDArray, NDArray]): Merged PCs and corresponding singular values.\n        \"\"\"\n        x = [u @ np.diag(s) for u, s in zip(client_singular_vectors, client_singular_values)]\n        svd_input = np.concatenate(x, axis=1)\n        new_singular_vectors, new_singular_values, _ = np.linalg.svd(svd_input, full_matrices=True)\n        return new_singular_vectors, new_singular_values\n\n    def merge_subspaces_qr(\n        self, client_singular_vectors: NDArrays, client_singular_values: NDArrays\n    ) -&gt; tuple[NDArray, NDArray]:\n        \"\"\"\n        Produce the principal components (PCs) for all the data distributed across clients by merging the PCs\n        belonging to each local dataset.\n\n        Each clients sends a matrix whose columns are the local principal components to the server. The corresponding\n        singular values are also shared.\n\n        This implementation can be viewed as a more efficient approximation to  the SVD-based merging in that it does\n        not require performing SVD on a large matrix.\n\n        Directly performing SVD does not take into account the following two observations, suggesting there are more\n        efficient algorithms for merging:\n\n        1. Each client's singular vectors are already orthonormal.\n        2. The right singular vectors do not need to be computed since only the left singular vectors are returned as\n           the merging result.\n\n        In contrast, the algorithm here performs a QR decomposition on the large data matrix, which is more efficient\n        than SVD, and SVD is only performed on a much smaller matrix.\n\n        Similarly to the SVD-based merging, it returns an approximation of the true principal components\n        of the aggregated data up to the multiplication of some block unitary matrix.\n\n        For the theoretical justification behind this approach, see the paper \"Subspace Tracking for Latent\n        Semantic Analysis\".\n\n        **NOTE**: Similar to ``merge_subspaces_svd``, this method assumes that the **columns** of ``U_i``'s are the\n        local principal components.\n\n        Args:\n            client_singular_vectors (NDArrays): Local PCs.\n            client_singular_values (NDArrays): Singular values corresponding to local PCs.\n\n        Returns:\n            (tuple[NDArray, NDArray]): Merged PCs and corresponding singular values.\n        \"\"\"\n        assert len(client_singular_values) &gt;= MINIMUM_PCA_ClIENTS\n        if len(client_singular_values) == MINIMUM_PCA_ClIENTS:\n            u1, s1 = client_singular_vectors[0], np.diag(client_singular_values[0])\n            u2, s2 = client_singular_vectors[1], np.diag(client_singular_values[1])\n            return self.merge_two_subspaces_qr((u1, s1), (u2, s2))\n        u, s = self.merge_subspaces_qr(client_singular_vectors[:-1], client_singular_values[:-1])\n        u_last, s_last = client_singular_vectors[-1], client_singular_values[-1]\n        return self.merge_two_subspaces_qr((u, np.diag(s)), (u_last, np.diag(s_last)))\n\n    def merge_two_subspaces_qr(\n        self, subspace1: tuple[NDArray, NDArray], subspace2: tuple[NDArray, NDArray]\n    ) -&gt; tuple[NDArray, NDArray]:\n        u1, s1 = subspace1\n        u2, s2 = subspace2\n\n        z = u1.T @ u2\n        q, r = np.linalg.qr(u2 - u1 @ z)\n\n        d2 = s1.shape[1]\n        d1 = r.shape[0]\n        zeros = np.zeros(shape=(d1, d2))\n        a = np.concatenate((s1, zeros), axis=0)\n        b = np.concatenate(((z @ s2), (r @ s2)), axis=0)\n        svd_input = np.concatenate((a, b), axis=1)\n\n        u3, s_final, _ = np.linalg.svd(svd_input, full_matrices=False)\n\n        u_final = (np.concatenate((u1, q), axis=1)) @ u3\n\n        m, n = u1.shape[0], u1.shape[1] + u2.shape[1]\n        rank = min(m, n)\n        return u_final[:, :rank], s_final[:rank]\n</code></pre>"},{"location":"api/#fl4health.strategies.fedpca.FedPCA.__init__","title":"<code>__init__(*, fraction_fit=1.0, fraction_evaluate=1.0, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters=None, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, weighted_aggregation=True, weighted_eval_losses=True, svd_merging=True)</code>","text":"<p>Strategy responsible for performing federated Principal Component Analysis. More specifically, this strategy merges client-computed local principal components to obtain the principal components for all data.</p> <p>Parameters:</p> Name Type Description Default <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. Defaults to 1.0.</p> <code>1.0</code> <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during fit. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of clients before starting FL. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>EVALUATE_FN_TYPE</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>initial_parameters</code> <code>Parameters | None</code> <p>Initial global model parameters. Defaults to None.</p> <code>None</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>weighted_aggregation</code> <code>bool</code> <p>Determines whether parameter aggregation is a linearly weighted average or a uniform average. FedAvg default is weighted average by client dataset counts. Defaults to True.</p> <code>True</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. FedAvg default is weighted average of the losses by client dataset counts. Defaults to True.</p> <code>True</code> <code>svd_merging</code> <code>bool</code> <p>Indicates whether merging of client principal components is done by directly performing SVD or using a procedure based on QR decomposition. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/strategies/fedpca.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: EVALUATE_FN_TYPE = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters | None = None,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    weighted_aggregation: bool = True,\n    weighted_eval_losses: bool = True,\n    svd_merging: bool = True,\n) -&gt; None:\n    \"\"\"\n    Strategy responsible for performing federated Principal Component Analysis. More specifically, this strategy\n    merges client-computed local principal components to obtain the principal components for all data.\n\n    Args:\n        fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n        min_fit_clients (int, optional): Minimum number of clients used during fit. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of clients before starting FL. Defaults to 2.\n        evaluate_fn (EVALUATE_FN_TYPE, optional): Optional function used for central server-side evaluation.\n            Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            client-side validation by providing a ``Config`` dictionary. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function. Defaults\n            to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n            average or a uniform average. FedAvg default is weighted average by client dataset counts. Defaults to\n            True.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n            counts. Defaults to True.\n        svd_merging (bool, optional): Indicates whether merging of client principal components is done by directly\n            performing SVD or using a procedure based on QR decomposition. Defaults to True.\n    \"\"\"\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_fit_clients=min_fit_clients,\n        min_evaluate_clients=min_evaluate_clients,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        weighted_aggregation=weighted_aggregation,\n        weighted_eval_losses=weighted_eval_losses,\n    )\n    # Since federated PCA does not use initial parameters, we fix it here.\n    self.initial_parameters = Parameters(tensors=[], tensor_type=\"numpy.ndarray\")\n    self.svd_merging = svd_merging\n</code></pre>"},{"location":"api/#fl4health.strategies.fedpca.FedPCA.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Aggregate client parameters. In this case, merge all clients' local principal components.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The client identifiers and the results of their local training that need to be aggregated on the server-side. In this scheme, the clients pack the layer weights into the results object along with the weight values to allow for alignment during aggregation.</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>These are the results and exceptions from clients that experienced an issue during training, such as timeouts or exceptions.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar]]</code> <p>The aggregated parameters and the metrics dictionary. In this case, the parameters are the new singular vectors and their corresponding singular values.</p> Source code in <code>fl4health/strategies/fedpca.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate client parameters. In this case, merge all clients' local principal components.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local training\n            that need to be aggregated on the server-side. In this scheme, the clients pack the layer weights into\n            the results object along with the weight values to allow for alignment during aggregation.\n        failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n            from clients that experienced an issue during training, such as timeouts or exceptions.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar]]): The aggregated parameters and the metrics dictionary.\n            In this case, the parameters are the new singular vectors and their corresponding singular values.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n    # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n    # reducing numerical fluctuation.\n    decoded_and_sorted_results = [weights for _, weights, _ in decode_and_pseudo_sort_results(results)]\n\n    client_singular_values = []\n    client_singular_vectors = []\n    for a in decoded_and_sorted_results:\n        singular_vectors, singular_values = a[0], a[1]\n        client_singular_vectors.append(singular_vectors)\n        client_singular_values.append(singular_values)\n\n    if self.svd_merging:\n        log(INFO, \"Performing SVD-based merging.\")\n        merged_singular_vectors, merged_singular_values = self.merge_subspaces_svd(\n            client_singular_vectors, client_singular_values\n        )\n    else:\n        # use qr merging instead\n        log(INFO, \"Performing QR-based merging.\")\n        merged_singular_vectors, merged_singular_values = self.merge_subspaces_qr(\n            client_singular_vectors, client_singular_values\n        )\n    parameters = ndarrays_to_parameters([merged_singular_vectors, merged_singular_values])\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.fit_metrics_aggregation_fn:\n        fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n    return parameters, metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.fedpca.FedPCA.merge_subspaces_svd","title":"<code>merge_subspaces_svd(client_singular_vectors, client_singular_values)</code>","text":"<p>Produce the principal components for all the data distributed across clients by merging the principal components belonging to each local dataset.</p> <p>Each clients sends a matrix whose columns are its local principal components to the server. The corresponding singular values are also shared.</p> <p>The server arranges the local principal components into a block matrix, then performs SVD.</p> <p>More precisely, if <code>U_i</code> denotes the matrix of the principal components of client i, and <code>S_i</code> denotes the corresponding diagonal matrix of singular values, and there are n clients, then merging is done by performing SVD on the matrix</p> <p><code>B = [U_1 @ S_1 | U_2 @ S_2 | ... | U_n @ S_n],</code></p> <p>where the new left singular vectors are returned as the merging result.</p> <p>Notes: 1. If <code>U_i @ S_i</code> is of size <code>d</code> by <code>N_i</code>, then <code>B</code> has size <code>d</code> by <code>N</code>, where    <code>N = N_1 + N_2 + ... + N_n.</code> 2. If <code>U @ S @ V.T = B</code> is the SVD of <code>B</code>, then it turns out that <code>U = A @ U'</code>, where the columns    of <code>U'</code> are the true principal components of the aggregated data, and <code>A</code> is some block unitary matrix.</p> <p>For the theoretical justification behind this procedure, see the paper \"A Distributed and Incremental SVD Algorithm for Agglomerative Data Analysis on Large Networks\".</p> <p>NOTE: This method assumes that the columns of <code>U_i</code>'s are the local principal components. Thus, after performing SVD on the matrix <code>B</code> (defined above), the merging result is the left singular vectors.</p> <p>This is in contrast with the client-side implementation of PCA (contained in class <code>PcaModule</code>), which assumes that the rows of the input data matrix are the data points. Hence, in <code>PcaModule</code>, the right singular vectors of the SVD of each client's data matrix are the principal components. (In a nutshell, the input data matrices in these two cases are \"transposes\" of each other.)</p> <p>Parameters:</p> Name Type Description Default <code>client_singular_vectors</code> <code>NDArrays</code> <p>Local PCs.</p> required <code>client_singular_values</code> <code>NDArrays</code> <p>Singular values corresponding to local PCs.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray, NDArray]</code> <p>Merged PCs and corresponding singular values.</p> Source code in <code>fl4health/strategies/fedpca.py</code> <pre><code>def merge_subspaces_svd(\n    self, client_singular_vectors: NDArrays, client_singular_values: NDArrays\n) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"\n    Produce the principal components for all the data distributed across clients by merging the principal\n    components belonging to each local dataset.\n\n    Each clients sends a matrix whose columns are its local principal components to the server. The corresponding\n    singular values are also shared.\n\n    The server arranges the local principal components into a block matrix, then performs SVD.\n\n    More precisely, if ``U_i`` denotes the matrix of the principal components of client i, and ``S_i`` denotes\n    the corresponding diagonal matrix of singular values, and there are n clients, then merging is done by\n    performing SVD on the matrix\n\n    ``B = [U_1 @ S_1 | U_2 @ S_2 | ... | U_n @ S_n],``\n\n    where the new left singular vectors are returned as the merging result.\n\n    Notes:\n    1. If ``U_i @ S_i`` is of size ``d`` by ``N_i``, then ``B`` has size ``d`` by ``N``, where\n       ``N = N_1 + N_2 + ... + N_n.``\n    2. If ``U @ S @ V.T = B`` is the SVD of ``B``, then it turns out that ``U = A @ U'``, where the columns\n       of ``U'`` are the true principal components of the aggregated data, and ``A`` is some block unitary matrix.\n\n    For the theoretical justification behind this procedure, see the paper\n    \"A Distributed and Incremental SVD Algorithm for Agglomerative Data Analysis on Large Networks\".\n\n    **NOTE**: This method assumes that the *columns* of ``U_i``'s are the local principal components. Thus, after\n    performing SVD on the matrix ``B`` (defined above), the merging result is the **left** singular vectors.\n\n    This is in contrast with the client-side implementation of PCA (contained in class ``PcaModule``), which\n    assumes that the **rows** of the input data matrix are the data points. Hence, in ``PcaModule``, the **right**\n    singular vectors of the SVD of each client's data matrix are the principal components. (In a nutshell, the\n    input data matrices in these two cases are \"transposes\" of each other.)\n\n    Args:\n        client_singular_vectors (NDArrays): Local PCs.\n        client_singular_values (NDArrays): Singular values corresponding to local PCs.\n\n    Returns:\n        (tuple[NDArray, NDArray]): Merged PCs and corresponding singular values.\n    \"\"\"\n    x = [u @ np.diag(s) for u, s in zip(client_singular_vectors, client_singular_values)]\n    svd_input = np.concatenate(x, axis=1)\n    new_singular_vectors, new_singular_values, _ = np.linalg.svd(svd_input, full_matrices=True)\n    return new_singular_vectors, new_singular_values\n</code></pre>"},{"location":"api/#fl4health.strategies.fedpca.FedPCA.merge_subspaces_qr","title":"<code>merge_subspaces_qr(client_singular_vectors, client_singular_values)</code>","text":"<p>Produce the principal components (PCs) for all the data distributed across clients by merging the PCs belonging to each local dataset.</p> <p>Each clients sends a matrix whose columns are the local principal components to the server. The corresponding singular values are also shared.</p> <p>This implementation can be viewed as a more efficient approximation to  the SVD-based merging in that it does not require performing SVD on a large matrix.</p> <p>Directly performing SVD does not take into account the following two observations, suggesting there are more efficient algorithms for merging:</p> <ol> <li>Each client's singular vectors are already orthonormal.</li> <li>The right singular vectors do not need to be computed since only the left singular vectors are returned as    the merging result.</li> </ol> <p>In contrast, the algorithm here performs a QR decomposition on the large data matrix, which is more efficient than SVD, and SVD is only performed on a much smaller matrix.</p> <p>Similarly to the SVD-based merging, it returns an approximation of the true principal components of the aggregated data up to the multiplication of some block unitary matrix.</p> <p>For the theoretical justification behind this approach, see the paper \"Subspace Tracking for Latent Semantic Analysis\".</p> <p>NOTE: Similar to <code>merge_subspaces_svd</code>, this method assumes that the columns of <code>U_i</code>'s are the local principal components.</p> <p>Parameters:</p> Name Type Description Default <code>client_singular_vectors</code> <code>NDArrays</code> <p>Local PCs.</p> required <code>client_singular_values</code> <code>NDArrays</code> <p>Singular values corresponding to local PCs.</p> required <p>Returns:</p> Type Description <code>tuple[NDArray, NDArray]</code> <p>Merged PCs and corresponding singular values.</p> Source code in <code>fl4health/strategies/fedpca.py</code> <pre><code>def merge_subspaces_qr(\n    self, client_singular_vectors: NDArrays, client_singular_values: NDArrays\n) -&gt; tuple[NDArray, NDArray]:\n    \"\"\"\n    Produce the principal components (PCs) for all the data distributed across clients by merging the PCs\n    belonging to each local dataset.\n\n    Each clients sends a matrix whose columns are the local principal components to the server. The corresponding\n    singular values are also shared.\n\n    This implementation can be viewed as a more efficient approximation to  the SVD-based merging in that it does\n    not require performing SVD on a large matrix.\n\n    Directly performing SVD does not take into account the following two observations, suggesting there are more\n    efficient algorithms for merging:\n\n    1. Each client's singular vectors are already orthonormal.\n    2. The right singular vectors do not need to be computed since only the left singular vectors are returned as\n       the merging result.\n\n    In contrast, the algorithm here performs a QR decomposition on the large data matrix, which is more efficient\n    than SVD, and SVD is only performed on a much smaller matrix.\n\n    Similarly to the SVD-based merging, it returns an approximation of the true principal components\n    of the aggregated data up to the multiplication of some block unitary matrix.\n\n    For the theoretical justification behind this approach, see the paper \"Subspace Tracking for Latent\n    Semantic Analysis\".\n\n    **NOTE**: Similar to ``merge_subspaces_svd``, this method assumes that the **columns** of ``U_i``'s are the\n    local principal components.\n\n    Args:\n        client_singular_vectors (NDArrays): Local PCs.\n        client_singular_values (NDArrays): Singular values corresponding to local PCs.\n\n    Returns:\n        (tuple[NDArray, NDArray]): Merged PCs and corresponding singular values.\n    \"\"\"\n    assert len(client_singular_values) &gt;= MINIMUM_PCA_ClIENTS\n    if len(client_singular_values) == MINIMUM_PCA_ClIENTS:\n        u1, s1 = client_singular_vectors[0], np.diag(client_singular_values[0])\n        u2, s2 = client_singular_vectors[1], np.diag(client_singular_values[1])\n        return self.merge_two_subspaces_qr((u1, s1), (u2, s2))\n    u, s = self.merge_subspaces_qr(client_singular_vectors[:-1], client_singular_values[:-1])\n    u_last, s_last = client_singular_vectors[-1], client_singular_values[-1]\n    return self.merge_two_subspaces_qr((u, np.diag(s)), (u_last, np.diag(s_last)))\n</code></pre>"},{"location":"api/#fl4health.strategies.fedpm","title":"<code>fedpm</code>","text":""},{"location":"api/#fl4health.strategies.fedpm.FedPm","title":"<code>FedPm</code>","text":"<p>               Bases: <code>FedAvgDynamicLayer</code></p> Source code in <code>fl4health/strategies/fedpm.py</code> <pre><code>class FedPm(FedAvgDynamicLayer):\n    def __init__(\n        self,\n        *,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters | None = None,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        weighted_eval_losses: bool = True,\n        bayesian_aggregation: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        A strategy that is used for aggregating probability masks in the \"Federated Probabilistic Mask Training\"\n        paradigm, as detailed in http://arxiv.org/pdf/2209.15328.\n\n        The implementation here allows for simply averaging the probability masks, as well as the more sophisticated\n        Bayesian aggregation approach.\n\n        **NOTE**: Since the parameters aggregated by this strategy are supposed to be binary masks, by default\n        FedPM performs uniformed averaging. The effect of weighted averaging is also not covered in the original work.\n\n        Args:\n            fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0. Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n            min_fit_clients (int, optional): Minimum number of clients used during fitting. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for central server-side evaluation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                client-side validation by providing a ``Config`` dictionary. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n                counts. Defaults to True.\n            bayesian_aggregation (bool): Determines whether Bayesian aggregation is used.\n        \"\"\"\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_fit_clients=min_fit_clients,\n            min_evaluate_clients=min_evaluate_clients,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            weighted_aggregation=False,\n            weighted_eval_losses=weighted_eval_losses,\n        )\n        # Parameters for Beta distribution.\n        self.beta_parameters: dict[str, tuple[NDArray, NDArray]] = {}\n        self.bayesian_aggregation = bayesian_aggregation\n\n    def aggregate(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, NDArray]:\n        if not self.bayesian_aggregation:\n            return super().aggregate(results)\n        return self.aggregate_bayesian(results)\n\n    def aggregate_bayesian(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, NDArray]:\n        \"\"\"\n        Perform posterior update to the Beta distribution parameters based on the binary masks sent by the clients.\n\n        More precisely, each client maintains for each one of its parameter tensors a \"probability score tensor\".\n        These scores (after applying the Sigmoid function to them) are Bernoulli probabilities which indicate how\n        likely their corresponding parameters are to be pruned or kept. Each client samples a binary mask for every\n        one of its parameter tensors based on the corresponding Bernoulli probabilities. These masks are sent to the\n        server for aggregation.\n\n        Here, we assume that the bernoulli probabilities of each client themselves follow a Beta distribution with\n        parameters alpha and beta. Then the binary masks may be viewed as data that can be used to update alpha and\n        beta, and this corresponds to a posterior update. Due to the conjugate relation between the Beta and\n        Bernoulli distributions, the posterior distribution is still a Beta distribution, so we can perform the\n        aggregation in this manner every round.\n\n        In this case, the updates performed are:\n\n        ```python\n        alpha_new = alpha + M\n\n        beta_new = beta + K * 1 - M\n\n        theta = (alpha_new - 1) / (alpha_new + beta_new - 2)\n        ```\n\n        where ``M`` is the sum of all binary masks corresponding to a particular parameter tensor, ``K`` is the number\n        of clients, and \"1\" in the second equation refers to an array of all ones of the same shape as ``M``.\n\n        In the beginning, alpha and beta are initialized to arrays of all ones\n\n        Args:\n            results (list[tuple[NDArrays, int]]): Binary masks sent to the server for aggregation\n\n        Returns:\n            (dict[str, NDArray]): Aggregated binary masks\n        \"\"\"\n        names_to_layers: defaultdict[str, list[NDArray]] = defaultdict(list)\n        total_num_clients: defaultdict[str, int] = defaultdict(int)\n\n        # unpack the parameters and initialize the beta parameters to be all ones if they have not already\n        # been initialized.\n        for packed_layers, _ in results:\n            layers, names = self.parameter_packer.unpack_parameters(packed_layers)\n            for layer, name in zip(layers, names):\n                names_to_layers[name].append(layer)\n                total_num_clients[name] += 1\n                if name not in self.beta_parameters:\n                    alpha = np.ones_like(layer)\n                    beta = np.ones_like(layer)\n                    self.beta_parameters[name] = (alpha, beta)\n\n        aggregation_result: dict[str, NDArray] = {}\n\n        # posterior update of the beta parameters and using them\n        # to compute the final result.\n        for parameter_name in self.beta_parameters:\n            m_agg = reduce(np.add, names_to_layers[parameter_name])\n            n_clients = total_num_clients[parameter_name]\n            alpha, beta = self.beta_parameters[parameter_name]\n            alpha_new = alpha + m_agg\n            beta_new = beta + np.ones_like(beta) * n_clients - m_agg\n            self.beta_parameters[parameter_name] = (alpha_new, beta_new)\n            aggregation_result[parameter_name] = (alpha_new - 1) / (alpha_new + beta_new - 2)\n\n        return aggregation_result\n\n    def reset_beta_priors(self) -&gt; None:\n        \"\"\"Reset the alpha and beta parameters for the Beta distribution to be arrays of all ones.\"\"\"\n        for parameter_name in self.beta_parameters:\n            alpha, beta = self.beta_parameters[parameter_name]\n            self.beta_parameters[parameter_name] = (np.ones_like(alpha), np.ones_like(beta))\n</code></pre>"},{"location":"api/#fl4health.strategies.fedpm.FedPm.__init__","title":"<code>__init__(*, fraction_fit=1.0, fraction_evaluate=1.0, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters=None, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, weighted_eval_losses=True, bayesian_aggregation=True)</code>","text":"<p>A strategy that is used for aggregating probability masks in the \"Federated Probabilistic Mask Training\" paradigm, as detailed in http://arxiv.org/pdf/2209.15328.</p> <p>The implementation here allows for simply averaging the probability masks, as well as the more sophisticated Bayesian aggregation approach.</p> <p>NOTE: Since the parameters aggregated by this strategy are supposed to be binary masks, by default FedPM performs uniformed averaging. The effect of weighted averaging is also not covered in the original work.</p> <p>Parameters:</p> Name Type Description Default <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. Defaults to 1.0. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. Defaults to 1.0.</p> <code>1.0</code> <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during fitting. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>initial_parameters</code> <code>Parameters | None</code> <p>Initial global model parameters. Defaults to None.</p> <code>None</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. FedAvg default is weighted average of the losses by client dataset counts. Defaults to True.</p> <code>True</code> <code>bayesian_aggregation</code> <code>bool</code> <p>Determines whether Bayesian aggregation is used.</p> <code>True</code> Source code in <code>fl4health/strategies/fedpm.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters | None = None,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    weighted_eval_losses: bool = True,\n    bayesian_aggregation: bool = True,\n) -&gt; None:\n    \"\"\"\n    A strategy that is used for aggregating probability masks in the \"Federated Probabilistic Mask Training\"\n    paradigm, as detailed in http://arxiv.org/pdf/2209.15328.\n\n    The implementation here allows for simply averaging the probability masks, as well as the more sophisticated\n    Bayesian aggregation approach.\n\n    **NOTE**: Since the parameters aggregated by this strategy are supposed to be binary masks, by default\n    FedPM performs uniformed averaging. The effect of weighted averaging is also not covered in the original work.\n\n    Args:\n        fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0. Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n        min_fit_clients (int, optional): Minimum number of clients used during fitting. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for central server-side evaluation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            client-side validation by providing a ``Config`` dictionary. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        initial_parameters (Parameters | None, optional): Initial global model parameters. Defaults to None.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n            counts. Defaults to True.\n        bayesian_aggregation (bool): Determines whether Bayesian aggregation is used.\n    \"\"\"\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_fit_clients=min_fit_clients,\n        min_evaluate_clients=min_evaluate_clients,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        weighted_aggregation=False,\n        weighted_eval_losses=weighted_eval_losses,\n    )\n    # Parameters for Beta distribution.\n    self.beta_parameters: dict[str, tuple[NDArray, NDArray]] = {}\n    self.bayesian_aggregation = bayesian_aggregation\n</code></pre>"},{"location":"api/#fl4health.strategies.fedpm.FedPm.aggregate_bayesian","title":"<code>aggregate_bayesian(results)</code>","text":"<p>Perform posterior update to the Beta distribution parameters based on the binary masks sent by the clients.</p> <p>More precisely, each client maintains for each one of its parameter tensors a \"probability score tensor\". These scores (after applying the Sigmoid function to them) are Bernoulli probabilities which indicate how likely their corresponding parameters are to be pruned or kept. Each client samples a binary mask for every one of its parameter tensors based on the corresponding Bernoulli probabilities. These masks are sent to the server for aggregation.</p> <p>Here, we assume that the bernoulli probabilities of each client themselves follow a Beta distribution with parameters alpha and beta. Then the binary masks may be viewed as data that can be used to update alpha and beta, and this corresponds to a posterior update. Due to the conjugate relation between the Beta and Bernoulli distributions, the posterior distribution is still a Beta distribution, so we can perform the aggregation in this manner every round.</p> <p>In this case, the updates performed are:</p> <pre><code>alpha_new = alpha + M\n\nbeta_new = beta + K * 1 - M\n\ntheta = (alpha_new - 1) / (alpha_new + beta_new - 2)\n</code></pre> <p>where <code>M</code> is the sum of all binary masks corresponding to a particular parameter tensor, <code>K</code> is the number of clients, and \"1\" in the second equation refers to an array of all ones of the same shape as <code>M</code>.</p> <p>In the beginning, alpha and beta are initialized to arrays of all ones</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[NDArrays, int]]</code> <p>Binary masks sent to the server for aggregation</p> required <p>Returns:</p> Type Description <code>dict[str, NDArray]</code> <p>Aggregated binary masks</p> Source code in <code>fl4health/strategies/fedpm.py</code> <pre><code>def aggregate_bayesian(self, results: list[tuple[NDArrays, int]]) -&gt; dict[str, NDArray]:\n    \"\"\"\n    Perform posterior update to the Beta distribution parameters based on the binary masks sent by the clients.\n\n    More precisely, each client maintains for each one of its parameter tensors a \"probability score tensor\".\n    These scores (after applying the Sigmoid function to them) are Bernoulli probabilities which indicate how\n    likely their corresponding parameters are to be pruned or kept. Each client samples a binary mask for every\n    one of its parameter tensors based on the corresponding Bernoulli probabilities. These masks are sent to the\n    server for aggregation.\n\n    Here, we assume that the bernoulli probabilities of each client themselves follow a Beta distribution with\n    parameters alpha and beta. Then the binary masks may be viewed as data that can be used to update alpha and\n    beta, and this corresponds to a posterior update. Due to the conjugate relation between the Beta and\n    Bernoulli distributions, the posterior distribution is still a Beta distribution, so we can perform the\n    aggregation in this manner every round.\n\n    In this case, the updates performed are:\n\n    ```python\n    alpha_new = alpha + M\n\n    beta_new = beta + K * 1 - M\n\n    theta = (alpha_new - 1) / (alpha_new + beta_new - 2)\n    ```\n\n    where ``M`` is the sum of all binary masks corresponding to a particular parameter tensor, ``K`` is the number\n    of clients, and \"1\" in the second equation refers to an array of all ones of the same shape as ``M``.\n\n    In the beginning, alpha and beta are initialized to arrays of all ones\n\n    Args:\n        results (list[tuple[NDArrays, int]]): Binary masks sent to the server for aggregation\n\n    Returns:\n        (dict[str, NDArray]): Aggregated binary masks\n    \"\"\"\n    names_to_layers: defaultdict[str, list[NDArray]] = defaultdict(list)\n    total_num_clients: defaultdict[str, int] = defaultdict(int)\n\n    # unpack the parameters and initialize the beta parameters to be all ones if they have not already\n    # been initialized.\n    for packed_layers, _ in results:\n        layers, names = self.parameter_packer.unpack_parameters(packed_layers)\n        for layer, name in zip(layers, names):\n            names_to_layers[name].append(layer)\n            total_num_clients[name] += 1\n            if name not in self.beta_parameters:\n                alpha = np.ones_like(layer)\n                beta = np.ones_like(layer)\n                self.beta_parameters[name] = (alpha, beta)\n\n    aggregation_result: dict[str, NDArray] = {}\n\n    # posterior update of the beta parameters and using them\n    # to compute the final result.\n    for parameter_name in self.beta_parameters:\n        m_agg = reduce(np.add, names_to_layers[parameter_name])\n        n_clients = total_num_clients[parameter_name]\n        alpha, beta = self.beta_parameters[parameter_name]\n        alpha_new = alpha + m_agg\n        beta_new = beta + np.ones_like(beta) * n_clients - m_agg\n        self.beta_parameters[parameter_name] = (alpha_new, beta_new)\n        aggregation_result[parameter_name] = (alpha_new - 1) / (alpha_new + beta_new - 2)\n\n    return aggregation_result\n</code></pre>"},{"location":"api/#fl4health.strategies.fedpm.FedPm.reset_beta_priors","title":"<code>reset_beta_priors()</code>","text":"<p>Reset the alpha and beta parameters for the Beta distribution to be arrays of all ones.</p> Source code in <code>fl4health/strategies/fedpm.py</code> <pre><code>def reset_beta_priors(self) -&gt; None:\n    \"\"\"Reset the alpha and beta parameters for the Beta distribution to be arrays of all ones.\"\"\"\n    for parameter_name in self.beta_parameters:\n        alpha, beta = self.beta_parameters[parameter_name]\n        self.beta_parameters[parameter_name] = (np.ones_like(alpha), np.ones_like(beta))\n</code></pre>"},{"location":"api/#fl4health.strategies.flash","title":"<code>flash</code>","text":""},{"location":"api/#fl4health.strategies.flash.Flash","title":"<code>Flash</code>","text":"<p>               Bases: <code>BasicFedAvg</code></p> Source code in <code>fl4health/strategies/flash.py</code> <pre><code>class Flash(BasicFedAvg):\n    def __init__(\n        self,\n        *,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: EVALUATE_FN_TYPE = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters | None,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        eta: float = 1e-1,\n        eta_l: float = 1e-1,\n        beta_1: float = 0.9,\n        beta_2: float = 0.99,\n        tau: float = 1e-9,\n        weighted_aggregation: bool = False,\n        weighted_eval_losses: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Flash: Concept Drift Adaptation in Federated Learning.\n\n        Implementation based on https://proceedings.mlr.press/v202/panchal23a/panchal23a.pdf\n\n        Args:\n            initial_parameters (Parameters | None): Initial global model parameters.\n            fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n            min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 2.\n            evaluate_fn (EVALUATE_FN_TYPE, optional): Optional function used for validation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                validation. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function. Defaults\n                to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            eta (float, optional): Server-side learning rate. Defaults to 1e-1.\n            eta_l (float, optional): Client-side learning rate. Defaults to 1e-1.\n            beta_1 (float, optional): Momentum parameter. Defaults to 0.9.\n            beta_2 (float, optional): Second moment parameter. Defaults to 0.99.\n            tau (float, optional): Controls the algorithm's degree of adaptability. Defaults to 1e-9.\n            weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n                average or a uniform average. Flash default is a uniform average by the number of clients.\n                Defaults to False.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. Flash default is a uniform average of the losses by dividing\n                the total loss by the number of clients. Defaults to False.\n        \"\"\"\n        if initial_parameters:\n            self.current_weights = parameters_to_ndarrays(initial_parameters)\n\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_fit_clients=min_fit_clients,\n            min_evaluate_clients=min_evaluate_clients,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            weighted_aggregation=weighted_aggregation,\n            weighted_eval_losses=weighted_eval_losses,\n        )\n        self.eta = eta\n        self.eta_l = eta_l\n        self.tau = tau\n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        zero_weights = [np.zeros_like(x) for x in self.current_weights]\n        self.m_t: NDArrays = zero_weights.copy()\n        self.v_t: NDArrays = zero_weights.copy()\n        self.d_t: NDArrays = zero_weights.copy()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Compute a string representation of the strategy.\"\"\"\n        return f\"Flash(accept_failures={self.accept_failures})\"\n\n    def add_auxiliary_information(self, original_parameters: Parameters) -&gt; None:\n        \"\"\"\n        Function for saving the ``original_parameters`` as the current weights after asking a client for model\n        weight initialization. Unlike other strategies that leverage this function, we don't need to pack in new\n        information, just save the client-side initialized model parameters.\n\n        Args:\n            original_parameters (Parameters): Original set of parameters provided by a client for model weight\n                initialization\n        \"\"\"\n        # Copy the model parameters into NDArrays for storage\n        self.current_weights = parameters_to_ndarrays(original_parameters)\n\n    def _update_parameters(self, delta_t: NDArrays) -&gt; None:\n        \"\"\"Update m_t, v_t, beta_3, and d_t per-element.\"\"\"\n        for i, (delta, m_prev, v_prev, d_prev) in enumerate(zip(delta_t, self.m_t, self.v_t, self.d_t)):\n            delta_squared = np.square(delta)\n            # Update m_t\n            self.m_t[i] = self.beta_1 * m_prev + (1 - self.beta_1) * delta\n\n            # Update v_t\n            self.v_t[i] = self.beta_2 * v_prev + (1 - self.beta_2) * delta_squared\n\n            # Compute beta_3\n            norm_v_prev = np.abs(v_prev)\n            norm_diff = np.abs(delta_squared - self.v_t[i])\n            beta_3_matrix = norm_v_prev / (norm_diff + norm_v_prev)\n\n            # Update d_t\n            self.d_t[i] = beta_3_matrix * d_prev + (1 - beta_3_matrix) * (delta_squared - self.v_t[i])\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"Aggregate fit results using the Flash method.\"\"\"\n        fedavg_parameters_aggregated, metrics_aggregated = super().aggregate_fit(\n            server_round=server_round, results=results, failures=failures\n        )\n\n        if fedavg_parameters_aggregated is None:\n            return None, {}\n\n        fedavg_weights_aggregate = parameters_to_ndarrays(fedavg_parameters_aggregated)\n\n        delta_t: NDArrays = [x - y for x, y in zip(fedavg_weights_aggregate, self.current_weights)]\n\n        self._update_parameters(delta_t)\n\n        new_weights = [\n            current_weight + self.eta * m_t / (np.sqrt(v_t) - d_t + self.tau)\n            for current_weight, m_t, v_t, d_t in zip(self.current_weights, self.m_t, self.v_t, self.d_t)\n        ]\n\n        self.current_weights = new_weights\n\n        return ndarrays_to_parameters(self.current_weights), metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.flash.Flash.__init__","title":"<code>__init__(*, fraction_fit=1.0, fraction_evaluate=1.0, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, eta=0.1, eta_l=0.1, beta_1=0.9, beta_2=0.99, tau=1e-09, weighted_aggregation=False, weighted_eval_losses=False)</code>","text":"<p>Flash: Concept Drift Adaptation in Federated Learning.</p> <p>Implementation based on https://proceedings.mlr.press/v202/panchal23a/panchal23a.pdf</p> <p>Parameters:</p> Name Type Description Default <code>initial_parameters</code> <code>Parameters | None</code> <p>Initial global model parameters.</p> required <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. Defaults to 1.0.</p> <code>1.0</code> <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during training. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of total clients in the system. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>EVALUATE_FN_TYPE</code> <p>Optional function used for validation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure validation. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>eta</code> <code>float</code> <p>Server-side learning rate. Defaults to 1e-1.</p> <code>0.1</code> <code>eta_l</code> <code>float</code> <p>Client-side learning rate. Defaults to 1e-1.</p> <code>0.1</code> <code>beta_1</code> <code>float</code> <p>Momentum parameter. Defaults to 0.9.</p> <code>0.9</code> <code>beta_2</code> <code>float</code> <p>Second moment parameter. Defaults to 0.99.</p> <code>0.99</code> <code>tau</code> <code>float</code> <p>Controls the algorithm's degree of adaptability. Defaults to 1e-9.</p> <code>1e-09</code> <code>weighted_aggregation</code> <code>bool</code> <p>Determines whether parameter aggregation is a linearly weighted average or a uniform average. Flash default is a uniform average by the number of clients. Defaults to False.</p> <code>False</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. Flash default is a uniform average of the losses by dividing the total loss by the number of clients. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/strategies/flash.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: EVALUATE_FN_TYPE = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters | None,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    eta: float = 1e-1,\n    eta_l: float = 1e-1,\n    beta_1: float = 0.9,\n    beta_2: float = 0.99,\n    tau: float = 1e-9,\n    weighted_aggregation: bool = False,\n    weighted_eval_losses: bool = False,\n) -&gt; None:\n    \"\"\"\n    Flash: Concept Drift Adaptation in Federated Learning.\n\n    Implementation based on https://proceedings.mlr.press/v202/panchal23a/panchal23a.pdf\n\n    Args:\n        initial_parameters (Parameters | None): Initial global model parameters.\n        fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n        min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of total clients in the system. Defaults to 2.\n        evaluate_fn (EVALUATE_FN_TYPE, optional): Optional function used for validation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            validation. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function. Defaults\n            to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        eta (float, optional): Server-side learning rate. Defaults to 1e-1.\n        eta_l (float, optional): Client-side learning rate. Defaults to 1e-1.\n        beta_1 (float, optional): Momentum parameter. Defaults to 0.9.\n        beta_2 (float, optional): Second moment parameter. Defaults to 0.99.\n        tau (float, optional): Controls the algorithm's degree of adaptability. Defaults to 1e-9.\n        weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n            average or a uniform average. Flash default is a uniform average by the number of clients.\n            Defaults to False.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. Flash default is a uniform average of the losses by dividing\n            the total loss by the number of clients. Defaults to False.\n    \"\"\"\n    if initial_parameters:\n        self.current_weights = parameters_to_ndarrays(initial_parameters)\n\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_fit_clients=min_fit_clients,\n        min_evaluate_clients=min_evaluate_clients,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        weighted_aggregation=weighted_aggregation,\n        weighted_eval_losses=weighted_eval_losses,\n    )\n    self.eta = eta\n    self.eta_l = eta_l\n    self.tau = tau\n    self.beta_1 = beta_1\n    self.beta_2 = beta_2\n    zero_weights = [np.zeros_like(x) for x in self.current_weights]\n    self.m_t: NDArrays = zero_weights.copy()\n    self.v_t: NDArrays = zero_weights.copy()\n    self.d_t: NDArrays = zero_weights.copy()\n</code></pre>"},{"location":"api/#fl4health.strategies.flash.Flash.__repr__","title":"<code>__repr__()</code>","text":"<p>Compute a string representation of the strategy.</p> Source code in <code>fl4health/strategies/flash.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Compute a string representation of the strategy.\"\"\"\n    return f\"Flash(accept_failures={self.accept_failures})\"\n</code></pre>"},{"location":"api/#fl4health.strategies.flash.Flash.add_auxiliary_information","title":"<code>add_auxiliary_information(original_parameters)</code>","text":"<p>Function for saving the <code>original_parameters</code> as the current weights after asking a client for model weight initialization. Unlike other strategies that leverage this function, we don't need to pack in new information, just save the client-side initialized model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>original_parameters</code> <code>Parameters</code> <p>Original set of parameters provided by a client for model weight initialization</p> required Source code in <code>fl4health/strategies/flash.py</code> <pre><code>def add_auxiliary_information(self, original_parameters: Parameters) -&gt; None:\n    \"\"\"\n    Function for saving the ``original_parameters`` as the current weights after asking a client for model\n    weight initialization. Unlike other strategies that leverage this function, we don't need to pack in new\n    information, just save the client-side initialized model parameters.\n\n    Args:\n        original_parameters (Parameters): Original set of parameters provided by a client for model weight\n            initialization\n    \"\"\"\n    # Copy the model parameters into NDArrays for storage\n    self.current_weights = parameters_to_ndarrays(original_parameters)\n</code></pre>"},{"location":"api/#fl4health.strategies.flash.Flash.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Aggregate fit results using the Flash method.</p> Source code in <code>fl4health/strategies/flash.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"Aggregate fit results using the Flash method.\"\"\"\n    fedavg_parameters_aggregated, metrics_aggregated = super().aggregate_fit(\n        server_round=server_round, results=results, failures=failures\n    )\n\n    if fedavg_parameters_aggregated is None:\n        return None, {}\n\n    fedavg_weights_aggregate = parameters_to_ndarrays(fedavg_parameters_aggregated)\n\n    delta_t: NDArrays = [x - y for x, y in zip(fedavg_weights_aggregate, self.current_weights)]\n\n    self._update_parameters(delta_t)\n\n    new_weights = [\n        current_weight + self.eta * m_t / (np.sqrt(v_t) - d_t + self.tau)\n        for current_weight, m_t, v_t, d_t in zip(self.current_weights, self.m_t, self.v_t, self.d_t)\n    ]\n\n    self.current_weights = new_weights\n\n    return ndarrays_to_parameters(self.current_weights), metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.model_merge_strategy","title":"<code>model_merge_strategy</code>","text":""},{"location":"api/#fl4health.strategies.model_merge_strategy.ModelMergeStrategy","title":"<code>ModelMergeStrategy</code>","text":"<p>               Bases: <code>Strategy</code></p> Source code in <code>fl4health/strategies/model_merge_strategy.py</code> <pre><code>class ModelMergeStrategy(Strategy):\n    # pylint: disable=too-many-arguments,too-many-instance-attributes\n    def __init__(\n        self,\n        *,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_fit_clients: int = 2,\n        min_evaluate_clients: int = 2,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[\n                [int, NDArrays, dict[str, Scalar]],\n                tuple[float, dict[str, Scalar]] | None,\n            ]\n            | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        weighted_aggregation: bool = True,\n    ) -&gt; None:\n        \"\"\"\n        Model Merging strategy in which weights are loaded from clients, averaged (weighted or unweighted) and\n        redistributed to the clients for evaluation.\n\n        Args:\n            fraction_fit (float, optional): Fraction of clients used during training. In case ``min_fit_clients`` is\n                larger than ``fraction_fit * available_clients``, ``min_fit_clients`` will still be sampled.\n                Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. In case\n                ``min_evaluate_clients`` is larger than ``fraction_evaluate * available_clients``,\n                ``min_evaluate_clients`` will still be sampled. Defaults to 1.0.\n            min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n            min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n            min_available_clients (int, optional): Minimum number of total clients in the system.\n                Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for central server-side evaluation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                client-side validation by providing a ``Config`` dictionary. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n                average or a uniform average. Important to note that weighting is based on number of samples in the\n                test dataset for the ``ModelMergeStrategy``. Defaults to True.\n        \"\"\"\n        self.fraction_fit = fraction_fit\n        self.fraction_evaluate = fraction_evaluate\n        self.min_fit_clients = min_fit_clients\n        self.min_evaluate_clients = min_evaluate_clients\n        self.min_available_clients = min_available_clients\n        self.evaluate_fn = evaluate_fn\n        self.on_fit_config_fn = on_fit_config_fn\n        self.on_evaluate_config_fn = on_evaluate_config_fn\n        self.accept_failures = accept_failures\n        self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n        self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n        self.weighted_aggregation = weighted_aggregation\n\n    def configure_fit(\n        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n    ) -&gt; list[tuple[ClientProxy, FitIns]]:\n        \"\"\"\n        Sample and configure clients for a fit round.\n\n        In ``ModelMergeStrategy``, it is assumed that server side parameters are empty and clients will be initialized\n        with their weights locally.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            parameters (Parameters): Not used.\n            client_manager (ClientManager): The manager used to sample from the available clients.\n\n        Returns:\n            (list[tuple[ClientProxy, FitIns]]): List of sampled client identifiers and the configuration/parameters to\n                be sent to each client (packaged as ``FitIns``).\n        \"\"\"\n        config = {}\n        if self.on_fit_config_fn is not None:\n            # Custom fit config function provided\n            config = self.on_fit_config_fn(server_round)\n        fit_ins = FitIns(Parameters([], \"\"), config)\n\n        # Sample clients\n        if isinstance(client_manager, BaseFractionSamplingManager):\n            clients = client_manager.sample_fraction(self.fraction_fit, self.min_available_clients)\n        else:\n            sample_size = max(int(client_manager.num_available() * self.fraction_fit), self.min_fit_clients)\n            clients = client_manager.sample(num_clients=sample_size, min_num_clients=self.min_available_clients)\n\n        # Return client/config pairs\n        return [(client, fit_ins) for client in clients]\n\n    def configure_evaluate(\n        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n    ) -&gt; list[tuple[ClientProxy, EvaluateIns]]:\n        \"\"\"\n        Sample and configure clients for a evaluation round.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on. Only one round for\n                ``ModelMergeStrategy``\n            parameters (Parameters): The parameters to be used to initialize the clients for the eval round.\n                This will only occur following model merging.\n            client_manager (ClientManager): The manager used to sample from the available clients.\n\n        Returns:\n            (list[tuple[ClientProxy, EvaluateIns]]): List of sampled client identifiers and the\n                configuration/parameters to be sent to each client (packaged as ``EvaluateIns``).\n        \"\"\"\n        # Do not configure federated evaluation if fraction eval is 0.\n        if self.fraction_evaluate == 0.0:\n            return []\n\n        # Parameters and config\n        config = {}\n        if self.on_evaluate_config_fn is not None:\n            # Custom evaluation config function provided\n            config = self.on_evaluate_config_fn(server_round)\n        evaluate_ins = EvaluateIns(parameters, config)\n\n        # Sample clients\n        if isinstance(client_manager, BaseFractionSamplingManager):\n            clients = client_manager.sample_fraction(self.fraction_evaluate, self.min_available_clients)\n        else:\n            sample_size = max(int(client_manager.num_available() * self.fraction_evaluate), self.min_evaluate_clients)\n            clients = client_manager.sample(num_clients=sample_size, min_num_clients=self.min_available_clients)\n\n        # Return client/config pairs\n        return [(client, evaluate_ins) for client in clients]\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"\n        Performs model merging by taking an unweighted average of client weights and metrics.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on. Only one round for\n                ``ModelMergeStrategy``.\n            results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local fit\n                that need to be aggregated on the server-side.\n            failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n                from clients that experienced an issue during fit, such as timeouts or exceptions.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n        # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n        # reducing numerical fluctuation.\n        decoded_and_sorted_results = [\n            (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n        ]\n\n        # Aggregate them in an weighted or unweighted fashion based on self.weighted_aggregation.\n        aggregated_arrays = aggregate_results(decoded_and_sorted_results, self.weighted_aggregation)\n        # Convert back to parameters\n        parameters_aggregated = ndarrays_to_parameters(aggregated_arrays)\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.fit_metrics_aggregation_fn:\n            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n        return parameters_aggregated, metrics_aggregated\n\n    def aggregate_evaluate(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, EvaluateRes]],\n        failures: list[tuple[ClientProxy, EvaluateRes] | BaseException],\n    ) -&gt; tuple[float | None, dict[str, Scalar]]:\n        \"\"\"\n        Aggregate the metrics returned from the clients as a result of the evaluation round. ``ModelMergeStrategy``\n        assumes only metrics will be computed on client and loss is set to None.\n\n        Args:\n            server_round (int): Server round we're currently on..\n            results (list[tuple[ClientProxy, EvaluateRes]]): The client identifiers and the results of their local\n                evaluation that need to be aggregated on the server-side. These results are loss values\n                (None in this case) and the metrics dictionary.\n            failures (list[tuple[ClientProxy, EvaluateRes]  |  BaseException]): These are the results and exceptions\n                from clients that experienced an issue during evaluation, such as timeouts or exceptions.\n\n        Returns:\n            (tuple[float | None, dict[str, Scalar]]): Aggregated loss values and the aggregated metrics. The metrics\n                are aggregated according to ``evaluate_metrics_aggregation_fn``.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.evaluate_metrics_aggregation_fn:\n            eval_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.evaluate_metrics_aggregation_fn(eval_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No evaluate_metrics_aggregation_fn provided\")\n\n        return None, metrics_aggregated\n\n    def evaluate(self, server_round: int, parameters: Parameters) -&gt; tuple[float, dict[str, Scalar]] | None:\n        \"\"\"\n        Evaluate the model parameters after the merging has occurred. This function can be used to perform centralized\n        (i.e., server-side) evaluation of model parameters.\n\n        Args:\n            server_round (int): Server round. Only one round in ``ModelMergeStrategy``.\n            parameters: Parameters The current model parameters after merging has occurred.\n\n        Returns:\n            (tuple[float, dict[str, Scalar]] | None): A Tuple containing loss and a dictionary containing task-specific\n                metrics (e.g., accuracy).\n        \"\"\"\n        if self.evaluate_fn is None:\n            return None\n\n        eval_res = self.evaluate_fn(server_round, parameters_to_ndarrays(parameters), {})\n\n        if eval_res is None:\n            return None\n        loss, metrics = eval_res\n        return loss, metrics\n\n    def initialize_parameters(self, client_manager: ClientManager) -&gt; Parameters | None:\n        \"\"\"\n        Required definition of parent class. ``ModelMergeStrategy`` does not support server side initialization.\n        Parameters are always set to None.\n\n        Args:\n            client_manager (ClientManager): Unused.\n        \"\"\"\n        return None\n</code></pre>"},{"location":"api/#fl4health.strategies.model_merge_strategy.ModelMergeStrategy.__init__","title":"<code>__init__(*, fraction_fit=1.0, fraction_evaluate=1.0, min_fit_clients=2, min_evaluate_clients=2, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, weighted_aggregation=True)</code>","text":"<p>Model Merging strategy in which weights are loaded from clients, averaged (weighted or unweighted) and redistributed to the clients for evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. In case <code>min_fit_clients</code> is larger than <code>fraction_fit * available_clients</code>, <code>min_fit_clients</code> will still be sampled. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. In case <code>min_evaluate_clients</code> is larger than <code>fraction_evaluate * available_clients</code>, <code>min_evaluate_clients</code> will still be sampled. Defaults to 1.0.</p> <code>1.0</code> <code>min_fit_clients</code> <code>int</code> <p>Minimum number of clients used during training. Defaults to 2.</p> <code>2</code> <code>min_evaluate_clients</code> <code>int</code> <p>Minimum number of clients used during validation. Defaults to 2.</p> <code>2</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of total clients in the system. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>weighted_aggregation</code> <code>bool</code> <p>Determines whether parameter aggregation is a linearly weighted average or a uniform average. Important to note that weighting is based on number of samples in the test dataset for the <code>ModelMergeStrategy</code>. Defaults to True.</p> <code>True</code> Source code in <code>fl4health/strategies/model_merge_strategy.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_fit_clients: int = 2,\n    min_evaluate_clients: int = 2,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[\n            [int, NDArrays, dict[str, Scalar]],\n            tuple[float, dict[str, Scalar]] | None,\n        ]\n        | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    weighted_aggregation: bool = True,\n) -&gt; None:\n    \"\"\"\n    Model Merging strategy in which weights are loaded from clients, averaged (weighted or unweighted) and\n    redistributed to the clients for evaluation.\n\n    Args:\n        fraction_fit (float, optional): Fraction of clients used during training. In case ``min_fit_clients`` is\n            larger than ``fraction_fit * available_clients``, ``min_fit_clients`` will still be sampled.\n            Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. In case\n            ``min_evaluate_clients`` is larger than ``fraction_evaluate * available_clients``,\n            ``min_evaluate_clients`` will still be sampled. Defaults to 1.0.\n        min_fit_clients (int, optional): Minimum number of clients used during training. Defaults to 2.\n        min_evaluate_clients (int, optional): Minimum number of clients used during validation. Defaults to 2.\n        min_available_clients (int, optional): Minimum number of total clients in the system.\n            Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for central server-side evaluation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            client-side validation by providing a ``Config`` dictionary. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        weighted_aggregation (bool, optional): Determines whether parameter aggregation is a linearly weighted\n            average or a uniform average. Important to note that weighting is based on number of samples in the\n            test dataset for the ``ModelMergeStrategy``. Defaults to True.\n    \"\"\"\n    self.fraction_fit = fraction_fit\n    self.fraction_evaluate = fraction_evaluate\n    self.min_fit_clients = min_fit_clients\n    self.min_evaluate_clients = min_evaluate_clients\n    self.min_available_clients = min_available_clients\n    self.evaluate_fn = evaluate_fn\n    self.on_fit_config_fn = on_fit_config_fn\n    self.on_evaluate_config_fn = on_evaluate_config_fn\n    self.accept_failures = accept_failures\n    self.fit_metrics_aggregation_fn = fit_metrics_aggregation_fn\n    self.evaluate_metrics_aggregation_fn = evaluate_metrics_aggregation_fn\n    self.weighted_aggregation = weighted_aggregation\n</code></pre>"},{"location":"api/#fl4health.strategies.model_merge_strategy.ModelMergeStrategy.configure_fit","title":"<code>configure_fit(server_round, parameters, client_manager)</code>","text":"<p>Sample and configure clients for a fit round.</p> <p>In <code>ModelMergeStrategy</code>, it is assumed that server side parameters are empty and clients will be initialized with their weights locally.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>parameters</code> <code>Parameters</code> <p>Not used.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The manager used to sample from the available clients.</p> required <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, FitIns]]</code> <p>List of sampled client identifiers and the configuration/parameters to be sent to each client (packaged as <code>FitIns</code>).</p> Source code in <code>fl4health/strategies/model_merge_strategy.py</code> <pre><code>def configure_fit(\n    self, server_round: int, parameters: Parameters, client_manager: ClientManager\n) -&gt; list[tuple[ClientProxy, FitIns]]:\n    \"\"\"\n    Sample and configure clients for a fit round.\n\n    In ``ModelMergeStrategy``, it is assumed that server side parameters are empty and clients will be initialized\n    with their weights locally.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        parameters (Parameters): Not used.\n        client_manager (ClientManager): The manager used to sample from the available clients.\n\n    Returns:\n        (list[tuple[ClientProxy, FitIns]]): List of sampled client identifiers and the configuration/parameters to\n            be sent to each client (packaged as ``FitIns``).\n    \"\"\"\n    config = {}\n    if self.on_fit_config_fn is not None:\n        # Custom fit config function provided\n        config = self.on_fit_config_fn(server_round)\n    fit_ins = FitIns(Parameters([], \"\"), config)\n\n    # Sample clients\n    if isinstance(client_manager, BaseFractionSamplingManager):\n        clients = client_manager.sample_fraction(self.fraction_fit, self.min_available_clients)\n    else:\n        sample_size = max(int(client_manager.num_available() * self.fraction_fit), self.min_fit_clients)\n        clients = client_manager.sample(num_clients=sample_size, min_num_clients=self.min_available_clients)\n\n    # Return client/config pairs\n    return [(client, fit_ins) for client in clients]\n</code></pre>"},{"location":"api/#fl4health.strategies.model_merge_strategy.ModelMergeStrategy.configure_evaluate","title":"<code>configure_evaluate(server_round, parameters, client_manager)</code>","text":"<p>Sample and configure clients for a evaluation round.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on. Only one round for <code>ModelMergeStrategy</code></p> required <code>parameters</code> <code>Parameters</code> <p>The parameters to be used to initialize the clients for the eval round. This will only occur following model merging.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The manager used to sample from the available clients.</p> required <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, EvaluateIns]]</code> <p>List of sampled client identifiers and the configuration/parameters to be sent to each client (packaged as <code>EvaluateIns</code>).</p> Source code in <code>fl4health/strategies/model_merge_strategy.py</code> <pre><code>def configure_evaluate(\n    self, server_round: int, parameters: Parameters, client_manager: ClientManager\n) -&gt; list[tuple[ClientProxy, EvaluateIns]]:\n    \"\"\"\n    Sample and configure clients for a evaluation round.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on. Only one round for\n            ``ModelMergeStrategy``\n        parameters (Parameters): The parameters to be used to initialize the clients for the eval round.\n            This will only occur following model merging.\n        client_manager (ClientManager): The manager used to sample from the available clients.\n\n    Returns:\n        (list[tuple[ClientProxy, EvaluateIns]]): List of sampled client identifiers and the\n            configuration/parameters to be sent to each client (packaged as ``EvaluateIns``).\n    \"\"\"\n    # Do not configure federated evaluation if fraction eval is 0.\n    if self.fraction_evaluate == 0.0:\n        return []\n\n    # Parameters and config\n    config = {}\n    if self.on_evaluate_config_fn is not None:\n        # Custom evaluation config function provided\n        config = self.on_evaluate_config_fn(server_round)\n    evaluate_ins = EvaluateIns(parameters, config)\n\n    # Sample clients\n    if isinstance(client_manager, BaseFractionSamplingManager):\n        clients = client_manager.sample_fraction(self.fraction_evaluate, self.min_available_clients)\n    else:\n        sample_size = max(int(client_manager.num_available() * self.fraction_evaluate), self.min_evaluate_clients)\n        clients = client_manager.sample(num_clients=sample_size, min_num_clients=self.min_available_clients)\n\n    # Return client/config pairs\n    return [(client, evaluate_ins) for client in clients]\n</code></pre>"},{"location":"api/#fl4health.strategies.model_merge_strategy.ModelMergeStrategy.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Performs model merging by taking an unweighted average of client weights and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on. Only one round for <code>ModelMergeStrategy</code>.</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>The client identifiers and the results of their local fit that need to be aggregated on the server-side.</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>These are the results and exceptions from clients that experienced an issue during fit, such as timeouts or exceptions.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar]]</code> <p>The aggregated model weights and the metrics dictionary.</p> Source code in <code>fl4health/strategies/model_merge_strategy.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"\n    Performs model merging by taking an unweighted average of client weights and metrics.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on. Only one round for\n            ``ModelMergeStrategy``.\n        results (list[tuple[ClientProxy, FitRes]]): The client identifiers and the results of their local fit\n            that need to be aggregated on the server-side.\n        failures (list[tuple[ClientProxy, FitRes] | BaseException]): These are the results and exceptions\n            from clients that experienced an issue during fit, such as timeouts or exceptions.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar]]): The aggregated model weights and the metrics dictionary.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n    # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n    # reducing numerical fluctuation.\n    decoded_and_sorted_results = [\n        (weights, sample_counts) for _, weights, sample_counts in decode_and_pseudo_sort_results(results)\n    ]\n\n    # Aggregate them in an weighted or unweighted fashion based on self.weighted_aggregation.\n    aggregated_arrays = aggregate_results(decoded_and_sorted_results, self.weighted_aggregation)\n    # Convert back to parameters\n    parameters_aggregated = ndarrays_to_parameters(aggregated_arrays)\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.fit_metrics_aggregation_fn:\n        fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n    return parameters_aggregated, metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.model_merge_strategy.ModelMergeStrategy.aggregate_evaluate","title":"<code>aggregate_evaluate(server_round, results, failures)</code>","text":"<p>Aggregate the metrics returned from the clients as a result of the evaluation round. <code>ModelMergeStrategy</code> assumes only metrics will be computed on client and loss is set to None.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Server round we're currently on..</p> required <code>results</code> <code>list[tuple[ClientProxy, EvaluateRes]]</code> <p>The client identifiers and the results of their local evaluation that need to be aggregated on the server-side. These results are loss values (None in this case) and the metrics dictionary.</p> required <code>failures</code> <code>list[tuple[ClientProxy, EvaluateRes] | BaseException]</code> <p>These are the results and exceptions from clients that experienced an issue during evaluation, such as timeouts or exceptions.</p> required <p>Returns:</p> Type Description <code>tuple[float | None, dict[str, Scalar]]</code> <p>Aggregated loss values and the aggregated metrics. The metrics are aggregated according to <code>evaluate_metrics_aggregation_fn</code>.</p> Source code in <code>fl4health/strategies/model_merge_strategy.py</code> <pre><code>def aggregate_evaluate(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, EvaluateRes]],\n    failures: list[tuple[ClientProxy, EvaluateRes] | BaseException],\n) -&gt; tuple[float | None, dict[str, Scalar]]:\n    \"\"\"\n    Aggregate the metrics returned from the clients as a result of the evaluation round. ``ModelMergeStrategy``\n    assumes only metrics will be computed on client and loss is set to None.\n\n    Args:\n        server_round (int): Server round we're currently on..\n        results (list[tuple[ClientProxy, EvaluateRes]]): The client identifiers and the results of their local\n            evaluation that need to be aggregated on the server-side. These results are loss values\n            (None in this case) and the metrics dictionary.\n        failures (list[tuple[ClientProxy, EvaluateRes]  |  BaseException]): These are the results and exceptions\n            from clients that experienced an issue during evaluation, such as timeouts or exceptions.\n\n    Returns:\n        (tuple[float | None, dict[str, Scalar]]): Aggregated loss values and the aggregated metrics. The metrics\n            are aggregated according to ``evaluate_metrics_aggregation_fn``.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.evaluate_metrics_aggregation_fn:\n        eval_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.evaluate_metrics_aggregation_fn(eval_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No evaluate_metrics_aggregation_fn provided\")\n\n    return None, metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.model_merge_strategy.ModelMergeStrategy.evaluate","title":"<code>evaluate(server_round, parameters)</code>","text":"<p>Evaluate the model parameters after the merging has occurred. This function can be used to perform centralized (i.e., server-side) evaluation of model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Server round. Only one round in <code>ModelMergeStrategy</code>.</p> required <code>parameters</code> <code>Parameters</code> <p>Parameters The current model parameters after merging has occurred.</p> required <p>Returns:</p> Type Description <code>tuple[float, dict[str, Scalar]] | None</code> <p>A Tuple containing loss and a dictionary containing task-specific metrics (e.g., accuracy).</p> Source code in <code>fl4health/strategies/model_merge_strategy.py</code> <pre><code>def evaluate(self, server_round: int, parameters: Parameters) -&gt; tuple[float, dict[str, Scalar]] | None:\n    \"\"\"\n    Evaluate the model parameters after the merging has occurred. This function can be used to perform centralized\n    (i.e., server-side) evaluation of model parameters.\n\n    Args:\n        server_round (int): Server round. Only one round in ``ModelMergeStrategy``.\n        parameters: Parameters The current model parameters after merging has occurred.\n\n    Returns:\n        (tuple[float, dict[str, Scalar]] | None): A Tuple containing loss and a dictionary containing task-specific\n            metrics (e.g., accuracy).\n    \"\"\"\n    if self.evaluate_fn is None:\n        return None\n\n    eval_res = self.evaluate_fn(server_round, parameters_to_ndarrays(parameters), {})\n\n    if eval_res is None:\n        return None\n    loss, metrics = eval_res\n    return loss, metrics\n</code></pre>"},{"location":"api/#fl4health.strategies.model_merge_strategy.ModelMergeStrategy.initialize_parameters","title":"<code>initialize_parameters(client_manager)</code>","text":"<p>Required definition of parent class. <code>ModelMergeStrategy</code> does not support server side initialization. Parameters are always set to None.</p> <p>Parameters:</p> Name Type Description Default <code>client_manager</code> <code>ClientManager</code> <p>Unused.</p> required Source code in <code>fl4health/strategies/model_merge_strategy.py</code> <pre><code>def initialize_parameters(self, client_manager: ClientManager) -&gt; Parameters | None:\n    \"\"\"\n    Required definition of parent class. ``ModelMergeStrategy`` does not support server side initialization.\n    Parameters are always set to None.\n\n    Args:\n        client_manager (ClientManager): Unused.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/#fl4health.strategies.noisy_aggregate","title":"<code>noisy_aggregate</code>","text":""},{"location":"api/#fl4health.strategies.noisy_aggregate.add_noise_to_array","title":"<code>add_noise_to_array(layer, noise_std_dev, denominator)</code>","text":"<p>For a given numpy array, this adds centered gaussian noise with a provided standard deviation to each element of the provided array. This noise is normalized by some value, as given in the denominator.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>NDArray</code> <p>The numpy array to have element-wise noise added to it.</p> required <code>noise_std_dev</code> <code>float</code> <p>The standard deviation of the centered gaussian noise to be added to each element</p> required <code>denominator</code> <code>int</code> <p>Normalization value for scaling down the values in the array.</p> required <p>Returns:</p> Type Description <code>NDArray</code> <p>The element-wise noised array, scaled by the denominator value.</p> Source code in <code>fl4health/strategies/noisy_aggregate.py</code> <pre><code>def add_noise_to_array(layer: NDArray, noise_std_dev: float, denominator: int) -&gt; NDArray:\n    \"\"\"\n    For a given numpy array, this adds centered gaussian noise with a provided standard deviation to each element of\n    the provided array. This noise is normalized by some value, as given in the denominator.\n\n    Args:\n        layer (NDArray): The numpy array to have element-wise noise added to it.\n        noise_std_dev (float): The standard deviation of the centered gaussian noise to be added to each element\n        denominator (int): Normalization value for scaling down the values in the array.\n\n    Returns:\n        (NDArray): The element-wise noised array, scaled by the denominator value.\n    \"\"\"\n    layer_noise = np.random.normal(0.0, noise_std_dev, layer.shape)\n    return (1.0 / denominator) * (layer + layer_noise)\n</code></pre>"},{"location":"api/#fl4health.strategies.noisy_aggregate.add_noise_to_ndarrays","title":"<code>add_noise_to_ndarrays(client_model_updates, sigma, n_clients)</code>","text":"<p>This function adds centered Gaussian noise (with standard deviation sigma) to the uniform average  of the list of the numpy arrays provided.</p> <p>Parameters:</p> Name Type Description Default <code>client_model_updates</code> <code>list[NDArrays]</code> <p>List of lists of numpy arrays. Each member of the list represents a set of numpy arrays, each of which should be averaged element-wise with the corresponding array from the other lists. These will have centered Gaussian noise added.</p> required <code>sigma</code> <code>float</code> <p>The standard deviation of the centered Gaussian noise to be added to each element.</p> required <code>n_clients</code> <code>int</code> <p>The number of arrays in the average. This should be the same as the size of <code>client_model_updates</code> in almost all cases.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Average of the centered Gaussian noised arrays.</p> Source code in <code>fl4health/strategies/noisy_aggregate.py</code> <pre><code>def add_noise_to_ndarrays(client_model_updates: list[NDArrays], sigma: float, n_clients: int) -&gt; NDArrays:\n    \"\"\"\n    This function adds centered Gaussian noise (with standard deviation sigma) to the uniform average  of the list\n    of the numpy arrays provided.\n\n    Args:\n        client_model_updates (list[NDArrays]): List of lists of numpy arrays. Each member of the list represents a\n            set of numpy arrays, each of which should be averaged element-wise with the corresponding array from the\n            other lists. These will have centered Gaussian noise added.\n        sigma (float): The standard deviation of the centered Gaussian noise to be added to each element.\n        n_clients (int): The number of arrays in the average. This should be the same as the size of\n            ``client_model_updates`` in almost all cases.\n\n    Returns:\n        (NDArrays): Average of the centered Gaussian noised arrays.\n    \"\"\"\n    layer_sums: NDArrays = [\n        add_noise_to_array(reduce(np.add, layer_updates), sigma, n_clients)\n        for layer_updates in zip(*client_model_updates)\n    ]\n    return layer_sums\n</code></pre>"},{"location":"api/#fl4health.strategies.noisy_aggregate.gaussian_noisy_unweighted_aggregate","title":"<code>gaussian_noisy_unweighted_aggregate(results, noise_multiplier, clipping_bound)</code>","text":"<p>Compute unweighted average of weights. Apply Gaussian noise to the sum of these weights prior to normalizing.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[NDArrays, int]]</code> <p>List of tuples containing the model updates and the number of samples for each client.</p> required <code>noise_multiplier</code> <code>float</code> <p>The multiplier on the clipping bound to determine the std. dev. of noise applied to weight updates.</p> required <code>clipping_bound</code> <code>float</code> <p>The clipping bound applied to client model updates.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Model update for a given round.</p> Source code in <code>fl4health/strategies/noisy_aggregate.py</code> <pre><code>def gaussian_noisy_unweighted_aggregate(\n    results: list[tuple[NDArrays, int]], noise_multiplier: float, clipping_bound: float\n) -&gt; NDArrays:\n    \"\"\"\n    Compute unweighted average of weights. Apply Gaussian noise to the sum of these weights prior to normalizing.\n\n    Args:\n        results (list[tuple[NDArrays, int]]): List of tuples containing the model updates and the number of samples\n            for each client.\n        noise_multiplier (float): The multiplier on the clipping bound to determine the std. dev. of noise applied to\n            weight updates.\n        clipping_bound (float): The clipping bound applied to client model updates.\n\n    Returns:\n        (NDArrays): Model update for a given round.\n    \"\"\"\n    n_clients = len(results)\n    # dropping number of data points component\n    client_model_updates = [ndarrays for ndarrays, _ in results]\n    sigma = noise_multiplier * clipping_bound\n    return add_noise_to_ndarrays(client_model_updates, sigma, n_clients)\n</code></pre>"},{"location":"api/#fl4health.strategies.noisy_aggregate.gaussian_noisy_weighted_aggregate","title":"<code>gaussian_noisy_weighted_aggregate(results, noise_multiplier, clipping_bound, fraction_fit, per_client_example_cap, total_client_weight)</code>","text":"<p>Compute weighted average of weights. Apply Gaussian noise to the sum of these weights prior to normalizing.</p> <p>Weighted Implementation based on https://arxiv.org/pdf/1710.06963.pdf.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[NDArrays, int]]</code> <p>List of tuples containing the model updates and the number of samples for each client.</p> required <code>noise_multiplier</code> <code>float</code> <p>The multiplier on the clipping bound to determine the std. dev. of noise applied to weight updates.</p> required <code>clipping_bound</code> <code>float</code> <p>The clipping bound applied to client model updates.</p> required <code>fraction_fit</code> <code>float</code> <p>Fraction of clients sampled each round.</p> required <code>per_client_example_cap</code> <code>float</code> <p>The maximum number samples per client.</p> required <code>total_client_weight</code> <code>float</code> <p>The total client weight across samples.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Noised model update for a given round.</p> Source code in <code>fl4health/strategies/noisy_aggregate.py</code> <pre><code>def gaussian_noisy_weighted_aggregate(\n    results: list[tuple[NDArrays, int]],\n    noise_multiplier: float,\n    clipping_bound: float,\n    fraction_fit: float,\n    per_client_example_cap: float,\n    total_client_weight: float,\n) -&gt; NDArrays:\n    \"\"\"\n    Compute weighted average of weights. Apply Gaussian noise to the sum of these weights prior to normalizing.\n\n    Weighted Implementation based on https://arxiv.org/pdf/1710.06963.pdf.\n\n\n    Args:\n        results (list[tuple[NDArrays, int]]): List of tuples containing the model updates and the number of samples\n            for each client.\n        noise_multiplier (float): The multiplier on the clipping bound to determine the std. dev. of noise applied to\n            weight updates.\n        clipping_bound (float):  The clipping bound applied to client model updates.\n        fraction_fit (float): Fraction of clients sampled each round.\n        per_client_example_cap (float): The maximum number samples per client.\n        total_client_weight (float): The total client weight across samples.\n\n    Returns:\n        (NDArrays): Noised model update for a given round.\n    \"\"\"\n    n_clients = len(results)\n    client_model_updates: list[NDArrays] = []\n    client_n_points: list[int] = []\n    for weights, n_points in results:\n        client_model_updates.append(weights)\n        client_n_points.append(n_points)\n\n    # Calculate coefficients (w_k) by taking the minimum of the sample counts divided by example cap and 1\n    client_coefficients = [min((n_points / per_client_example_cap, 1.0)) for n_points in client_n_points]\n\n    # Scale coefficients by total expected client weight\n    client_coefficients_scaled = [coef / (fraction_fit * total_client_weight) for coef in client_coefficients]\n\n    # Scale updates by coef for each client\n    client_model_updates = [\n        [layer_update * client_coef for layer_update in client_model_update]\n        for client_model_update, client_coef in zip(client_model_updates, client_coefficients_scaled)\n    ]  # Calculate model updates as linear combination of updates\n\n    # Update clipping bound as max(w_k) * clipping bound\n    # We only require w_k * update is bounded\n    # Refer to the footnote on page 4 in https://arxiv.org/pdf/1710.06963.pdf\n    updated_clipping_bound = clipping_bound * max(client_coefficients)\n\n    sigma = (noise_multiplier * updated_clipping_bound) / fraction_fit\n    return add_noise_to_ndarrays(client_model_updates, sigma, n_clients)\n</code></pre>"},{"location":"api/#fl4health.strategies.noisy_aggregate.gaussian_noisy_aggregate_clipping_bits","title":"<code>gaussian_noisy_aggregate_clipping_bits(bits, noise_std_dev)</code>","text":"<p>Computes the noisy aggregate of the clipping bits returned by each client as a list of numpy arrays. Note that each array should only have a single bit value. This returns the noisy unweighted average of these bits. The noise is centered Gaussian.</p> <p>Parameters:</p> Name Type Description Default <code>bits</code> <code>NDArrays</code> <p>Clipping bit returned by each client.</p> required <code>noise_std_dev</code> <code>float</code> <p>The standard deviation of the centered Gaussian noise applied to the bits.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The uniformly averaged noisy bit.</p> Source code in <code>fl4health/strategies/noisy_aggregate.py</code> <pre><code>def gaussian_noisy_aggregate_clipping_bits(bits: NDArrays, noise_std_dev: float) -&gt; float:\n    \"\"\"\n    Computes the noisy aggregate of the clipping bits returned by each client as a list of numpy arrays. Note that each\n    array should only have a single bit value. This returns the noisy unweighted average of these bits. The noise is\n    centered Gaussian.\n\n    Args:\n        bits (NDArrays): Clipping bit returned by each client.\n        noise_std_dev (float): The standard deviation of the centered Gaussian noise applied to the bits.\n\n    Returns:\n        (float): The uniformly averaged noisy bit.\n    \"\"\"\n    n_clients = len(bits)\n    bit_sum = reduce(np.add, bits)\n    # This should be \"shapeless\" since each client returns a single bit as a numpy array.\n    assert bit_sum.shape == ()\n    noised_bit_sum = add_noise_to_array(bit_sum, noise_std_dev, n_clients)\n    return float(noised_bit_sum)\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold","title":"<code>scaffold</code>","text":""},{"location":"api/#fl4health.strategies.scaffold.Scaffold","title":"<code>Scaffold</code>","text":"<p>               Bases: <code>BasicFedAvg</code></p> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>class Scaffold(BasicFedAvg):\n    def __init__(\n        self,\n        *,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        initial_parameters: Parameters,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        weighted_eval_losses: bool = True,\n        learning_rate: float = 1.0,\n        initial_control_variates: Parameters | None = None,\n        model: nn.Module | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Scaffold Federated Learning strategy. Implementation based on https://arxiv.org/pdf/1910.06378.pdf.\n\n        Args:\n            initial_parameters (Parameters): Initial model parameters to which all client models are set.\n            fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n            min_available_clients (int, optional): Minimum number of total clients in the system.\n                Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for central server-side evaluation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                client-side validation by providing a ``Config`` dictionary. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n                counts. Defaults to True.\n            learning_rate (float, optional): Learning rate for server side optimization. Defaults to 1.0.\n            initial_control_variates (Parameters | None, optional): These are the initial set of control variates\n                to use for the scaffold strategy both on the server and client sides. It is optional, but if it is not\n                provided, the strategy must receive a model that reflects the architecture to be used on the clients.\n                Defaults to None.\n            model (nn.Module | None, optional): If provided and ``initial_control_variates`` is not, this is used to\n                set the server control variates and the initial control variates on the client side to all zeros.\n                If ``initial_control_variates`` are provided, they take precedence. Defaults to None.\n        \"\"\"\n        self.server_model_weights = parameters_to_ndarrays(initial_parameters)\n        # Setup the initial control variates on the server-side and store them to be transmitted to the clients\n        initial_control_variates = self.initialize_control_variates(initial_control_variates, model)\n        initial_parameters.tensors.extend(initial_control_variates.tensors)\n\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            weighted_aggregation=False,\n            weighted_eval_losses=weighted_eval_losses,\n        )\n        self.learning_rate = learning_rate\n        self.parameter_packer = ParameterPackerWithControlVariates(len(self.server_model_weights))\n\n    def initialize_control_variates(\n        self, initial_control_variates: Parameters | None, model: nn.Module | None\n    ) -&gt; Parameters:\n        \"\"\"\n        This is a helper function for the SCAFFOLD strategy init function to initialize the\n        ``server_control_variates``. It either initializes the control variates with custom provided variates or using\n        the provided model architecture.\n\n        Args:\n            initial_control_variates (Parameters | None): These are the initial set of control variates\n                to use for the scaffold strategy both on the server and client sides. It is optional, but if it is not\n                provided, the strategy must receive a model that reflects the architecture to be used on the clients.\n                Defaults to None.\n            model (nn.Module | None): If provided and ``initial_control_variates`` is not, this is used to\n                set the server control variates and the initial control variates on the client side to all zeros.\n                If ``initial_control_variates`` are provided, they take precedence. Defaults to None.\n\n        Returns:\n            (Parameters): This quantity represents the initial values for the control variates for the server and on\n                the client-side.\n\n        Raises:\n            ValueError: This error will be raised if neither a model nor initial control variates are provided.\n        \"\"\"\n        if initial_control_variates is not None:\n            # If we've been provided with a set of initial control variates, we use those values\n            self.server_control_variates = parameters_to_ndarrays(initial_control_variates)\n            return initial_control_variates\n        if model is not None:\n            # If no initial values are provided but a model structure has been given, we initialize the control\n            # variates to zeros as recommended in the SCAFFOLD paper.\n            zero_control_variates = [np.zeros_like(val.data) for val in model.parameters() if val.requires_grad]\n            self.server_control_variates = zero_control_variates\n            return ndarrays_to_parameters(zero_control_variates)\n        # Either a model structure or custom initial values for the control variates must be provided to run\n        # SCAFFOLD\n        raise ValueError(\n            \"Both initial_control_variates and model are None. One must be defined in order to establish \"\n            \"initial values for the control variates.\"\n        )\n\n    def aggregate_fit(\n        self,\n        server_round: int,\n        results: list[tuple[ClientProxy, FitRes]],\n        failures: list[tuple[ClientProxy, FitRes] | BaseException],\n    ) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n        \"\"\"\n        Performs server-side aggregation of model weights and control variates associated with the SCAFFOLD method\n        Both model weights and control variates are aggregated through **UNWEIGHTED** averaging consistent with the\n        paper. The newly aggregated weights and control variates are then repacked and sent back to the clients.\n\n        This function also handles aggregation of training run metrics (i.e. accuracy over the local training etc.)\n        through the ``fit_metrics_aggregation_fn`` provided in constructing the strategy.\n\n        Args:\n            server_round (int): What round of FL we're on (from servers perspective).\n            results (list[tuple[ClientProxy, FitRes]]): These are the \"successful\" training run results. By default\n                these results are the only ones used in aggregation, even if some of the failed clients have partial\n                results (in the failures list).\n            failures (list[tuple[ClientProxy, FitRes] | BaseException]): This is the list of clients that\n                \"failed\" during the training phase for one reason or another, including timeouts and exceptions.\n\n        Returns:\n            (tuple[Parameters | None, dict[str, Scalar]]): The aggregated weighted and metrics dictionary. The\n                parameters are optional and will be none in the even that there are no successful clients or there\n                were failures and they are not accepted.\n        \"\"\"\n        if not results:\n            return None, {}\n        # Do not aggregate if there are failures and failures are not accepted\n        if not self.accept_failures and failures:\n            return None, {}\n\n        # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n        # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n        # reducing numerical fluctuation.\n        decoded_and_sorted_results = [weights for _, weights, _ in decode_and_pseudo_sort_results(results)]\n\n        # x = 1 / |S| * sum(x_i) and c = 1 / |S| * sum(delta_c_i)\n        # Aggregation operation over packed params (includes both weights and control variate updates)\n        aggregated_params = self.aggregate(decoded_and_sorted_results)\n\n        weights, control_variates_update = self.parameter_packer.unpack_parameters(aggregated_params)\n\n        self.server_model_weights = self.compute_updated_weights(weights)\n        self.server_control_variates = self.compute_updated_control_variates(control_variates_update)\n\n        parameters = self.parameter_packer.pack_parameters(self.server_model_weights, self.server_control_variates)\n\n        # Aggregate custom metrics if aggregation fn was provided\n        metrics_aggregated = {}\n        if self.fit_metrics_aggregation_fn:\n            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n        elif server_round == 1:  # Only log this warning once\n            log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n        return ndarrays_to_parameters(parameters), metrics_aggregated\n\n    def compute_parameter_delta(self, params_1: NDArrays, params_2: NDArrays) -&gt; NDArrays:\n        \"\"\"\n        Computes element-wise difference of two lists of NDarray where elements in ``params_2`` are subtracted from\n        elements in ``params_1``.\n\n        Args:\n            params_1 (NDArrays): Parameters to be subtracted from.\n            params_2 (NDArrays): Parameters to subtract from ``params_1``.\n\n        Returns:\n            (NDArrays): Element-wise subtraction result across all numpy arrays.\n        \"\"\"\n        parameter_delta: NDArrays = [param_1 - param_2 for param_1, param_2 in zip(params_1, params_2)]\n\n        return parameter_delta\n\n    def compute_updated_parameters(\n        self, scaling_coefficient: float, original_params: NDArrays, parameter_updates: NDArrays\n    ) -&gt; NDArrays:\n        \"\"\"\n        Computes updated_params by moving in the direction of parameter_updates with a step proportional the scaling\n        coefficient.\n\n        Calculates\n\n        \\\\[\\\\text{original_params} + \\\\text{scaling_coefficient} \\\\cdot \\\\text{parameter_updates}.\\\\]\n\n        Args:\n            scaling_coefficient (float): Scaling length for the parameter updates (can be thought of as\n                \"learning rate\").\n            original_params (NDArrays): Parameters to be updated.\n            parameter_updates (NDArrays): Update direction to update the ``original_params``.\n\n        Returns:\n            (NDArrays): Updated numpy arrays according to\n                \\\\(\\\\text{original_params} + \\\\text{scaling_coefficient} \\\\cdot \\\\text{parameter_updates}\\\\).\n        \"\"\"\n        return [\n            original_param + scaling_coefficient * update\n            for original_param, update in zip(original_params, parameter_updates)\n        ]\n\n    def aggregate(self, params: list[NDArrays]) -&gt; NDArrays:\n        \"\"\"\n        Simple unweighted average to aggregate params, consistent with SCAFFOLD paper. This is \"element-wise\"\n        averaging.\n\n        Args:\n            params (list[NDArrays]): numpy arrays whose entries are to be averaged together.\n\n        Returns:\n            (NDArrays): Element-wise average over the list of numpy arrays.\n        \"\"\"\n        num_clients = len(params)\n\n        # Compute average weights of each layer\n        params_prime: NDArrays = [reduce(np.add, layer_updates) / num_clients for layer_updates in zip(*params)]\n\n        return params_prime\n\n    def configure_fit_all(\n        self, server_round: int, parameters: Parameters, client_manager: ClientManager\n    ) -&gt; list[tuple[ClientProxy, FitIns]]:\n        \"\"\"\n        This function configures **ALL** clients for a training round. That is, it forces the client manager to grab\n        all of the available clients to participate in the training round. By default, the manager will at least wait\n        for the ``min_available_clients`` threshold to be met. Thereafter it will simply grab all available clients for\n        participation.\n\n        The function follows the standard configuration flow where the ``on_fit_config_fn`` function is used to produce\n        configurations to be sent to all clients. These are packaged with the provided parameters and set over to the\n        clients.\n\n        Args:\n            server_round (int): Indicates the server round we're currently on.\n            parameters (Parameters): The parameters to be used to initialize the clients for the fit round.\n            client_manager (ClientManager): The manager used to grab all of the clients. Currently we restrict this to\n                be ``BaseFractionSamplingManager``, which has a \"sample all\" function built in.\n\n        Returns:\n            (list[tuple[ClientProxy, FitIns]]): List of sampled client identifiers and the configuration/parameters to\n                be sent to each client (packaged as ``FitIns``).\n        \"\"\"\n        # This strategy requires the client manager to be of type at least BaseFractionSamplingManager\n        assert isinstance(client_manager, BaseFractionSamplingManager)\n\n        config = {}\n        if self.on_fit_config_fn is not None:\n            # Custom fit config function provided\n            config = self.on_fit_config_fn(server_round)\n        else:\n            config = {\"current_server_round\": server_round}\n\n        fit_ins = FitIns(parameters, config)\n\n        clients = client_manager.sample_all(self.min_available_clients)\n\n        # Return client/config pairs\n        return [(client, fit_ins) for client in clients]\n\n    def compute_updated_weights(self, weights: NDArrays) -&gt; NDArrays:\n        \"\"\"\n        Computes and update to the current ``self.server_model_weights``. This assumes that the weights represents the\n        raw weights aggregated from the client. Therefore it first needs to be turned into a \"delta\" with\n        ``weights - self.server_model_weights``.\n\n        Then this is used to update with a learning rate scalar (set by ``self.learning_rate``) as\n        ``self.server_model_weights + self.learning_rate * (weights - self.server_model_weights)``.\n\n        Args:\n            weights (NDArrays): The updated weights (aggregated from the clients).\n\n        Returns:\n            (NDArrays): ``self.server_model_weights + self.learning_rate * (weights - self.server_model_weights)``\n                These are the updated server model weights.\n        \"\"\"\n        # x_update = y_i - x\n        delta_weights = self.compute_parameter_delta(weights, self.server_model_weights)\n\n        # x = x + lr * x_update\n        return self.compute_updated_parameters(self.learning_rate, self.server_model_weights, delta_weights)\n\n    def compute_updated_control_variates(self, control_variates_update: NDArrays) -&gt; NDArrays:\n        \"\"\"\n        Given the aggregated control variates from the clients, this updates the server control variates in line with\n        the paper. If \\\\(c\\\\) is the server control variates and ``c_update`` is the client control variates, then\n        this update takes the following form.\n\n        \\\\[c + \\\\frac{\\\\vert S \\\\vert}{N} \\\\cdot c_{\\\\text{update}},\\\\]\n\n        where \\\\(\\\\vert S\\\\vert\\\\) is the number of clients that participated and N is\n        the total number of clients \\\\(\\\\frac{\\\\vert S \\\\vert}{N}\\\\) is the proportion given by fraction fit.\n\n        Args:\n            control_variates_update (NDArrays): Aggregated control variates received from the clients (uniformly\n                averaged).\n\n        Returns:\n            (NDArrays): Updated server control variates according to the formula.\n        \"\"\"\n        # c = c + |S| / N * c_update\n        return self.compute_updated_parameters(\n            self.fraction_fit, self.server_control_variates, control_variates_update\n        )\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.Scaffold.__init__","title":"<code>__init__(*, fraction_fit=1.0, fraction_evaluate=1.0, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, initial_parameters, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, weighted_eval_losses=True, learning_rate=1.0, initial_control_variates=None, model=None)</code>","text":"<p>Scaffold Federated Learning strategy. Implementation based on https://arxiv.org/pdf/1910.06378.pdf.</p> <p>Parameters:</p> Name Type Description Default <code>initial_parameters</code> <code>Parameters</code> <p>Initial model parameters to which all client models are set.</p> required <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. Defaults to 1.0.</p> <code>1.0</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of total clients in the system. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. FedAvg default is weighted average of the losses by client dataset counts. Defaults to True.</p> <code>True</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for server side optimization. Defaults to 1.0.</p> <code>1.0</code> <code>initial_control_variates</code> <code>Parameters | None</code> <p>These are the initial set of control variates to use for the scaffold strategy both on the server and client sides. It is optional, but if it is not provided, the strategy must receive a model that reflects the architecture to be used on the clients. Defaults to None.</p> <code>None</code> <code>model</code> <code>Module | None</code> <p>If provided and <code>initial_control_variates</code> is not, this is used to set the server control variates and the initial control variates on the client side to all zeros. If <code>initial_control_variates</code> are provided, they take precedence. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>def __init__(\n    self,\n    *,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    initial_parameters: Parameters,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    weighted_eval_losses: bool = True,\n    learning_rate: float = 1.0,\n    initial_control_variates: Parameters | None = None,\n    model: nn.Module | None = None,\n) -&gt; None:\n    \"\"\"\n    Scaffold Federated Learning strategy. Implementation based on https://arxiv.org/pdf/1910.06378.pdf.\n\n    Args:\n        initial_parameters (Parameters): Initial model parameters to which all client models are set.\n        fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n        min_available_clients (int, optional): Minimum number of total clients in the system.\n            Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for central server-side evaluation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            client-side validation by providing a ``Config`` dictionary. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n            counts. Defaults to True.\n        learning_rate (float, optional): Learning rate for server side optimization. Defaults to 1.0.\n        initial_control_variates (Parameters | None, optional): These are the initial set of control variates\n            to use for the scaffold strategy both on the server and client sides. It is optional, but if it is not\n            provided, the strategy must receive a model that reflects the architecture to be used on the clients.\n            Defaults to None.\n        model (nn.Module | None, optional): If provided and ``initial_control_variates`` is not, this is used to\n            set the server control variates and the initial control variates on the client side to all zeros.\n            If ``initial_control_variates`` are provided, they take precedence. Defaults to None.\n    \"\"\"\n    self.server_model_weights = parameters_to_ndarrays(initial_parameters)\n    # Setup the initial control variates on the server-side and store them to be transmitted to the clients\n    initial_control_variates = self.initialize_control_variates(initial_control_variates, model)\n    initial_parameters.tensors.extend(initial_control_variates.tensors)\n\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        weighted_aggregation=False,\n        weighted_eval_losses=weighted_eval_losses,\n    )\n    self.learning_rate = learning_rate\n    self.parameter_packer = ParameterPackerWithControlVariates(len(self.server_model_weights))\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.Scaffold.initialize_control_variates","title":"<code>initialize_control_variates(initial_control_variates, model)</code>","text":"<p>This is a helper function for the SCAFFOLD strategy init function to initialize the <code>server_control_variates</code>. It either initializes the control variates with custom provided variates or using the provided model architecture.</p> <p>Parameters:</p> Name Type Description Default <code>initial_control_variates</code> <code>Parameters | None</code> <p>These are the initial set of control variates to use for the scaffold strategy both on the server and client sides. It is optional, but if it is not provided, the strategy must receive a model that reflects the architecture to be used on the clients. Defaults to None.</p> required <code>model</code> <code>Module | None</code> <p>If provided and <code>initial_control_variates</code> is not, this is used to set the server control variates and the initial control variates on the client side to all zeros. If <code>initial_control_variates</code> are provided, they take precedence. Defaults to None.</p> required <p>Returns:</p> Type Description <code>Parameters</code> <p>This quantity represents the initial values for the control variates for the server and on the client-side.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>This error will be raised if neither a model nor initial control variates are provided.</p> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>def initialize_control_variates(\n    self, initial_control_variates: Parameters | None, model: nn.Module | None\n) -&gt; Parameters:\n    \"\"\"\n    This is a helper function for the SCAFFOLD strategy init function to initialize the\n    ``server_control_variates``. It either initializes the control variates with custom provided variates or using\n    the provided model architecture.\n\n    Args:\n        initial_control_variates (Parameters | None): These are the initial set of control variates\n            to use for the scaffold strategy both on the server and client sides. It is optional, but if it is not\n            provided, the strategy must receive a model that reflects the architecture to be used on the clients.\n            Defaults to None.\n        model (nn.Module | None): If provided and ``initial_control_variates`` is not, this is used to\n            set the server control variates and the initial control variates on the client side to all zeros.\n            If ``initial_control_variates`` are provided, they take precedence. Defaults to None.\n\n    Returns:\n        (Parameters): This quantity represents the initial values for the control variates for the server and on\n            the client-side.\n\n    Raises:\n        ValueError: This error will be raised if neither a model nor initial control variates are provided.\n    \"\"\"\n    if initial_control_variates is not None:\n        # If we've been provided with a set of initial control variates, we use those values\n        self.server_control_variates = parameters_to_ndarrays(initial_control_variates)\n        return initial_control_variates\n    if model is not None:\n        # If no initial values are provided but a model structure has been given, we initialize the control\n        # variates to zeros as recommended in the SCAFFOLD paper.\n        zero_control_variates = [np.zeros_like(val.data) for val in model.parameters() if val.requires_grad]\n        self.server_control_variates = zero_control_variates\n        return ndarrays_to_parameters(zero_control_variates)\n    # Either a model structure or custom initial values for the control variates must be provided to run\n    # SCAFFOLD\n    raise ValueError(\n        \"Both initial_control_variates and model are None. One must be defined in order to establish \"\n        \"initial values for the control variates.\"\n    )\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.Scaffold.aggregate_fit","title":"<code>aggregate_fit(server_round, results, failures)</code>","text":"<p>Performs server-side aggregation of model weights and control variates associated with the SCAFFOLD method Both model weights and control variates are aggregated through UNWEIGHTED averaging consistent with the paper. The newly aggregated weights and control variates are then repacked and sent back to the clients.</p> <p>This function also handles aggregation of training run metrics (i.e. accuracy over the local training etc.) through the <code>fit_metrics_aggregation_fn</code> provided in constructing the strategy.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>What round of FL we're on (from servers perspective).</p> required <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>These are the \"successful\" training run results. By default these results are the only ones used in aggregation, even if some of the failed clients have partial results (in the failures list).</p> required <code>failures</code> <code>list[tuple[ClientProxy, FitRes] | BaseException]</code> <p>This is the list of clients that \"failed\" during the training phase for one reason or another, including timeouts and exceptions.</p> required <p>Returns:</p> Type Description <code>tuple[Parameters | None, dict[str, Scalar]]</code> <p>The aggregated weighted and metrics dictionary. The parameters are optional and will be none in the even that there are no successful clients or there were failures and they are not accepted.</p> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>def aggregate_fit(\n    self,\n    server_round: int,\n    results: list[tuple[ClientProxy, FitRes]],\n    failures: list[tuple[ClientProxy, FitRes] | BaseException],\n) -&gt; tuple[Parameters | None, dict[str, Scalar]]:\n    \"\"\"\n    Performs server-side aggregation of model weights and control variates associated with the SCAFFOLD method\n    Both model weights and control variates are aggregated through **UNWEIGHTED** averaging consistent with the\n    paper. The newly aggregated weights and control variates are then repacked and sent back to the clients.\n\n    This function also handles aggregation of training run metrics (i.e. accuracy over the local training etc.)\n    through the ``fit_metrics_aggregation_fn`` provided in constructing the strategy.\n\n    Args:\n        server_round (int): What round of FL we're on (from servers perspective).\n        results (list[tuple[ClientProxy, FitRes]]): These are the \"successful\" training run results. By default\n            these results are the only ones used in aggregation, even if some of the failed clients have partial\n            results (in the failures list).\n        failures (list[tuple[ClientProxy, FitRes] | BaseException]): This is the list of clients that\n            \"failed\" during the training phase for one reason or another, including timeouts and exceptions.\n\n    Returns:\n        (tuple[Parameters | None, dict[str, Scalar]]): The aggregated weighted and metrics dictionary. The\n            parameters are optional and will be none in the even that there are no successful clients or there\n            were failures and they are not accepted.\n    \"\"\"\n    if not results:\n        return None, {}\n    # Do not aggregate if there are failures and failures are not accepted\n    if not self.accept_failures and failures:\n        return None, {}\n\n    # Sorting the results by elements and sample counts. This is primarily to reduce numerical fluctuations in\n    # summing the numpy arrays during aggregation. This ensures that addition will occur in the same order,\n    # reducing numerical fluctuation.\n    decoded_and_sorted_results = [weights for _, weights, _ in decode_and_pseudo_sort_results(results)]\n\n    # x = 1 / |S| * sum(x_i) and c = 1 / |S| * sum(delta_c_i)\n    # Aggregation operation over packed params (includes both weights and control variate updates)\n    aggregated_params = self.aggregate(decoded_and_sorted_results)\n\n    weights, control_variates_update = self.parameter_packer.unpack_parameters(aggregated_params)\n\n    self.server_model_weights = self.compute_updated_weights(weights)\n    self.server_control_variates = self.compute_updated_control_variates(control_variates_update)\n\n    parameters = self.parameter_packer.pack_parameters(self.server_model_weights, self.server_control_variates)\n\n    # Aggregate custom metrics if aggregation fn was provided\n    metrics_aggregated = {}\n    if self.fit_metrics_aggregation_fn:\n        fit_metrics = [(res.num_examples, res.metrics) for _, res in results]\n        metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)\n    elif server_round == 1:  # Only log this warning once\n        log(WARNING, \"No fit_metrics_aggregation_fn provided\")\n\n    return ndarrays_to_parameters(parameters), metrics_aggregated\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.Scaffold.compute_parameter_delta","title":"<code>compute_parameter_delta(params_1, params_2)</code>","text":"<p>Computes element-wise difference of two lists of NDarray where elements in <code>params_2</code> are subtracted from elements in <code>params_1</code>.</p> <p>Parameters:</p> Name Type Description Default <code>params_1</code> <code>NDArrays</code> <p>Parameters to be subtracted from.</p> required <code>params_2</code> <code>NDArrays</code> <p>Parameters to subtract from <code>params_1</code>.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Element-wise subtraction result across all numpy arrays.</p> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>def compute_parameter_delta(self, params_1: NDArrays, params_2: NDArrays) -&gt; NDArrays:\n    \"\"\"\n    Computes element-wise difference of two lists of NDarray where elements in ``params_2`` are subtracted from\n    elements in ``params_1``.\n\n    Args:\n        params_1 (NDArrays): Parameters to be subtracted from.\n        params_2 (NDArrays): Parameters to subtract from ``params_1``.\n\n    Returns:\n        (NDArrays): Element-wise subtraction result across all numpy arrays.\n    \"\"\"\n    parameter_delta: NDArrays = [param_1 - param_2 for param_1, param_2 in zip(params_1, params_2)]\n\n    return parameter_delta\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.Scaffold.compute_updated_parameters","title":"<code>compute_updated_parameters(scaling_coefficient, original_params, parameter_updates)</code>","text":"<p>Computes updated_params by moving in the direction of parameter_updates with a step proportional the scaling coefficient.</p> <p>Calculates</p> \\[\\text{original_params} + \\text{scaling_coefficient} \\cdot \\text{parameter_updates}.\\] <p>Parameters:</p> Name Type Description Default <code>scaling_coefficient</code> <code>float</code> <p>Scaling length for the parameter updates (can be thought of as \"learning rate\").</p> required <code>original_params</code> <code>NDArrays</code> <p>Parameters to be updated.</p> required <code>parameter_updates</code> <code>NDArrays</code> <p>Update direction to update the <code>original_params</code>.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Updated numpy arrays according to \\(\\text{original_params} + \\text{scaling_coefficient} \\cdot \\text{parameter_updates}\\).</p> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>def compute_updated_parameters(\n    self, scaling_coefficient: float, original_params: NDArrays, parameter_updates: NDArrays\n) -&gt; NDArrays:\n    \"\"\"\n    Computes updated_params by moving in the direction of parameter_updates with a step proportional the scaling\n    coefficient.\n\n    Calculates\n\n    \\\\[\\\\text{original_params} + \\\\text{scaling_coefficient} \\\\cdot \\\\text{parameter_updates}.\\\\]\n\n    Args:\n        scaling_coefficient (float): Scaling length for the parameter updates (can be thought of as\n            \"learning rate\").\n        original_params (NDArrays): Parameters to be updated.\n        parameter_updates (NDArrays): Update direction to update the ``original_params``.\n\n    Returns:\n        (NDArrays): Updated numpy arrays according to\n            \\\\(\\\\text{original_params} + \\\\text{scaling_coefficient} \\\\cdot \\\\text{parameter_updates}\\\\).\n    \"\"\"\n    return [\n        original_param + scaling_coefficient * update\n        for original_param, update in zip(original_params, parameter_updates)\n    ]\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.Scaffold.aggregate","title":"<code>aggregate(params)</code>","text":"<p>Simple unweighted average to aggregate params, consistent with SCAFFOLD paper. This is \"element-wise\" averaging.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[NDArrays]</code> <p>numpy arrays whose entries are to be averaged together.</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Element-wise average over the list of numpy arrays.</p> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>def aggregate(self, params: list[NDArrays]) -&gt; NDArrays:\n    \"\"\"\n    Simple unweighted average to aggregate params, consistent with SCAFFOLD paper. This is \"element-wise\"\n    averaging.\n\n    Args:\n        params (list[NDArrays]): numpy arrays whose entries are to be averaged together.\n\n    Returns:\n        (NDArrays): Element-wise average over the list of numpy arrays.\n    \"\"\"\n    num_clients = len(params)\n\n    # Compute average weights of each layer\n    params_prime: NDArrays = [reduce(np.add, layer_updates) / num_clients for layer_updates in zip(*params)]\n\n    return params_prime\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.Scaffold.configure_fit_all","title":"<code>configure_fit_all(server_round, parameters, client_manager)</code>","text":"<p>This function configures ALL clients for a training round. That is, it forces the client manager to grab all of the available clients to participate in the training round. By default, the manager will at least wait for the <code>min_available_clients</code> threshold to be met. Thereafter it will simply grab all available clients for participation.</p> <p>The function follows the standard configuration flow where the <code>on_fit_config_fn</code> function is used to produce configurations to be sent to all clients. These are packaged with the provided parameters and set over to the clients.</p> <p>Parameters:</p> Name Type Description Default <code>server_round</code> <code>int</code> <p>Indicates the server round we're currently on.</p> required <code>parameters</code> <code>Parameters</code> <p>The parameters to be used to initialize the clients for the fit round.</p> required <code>client_manager</code> <code>ClientManager</code> <p>The manager used to grab all of the clients. Currently we restrict this to be <code>BaseFractionSamplingManager</code>, which has a \"sample all\" function built in.</p> required <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, FitIns]]</code> <p>List of sampled client identifiers and the configuration/parameters to be sent to each client (packaged as <code>FitIns</code>).</p> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>def configure_fit_all(\n    self, server_round: int, parameters: Parameters, client_manager: ClientManager\n) -&gt; list[tuple[ClientProxy, FitIns]]:\n    \"\"\"\n    This function configures **ALL** clients for a training round. That is, it forces the client manager to grab\n    all of the available clients to participate in the training round. By default, the manager will at least wait\n    for the ``min_available_clients`` threshold to be met. Thereafter it will simply grab all available clients for\n    participation.\n\n    The function follows the standard configuration flow where the ``on_fit_config_fn`` function is used to produce\n    configurations to be sent to all clients. These are packaged with the provided parameters and set over to the\n    clients.\n\n    Args:\n        server_round (int): Indicates the server round we're currently on.\n        parameters (Parameters): The parameters to be used to initialize the clients for the fit round.\n        client_manager (ClientManager): The manager used to grab all of the clients. Currently we restrict this to\n            be ``BaseFractionSamplingManager``, which has a \"sample all\" function built in.\n\n    Returns:\n        (list[tuple[ClientProxy, FitIns]]): List of sampled client identifiers and the configuration/parameters to\n            be sent to each client (packaged as ``FitIns``).\n    \"\"\"\n    # This strategy requires the client manager to be of type at least BaseFractionSamplingManager\n    assert isinstance(client_manager, BaseFractionSamplingManager)\n\n    config = {}\n    if self.on_fit_config_fn is not None:\n        # Custom fit config function provided\n        config = self.on_fit_config_fn(server_round)\n    else:\n        config = {\"current_server_round\": server_round}\n\n    fit_ins = FitIns(parameters, config)\n\n    clients = client_manager.sample_all(self.min_available_clients)\n\n    # Return client/config pairs\n    return [(client, fit_ins) for client in clients]\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.Scaffold.compute_updated_weights","title":"<code>compute_updated_weights(weights)</code>","text":"<p>Computes and update to the current <code>self.server_model_weights</code>. This assumes that the weights represents the raw weights aggregated from the client. Therefore it first needs to be turned into a \"delta\" with <code>weights - self.server_model_weights</code>.</p> <p>Then this is used to update with a learning rate scalar (set by <code>self.learning_rate</code>) as <code>self.server_model_weights + self.learning_rate * (weights - self.server_model_weights)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>NDArrays</code> <p>The updated weights (aggregated from the clients).</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p><code>self.server_model_weights + self.learning_rate * (weights - self.server_model_weights)</code> These are the updated server model weights.</p> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>def compute_updated_weights(self, weights: NDArrays) -&gt; NDArrays:\n    \"\"\"\n    Computes and update to the current ``self.server_model_weights``. This assumes that the weights represents the\n    raw weights aggregated from the client. Therefore it first needs to be turned into a \"delta\" with\n    ``weights - self.server_model_weights``.\n\n    Then this is used to update with a learning rate scalar (set by ``self.learning_rate``) as\n    ``self.server_model_weights + self.learning_rate * (weights - self.server_model_weights)``.\n\n    Args:\n        weights (NDArrays): The updated weights (aggregated from the clients).\n\n    Returns:\n        (NDArrays): ``self.server_model_weights + self.learning_rate * (weights - self.server_model_weights)``\n            These are the updated server model weights.\n    \"\"\"\n    # x_update = y_i - x\n    delta_weights = self.compute_parameter_delta(weights, self.server_model_weights)\n\n    # x = x + lr * x_update\n    return self.compute_updated_parameters(self.learning_rate, self.server_model_weights, delta_weights)\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.Scaffold.compute_updated_control_variates","title":"<code>compute_updated_control_variates(control_variates_update)</code>","text":"<p>Given the aggregated control variates from the clients, this updates the server control variates in line with the paper. If \\(c\\) is the server control variates and <code>c_update</code> is the client control variates, then this update takes the following form.</p> \\[c + \\frac{\\vert S \\vert}{N} \\cdot c_{\\text{update}},\\] <p>where \\(\\vert S\\vert\\) is the number of clients that participated and N is the total number of clients \\(\\frac{\\vert S \\vert}{N}\\) is the proportion given by fraction fit.</p> <p>Parameters:</p> Name Type Description Default <code>control_variates_update</code> <code>NDArrays</code> <p>Aggregated control variates received from the clients (uniformly averaged).</p> required <p>Returns:</p> Type Description <code>NDArrays</code> <p>Updated server control variates according to the formula.</p> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>def compute_updated_control_variates(self, control_variates_update: NDArrays) -&gt; NDArrays:\n    \"\"\"\n    Given the aggregated control variates from the clients, this updates the server control variates in line with\n    the paper. If \\\\(c\\\\) is the server control variates and ``c_update`` is the client control variates, then\n    this update takes the following form.\n\n    \\\\[c + \\\\frac{\\\\vert S \\\\vert}{N} \\\\cdot c_{\\\\text{update}},\\\\]\n\n    where \\\\(\\\\vert S\\\\vert\\\\) is the number of clients that participated and N is\n    the total number of clients \\\\(\\\\frac{\\\\vert S \\\\vert}{N}\\\\) is the proportion given by fraction fit.\n\n    Args:\n        control_variates_update (NDArrays): Aggregated control variates received from the clients (uniformly\n            averaged).\n\n    Returns:\n        (NDArrays): Updated server control variates according to the formula.\n    \"\"\"\n    # c = c + |S| / N * c_update\n    return self.compute_updated_parameters(\n        self.fraction_fit, self.server_control_variates, control_variates_update\n    )\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.OpacusScaffold","title":"<code>OpacusScaffold</code>","text":"<p>               Bases: <code>Scaffold</code></p> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>class OpacusScaffold(Scaffold):\n    def __init__(\n        self,\n        *,\n        model: GradSampleModule,\n        fraction_fit: float = 1.0,\n        fraction_evaluate: float = 1.0,\n        min_available_clients: int = 2,\n        evaluate_fn: (\n            Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n        ) = None,\n        on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n        accept_failures: bool = True,\n        fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n        weighted_eval_losses: bool = True,\n        learning_rate: float = 1.0,\n    ) -&gt; None:\n        \"\"\"\n        A simple extension of the Scaffold strategy to force the model being federally trained to be an valid Opacus\n        ``GradSamplingModule`` and, thereby, ensure that associated the parameters are aligned with those of Opacus\n        based models used by the ``InstanceLevelDpClient``.\n\n        **NOTE**: The ``initial_control_variates`` are all initialized to zero, as recommended in the SCAFFOLD paper.\n        If one wants a specific type of control variate initialization, this class will need to be overridden.\n\n        Args:\n            model (nn.Module): The model architecture to be federally trained. When using this strategy, the provided\n                model must be of type Opacus ``GradSampleModule``. This model will then be used to set\n                ``initialize_parameters`` as the initial parameters to be used by all clients AND the\n                ``initial_control_variates``.\n            fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n            fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n            min_available_clients (int, optional): Minimum number of total clients in the system.\n                Defaults to 2.\n            evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n                Optional function used for central server-side evaluation. Defaults to None.\n            on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                training by providing a configuration dictionary. Defaults to None.\n            on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n                client-side validation by providing a ``Config`` dictionary. Defaults to None.\n            accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n            fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n                Defaults to None.\n            weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n                averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n                counts. Defaults to True.\n            learning_rate (float, optional): Learning rate for server side optimization. Defaults to 1.0.\n        \"\"\"\n        assert isinstance(model, GradSampleModule), \"Provided model must be Opacus type GradSampleModule\"\n        # Setting the initial parameters to correspond with those of the provided model\n        initial_parameters = get_all_model_parameters(model)\n        # Initializing the control variates to be uniformly zero using the structure of the provided model.\n        initial_control_variates = ndarrays_to_parameters(\n            [np.zeros_like(val.data) for val in model.parameters() if val.requires_grad]\n        )\n\n        super().__init__(\n            fraction_fit=fraction_fit,\n            fraction_evaluate=fraction_evaluate,\n            min_available_clients=min_available_clients,\n            evaluate_fn=evaluate_fn,\n            on_fit_config_fn=on_fit_config_fn,\n            on_evaluate_config_fn=on_evaluate_config_fn,\n            accept_failures=accept_failures,\n            initial_parameters=initial_parameters,\n            fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n            evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n            weighted_eval_losses=weighted_eval_losses,\n            initial_control_variates=initial_control_variates,\n            model=None,\n            learning_rate=learning_rate,\n        )\n</code></pre>"},{"location":"api/#fl4health.strategies.scaffold.OpacusScaffold.__init__","title":"<code>__init__(*, model, fraction_fit=1.0, fraction_evaluate=1.0, min_available_clients=2, evaluate_fn=None, on_fit_config_fn=None, on_evaluate_config_fn=None, accept_failures=True, fit_metrics_aggregation_fn=None, evaluate_metrics_aggregation_fn=None, weighted_eval_losses=True, learning_rate=1.0)</code>","text":"<p>A simple extension of the Scaffold strategy to force the model being federally trained to be an valid Opacus <code>GradSamplingModule</code> and, thereby, ensure that associated the parameters are aligned with those of Opacus based models used by the <code>InstanceLevelDpClient</code>.</p> <p>NOTE: The <code>initial_control_variates</code> are all initialized to zero, as recommended in the SCAFFOLD paper. If one wants a specific type of control variate initialization, this class will need to be overridden.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model architecture to be federally trained. When using this strategy, the provided model must be of type Opacus <code>GradSampleModule</code>. This model will then be used to set <code>initialize_parameters</code> as the initial parameters to be used by all clients AND the <code>initial_control_variates</code>.</p> required <code>fraction_fit</code> <code>float</code> <p>Fraction of clients used during training. Defaults to 1.0.</p> <code>1.0</code> <code>fraction_evaluate</code> <code>float</code> <p>Fraction of clients used during validation. Defaults to 1.0.</p> <code>1.0</code> <code>min_available_clients</code> <code>int</code> <p>Minimum number of total clients in the system. Defaults to 2.</p> <code>2</code> <code>evaluate_fn</code> <code>Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None</code> <p>Optional function used for central server-side evaluation. Defaults to None.</p> <code>None</code> <code>on_fit_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure training by providing a configuration dictionary. Defaults to None.</p> <code>None</code> <code>on_evaluate_config_fn</code> <code>Callable[[int], dict[str, Scalar]] | None</code> <p>Function used to configure client-side validation by providing a <code>Config</code> dictionary. Defaults to None.</p> <code>None</code> <code>accept_failures</code> <code>bool</code> <p>Whether or not accept rounds containing failures. Defaults to True.</p> <code>True</code> <code>fit_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>evaluate_metrics_aggregation_fn</code> <code>MetricsAggregationFn | None</code> <p>Metrics aggregation function. Defaults to None.</p> <code>None</code> <code>weighted_eval_losses</code> <code>bool</code> <p>Determines whether losses during evaluation are linearly weighted averages or a uniform average. FedAvg default is weighted average of the losses by client dataset counts. Defaults to True.</p> <code>True</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for server side optimization. Defaults to 1.0.</p> <code>1.0</code> Source code in <code>fl4health/strategies/scaffold.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model: GradSampleModule,\n    fraction_fit: float = 1.0,\n    fraction_evaluate: float = 1.0,\n    min_available_clients: int = 2,\n    evaluate_fn: (\n        Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None\n    ) = None,\n    on_fit_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    on_evaluate_config_fn: Callable[[int], dict[str, Scalar]] | None = None,\n    accept_failures: bool = True,\n    fit_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    evaluate_metrics_aggregation_fn: MetricsAggregationFn | None = None,\n    weighted_eval_losses: bool = True,\n    learning_rate: float = 1.0,\n) -&gt; None:\n    \"\"\"\n    A simple extension of the Scaffold strategy to force the model being federally trained to be an valid Opacus\n    ``GradSamplingModule`` and, thereby, ensure that associated the parameters are aligned with those of Opacus\n    based models used by the ``InstanceLevelDpClient``.\n\n    **NOTE**: The ``initial_control_variates`` are all initialized to zero, as recommended in the SCAFFOLD paper.\n    If one wants a specific type of control variate initialization, this class will need to be overridden.\n\n    Args:\n        model (nn.Module): The model architecture to be federally trained. When using this strategy, the provided\n            model must be of type Opacus ``GradSampleModule``. This model will then be used to set\n            ``initialize_parameters`` as the initial parameters to be used by all clients AND the\n            ``initial_control_variates``.\n        fraction_fit (float, optional): Fraction of clients used during training. Defaults to 1.0.\n        fraction_evaluate (float, optional): Fraction of clients used during validation. Defaults to 1.0.\n        min_available_clients (int, optional): Minimum number of total clients in the system.\n            Defaults to 2.\n        evaluate_fn (Callable[[int, NDArrays, dict[str, Scalar]], tuple[float, dict[str, Scalar]] | None] | None):\n            Optional function used for central server-side evaluation. Defaults to None.\n        on_fit_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            training by providing a configuration dictionary. Defaults to None.\n        on_evaluate_config_fn (Callable[[int], dict[str, Scalar]] | None, optional): Function used to configure\n            client-side validation by providing a ``Config`` dictionary. Defaults to None.\n        accept_failures (bool, optional): Whether or not accept rounds containing failures. Defaults to True.\n        fit_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        evaluate_metrics_aggregation_fn (MetricsAggregationFn | None, optional): Metrics aggregation function.\n            Defaults to None.\n        weighted_eval_losses (bool, optional): Determines whether losses during evaluation are linearly weighted\n            averages or a uniform average. FedAvg default is weighted average of the losses by client dataset\n            counts. Defaults to True.\n        learning_rate (float, optional): Learning rate for server side optimization. Defaults to 1.0.\n    \"\"\"\n    assert isinstance(model, GradSampleModule), \"Provided model must be Opacus type GradSampleModule\"\n    # Setting the initial parameters to correspond with those of the provided model\n    initial_parameters = get_all_model_parameters(model)\n    # Initializing the control variates to be uniformly zero using the structure of the provided model.\n    initial_control_variates = ndarrays_to_parameters(\n        [np.zeros_like(val.data) for val in model.parameters() if val.requires_grad]\n    )\n\n    super().__init__(\n        fraction_fit=fraction_fit,\n        fraction_evaluate=fraction_evaluate,\n        min_available_clients=min_available_clients,\n        evaluate_fn=evaluate_fn,\n        on_fit_config_fn=on_fit_config_fn,\n        on_evaluate_config_fn=on_evaluate_config_fn,\n        accept_failures=accept_failures,\n        initial_parameters=initial_parameters,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        weighted_eval_losses=weighted_eval_losses,\n        initial_control_variates=initial_control_variates,\n        model=None,\n        learning_rate=learning_rate,\n    )\n</code></pre>"},{"location":"api/#fl4health.strategies.strategy_with_poll","title":"<code>strategy_with_poll</code>","text":""},{"location":"api/#fl4health.strategies.strategy_with_poll.StrategyWithPolling","title":"<code>StrategyWithPolling</code>","text":"<p>               Bases: <code>ABC</code></p> <p>This abstract base class is used to ensure that an FL strategy class implements configure polling when it should and that any server that wants to do polling can use this function when it's expected to.</p> Source code in <code>fl4health/strategies/strategy_with_poll.py</code> <pre><code>class StrategyWithPolling(ABC):\n    \"\"\"\n    This abstract base class is used to ensure that an FL strategy class implements configure polling when it should\n    and that any server that wants to do polling can use this function when it's expected to.\n    \"\"\"\n\n    @abstractmethod\n    def configure_poll(\n        self, server_round: int, client_manager: ClientManager\n    ) -&gt; list[tuple[ClientProxy, GetPropertiesIns]]:\n        pass\n</code></pre>"},{"location":"api/#fl4health.utils","title":"<code>utils</code>","text":""},{"location":"api/#fl4health.utils.client","title":"<code>client</code>","text":""},{"location":"api/#fl4health.utils.client.move_data_to_device","title":"<code>move_data_to_device(data, device)</code>","text":"<p>Moves data to the target device.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>T</code> <p>The data to move to self.device. Can be a <code>TorchInputType</code> or a <code>TorchTargetType</code></p> required <code>device</code> <code>device</code> <p>Device indicator for where to send the model, batches, labels etc. Often 'cpu' or 'cuda'</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>Raised if data is not one of the types specified by <code>TorchInputType</code> or <code>TorchTargetType</code></p> <p>Returns:</p> Type Description <code>T</code> <p>The data argument except now it's been moved to <code>self.device</code></p> Source code in <code>fl4health/utils/client.py</code> <pre><code>def move_data_to_device(data: T, device: torch.device) -&gt; T:\n    \"\"\"\n    Moves data to the target device.\n\n    Args:\n        data (T): The data to move to self.device. Can be a ``TorchInputType`` or a ``TorchTargetType``\n        device (torch.device): Device indicator for where to send the model, batches, labels etc. Often 'cpu' or\n            'cuda'\n\n    Raises:\n        TypeError: Raised if data is not one of the types specified by ``TorchInputType`` or ``TorchTargetType``\n\n    Returns:\n        (T): The data argument except now it's been moved to ``self.device``\n    \"\"\"\n    # Currently we expect both inputs and targets to be either tensors\n    # or dictionaries of tensors\n    if isinstance(data, torch.Tensor):\n        return data.to(device)\n    if isinstance(data, dict):\n        return {key: value.to(device) for key, value in data.items()}\n    raise TypeError(\n        \"data must be of type torch.Tensor or dict[str, torch.Tensor]. If definition of TorchInputType or \"\n        \"TorchTargetType has changed this method might need to be updated or split into two.\"\n    )\n</code></pre>"},{"location":"api/#fl4health.utils.client.check_if_batch_is_empty_and_verify_input","title":"<code>check_if_batch_is_empty_and_verify_input(input)</code>","text":"<p>This function checks whether the provided batch (input) is empty. If the input is a dictionary of inputs, it first verifies that the length of all inputs is the same, then checks if they are non-empty. NOTE: This function assumes the input is BATCH FIRST.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>TorchInputType</code> <p>Input batch. Input can be of type <code>torch.Tensor</code> or <code>dict[str, torch.Tensor]</code>, and in the latter case, the batch is considered to be empty if all tensors in the dictionary have length zero.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>Raised if input is not of type <code>torch.Tensor</code> or <code>dict[str, torch.Tensor]</code>.</p> <code>ValueError</code> <p>Raised if input has type <code>dict[str, torch.Tensor]</code> and not all tensors within the dictionary have the same size.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if input is an empty batch.</p> Source code in <code>fl4health/utils/client.py</code> <pre><code>def check_if_batch_is_empty_and_verify_input(input: TorchInputType) -&gt; bool:\n    \"\"\"\n    This function checks whether the provided batch (input) is empty. If the input is a dictionary of inputs, it\n    first verifies that the length of all inputs is the same, then checks if they are non-empty.\n    **NOTE**: This function assumes the input is **BATCH FIRST**.\n\n    Args:\n        input (TorchInputType): Input batch. Input can be of type ``torch.Tensor`` or ``dict[str, torch.Tensor]``,\n            and in the latter case, the batch is considered to be empty if all tensors in the dictionary have length\n            zero.\n\n    Raises:\n        TypeError: Raised if input is not of type ``torch.Tensor`` or ``dict[str, torch.Tensor]``.\n        ValueError: Raised if input has type ``dict[str, torch.Tensor]`` and not all tensors within the dictionary have\n            the same size.\n\n    Returns:\n        (bool): True if input is an empty batch.\n    \"\"\"\n    if isinstance(input, torch.Tensor):\n        return len(input) == 0\n    if isinstance(input, dict):\n        input_iter = iter(input.items())\n        _, first_val = next(input_iter)\n        first_val_len = len(first_val)\n        if not all(len(val) == first_val_len for _, val in input_iter):\n            raise ValueError(\"Not all tensors in the dictionary have the same size.\")\n        return first_val_len == 0\n    raise TypeError(\"Input must be of type torch.Tensor or dict[str, torch.Tensor].\")\n</code></pre>"},{"location":"api/#fl4health.utils.client.clone_and_freeze_model","title":"<code>clone_and_freeze_model(model)</code>","text":"<p>Creates a clone of the model with frozen weights to be used in loss calculations so the original model is preserved in its current state.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Model to clone and freeze.</p> required <p>Returns:</p> Type Description <code>Module</code> <p>Cloned and frozen model.</p> Source code in <code>fl4health/utils/client.py</code> <pre><code>def clone_and_freeze_model(model: nn.Module) -&gt; nn.Module:\n    \"\"\"\n    Creates a clone of the model with frozen weights to be used in loss calculations so the original model is\n    preserved in its current state.\n\n    Args:\n        model (nn.Module): Model to clone and freeze.\n\n    Returns:\n        (nn.Module): Cloned and frozen model.\n    \"\"\"\n    cloned_model = copy.deepcopy(model)\n    for param in cloned_model.parameters():\n        param.requires_grad = False\n    cloned_model.eval()\n\n    return cloned_model\n</code></pre>"},{"location":"api/#fl4health.utils.client.maybe_progress_bar","title":"<code>maybe_progress_bar(iterable, display_progress_bar)</code>","text":"<p>Used to print progress bars during client training and validation. If <code>self.progress_bar</code> is false, just returns the original input iterable without modifying it.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable</code> <p>The iterable to wrap</p> required <code>display_progress_bar</code> <code>bool</code> <p>Whether we want to display a progress bar or not.</p> required <p>Returns:</p> Type Description <code>Iterable</code> <p>An iterator which acts exactly like the original iterable, but prints a dynamically updating</p> <code>Iterable</code> <p>progress bar every time a value is requested. Or the original iterable if <code>self.progress_bar</code> is False</p> Source code in <code>fl4health/utils/client.py</code> <pre><code>def maybe_progress_bar(iterable: Iterable, display_progress_bar: bool) -&gt; Iterable:\n    \"\"\"\n    Used to print progress bars during client training and validation. If ``self.progress_bar`` is false, just returns\n    the original input iterable without modifying it.\n\n    Args:\n        iterable (Iterable): The iterable to wrap\n        display_progress_bar (bool): Whether we want to display a progress bar or not.\n\n    Returns:\n        (Iterable): An iterator which acts exactly like the original iterable, but prints a dynamically updating\n        progress bar every time a value is requested. Or the original iterable if ``self.progress_bar`` is False\n    \"\"\"\n    if not display_progress_bar:\n        return iterable\n    # We can use the flwr console handler to format progress bar\n    frame = currentframe()\n    lineno = 0 if frame is None else getframeinfo(frame).lineno\n    record = LogRecord(\n        name=LOGGER_NAME,\n        pathname=os.path.abspath(os.getcwd()),\n        lineno=lineno,\n        args={},\n        exc_info=None,\n        level=INFO,\n        msg=\"{l_bar}{bar}{r_bar}\",\n    )\n    format = console_handler.format(record)\n    # Create a clean looking tqdm instance that matches the flwr logging\n    kwargs: Any = {\n        \"leave\": True,\n        \"ascii\": \" &gt;=\",\n        \"unit\": \"steps\",\n        \"dynamic_ncols\": True,\n        \"bar_format\": format,\n    }\n    return tqdm(iterable, **kwargs)\n</code></pre>"},{"location":"api/#fl4health.utils.config","title":"<code>config</code>","text":""},{"location":"api/#fl4health.utils.config.load_config","title":"<code>load_config(config_path)</code>","text":"<p>Load Configuration Dictionary.</p> Source code in <code>fl4health/utils/config.py</code> <pre><code>def load_config(config_path: str) -&gt; dict[str, Any]:\n    \"\"\"Load Configuration Dictionary.\"\"\"\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    check_config(config)\n\n    return config\n</code></pre>"},{"location":"api/#fl4health.utils.config.check_config","title":"<code>check_config(config)</code>","text":"<p>Check if Configuration Dictionary is valid.</p> Source code in <code>fl4health/utils/config.py</code> <pre><code>def check_config(config: dict[str, Any]) -&gt; None:\n    \"\"\"Check if Configuration Dictionary is valid.\"\"\"\n    # Check for presence of required keys\n    for req_key in REQUIRED_CONFIG:\n        if req_key not in config:\n            raise InvalidConfigError(f\"{req_key} must be specified in Config File\")\n\n    # Check for invalid parameter value types\n    for req_key, val in REQUIRED_CONFIG.items():\n        if not isinstance(config[req_key], val):\n            raise InvalidConfigError(f\"{req_key} must be of type {str(val)}\")\n\n    # Check for invalid integer parameter values\n    for key in [\"n_server_rounds\", \"batch_size\"]:\n        if config[key] &lt;= 0:\n            raise InvalidConfigError(f\"{key} must be greater than 0\")\n</code></pre>"},{"location":"api/#fl4health.utils.config.narrow_dict_type","title":"<code>narrow_dict_type(dictionary, key, narrow_type_to)</code>","text":"<p>Checks if a key exists in dictionary and if so, verify it is of type <code>narrow_type_to</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict[str, Any]</code> <p>A dictionary with string keys.</p> required <code>key</code> <code>str</code> <p>The key to check dictionary for.</p> required <code>narrow_type_to</code> <code>type[T]</code> <p>The expected type of dictionary[key]</p> required <p>Returns:</p> Type Description <code>T</code> <p>The type-checked value at dictionary[key].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dictionary[key] is not of type <code>narrow_type_to</code> or if the key is not present in dictionary.</p> Source code in <code>fl4health/utils/config.py</code> <pre><code>def narrow_dict_type(dictionary: dict[str, Any], key: str, narrow_type_to: type[T]) -&gt; T:\n    \"\"\"\n    Checks if a key exists in dictionary and if so, verify it is of type ``narrow_type_to``.\n\n    Args:\n        dictionary (dict[str, Any]): A dictionary with string keys.\n        key (str): The key to check dictionary for.\n        narrow_type_to (type[T]): The expected type of dictionary[key]\n\n    Returns:\n        (T): The type-checked value at dictionary[key].\n\n    Raises:\n        ValueError: If dictionary[key] is not of type ``narrow_type_to`` or if the key is not present in dictionary.\n    \"\"\"\n    if key not in dictionary:\n        raise ValueError(f\"{key} is not present in the Dictionary.\")\n\n    value = dictionary[key]\n    if isinstance(value, narrow_type_to):\n        return value\n    raise ValueError(f\"Provided key ({key}) value does not have correct type\")\n</code></pre>"},{"location":"api/#fl4health.utils.config.narrow_dict_type_and_set_attribute","title":"<code>narrow_dict_type_and_set_attribute(self, dictionary, dictionary_key, attribute_name, narrow_type_to, func=None)</code>","text":"<p>Checks a key exists in dictionary, verify its type and sets the corresponding attribute. Optionally, passes narrowed value to function prior to setting attribute. If key is not present in dictionary or dictionary[dictionary_key] has the wrong type, a <code>ValueError</code> is thrown.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>object</code> <p>The object to set attribute to dictionary[dictionary_key].</p> required <code>dictionary</code> <code>dict</code> <p>A dictionary with string keys.</p> required <code>dictionary_key</code> <code>str</code> <p>A dictionary with string keys.</p> required <code>attribute_name</code> <code>str</code> <p>The key for which to check in dictionary.</p> required <code>narrow_type_to</code> <code>type[T]</code> <p>The expected type of dictionary[key].</p> required <code>func</code> <code>Callable[[Any], Any] | None</code> <p>Function to operate on the extracted value if desired. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/utils/config.py</code> <pre><code>def narrow_dict_type_and_set_attribute(\n    self: object,\n    dictionary: dict,\n    dictionary_key: str,\n    attribute_name: str,\n    narrow_type_to: type[T],\n    func: Callable[[Any], Any] | None = None,\n) -&gt; None:\n    \"\"\"\n    Checks a key exists in dictionary, verify its type and sets the corresponding attribute. Optionally, passes\n    narrowed value to function prior to setting attribute. If key is not present in dictionary or\n    dictionary[dictionary_key] has the wrong type, a ``ValueError`` is thrown.\n\n    Args:\n        self (object): The object to set attribute to dictionary[dictionary_key].\n        dictionary (dict): A dictionary with string keys.\n        dictionary_key (str): A dictionary with string keys.\n        attribute_name (str): The key for which to check in dictionary.\n        narrow_type_to (type[T]): The expected type of dictionary[key].\n        func (Callable[[Any], Any] | None, optional): Function to operate on the extracted value if desired. Defaults\n            to None.\n    \"\"\"\n    val = narrow_dict_type(dictionary, dictionary_key, narrow_type_to)\n    val = func(val) if func is not None else val\n    setattr(self, attribute_name, val)\n</code></pre>"},{"location":"api/#fl4health.utils.config.make_dict_with_epochs_or_steps","title":"<code>make_dict_with_epochs_or_steps(local_epochs=None, local_steps=None)</code>","text":"<p>Given two optional variables, this function will determine which, if any, are not None and create a dictionary from the value. If both are not None, it will prioritize <code>local_epochs</code>. If both are None, then the dictionary is empty.</p> <p>Parameters:</p> Name Type Description Default <code>local_epochs</code> <code>int | None</code> <p>Number of local epochs of training to perform in FL. Defaults to None.</p> <code>None</code> <code>local_steps</code> <code>int | None</code> <p>Number of local steps of training to perform in FL. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>Dictionary with at most one of the non-none values, keyed by the name of the non-none variable.</p> Source code in <code>fl4health/utils/config.py</code> <pre><code>def make_dict_with_epochs_or_steps(local_epochs: int | None = None, local_steps: int | None = None) -&gt; dict[str, int]:\n    \"\"\"\n    Given two optional variables, this function will determine which, if any, are not None and create a dictionary\n    from the value. If both are not None, it will prioritize ``local_epochs``. If both are None, then the dictionary\n    is empty.\n\n    Args:\n        local_epochs (int | None, optional): Number of local epochs of training to perform in FL. Defaults to None.\n        local_steps (int | None, optional): Number of local steps of training to perform in FL. Defaults to None.\n\n    Returns:\n        (dict[str, int]): Dictionary with at most one of the non-none values, keyed by the name of the non-none\n            variable.\n    \"\"\"\n    if local_epochs is not None:\n        return {\"local_epochs\": local_epochs}\n    if local_steps is not None:\n        return {\"local_steps\": local_steps}\n    return {}\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation","title":"<code>data_generation</code>","text":""},{"location":"api/#fl4health.utils.data_generation.SyntheticFedProxDataset","title":"<code>SyntheticFedProxDataset</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>class SyntheticFedProxDataset(ABC):\n    def __init__(\n        self,\n        num_clients: int,\n        temperature: float = 1.0,\n        input_dim: int = 60,\n        output_dim: int = 10,\n        hidden_dim: int | None = None,\n        samples_per_client: int = 1000,\n    ) -&gt; None:\n        \"\"\"\n        Abstract base class to support synthetic dataset generation in the style of the original FedProx paper.\n\n        Paper link: https://arxiv.org/abs/1812.06127\n\n        Reference code: https://github.com/litian96/FedProx/tree/master/data/synthetic_1_1\n\n        **NOTE**: In the implementations here, all clients receive the same number of samples. In the original FedProx\n        setup, they are sampled using a power law.\n\n        Args:\n            num_clients (int): Number of datasets (one per client) to generate.\n            temperature (float, optional): Temperature used for the softmax mapping to labels. Defaults to 1.0.\n            input_dim (int, optional): Dimension of the input features for the synthetic dataset. Default is as in the\n                FedProx paper. Defaults to 60.\n            output_dim (int, optional): Dimension of the output labels for the synthetic dataset. These are one-hot\n                encoding labels. Default is as in the FedProx paper. Defaults to 10.\n            hidden_dim (int | None, optional): Dimension of the hidden layer for the two-layer mapping. If None, a\n                one-layer mapping is used. Defaults to None.\n            samples_per_client (int, optional): Number of samples to generate in each client's dataset.\n                Defaults to 1000.\n        \"\"\"\n        self.num_clients = num_clients\n        self.temperature = temperature\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.samples_per_client = samples_per_client\n\n        # Sigma in the FedProx paper\n        self.input_covariance = self.construct_covariance_matrix()\n\n    def construct_covariance_matrix(self) -&gt; torch.Tensor:\n        r\"\"\"\n        This function generations the covariance matrix used in generating input features. It is fixed across all\n        datasets. It is a diagonal matrix with diagonal entries \\(x_{j, j} = j^{-1.2}\\), where \\(j\\) starts at\n        1 in this notation. The matrix is of dimension ``input_dim`` x ``input_dim``.\n\n        Returns:\n            (torch.Tensor): Covariance matrix for generation of input features.\n        \"\"\"\n        sigma_diagonal = torch.zeros(self.input_dim)\n        for i in range(self.input_dim):\n            # indexing in the original implementation starts at 1, so i+1\n            sigma_diagonal[i] = math.pow((i + 1), -1.2)\n        return torch.diag(sigma_diagonal)\n\n    def one_layer_map_inputs_to_outputs(self, x: torch.Tensor, w: torch.Tensor, b: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        This function maps features x to a label y as done in the original paper. The first stage is the affine\n        transformation.\n\n        \\\\[\\\\hat{y} = \\\\frac{1}{T} \\\\cdot (Wx + b).\\\\]\n\n        Then \\\\(y = \\\\text{softmax}(\\\\hat{y})\\\\). Getting the argmax from the distribution, we then\n        one hot encode the resulting label sample.\n\n        Args:\n            x (torch.Tensor): The input features to be mapped to output labels. Shape is (dataset size, ``input_dim``)\n            w (torch.Tensor): The linear transformation matrix. Shape is (``output_dim``, ``input_dim``)\n            b (torch.Tensor): The bias in the linear transformation. Shape is (``output_dim``, 1)\n\n        Returns:\n            (torch.Tensor): The labels associated with each of the inputs. The shape is (dataset size, ``output_dim``)\n        \"\"\"\n        raw_y = (torch.matmul(x, w.T) + b.T.repeat(self.samples_per_client, 1)) / self.temperature\n        distributions = F.softmax(raw_y, dim=1)\n        samples = torch.argmax(distributions, 1)\n        return F.one_hot(samples, num_classes=self.output_dim).squeeze()\n\n    def two_layer_map_inputs_to_outputs(\n        self, x: torch.Tensor, w_1: torch.Tensor, b_1: torch.Tensor, w_2: torch.Tensor, b_2: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        This function maps features x to a label y in an alternative way to include two layers. The first stage is two\n        affine transformations.\n\n        \\\\begin{align} &amp; \\\\text{latent} = \\\\frac{1}{T} \\\\cdot (W_1 \\\\cdot x + b_1) \\\\\\\\\n        &amp; \\\\hat{y} = (W_2 \\\\cdot \\\\text{latent} + b_2). \\\\end{align}\n\n        Then \\\\(y = \\\\text{softmax}(\\\\hat{y})\\\\). Getting the argmax from the distribution, we then\n        one hot encode the resulting label sample.\n\n        Args:\n            x (torch.Tensor): The input features to be mapped to output labels. Shape is (dataset size, ``input_dim``).\n            w_1 (torch.Tensor): The first linear transformation matrix. Shape is (``hidden_dim``, ``input_dim``).\n            b_1 (torch.Tensor): The bias in the first linear transformation. Shape is (``hidden_dim``, 1).\n            w_2 (torch.Tensor): The second linear transformation matrix. Shape is (``output_dim``, ``hidden_dim``).\n            b_2 (torch.Tensor): The bias in the second linear transformation. Shape is (``output_dim``, 1).\n\n        Returns:\n            (torch.Tensor): The labels associated with each of the inputs. The shape is (dataset size, ``output_dim``).\n        \"\"\"\n        latent = (torch.matmul(x, w_1.T) + b_1.T.repeat(self.samples_per_client, 1)) / self.temperature\n        raw_y = torch.matmul(latent, w_2.T) + b_2.T.repeat(self.samples_per_client, 1)\n        out_distributions = F.softmax(raw_y, dim=1)\n        samples = torch.argmax(out_distributions, 1)\n        return F.one_hot(samples, num_classes=self.output_dim).squeeze()\n\n    def generate(self) -&gt; list[TensorDataset]:\n        \"\"\"\n        Based on the class parameters, generate a list of synthetic ``TensorDatasets``, one for each client.\n\n        Returns:\n            (list[TensorDataset]): Synthetic datasets for each client.\n        \"\"\"\n        client_tensors = self.generate_client_tensors()\n        assert len(client_tensors) == self.num_clients, (\n            \"The tensors returned by generate_client_tensors should have the same length as self.num_clients\"\n        )\n        return [TensorDataset(x, y) for x, y in client_tensors]\n\n    @abstractmethod\n    def generate_client_tensors(self) -&gt; list[tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"\n        Method to be implemented determining how to generate the tensors in the subclasses. Each of the subclasses\n        uses the affine mapping, but the parameters for how that affine mapping is setup are different and determined\n        in this function.\n\n        Returns:\n            (list[tuple[torch.Tensor, torch.Tensor]]): Input and output tensors for each of the clients.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticFedProxDataset.__init__","title":"<code>__init__(num_clients, temperature=1.0, input_dim=60, output_dim=10, hidden_dim=None, samples_per_client=1000)</code>","text":"<p>Abstract base class to support synthetic dataset generation in the style of the original FedProx paper.</p> <p>Paper link: https://arxiv.org/abs/1812.06127</p> <p>Reference code: https://github.com/litian96/FedProx/tree/master/data/synthetic_1_1</p> <p>NOTE: In the implementations here, all clients receive the same number of samples. In the original FedProx setup, they are sampled using a power law.</p> <p>Parameters:</p> Name Type Description Default <code>num_clients</code> <code>int</code> <p>Number of datasets (one per client) to generate.</p> required <code>temperature</code> <code>float</code> <p>Temperature used for the softmax mapping to labels. Defaults to 1.0.</p> <code>1.0</code> <code>input_dim</code> <code>int</code> <p>Dimension of the input features for the synthetic dataset. Default is as in the FedProx paper. Defaults to 60.</p> <code>60</code> <code>output_dim</code> <code>int</code> <p>Dimension of the output labels for the synthetic dataset. These are one-hot encoding labels. Default is as in the FedProx paper. Defaults to 10.</p> <code>10</code> <code>hidden_dim</code> <code>int | None</code> <p>Dimension of the hidden layer for the two-layer mapping. If None, a one-layer mapping is used. Defaults to None.</p> <code>None</code> <code>samples_per_client</code> <code>int</code> <p>Number of samples to generate in each client's dataset. Defaults to 1000.</p> <code>1000</code> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def __init__(\n    self,\n    num_clients: int,\n    temperature: float = 1.0,\n    input_dim: int = 60,\n    output_dim: int = 10,\n    hidden_dim: int | None = None,\n    samples_per_client: int = 1000,\n) -&gt; None:\n    \"\"\"\n    Abstract base class to support synthetic dataset generation in the style of the original FedProx paper.\n\n    Paper link: https://arxiv.org/abs/1812.06127\n\n    Reference code: https://github.com/litian96/FedProx/tree/master/data/synthetic_1_1\n\n    **NOTE**: In the implementations here, all clients receive the same number of samples. In the original FedProx\n    setup, they are sampled using a power law.\n\n    Args:\n        num_clients (int): Number of datasets (one per client) to generate.\n        temperature (float, optional): Temperature used for the softmax mapping to labels. Defaults to 1.0.\n        input_dim (int, optional): Dimension of the input features for the synthetic dataset. Default is as in the\n            FedProx paper. Defaults to 60.\n        output_dim (int, optional): Dimension of the output labels for the synthetic dataset. These are one-hot\n            encoding labels. Default is as in the FedProx paper. Defaults to 10.\n        hidden_dim (int | None, optional): Dimension of the hidden layer for the two-layer mapping. If None, a\n            one-layer mapping is used. Defaults to None.\n        samples_per_client (int, optional): Number of samples to generate in each client's dataset.\n            Defaults to 1000.\n    \"\"\"\n    self.num_clients = num_clients\n    self.temperature = temperature\n    self.input_dim = input_dim\n    self.output_dim = output_dim\n    self.hidden_dim = hidden_dim\n    self.samples_per_client = samples_per_client\n\n    # Sigma in the FedProx paper\n    self.input_covariance = self.construct_covariance_matrix()\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticFedProxDataset.construct_covariance_matrix","title":"<code>construct_covariance_matrix()</code>","text":"<p>This function generations the covariance matrix used in generating input features. It is fixed across all datasets. It is a diagonal matrix with diagonal entries \\(x_{j, j} = j^{-1.2}\\), where \\(j\\) starts at 1 in this notation. The matrix is of dimension <code>input_dim</code> x <code>input_dim</code>.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>Covariance matrix for generation of input features.</p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def construct_covariance_matrix(self) -&gt; torch.Tensor:\n    r\"\"\"\n    This function generations the covariance matrix used in generating input features. It is fixed across all\n    datasets. It is a diagonal matrix with diagonal entries \\(x_{j, j} = j^{-1.2}\\), where \\(j\\) starts at\n    1 in this notation. The matrix is of dimension ``input_dim`` x ``input_dim``.\n\n    Returns:\n        (torch.Tensor): Covariance matrix for generation of input features.\n    \"\"\"\n    sigma_diagonal = torch.zeros(self.input_dim)\n    for i in range(self.input_dim):\n        # indexing in the original implementation starts at 1, so i+1\n        sigma_diagonal[i] = math.pow((i + 1), -1.2)\n    return torch.diag(sigma_diagonal)\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticFedProxDataset.one_layer_map_inputs_to_outputs","title":"<code>one_layer_map_inputs_to_outputs(x, w, b)</code>","text":"<p>This function maps features x to a label y as done in the original paper. The first stage is the affine transformation.</p> \\[\\hat{y} = \\frac{1}{T} \\cdot (Wx + b).\\] <p>Then \\(y = \\text{softmax}(\\hat{y})\\). Getting the argmax from the distribution, we then one hot encode the resulting label sample.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input features to be mapped to output labels. Shape is (dataset size, <code>input_dim</code>)</p> required <code>w</code> <code>Tensor</code> <p>The linear transformation matrix. Shape is (<code>output_dim</code>, <code>input_dim</code>)</p> required <code>b</code> <code>Tensor</code> <p>The bias in the linear transformation. Shape is (<code>output_dim</code>, 1)</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The labels associated with each of the inputs. The shape is (dataset size, <code>output_dim</code>)</p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def one_layer_map_inputs_to_outputs(self, x: torch.Tensor, w: torch.Tensor, b: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    This function maps features x to a label y as done in the original paper. The first stage is the affine\n    transformation.\n\n    \\\\[\\\\hat{y} = \\\\frac{1}{T} \\\\cdot (Wx + b).\\\\]\n\n    Then \\\\(y = \\\\text{softmax}(\\\\hat{y})\\\\). Getting the argmax from the distribution, we then\n    one hot encode the resulting label sample.\n\n    Args:\n        x (torch.Tensor): The input features to be mapped to output labels. Shape is (dataset size, ``input_dim``)\n        w (torch.Tensor): The linear transformation matrix. Shape is (``output_dim``, ``input_dim``)\n        b (torch.Tensor): The bias in the linear transformation. Shape is (``output_dim``, 1)\n\n    Returns:\n        (torch.Tensor): The labels associated with each of the inputs. The shape is (dataset size, ``output_dim``)\n    \"\"\"\n    raw_y = (torch.matmul(x, w.T) + b.T.repeat(self.samples_per_client, 1)) / self.temperature\n    distributions = F.softmax(raw_y, dim=1)\n    samples = torch.argmax(distributions, 1)\n    return F.one_hot(samples, num_classes=self.output_dim).squeeze()\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticFedProxDataset.two_layer_map_inputs_to_outputs","title":"<code>two_layer_map_inputs_to_outputs(x, w_1, b_1, w_2, b_2)</code>","text":"<p>This function maps features x to a label y in an alternative way to include two layers. The first stage is two affine transformations.</p> \\[\\begin{align} &amp; \\text{latent} = \\frac{1}{T} \\cdot (W_1 \\cdot x + b_1) \\\\ &amp; \\hat{y} = (W_2 \\cdot \\text{latent} + b_2). \\end{align}\\] <p>Then \\(y = \\text{softmax}(\\hat{y})\\). Getting the argmax from the distribution, we then one hot encode the resulting label sample.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input features to be mapped to output labels. Shape is (dataset size, <code>input_dim</code>).</p> required <code>w_1</code> <code>Tensor</code> <p>The first linear transformation matrix. Shape is (<code>hidden_dim</code>, <code>input_dim</code>).</p> required <code>b_1</code> <code>Tensor</code> <p>The bias in the first linear transformation. Shape is (<code>hidden_dim</code>, 1).</p> required <code>w_2</code> <code>Tensor</code> <p>The second linear transformation matrix. Shape is (<code>output_dim</code>, <code>hidden_dim</code>).</p> required <code>b_2</code> <code>Tensor</code> <p>The bias in the second linear transformation. Shape is (<code>output_dim</code>, 1).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The labels associated with each of the inputs. The shape is (dataset size, <code>output_dim</code>).</p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def two_layer_map_inputs_to_outputs(\n    self, x: torch.Tensor, w_1: torch.Tensor, b_1: torch.Tensor, w_2: torch.Tensor, b_2: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    This function maps features x to a label y in an alternative way to include two layers. The first stage is two\n    affine transformations.\n\n    \\\\begin{align} &amp; \\\\text{latent} = \\\\frac{1}{T} \\\\cdot (W_1 \\\\cdot x + b_1) \\\\\\\\\n    &amp; \\\\hat{y} = (W_2 \\\\cdot \\\\text{latent} + b_2). \\\\end{align}\n\n    Then \\\\(y = \\\\text{softmax}(\\\\hat{y})\\\\). Getting the argmax from the distribution, we then\n    one hot encode the resulting label sample.\n\n    Args:\n        x (torch.Tensor): The input features to be mapped to output labels. Shape is (dataset size, ``input_dim``).\n        w_1 (torch.Tensor): The first linear transformation matrix. Shape is (``hidden_dim``, ``input_dim``).\n        b_1 (torch.Tensor): The bias in the first linear transformation. Shape is (``hidden_dim``, 1).\n        w_2 (torch.Tensor): The second linear transformation matrix. Shape is (``output_dim``, ``hidden_dim``).\n        b_2 (torch.Tensor): The bias in the second linear transformation. Shape is (``output_dim``, 1).\n\n    Returns:\n        (torch.Tensor): The labels associated with each of the inputs. The shape is (dataset size, ``output_dim``).\n    \"\"\"\n    latent = (torch.matmul(x, w_1.T) + b_1.T.repeat(self.samples_per_client, 1)) / self.temperature\n    raw_y = torch.matmul(latent, w_2.T) + b_2.T.repeat(self.samples_per_client, 1)\n    out_distributions = F.softmax(raw_y, dim=1)\n    samples = torch.argmax(out_distributions, 1)\n    return F.one_hot(samples, num_classes=self.output_dim).squeeze()\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticFedProxDataset.generate","title":"<code>generate()</code>","text":"<p>Based on the class parameters, generate a list of synthetic <code>TensorDatasets</code>, one for each client.</p> <p>Returns:</p> Type Description <code>list[TensorDataset]</code> <p>Synthetic datasets for each client.</p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def generate(self) -&gt; list[TensorDataset]:\n    \"\"\"\n    Based on the class parameters, generate a list of synthetic ``TensorDatasets``, one for each client.\n\n    Returns:\n        (list[TensorDataset]): Synthetic datasets for each client.\n    \"\"\"\n    client_tensors = self.generate_client_tensors()\n    assert len(client_tensors) == self.num_clients, (\n        \"The tensors returned by generate_client_tensors should have the same length as self.num_clients\"\n    )\n    return [TensorDataset(x, y) for x, y in client_tensors]\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticFedProxDataset.generate_client_tensors","title":"<code>generate_client_tensors()</code>  <code>abstractmethod</code>","text":"<p>Method to be implemented determining how to generate the tensors in the subclasses. Each of the subclasses uses the affine mapping, but the parameters for how that affine mapping is setup are different and determined in this function.</p> <p>Returns:</p> Type Description <code>list[tuple[Tensor, Tensor]]</code> <p>Input and output tensors for each of the clients.</p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>@abstractmethod\ndef generate_client_tensors(self) -&gt; list[tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    Method to be implemented determining how to generate the tensors in the subclasses. Each of the subclasses\n    uses the affine mapping, but the parameters for how that affine mapping is setup are different and determined\n    in this function.\n\n    Returns:\n        (list[tuple[torch.Tensor, torch.Tensor]]): Input and output tensors for each of the clients.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticNonIidFedProxDataset","title":"<code>SyntheticNonIidFedProxDataset</code>","text":"<p>               Bases: <code>SyntheticFedProxDataset</code></p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>class SyntheticNonIidFedProxDataset(SyntheticFedProxDataset):\n    def __init__(\n        self,\n        num_clients: int,\n        alpha: float,\n        beta: float,\n        temperature: float = 1.0,\n        input_dim: int = 60,\n        output_dim: int = 10,\n        hidden_dim: int | None = None,\n        samples_per_client: int = 1000,\n    ) -&gt; None:\n        \"\"\"\n        NON-IID Synthetic dataset generator modeled after the implementation in the original FedProx paper. See Section\n        5.1 in the paper link below for additional details. The non-IID generation code is modeled after the code\n        housed in the github link below as well.\n\n        Paper link: https://arxiv.org/abs/1812.06127\n\n        Reference code: https://github.com/litian96/FedProx/tree/master/data/synthetic_1_1\n\n        **NOTE**: This generator ends up with fairly skewed labels in generation. That is, many of the clients will not\n        have representations of all the labels. This has been verified as also occurring in the reference code above\n        and is not a bug.\n\n        The larger alpha and beta are, the more heterogeneous the clients data is. The larger alpha is, the more\n        \"different\" the affine transformations are from one another. The larger beta is, the larger the variance in the\n        centers of the input features.\n\n        Args:\n            num_clients (int): Number of datasets (one per client) to generate.\n            alpha (float): This is the standard deviation for the mean (``u_k``), drawn from a centered normal\n                distribution, which is used to generate the elements of the affine transformation components ``W``,\n                ``b``.\n            beta (float): This is the standard deviation for each element of the multidimensional mean (``v_k``),\n                drawn from a centered normal distribution, which is used to generate the elements of the input features\n                for \\\\(x \\\\sim \\\\mathcal{N}(B_k, \\\\Sigma)\\\\).\n            temperature (float, optional): Temperature used for the softmax mapping to labels. Defaults to 1.0.\n            input_dim (int, optional): Dimension of the input features for the synthetic dataset. Default is as in the\n                FedProx paper. Defaults to 60.\n            output_dim (int, optional): Dimension of the output labels for the synthetic dataset. These are one-hot\n                encoding labels. Default is as in the FedProx paper. Defaults to 10.\n            hidden_dim (int | None, optional): Dimension of the hidden layer for the two-layer mapping. If None, a\n                one-layer mapping is used. Defaults to None.\n            samples_per_client (int, optional): Number of samples to generate in each client's dataset.\n                Defaults to 1000.\n        \"\"\"\n        super().__init__(\n            num_clients=num_clients,\n            temperature=temperature,\n            input_dim=input_dim,\n            output_dim=output_dim,\n            hidden_dim=hidden_dim,\n            samples_per_client=samples_per_client,\n        )\n        self.two_layer_generation = hidden_dim is not None\n        self.alpha = alpha\n        self.beta = beta\n\n    def get_input_output_tensors(\n        self, mu: list[float], v: torch.Tensor, sigma: torch.Tensor\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        This function takes values for the center of elements in the affine transformation elements (``mu``), the\n        centers feature each of the input feature dimensions (``v``), and the covariance of those features (``sigma``)\n        and produces the input, output tensor pairs with the appropriate dimensions.\n\n        Args:\n            mu (list[float]): List of the mean values from which each element of \\\\(W_i\\\\) and \\\\(b_i\\\\) are to be\n                drawn ~ \\\\(\\\\mathcal{N}(\\\\mu[i], 1)\\\\)\n            v (torch.Tensor): This is assumed to be a 1D tensor of size self.input_dim and represents the mean for the\n                multivariate normal from which to draw the input ``x``\n            sigma (torch.Tensor): This is assumed to be a 2D tensor of shape (``input_dim``, ``input_dim``) and\n                represents the covariance matrix \\\\(\\\\Sigma\\\\) of the multivariate normal from which to draw the\n                input ``x``. It  should be a diagonal matrix as well.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): ``X`` and ``Y`` for the clients synthetic dataset. Shape of ``X`` is\n                ``n_samples`` x input dimension. Shape of ``Y`` is ``n_samples`` x ``output_dim`` and is one-hot\n                encoded.\n        \"\"\"\n        multivariate_normal = MultivariateNormal(loc=v, covariance_matrix=sigma)\n        # size of x should be samples_per_client x input_dim\n        x = multivariate_normal.sample(torch.Size((self.samples_per_client,)))\n\n        if not self.two_layer_generation:\n            w = torch.normal(mu[0], torch.ones((self.output_dim, self.input_dim)))\n            b = torch.normal(mu[0], torch.ones(self.output_dim, 1))\n\n            return x, self.one_layer_map_inputs_to_outputs(x, w, b)\n        assert self.hidden_dim is not None\n        w_1 = torch.normal(mu[0], torch.ones((self.hidden_dim, self.input_dim)))\n        b_1 = torch.normal(mu[0], torch.ones(self.hidden_dim, 1))\n        w_2 = torch.normal(mu[1], torch.ones((self.output_dim, self.hidden_dim)))\n        b_2 = torch.normal(mu[1], torch.ones(self.output_dim, 1))\n\n        return x, self.two_layer_map_inputs_to_outputs(x, w_1, b_1, w_2, b_2)\n\n    def generate_client_tensors(self) -&gt; list[tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"\n        For the Non-IID synthetic generator, this function uses the values of alpha and beta to sample the parameters\n        that will be used to generate the synthetic datasets on each client. For each client, beta is used to sample\n        a mean value from which to generate the input features, alpha is used to sample a mean for the transformation\n        components of W and b. Note that sampling occurs for **EACH** client independently. The larger alpha and beta\n        the larger the variance in these values, implying higher probability of heterogeneity.\n\n        Returns:\n            (list[tuple[torch.Tensor, torch.Tensor]]): Set of input and output tensors for each client.\n        \"\"\"\n        tensors_per_client: list[tuple[torch.Tensor, torch.Tensor]] = []\n        for _ in range(self.num_clients):\n            b = torch.normal(0.0, self.beta, (1,))\n            # v_k in the FedProx paper\n            input_means = torch.normal(b, torch.ones(self.input_dim))\n\n            # u_k in the FedProx paper\n            affine_transform_means = []\n            affine_transform_means.append(torch.normal(0, self.alpha, (1,)).item())\n            if self.two_layer_generation:\n                affine_transform_means.append(torch.normal(0, self.alpha, (1,)).item())\n\n            client_x, client_y = self.get_input_output_tensors(\n                affine_transform_means, input_means, self.input_covariance\n            )\n            tensors_per_client.append((client_x, client_y))\n        return tensors_per_client\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticNonIidFedProxDataset.__init__","title":"<code>__init__(num_clients, alpha, beta, temperature=1.0, input_dim=60, output_dim=10, hidden_dim=None, samples_per_client=1000)</code>","text":"<p>NON-IID Synthetic dataset generator modeled after the implementation in the original FedProx paper. See Section 5.1 in the paper link below for additional details. The non-IID generation code is modeled after the code housed in the github link below as well.</p> <p>Paper link: https://arxiv.org/abs/1812.06127</p> <p>Reference code: https://github.com/litian96/FedProx/tree/master/data/synthetic_1_1</p> <p>NOTE: This generator ends up with fairly skewed labels in generation. That is, many of the clients will not have representations of all the labels. This has been verified as also occurring in the reference code above and is not a bug.</p> <p>The larger alpha and beta are, the more heterogeneous the clients data is. The larger alpha is, the more \"different\" the affine transformations are from one another. The larger beta is, the larger the variance in the centers of the input features.</p> <p>Parameters:</p> Name Type Description Default <code>num_clients</code> <code>int</code> <p>Number of datasets (one per client) to generate.</p> required <code>alpha</code> <code>float</code> <p>This is the standard deviation for the mean (<code>u_k</code>), drawn from a centered normal distribution, which is used to generate the elements of the affine transformation components <code>W</code>, <code>b</code>.</p> required <code>beta</code> <code>float</code> <p>This is the standard deviation for each element of the multidimensional mean (<code>v_k</code>), drawn from a centered normal distribution, which is used to generate the elements of the input features for \\(x \\sim \\mathcal{N}(B_k, \\Sigma)\\).</p> required <code>temperature</code> <code>float</code> <p>Temperature used for the softmax mapping to labels. Defaults to 1.0.</p> <code>1.0</code> <code>input_dim</code> <code>int</code> <p>Dimension of the input features for the synthetic dataset. Default is as in the FedProx paper. Defaults to 60.</p> <code>60</code> <code>output_dim</code> <code>int</code> <p>Dimension of the output labels for the synthetic dataset. These are one-hot encoding labels. Default is as in the FedProx paper. Defaults to 10.</p> <code>10</code> <code>hidden_dim</code> <code>int | None</code> <p>Dimension of the hidden layer for the two-layer mapping. If None, a one-layer mapping is used. Defaults to None.</p> <code>None</code> <code>samples_per_client</code> <code>int</code> <p>Number of samples to generate in each client's dataset. Defaults to 1000.</p> <code>1000</code> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def __init__(\n    self,\n    num_clients: int,\n    alpha: float,\n    beta: float,\n    temperature: float = 1.0,\n    input_dim: int = 60,\n    output_dim: int = 10,\n    hidden_dim: int | None = None,\n    samples_per_client: int = 1000,\n) -&gt; None:\n    \"\"\"\n    NON-IID Synthetic dataset generator modeled after the implementation in the original FedProx paper. See Section\n    5.1 in the paper link below for additional details. The non-IID generation code is modeled after the code\n    housed in the github link below as well.\n\n    Paper link: https://arxiv.org/abs/1812.06127\n\n    Reference code: https://github.com/litian96/FedProx/tree/master/data/synthetic_1_1\n\n    **NOTE**: This generator ends up with fairly skewed labels in generation. That is, many of the clients will not\n    have representations of all the labels. This has been verified as also occurring in the reference code above\n    and is not a bug.\n\n    The larger alpha and beta are, the more heterogeneous the clients data is. The larger alpha is, the more\n    \"different\" the affine transformations are from one another. The larger beta is, the larger the variance in the\n    centers of the input features.\n\n    Args:\n        num_clients (int): Number of datasets (one per client) to generate.\n        alpha (float): This is the standard deviation for the mean (``u_k``), drawn from a centered normal\n            distribution, which is used to generate the elements of the affine transformation components ``W``,\n            ``b``.\n        beta (float): This is the standard deviation for each element of the multidimensional mean (``v_k``),\n            drawn from a centered normal distribution, which is used to generate the elements of the input features\n            for \\\\(x \\\\sim \\\\mathcal{N}(B_k, \\\\Sigma)\\\\).\n        temperature (float, optional): Temperature used for the softmax mapping to labels. Defaults to 1.0.\n        input_dim (int, optional): Dimension of the input features for the synthetic dataset. Default is as in the\n            FedProx paper. Defaults to 60.\n        output_dim (int, optional): Dimension of the output labels for the synthetic dataset. These are one-hot\n            encoding labels. Default is as in the FedProx paper. Defaults to 10.\n        hidden_dim (int | None, optional): Dimension of the hidden layer for the two-layer mapping. If None, a\n            one-layer mapping is used. Defaults to None.\n        samples_per_client (int, optional): Number of samples to generate in each client's dataset.\n            Defaults to 1000.\n    \"\"\"\n    super().__init__(\n        num_clients=num_clients,\n        temperature=temperature,\n        input_dim=input_dim,\n        output_dim=output_dim,\n        hidden_dim=hidden_dim,\n        samples_per_client=samples_per_client,\n    )\n    self.two_layer_generation = hidden_dim is not None\n    self.alpha = alpha\n    self.beta = beta\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticNonIidFedProxDataset.get_input_output_tensors","title":"<code>get_input_output_tensors(mu, v, sigma)</code>","text":"<p>This function takes values for the center of elements in the affine transformation elements (<code>mu</code>), the centers feature each of the input feature dimensions (<code>v</code>), and the covariance of those features (<code>sigma</code>) and produces the input, output tensor pairs with the appropriate dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>list[float]</code> <p>List of the mean values from which each element of \\(W_i\\) and \\(b_i\\) are to be drawn ~ \\(\\mathcal{N}(\\mu[i], 1)\\)</p> required <code>v</code> <code>Tensor</code> <p>This is assumed to be a 1D tensor of size self.input_dim and represents the mean for the multivariate normal from which to draw the input <code>x</code></p> required <code>sigma</code> <code>Tensor</code> <p>This is assumed to be a 2D tensor of shape (<code>input_dim</code>, <code>input_dim</code>) and represents the covariance matrix \\(\\Sigma\\) of the multivariate normal from which to draw the input <code>x</code>. It  should be a diagonal matrix as well.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p><code>X</code> and <code>Y</code> for the clients synthetic dataset. Shape of <code>X</code> is <code>n_samples</code> x input dimension. Shape of <code>Y</code> is <code>n_samples</code> x <code>output_dim</code> and is one-hot encoded.</p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def get_input_output_tensors(\n    self, mu: list[float], v: torch.Tensor, sigma: torch.Tensor\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    This function takes values for the center of elements in the affine transformation elements (``mu``), the\n    centers feature each of the input feature dimensions (``v``), and the covariance of those features (``sigma``)\n    and produces the input, output tensor pairs with the appropriate dimensions.\n\n    Args:\n        mu (list[float]): List of the mean values from which each element of \\\\(W_i\\\\) and \\\\(b_i\\\\) are to be\n            drawn ~ \\\\(\\\\mathcal{N}(\\\\mu[i], 1)\\\\)\n        v (torch.Tensor): This is assumed to be a 1D tensor of size self.input_dim and represents the mean for the\n            multivariate normal from which to draw the input ``x``\n        sigma (torch.Tensor): This is assumed to be a 2D tensor of shape (``input_dim``, ``input_dim``) and\n            represents the covariance matrix \\\\(\\\\Sigma\\\\) of the multivariate normal from which to draw the\n            input ``x``. It  should be a diagonal matrix as well.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): ``X`` and ``Y`` for the clients synthetic dataset. Shape of ``X`` is\n            ``n_samples`` x input dimension. Shape of ``Y`` is ``n_samples`` x ``output_dim`` and is one-hot\n            encoded.\n    \"\"\"\n    multivariate_normal = MultivariateNormal(loc=v, covariance_matrix=sigma)\n    # size of x should be samples_per_client x input_dim\n    x = multivariate_normal.sample(torch.Size((self.samples_per_client,)))\n\n    if not self.two_layer_generation:\n        w = torch.normal(mu[0], torch.ones((self.output_dim, self.input_dim)))\n        b = torch.normal(mu[0], torch.ones(self.output_dim, 1))\n\n        return x, self.one_layer_map_inputs_to_outputs(x, w, b)\n    assert self.hidden_dim is not None\n    w_1 = torch.normal(mu[0], torch.ones((self.hidden_dim, self.input_dim)))\n    b_1 = torch.normal(mu[0], torch.ones(self.hidden_dim, 1))\n    w_2 = torch.normal(mu[1], torch.ones((self.output_dim, self.hidden_dim)))\n    b_2 = torch.normal(mu[1], torch.ones(self.output_dim, 1))\n\n    return x, self.two_layer_map_inputs_to_outputs(x, w_1, b_1, w_2, b_2)\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticNonIidFedProxDataset.generate_client_tensors","title":"<code>generate_client_tensors()</code>","text":"<p>For the Non-IID synthetic generator, this function uses the values of alpha and beta to sample the parameters that will be used to generate the synthetic datasets on each client. For each client, beta is used to sample a mean value from which to generate the input features, alpha is used to sample a mean for the transformation components of W and b. Note that sampling occurs for EACH client independently. The larger alpha and beta the larger the variance in these values, implying higher probability of heterogeneity.</p> <p>Returns:</p> Type Description <code>list[tuple[Tensor, Tensor]]</code> <p>Set of input and output tensors for each client.</p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def generate_client_tensors(self) -&gt; list[tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    For the Non-IID synthetic generator, this function uses the values of alpha and beta to sample the parameters\n    that will be used to generate the synthetic datasets on each client. For each client, beta is used to sample\n    a mean value from which to generate the input features, alpha is used to sample a mean for the transformation\n    components of W and b. Note that sampling occurs for **EACH** client independently. The larger alpha and beta\n    the larger the variance in these values, implying higher probability of heterogeneity.\n\n    Returns:\n        (list[tuple[torch.Tensor, torch.Tensor]]): Set of input and output tensors for each client.\n    \"\"\"\n    tensors_per_client: list[tuple[torch.Tensor, torch.Tensor]] = []\n    for _ in range(self.num_clients):\n        b = torch.normal(0.0, self.beta, (1,))\n        # v_k in the FedProx paper\n        input_means = torch.normal(b, torch.ones(self.input_dim))\n\n        # u_k in the FedProx paper\n        affine_transform_means = []\n        affine_transform_means.append(torch.normal(0, self.alpha, (1,)).item())\n        if self.two_layer_generation:\n            affine_transform_means.append(torch.normal(0, self.alpha, (1,)).item())\n\n        client_x, client_y = self.get_input_output_tensors(\n            affine_transform_means, input_means, self.input_covariance\n        )\n        tensors_per_client.append((client_x, client_y))\n    return tensors_per_client\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticIidFedProxDataset","title":"<code>SyntheticIidFedProxDataset</code>","text":"<p>               Bases: <code>SyntheticFedProxDataset</code></p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>class SyntheticIidFedProxDataset(SyntheticFedProxDataset):\n    def __init__(\n        self,\n        num_clients: int,\n        temperature: float = 1.0,\n        input_dim: int = 60,\n        output_dim: int = 10,\n        samples_per_client: int = 1000,\n    ) -&gt; None:\n        \"\"\"\n        IID Synthetic dataset generator modeled after the implementation in the original FedProx paper. See Appendix\n        C.1 in the paper link below for additional details. The IID generation code is based strictly on the\n        description in the appendix for IID dataset generation.\n\n        Paper link: https://arxiv.org/abs/1812.06127\n\n        **NOTE**: This generator ends up with fairly skewed labels in generation. That is, many of the clients will not\n        have representations of all the labels. This has been verified as also occurring in the reference code above\n        and is not a bug.\n\n        Args:\n            num_clients (int): Number of datasets (one per client) to generate.\n            temperature (float, optional): Temperature used for the softmax mapping to labels. Defaults to 1.0.\n            input_dim (int, optional): Dimension of the input features for the synthetic dataset. Default is as in the\n                FedProx paper. Defaults to 60.\n            output_dim (int, optional): Dimension of the output labels for the synthetic dataset. These are one-hot\n                encoding labels. Default is as in the FedProx paper. Defaults to 10.\n            samples_per_client (int, optional): Number of samples to generate in each client's dataset.\n                Defaults to 1000.\n        \"\"\"\n        super().__init__(\n            num_clients=num_clients,\n            temperature=temperature,\n            input_dim=input_dim,\n            output_dim=output_dim,\n            samples_per_client=samples_per_client,\n        )\n\n        # As described in the original paper, the affine transformation is SHARED by all clients and the elements\n        # of W and b are sampled from standard normal distributions.\n        self.w = torch.normal(0, torch.ones((self.output_dim, self.input_dim)))\n        self.b = torch.normal(0, torch.ones(self.output_dim, 1))\n        # Similarly, all input features across clients are all sampled from a centered multidimensional normal\n        # distribution with diagonal covariance matrix sigma (see base class for definition).\n        self.input_multivariate_normal = MultivariateNormal(\n            loc=torch.zeros(self.input_dim), covariance_matrix=self.input_covariance\n        )\n\n    def get_input_output_tensors(self) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        As described in the original FedProx paper (Appendix C.1), the features are all sampled from a centered\n        multidimensional normal distribution with diagonal covariance matrix shared across clients.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): ``X`` and ``Y`` for the clients synthetic dataset. Shape of ``X`` is\n                ``n_samples`` x input dimension. Shape of ``Y`` is ``n_samples`` x ``output_dim`` and is one-hot\n                encoded.\n        \"\"\"\n        # size of x should be samples_per_client x input_dim\n        x = self.input_multivariate_normal.sample(torch.Size((self.samples_per_client,)))\n        assert x.shape == (self.samples_per_client, self.input_dim)\n\n        return x, self.one_layer_map_inputs_to_outputs(x, self.w, self.b)\n\n    def generate_client_tensors(self) -&gt; list[tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"\n        For IID generation, this function is simple, as we need not sample any parameters per client for use in\n        generation, as these are all shared across clients.\n\n        Returns:\n            (list[tuple[torch.Tensor, torch.Tensor]]): Set of input and output tensors for each client.\n        \"\"\"\n        tensors_per_client: list[tuple[torch.Tensor, torch.Tensor]] = []\n        for _ in range(self.num_clients):\n            client_x, client_y = self.get_input_output_tensors()\n            tensors_per_client.append((client_x, client_y))\n        return tensors_per_client\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticIidFedProxDataset.__init__","title":"<code>__init__(num_clients, temperature=1.0, input_dim=60, output_dim=10, samples_per_client=1000)</code>","text":"<p>IID Synthetic dataset generator modeled after the implementation in the original FedProx paper. See Appendix C.1 in the paper link below for additional details. The IID generation code is based strictly on the description in the appendix for IID dataset generation.</p> <p>Paper link: https://arxiv.org/abs/1812.06127</p> <p>NOTE: This generator ends up with fairly skewed labels in generation. That is, many of the clients will not have representations of all the labels. This has been verified as also occurring in the reference code above and is not a bug.</p> <p>Parameters:</p> Name Type Description Default <code>num_clients</code> <code>int</code> <p>Number of datasets (one per client) to generate.</p> required <code>temperature</code> <code>float</code> <p>Temperature used for the softmax mapping to labels. Defaults to 1.0.</p> <code>1.0</code> <code>input_dim</code> <code>int</code> <p>Dimension of the input features for the synthetic dataset. Default is as in the FedProx paper. Defaults to 60.</p> <code>60</code> <code>output_dim</code> <code>int</code> <p>Dimension of the output labels for the synthetic dataset. These are one-hot encoding labels. Default is as in the FedProx paper. Defaults to 10.</p> <code>10</code> <code>samples_per_client</code> <code>int</code> <p>Number of samples to generate in each client's dataset. Defaults to 1000.</p> <code>1000</code> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def __init__(\n    self,\n    num_clients: int,\n    temperature: float = 1.0,\n    input_dim: int = 60,\n    output_dim: int = 10,\n    samples_per_client: int = 1000,\n) -&gt; None:\n    \"\"\"\n    IID Synthetic dataset generator modeled after the implementation in the original FedProx paper. See Appendix\n    C.1 in the paper link below for additional details. The IID generation code is based strictly on the\n    description in the appendix for IID dataset generation.\n\n    Paper link: https://arxiv.org/abs/1812.06127\n\n    **NOTE**: This generator ends up with fairly skewed labels in generation. That is, many of the clients will not\n    have representations of all the labels. This has been verified as also occurring in the reference code above\n    and is not a bug.\n\n    Args:\n        num_clients (int): Number of datasets (one per client) to generate.\n        temperature (float, optional): Temperature used for the softmax mapping to labels. Defaults to 1.0.\n        input_dim (int, optional): Dimension of the input features for the synthetic dataset. Default is as in the\n            FedProx paper. Defaults to 60.\n        output_dim (int, optional): Dimension of the output labels for the synthetic dataset. These are one-hot\n            encoding labels. Default is as in the FedProx paper. Defaults to 10.\n        samples_per_client (int, optional): Number of samples to generate in each client's dataset.\n            Defaults to 1000.\n    \"\"\"\n    super().__init__(\n        num_clients=num_clients,\n        temperature=temperature,\n        input_dim=input_dim,\n        output_dim=output_dim,\n        samples_per_client=samples_per_client,\n    )\n\n    # As described in the original paper, the affine transformation is SHARED by all clients and the elements\n    # of W and b are sampled from standard normal distributions.\n    self.w = torch.normal(0, torch.ones((self.output_dim, self.input_dim)))\n    self.b = torch.normal(0, torch.ones(self.output_dim, 1))\n    # Similarly, all input features across clients are all sampled from a centered multidimensional normal\n    # distribution with diagonal covariance matrix sigma (see base class for definition).\n    self.input_multivariate_normal = MultivariateNormal(\n        loc=torch.zeros(self.input_dim), covariance_matrix=self.input_covariance\n    )\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticIidFedProxDataset.get_input_output_tensors","title":"<code>get_input_output_tensors()</code>","text":"<p>As described in the original FedProx paper (Appendix C.1), the features are all sampled from a centered multidimensional normal distribution with diagonal covariance matrix shared across clients.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p><code>X</code> and <code>Y</code> for the clients synthetic dataset. Shape of <code>X</code> is <code>n_samples</code> x input dimension. Shape of <code>Y</code> is <code>n_samples</code> x <code>output_dim</code> and is one-hot encoded.</p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def get_input_output_tensors(self) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    As described in the original FedProx paper (Appendix C.1), the features are all sampled from a centered\n    multidimensional normal distribution with diagonal covariance matrix shared across clients.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): ``X`` and ``Y`` for the clients synthetic dataset. Shape of ``X`` is\n            ``n_samples`` x input dimension. Shape of ``Y`` is ``n_samples`` x ``output_dim`` and is one-hot\n            encoded.\n    \"\"\"\n    # size of x should be samples_per_client x input_dim\n    x = self.input_multivariate_normal.sample(torch.Size((self.samples_per_client,)))\n    assert x.shape == (self.samples_per_client, self.input_dim)\n\n    return x, self.one_layer_map_inputs_to_outputs(x, self.w, self.b)\n</code></pre>"},{"location":"api/#fl4health.utils.data_generation.SyntheticIidFedProxDataset.generate_client_tensors","title":"<code>generate_client_tensors()</code>","text":"<p>For IID generation, this function is simple, as we need not sample any parameters per client for use in generation, as these are all shared across clients.</p> <p>Returns:</p> Type Description <code>list[tuple[Tensor, Tensor]]</code> <p>Set of input and output tensors for each client.</p> Source code in <code>fl4health/utils/data_generation.py</code> <pre><code>def generate_client_tensors(self) -&gt; list[tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"\n    For IID generation, this function is simple, as we need not sample any parameters per client for use in\n    generation, as these are all shared across clients.\n\n    Returns:\n        (list[tuple[torch.Tensor, torch.Tensor]]): Set of input and output tensors for each client.\n    \"\"\"\n    tensors_per_client: list[tuple[torch.Tensor, torch.Tensor]] = []\n    for _ in range(self.num_clients):\n        client_x, client_y = self.get_input_output_tensors()\n        tensors_per_client.append((client_x, client_y))\n    return tensors_per_client\n</code></pre>"},{"location":"api/#fl4health.utils.dataset","title":"<code>dataset</code>","text":""},{"location":"api/#fl4health.utils.dataset.BaseDataset","title":"<code>BaseDataset</code>","text":"<p>               Bases: <code>ABC</code>, <code>Dataset</code></p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>class BaseDataset(ABC, Dataset):\n    def __init__(self, transform: Callable | None, target_transform: Callable | None) -&gt; None:\n        \"\"\"\n        Abstract base class for datasets used in this library. This class inherits from the torch Dataset base class.\n\n        Args:\n            transform (Callable | None, optional): Optional transformation to be applied to the input data.\n\n                **NOTE**: This transformation is applied at load time within ``__get_item__``\n\n                Defaults to None.\n            target_transform (Callable | None, optional): Optional transformation to be applied to the target data.\n\n                **NOTE**: This transformation is applied at load time within ``__get_item__``\n\n                Defaults to None.\n        \"\"\"\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def update_transform(self, f: Callable) -&gt; None:\n        if self.transform:\n            original_transform = self.transform\n            self.transform = lambda *x: f(original_transform(*x))\n        else:\n            self.transform = f\n\n    def update_target_transform(self, g: Callable) -&gt; None:\n        if self.target_transform:\n            original_target_transform = self.target_transform\n            self.target_transform = lambda *x: g(original_target_transform(*x))\n        else:\n            self.target_transform = g\n\n    @abstractmethod\n    def __getitem__(self, index: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Abstract method to be implemented by any inheriting dataset to produce a data value at provided index from\n        the underlying data.\n\n        Args:\n            index (int): Index at which to extract the data from the dataset.\n\n        Raises:\n            NotImplementedError: Throws if one attempts to use this function.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Input and target tensors extracted at the provided index.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Abstract method to be implemented by any inheriting dataset to produce a length value for the underlying data.\n\n        Raises:\n            NotImplementedError: Throws if one attempts to use this function.\n\n        Returns:\n            (int): Length of the underlying data.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.BaseDataset.__init__","title":"<code>__init__(transform, target_transform)</code>","text":"<p>Abstract base class for datasets used in this library. This class inherits from the torch Dataset base class.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable | None</code> <p>Optional transformation to be applied to the input data.</p> <p>NOTE: This transformation is applied at load time within <code>__get_item__</code></p> <p>Defaults to None.</p> required <code>target_transform</code> <code>Callable | None</code> <p>Optional transformation to be applied to the target data.</p> <p>NOTE: This transformation is applied at load time within <code>__get_item__</code></p> <p>Defaults to None.</p> required Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __init__(self, transform: Callable | None, target_transform: Callable | None) -&gt; None:\n    \"\"\"\n    Abstract base class for datasets used in this library. This class inherits from the torch Dataset base class.\n\n    Args:\n        transform (Callable | None, optional): Optional transformation to be applied to the input data.\n\n            **NOTE**: This transformation is applied at load time within ``__get_item__``\n\n            Defaults to None.\n        target_transform (Callable | None, optional): Optional transformation to be applied to the target data.\n\n            **NOTE**: This transformation is applied at load time within ``__get_item__``\n\n            Defaults to None.\n    \"\"\"\n    self.transform = transform\n    self.target_transform = target_transform\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.BaseDataset.__getitem__","title":"<code>__getitem__(index)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be implemented by any inheriting dataset to produce a data value at provided index from the underlying data.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index at which to extract the data from the dataset.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Throws if one attempts to use this function.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Input and target tensors extracted at the provided index.</p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, index: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Abstract method to be implemented by any inheriting dataset to produce a data value at provided index from\n    the underlying data.\n\n    Args:\n        index (int): Index at which to extract the data from the dataset.\n\n    Raises:\n        NotImplementedError: Throws if one attempts to use this function.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Input and target tensors extracted at the provided index.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.BaseDataset.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be implemented by any inheriting dataset to produce a length value for the underlying data.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Throws if one attempts to use this function.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of the underlying data.</p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n    \"\"\"\n    Abstract method to be implemented by any inheriting dataset to produce a length value for the underlying data.\n\n    Raises:\n        NotImplementedError: Throws if one attempts to use this function.\n\n    Returns:\n        (int): Length of the underlying data.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.TensorDataset","title":"<code>TensorDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>class TensorDataset(BaseDataset):\n    def __init__(\n        self,\n        data: torch.Tensor,\n        targets: torch.Tensor | None = None,\n        transform: Callable | None = None,\n        target_transform: Callable | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Basic dataset where the data and targets are assumed to be torch tensors. Optionally, this class allows the\n        user to perform transformations on both the data and the targets. This is useful, for example, in performing\n        data augmentation, label blurring, etc.\n\n        Args:\n            data (torch.Tensor): Input data for training.\n            targets (torch.Tensor | None, optional): Target data for training. Defaults to None.\n            transform (Callable | None, optional): Optional transformation to be applied to the input data.\n\n                **NOTE**: This transformation is applied at load time within ``__get_item__``\n\n                Defaults to None.\n            target_transform (Callable | None, optional): Optional transformation to be applied to the target data.\n\n                **NOTE**: This transformation is applied at load time within ``__get_item__``\n\n                Defaults to None.\n        \"\"\"\n        super().__init__(transform, target_transform)\n        self.data = data\n        self.targets = targets\n\n    def __getitem__(self, index: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Extracts the data and targets from the dataset at the provided index. Transformations are performed, as\n        specified in this datasets ``transform`` and ``target_transform`` functions. These are independently\n        applied to the data and targets, respectively.\n\n        Args:\n            index (int): Index in the dataset to extract.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Input data at the index after applying ``transform`` if any, targets\n                after applying ``target_transform`` if any.\n        \"\"\"\n        assert self.targets is not None\n\n        data, target = self.data[index], self.targets[index]\n\n        if self.transform is not None:\n            data = self.transform(data)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return data, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Length of the dataset as determined by len() applied to torch dataset.\n\n        Returns:\n            (int): Length of dataset.\n        \"\"\"\n        return len(self.data)\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.TensorDataset.__init__","title":"<code>__init__(data, targets=None, transform=None, target_transform=None)</code>","text":"<p>Basic dataset where the data and targets are assumed to be torch tensors. Optionally, this class allows the user to perform transformations on both the data and the targets. This is useful, for example, in performing data augmentation, label blurring, etc.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Input data for training.</p> required <code>targets</code> <code>Tensor | None</code> <p>Target data for training. Defaults to None.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Optional transformation to be applied to the input data.</p> <p>NOTE: This transformation is applied at load time within <code>__get_item__</code></p> <p>Defaults to None.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Optional transformation to be applied to the target data.</p> <p>NOTE: This transformation is applied at load time within <code>__get_item__</code></p> <p>Defaults to None.</p> <code>None</code> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __init__(\n    self,\n    data: torch.Tensor,\n    targets: torch.Tensor | None = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n) -&gt; None:\n    \"\"\"\n    Basic dataset where the data and targets are assumed to be torch tensors. Optionally, this class allows the\n    user to perform transformations on both the data and the targets. This is useful, for example, in performing\n    data augmentation, label blurring, etc.\n\n    Args:\n        data (torch.Tensor): Input data for training.\n        targets (torch.Tensor | None, optional): Target data for training. Defaults to None.\n        transform (Callable | None, optional): Optional transformation to be applied to the input data.\n\n            **NOTE**: This transformation is applied at load time within ``__get_item__``\n\n            Defaults to None.\n        target_transform (Callable | None, optional): Optional transformation to be applied to the target data.\n\n            **NOTE**: This transformation is applied at load time within ``__get_item__``\n\n            Defaults to None.\n    \"\"\"\n    super().__init__(transform, target_transform)\n    self.data = data\n    self.targets = targets\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.TensorDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Extracts the data and targets from the dataset at the provided index. Transformations are performed, as specified in this datasets <code>transform</code> and <code>target_transform</code> functions. These are independently applied to the data and targets, respectively.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index in the dataset to extract.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Input data at the index after applying <code>transform</code> if any, targets after applying <code>target_transform</code> if any.</p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Extracts the data and targets from the dataset at the provided index. Transformations are performed, as\n    specified in this datasets ``transform`` and ``target_transform`` functions. These are independently\n    applied to the data and targets, respectively.\n\n    Args:\n        index (int): Index in the dataset to extract.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Input data at the index after applying ``transform`` if any, targets\n            after applying ``target_transform`` if any.\n    \"\"\"\n    assert self.targets is not None\n\n    data, target = self.data[index], self.targets[index]\n\n    if self.transform is not None:\n        data = self.transform(data)\n\n    if self.target_transform is not None:\n        target = self.target_transform(target)\n\n    return data, target\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.TensorDataset.__len__","title":"<code>__len__()</code>","text":"<p>Length of the dataset as determined by len() applied to torch dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of dataset.</p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Length of the dataset as determined by len() applied to torch dataset.\n\n    Returns:\n        (int): Length of dataset.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.SslTensorDataset","title":"<code>SslTensorDataset</code>","text":"<p>               Bases: <code>TensorDataset</code></p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>class SslTensorDataset(TensorDataset):\n    def __init__(\n        self,\n        data: torch.Tensor,\n        targets: torch.Tensor | None = None,\n        transform: Callable | None = None,\n        target_transform: Callable | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Dataset specifically designed to perform self-supervised learning, where we don't have a specific set of\n        targets, because targets are derived from the data tensors.\n\n        Args:\n            data (torch.Tensor): Tensor representing the input data for the dataset.\n            targets (torch.Tensor | None, optional): **REQUIRED TO BE NONE**. The type and argument here is simply to\n                maintain compatibility with our ``TensorDataset`` base. Defaults to None.\n            transform (Callable | None, optional): Any transform to be applied to the data tensors. This transform is\n                performed BEFORE and target transforms that produce the self-supervised targets from the data.\n\n                **NOTE**: These transformations and the ``target_transform`` functions are applied **AT LOAD TIME**.\n\n                Defaults to None.\n            target_transform (Callable | None, optional): Any transform to be applied to the data tensors to produce\n                target tensors for training. This transform is performed after and transforms for the data tensors\n                themselves to produce the self-supervised targets from the data.\n\n                **NOTE**: These transformation functions are applied **AT LOAD TIME**.\n\n                Defaults to None.\n        \"\"\"\n        assert targets is None, \"SslTensorDataset targets must be None\"\n\n        super().__init__(data, targets, transform, target_transform)\n\n    def __getitem__(self, index: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Extracts the data and targets from the dataset at the provided index. Because this is an self-supervised\n        learning dataset. The input data also serves as the target (subject to transformations). Transformations\n        are performed, as specified in this datasets ``transform`` and ``target_transform`` functions. These are\n        applied first to the data, then targets are created by applying ``target_transform`` to the result.\n\n        Args:\n            index (int): Index in the dataset to extract.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Input data at the index after applying ``transform`` if any, targets\n                derived from data after applying ``transform`` and ``target_transform`` in sequence.\n        \"\"\"\n        data = self.data[index]\n\n        assert self.target_transform is not None, \"Target transform cannot be None.\"\n\n        if self.transform is not None:\n            data = self.transform(data)\n\n        # Perform transform on input to yield target during data loading\n        # More memory efficient than pre-computing transforms which requires\n        # storing multiple copies of each sample\n        transformed_data = self.target_transform(data)\n\n        return data, transformed_data\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.SslTensorDataset.__init__","title":"<code>__init__(data, targets=None, transform=None, target_transform=None)</code>","text":"<p>Dataset specifically designed to perform self-supervised learning, where we don't have a specific set of targets, because targets are derived from the data tensors.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Tensor representing the input data for the dataset.</p> required <code>targets</code> <code>Tensor | None</code> <p>REQUIRED TO BE NONE. The type and argument here is simply to maintain compatibility with our <code>TensorDataset</code> base. Defaults to None.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Any transform to be applied to the data tensors. This transform is performed BEFORE and target transforms that produce the self-supervised targets from the data.</p> <p>NOTE: These transformations and the <code>target_transform</code> functions are applied AT LOAD TIME.</p> <p>Defaults to None.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Any transform to be applied to the data tensors to produce target tensors for training. This transform is performed after and transforms for the data tensors themselves to produce the self-supervised targets from the data.</p> <p>NOTE: These transformation functions are applied AT LOAD TIME.</p> <p>Defaults to None.</p> <code>None</code> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __init__(\n    self,\n    data: torch.Tensor,\n    targets: torch.Tensor | None = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n) -&gt; None:\n    \"\"\"\n    Dataset specifically designed to perform self-supervised learning, where we don't have a specific set of\n    targets, because targets are derived from the data tensors.\n\n    Args:\n        data (torch.Tensor): Tensor representing the input data for the dataset.\n        targets (torch.Tensor | None, optional): **REQUIRED TO BE NONE**. The type and argument here is simply to\n            maintain compatibility with our ``TensorDataset`` base. Defaults to None.\n        transform (Callable | None, optional): Any transform to be applied to the data tensors. This transform is\n            performed BEFORE and target transforms that produce the self-supervised targets from the data.\n\n            **NOTE**: These transformations and the ``target_transform`` functions are applied **AT LOAD TIME**.\n\n            Defaults to None.\n        target_transform (Callable | None, optional): Any transform to be applied to the data tensors to produce\n            target tensors for training. This transform is performed after and transforms for the data tensors\n            themselves to produce the self-supervised targets from the data.\n\n            **NOTE**: These transformation functions are applied **AT LOAD TIME**.\n\n            Defaults to None.\n    \"\"\"\n    assert targets is None, \"SslTensorDataset targets must be None\"\n\n    super().__init__(data, targets, transform, target_transform)\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.SslTensorDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Extracts the data and targets from the dataset at the provided index. Because this is an self-supervised learning dataset. The input data also serves as the target (subject to transformations). Transformations are performed, as specified in this datasets <code>transform</code> and <code>target_transform</code> functions. These are applied first to the data, then targets are created by applying <code>target_transform</code> to the result.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index in the dataset to extract.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Input data at the index after applying <code>transform</code> if any, targets derived from data after applying <code>transform</code> and <code>target_transform</code> in sequence.</p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Extracts the data and targets from the dataset at the provided index. Because this is an self-supervised\n    learning dataset. The input data also serves as the target (subject to transformations). Transformations\n    are performed, as specified in this datasets ``transform`` and ``target_transform`` functions. These are\n    applied first to the data, then targets are created by applying ``target_transform`` to the result.\n\n    Args:\n        index (int): Index in the dataset to extract.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Input data at the index after applying ``transform`` if any, targets\n            derived from data after applying ``transform`` and ``target_transform`` in sequence.\n    \"\"\"\n    data = self.data[index]\n\n    assert self.target_transform is not None, \"Target transform cannot be None.\"\n\n    if self.transform is not None:\n        data = self.transform(data)\n\n    # Perform transform on input to yield target during data loading\n    # More memory efficient than pre-computing transforms which requires\n    # storing multiple copies of each sample\n    transformed_data = self.target_transform(data)\n\n    return data, transformed_data\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.DictionaryDataset","title":"<code>DictionaryDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>class DictionaryDataset(Dataset):\n    def __init__(self, data: dict[str, list[torch.Tensor]], targets: torch.Tensor) -&gt; None:\n        \"\"\"\n        A torch dataset that supports a dictionary of input data rather than just a ``torch.Tensor``. This kind of\n        dataset is useful when dealing with non-trivial inputs to a model. For example, a language model may require\n        token ids AND attention masks. This dataset supports that functionality.\n\n        Args:\n            data (dict[str, list[torch.Tensor]]): A set of data for model training/input in the form of a dictionary\n                of tensors.\n            targets (torch.Tensor): Target tensor.\n        \"\"\"\n        self.data = data\n        self.targets = targets\n\n    def __getitem__(self, index: int) -&gt; tuple[dict[str, torch.Tensor], torch.Tensor]:\n        \"\"\"\n        Extracts data from the ``DictionaryDataset`` at the provided index. The targets are simply extracted directly\n        using index. The input data dictionary is iterated through and each piece of data in the dictionary values\n        is extracted at the provided index and \"re-wrapped\" as a dictionary.\n\n        Args:\n            index (int): Index of the data to be extracted from the dataset.\n\n        Returns:\n            (tuple[dict[str, torch.Tensor], torch.Tensor]): Dictionary with the same keys as the dataset data\n                dictionary with data extracted at the provided index, target data extracted from the targets tensor at\n                index.\n        \"\"\"\n        return {key: val[index] for key, val in self.data.items()}, self.targets[index]\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Gets the length of the dataset as extracted from the first piece of data in the data dictionary.\n\n        **NOTE**: This implicitly assumes that the length of the data in each entry of the dictionary of data is\n        uniform.\n\n        Returns:\n            (int): Dataset length.\n        \"\"\"\n        first_key = list(self.data.keys())[0]\n        return len(self.data[first_key])\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.DictionaryDataset.__init__","title":"<code>__init__(data, targets)</code>","text":"<p>A torch dataset that supports a dictionary of input data rather than just a <code>torch.Tensor</code>. This kind of dataset is useful when dealing with non-trivial inputs to a model. For example, a language model may require token ids AND attention masks. This dataset supports that functionality.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, list[Tensor]]</code> <p>A set of data for model training/input in the form of a dictionary of tensors.</p> required <code>targets</code> <code>Tensor</code> <p>Target tensor.</p> required Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __init__(self, data: dict[str, list[torch.Tensor]], targets: torch.Tensor) -&gt; None:\n    \"\"\"\n    A torch dataset that supports a dictionary of input data rather than just a ``torch.Tensor``. This kind of\n    dataset is useful when dealing with non-trivial inputs to a model. For example, a language model may require\n    token ids AND attention masks. This dataset supports that functionality.\n\n    Args:\n        data (dict[str, list[torch.Tensor]]): A set of data for model training/input in the form of a dictionary\n            of tensors.\n        targets (torch.Tensor): Target tensor.\n    \"\"\"\n    self.data = data\n    self.targets = targets\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.DictionaryDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Extracts data from the <code>DictionaryDataset</code> at the provided index. The targets are simply extracted directly using index. The input data dictionary is iterated through and each piece of data in the dictionary values is extracted at the provided index and \"re-wrapped\" as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the data to be extracted from the dataset.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, Tensor], Tensor]</code> <p>Dictionary with the same keys as the dataset data dictionary with data extracted at the provided index, target data extracted from the targets tensor at index.</p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[dict[str, torch.Tensor], torch.Tensor]:\n    \"\"\"\n    Extracts data from the ``DictionaryDataset`` at the provided index. The targets are simply extracted directly\n    using index. The input data dictionary is iterated through and each piece of data in the dictionary values\n    is extracted at the provided index and \"re-wrapped\" as a dictionary.\n\n    Args:\n        index (int): Index of the data to be extracted from the dataset.\n\n    Returns:\n        (tuple[dict[str, torch.Tensor], torch.Tensor]): Dictionary with the same keys as the dataset data\n            dictionary with data extracted at the provided index, target data extracted from the targets tensor at\n            index.\n    \"\"\"\n    return {key: val[index] for key, val in self.data.items()}, self.targets[index]\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.DictionaryDataset.__len__","title":"<code>__len__()</code>","text":"<p>Gets the length of the dataset as extracted from the first piece of data in the data dictionary.</p> <p>NOTE: This implicitly assumes that the length of the data in each entry of the dictionary of data is uniform.</p> <p>Returns:</p> Type Description <code>int</code> <p>Dataset length.</p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Gets the length of the dataset as extracted from the first piece of data in the data dictionary.\n\n    **NOTE**: This implicitly assumes that the length of the data in each entry of the dictionary of data is\n    uniform.\n\n    Returns:\n        (int): Dataset length.\n    \"\"\"\n    first_key = list(self.data.keys())[0]\n    return len(self.data[first_key])\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.SyntheticDataset","title":"<code>SyntheticDataset</code>","text":"<p>               Bases: <code>TensorDataset</code></p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>class SyntheticDataset(TensorDataset):\n    def __init__(\n        self,\n        data: torch.Tensor,\n        targets: torch.Tensor,\n    ):\n        \"\"\"\n        A dataset for synthetically created data strictly in the form of pytorch tensors. Generally, this dataset\n        is just used for tests.\n\n        Args:\n            data (torch.Tensor): Data tensor with first dimension corresponding to the number of datapoints\n            targets (torch.Tensor): Target tensor with first dimension corresponding to the number of datapoints\n        \"\"\"\n        assert data.shape[0] == targets.shape[0]\n        self.data = data\n        self.targets = targets\n\n    def __getitem__(self, index: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Extracts the data at the provided index.\n\n        Args:\n            index (int): Index of the data in the dataset to be returned\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Input and targets at the provided index.\n        \"\"\"\n        assert self.targets is not None\n\n        data, target = self.data[index], self.targets[index]\n        return data, target\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset. Identical to the pytorch dataset length function.\n\n        Returns:\n            (int): Length of the data.\n        \"\"\"\n        return len(self.data)\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.SyntheticDataset.__init__","title":"<code>__init__(data, targets)</code>","text":"<p>A dataset for synthetically created data strictly in the form of pytorch tensors. Generally, this dataset is just used for tests.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tensor</code> <p>Data tensor with first dimension corresponding to the number of datapoints</p> required <code>targets</code> <code>Tensor</code> <p>Target tensor with first dimension corresponding to the number of datapoints</p> required Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __init__(\n    self,\n    data: torch.Tensor,\n    targets: torch.Tensor,\n):\n    \"\"\"\n    A dataset for synthetically created data strictly in the form of pytorch tensors. Generally, this dataset\n    is just used for tests.\n\n    Args:\n        data (torch.Tensor): Data tensor with first dimension corresponding to the number of datapoints\n        targets (torch.Tensor): Target tensor with first dimension corresponding to the number of datapoints\n    \"\"\"\n    assert data.shape[0] == targets.shape[0]\n    self.data = data\n    self.targets = targets\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.SyntheticDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Extracts the data at the provided index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the data in the dataset to be returned</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Input and targets at the provided index.</p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Extracts the data at the provided index.\n\n    Args:\n        index (int): Index of the data in the dataset to be returned\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Input and targets at the provided index.\n    \"\"\"\n    assert self.targets is not None\n\n    data, target = self.data[index], self.targets[index]\n    return data, target\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.SyntheticDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset. Identical to the pytorch dataset length function.</p> <p>Returns:</p> Type Description <code>int</code> <p>Length of the data.</p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset. Identical to the pytorch dataset length function.\n\n    Returns:\n        (int): Length of the data.\n    \"\"\"\n    return len(self.data)\n</code></pre>"},{"location":"api/#fl4health.utils.dataset.select_by_indices","title":"<code>select_by_indices(dataset, selected_indices)</code>","text":"<p>This function is used to extract a subset of a dataset sliced by the indices in the tensor <code>selected_indices</code>. The dataset returned should be of the same type as the input but with only data associated with the given indices.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>D</code> <p>Dataset to be \"subsampled\" using the provided indices.</p> required <code>selected_indices</code> <code>Tensor</code> <p>Indices within the datasets data and targets (if they exist) to select.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>Will throw an error if the dataset provided is not supported.</p> <p>Returns:</p> Type Description <code>D</code> <p>Dataset with only the data associated with the provided indices. Must be of a supported type.</p> Source code in <code>fl4health/utils/dataset.py</code> <pre><code>def select_by_indices(dataset: D, selected_indices: torch.Tensor) -&gt; D:\n    \"\"\"\n    This function is used to extract a subset of a dataset sliced by the indices in the tensor ``selected_indices``.\n    The dataset returned should be of the same type as the input but with only data associated with the given indices.\n\n    Args:\n        dataset (D): Dataset to be \"subsampled\" using the provided indices.\n        selected_indices (torch.Tensor): Indices within the datasets data and targets (if they exist) to select.\n\n    Raises:\n        TypeError: Will throw an error if the dataset provided is not supported.\n\n    Returns:\n        (D): Dataset with only the data associated with the provided indices. Must be of a supported type.\n    \"\"\"\n    if isinstance(dataset, TensorDataset):\n        modified_dataset = copy.deepcopy(dataset)\n        modified_dataset.data = dataset.data[selected_indices]\n        if dataset.targets is not None:\n            modified_dataset.targets = dataset.targets[selected_indices]\n        # cast being used here until the mypy bug mentioned in https://github.com/python/mypy/issues/12800 and the\n        # duplicate ticket https://github.com/python/mypy/issues/10817 are fixed\n        return cast(D, modified_dataset)\n    if isinstance(dataset, DictionaryDataset):\n        new_targets = dataset.targets[selected_indices]\n        new_data: dict[str, list[torch.Tensor]] = {}\n        for key, val in dataset.data.items():\n            # Since val is a list of tensors, we can't directly index into it\n            # using selected_indices.\n            new_data[key] = [val[i] for i in selected_indices]\n        # cast being used here until the mypy bug mentioned in https://github.com/python/mypy/issues/12800 and the\n        # duplicate ticket https://github.com/python/mypy/issues/10817 are fixed\n        return cast(D, DictionaryDataset(new_data, new_targets))\n    raise TypeError(\"Dataset type is not supported by this function.\")\n</code></pre>"},{"location":"api/#fl4health.utils.dataset_converter","title":"<code>dataset_converter</code>","text":""},{"location":"api/#fl4health.utils.dataset_converter.DatasetConverter","title":"<code>DatasetConverter</code>","text":"<p>               Bases: <code>TensorDataset</code></p> Source code in <code>fl4health/utils/dataset_converter.py</code> <pre><code>class DatasetConverter(TensorDataset):\n    def __init__(\n        self,\n        converter_function: Callable[[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor]],\n        dataset: TensorDataset | None,\n    ) -&gt; None:\n        \"\"\"\n        Dataset converter classes are designed to re-format any dataset for a given training task, and to fit it\n        into the unified training scheme of supervised learning in clients. Converters can be used in the data\n        loading step. They can also apply a light pre-processing step on datasets before the training process.\n\n        Args:\n            converter_function (Callable[[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]): Function\n                defining how the dataset should be converted.\n            dataset (TensorDataset | None): Dataset to be converted.\n        \"\"\"\n        assert dataset is None or dataset.targets is not None\n        self.converter_function = converter_function\n        self.dataset = dataset\n\n    def __getitem__(self, index: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Overriding this function from BaseDataset allows the converter to be compatible with the data transformers.\n        ``converter_function`` is applied after the transformers.\n\n        Args:\n            index (int): The index of the batch of data to be extracted.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Get the raw batch data (input, target) and apply the\n                ``converter_function`` before returning.\n        \"\"\"\n        assert self.dataset is not None, \"Error: no dataset is set, use convert_dataset(your_dataset: TensorDataset)\"\n        data, target = self.dataset.__getitem__(index)\n        return self.converter_function(data, target)\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the length of the dataset. Mostly just a wrapper on the standard pytorch dataset length to ensure that\n        the dataset is not None.\n\n        Returns:\n            (int): Dataset length.\n        \"\"\"\n        assert self.dataset is not None, \"Error: dataset is should be either converted or initiated.\"\n        return len(self.dataset)\n\n    def convert_dataset(self, dataset: TensorDataset) -&gt; TensorDataset:\n        \"\"\"\n        Applies the converter function over the dataset when the dataset is used\n        (i.e. during the dataloader creation step).\n        \"\"\"\n        # Dataset can be added/changed at any point in the pipeline.\n        self.dataset = dataset\n        # Returning this object as the converted dataset since this class overrides\n        # the __getitem__ function of BaseDataset.\n        return self\n</code></pre>"},{"location":"api/#fl4health.utils.dataset_converter.DatasetConverter.__init__","title":"<code>__init__(converter_function, dataset)</code>","text":"<p>Dataset converter classes are designed to re-format any dataset for a given training task, and to fit it into the unified training scheme of supervised learning in clients. Converters can be used in the data loading step. They can also apply a light pre-processing step on datasets before the training process.</p> <p>Parameters:</p> Name Type Description Default <code>converter_function</code> <code>Callable[[Tensor, Tensor], tuple[Tensor, Tensor]]</code> <p>Function defining how the dataset should be converted.</p> required <code>dataset</code> <code>TensorDataset | None</code> <p>Dataset to be converted.</p> required Source code in <code>fl4health/utils/dataset_converter.py</code> <pre><code>def __init__(\n    self,\n    converter_function: Callable[[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor]],\n    dataset: TensorDataset | None,\n) -&gt; None:\n    \"\"\"\n    Dataset converter classes are designed to re-format any dataset for a given training task, and to fit it\n    into the unified training scheme of supervised learning in clients. Converters can be used in the data\n    loading step. They can also apply a light pre-processing step on datasets before the training process.\n\n    Args:\n        converter_function (Callable[[torch.Tensor, torch.Tensor], tuple[torch.Tensor, torch.Tensor]]): Function\n            defining how the dataset should be converted.\n        dataset (TensorDataset | None): Dataset to be converted.\n    \"\"\"\n    assert dataset is None or dataset.targets is not None\n    self.converter_function = converter_function\n    self.dataset = dataset\n</code></pre>"},{"location":"api/#fl4health.utils.dataset_converter.DatasetConverter.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Overriding this function from BaseDataset allows the converter to be compatible with the data transformers. <code>converter_function</code> is applied after the transformers.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The index of the batch of data to be extracted.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Get the raw batch data (input, target) and apply the <code>converter_function</code> before returning.</p> Source code in <code>fl4health/utils/dataset_converter.py</code> <pre><code>def __getitem__(self, index: int) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Overriding this function from BaseDataset allows the converter to be compatible with the data transformers.\n    ``converter_function`` is applied after the transformers.\n\n    Args:\n        index (int): The index of the batch of data to be extracted.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Get the raw batch data (input, target) and apply the\n            ``converter_function`` before returning.\n    \"\"\"\n    assert self.dataset is not None, \"Error: no dataset is set, use convert_dataset(your_dataset: TensorDataset)\"\n    data, target = self.dataset.__getitem__(index)\n    return self.converter_function(data, target)\n</code></pre>"},{"location":"api/#fl4health.utils.dataset_converter.DatasetConverter.__len__","title":"<code>__len__()</code>","text":"<p>Returns the length of the dataset. Mostly just a wrapper on the standard pytorch dataset length to ensure that the dataset is not None.</p> <p>Returns:</p> Type Description <code>int</code> <p>Dataset length.</p> Source code in <code>fl4health/utils/dataset_converter.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the length of the dataset. Mostly just a wrapper on the standard pytorch dataset length to ensure that\n    the dataset is not None.\n\n    Returns:\n        (int): Dataset length.\n    \"\"\"\n    assert self.dataset is not None, \"Error: dataset is should be either converted or initiated.\"\n    return len(self.dataset)\n</code></pre>"},{"location":"api/#fl4health.utils.dataset_converter.DatasetConverter.convert_dataset","title":"<code>convert_dataset(dataset)</code>","text":"<p>Applies the converter function over the dataset when the dataset is used (i.e. during the dataloader creation step).</p> Source code in <code>fl4health/utils/dataset_converter.py</code> <pre><code>def convert_dataset(self, dataset: TensorDataset) -&gt; TensorDataset:\n    \"\"\"\n    Applies the converter function over the dataset when the dataset is used\n    (i.e. during the dataloader creation step).\n    \"\"\"\n    # Dataset can be added/changed at any point in the pipeline.\n    self.dataset = dataset\n    # Returning this object as the converted dataset since this class overrides\n    # the __getitem__ function of BaseDataset.\n    return self\n</code></pre>"},{"location":"api/#fl4health.utils.dataset_converter.AutoEncoderDatasetConverter","title":"<code>AutoEncoderDatasetConverter</code>","text":"<p>               Bases: <code>DatasetConverter</code></p> Source code in <code>fl4health/utils/dataset_converter.py</code> <pre><code>class AutoEncoderDatasetConverter(DatasetConverter):\n    def __init__(\n        self,\n        condition: str | torch.Tensor | None = None,\n        do_one_hot_encoding: bool = False,\n        custom_converter_function: Callable | None = None,\n        condition_vector_size: int | None = None,\n    ) -&gt; None:\n        \"\"\"\n        A dataset converter specific to formatting supervised data such as MNIST for self-supervised training in\n        autoencoder-based models, and potentially handling the existence of additional input (i.e. condition).\n\n        This class includes three converter functions that are chosen based on the condition, other converter\n        functions can be added or passed to support other conditions.\n\n        Args:\n            condition (str | torch.Tensor | None, optional): Could be a fixed tensor used for all the data samples,\n                None for non-conditional models, or a name (str) passed for other custom conversions like \"label\".\n                Defaults to None.\n            do_one_hot_encoding (bool, optional): Should converter perform one hot encoding on the condition or not.\n                Defaults to False.\n            custom_converter_function (Callable | None, optional): User can define a new converter function. Defaults\n                to None.\n            condition_vector_size (int | None, optional): Size of the conditioning vector if available. Defaults to\n                None.\n        \"\"\"\n        self.condition = condition\n        if isinstance(self.condition, torch.Tensor):\n            # Condition should be a ready to use 1D tensor set in the client.\n            assert self.condition.dim() == 1, (\n                f\"Error: condition should be a 1D vector instead of {self.condition.dim()}D tensor.\"\n            )\n        # Will be set in convert_dataset.\n        self.data_shape: torch.Size\n        self.do_one_hot_encoding = do_one_hot_encoding\n        # If no converter function is passed, it should be defined here.\n        if custom_converter_function is None:\n            self.converter_function = self._setup_converter_function()\n        else:\n            self.converter_function = custom_converter_function\n            assert condition_vector_size is not None, (\n                \"Error: The condition should be specified for a custom converter function.\"\n            )\n            self.condition_vector_size = condition_vector_size\n        super().__init__(self.converter_function, dataset=None)\n\n    def convert_dataset(self, dataset: TensorDataset) -&gt; TensorDataset:\n        assert dataset.targets is not None\n\n        self.dataset = dataset\n        # Data shape is saved to be used in the pack-unpack process.\n        # This is the shape of the data after getting transformed by torch transforms.\n        data, _ = self.dataset[0]\n        self.data_shape = data.shape\n        return self\n\n    def get_condition_vector_size(self) -&gt; int:\n        if self.condition == \"label\":\n            assert self.dataset is not None, \"Error: no dataset is passed to the converter.\"\n            assert self.dataset.targets is not None\n            if self.do_one_hot_encoding:\n                return len(torch.unique(self.dataset.targets))\n            return len(self.dataset.targets[0])\n        if isinstance(self.condition, torch.Tensor):\n            return self.condition.size(0)\n        if self.condition_vector_size is not None:\n            return self.condition_vector_size\n        if self.condition is None:\n            return 0\n        raise NotImplementedError(\"Error: support for this type of condition is not added to the data converter.\")\n\n    def _setup_converter_function(self) -&gt; Callable:\n        \"\"\"\n        Sets the converter function for autoencoder based models (if it is not already specified by the user).\n        If condition is not None, the respective converter function accounts for the type of\n        condition and handles the concatenation.\n\n        Returns:\n            (Callable): The suitable converter function based on the condition.\n        \"\"\"\n        # If the autoencoder is conditional\n        if self.condition is not None:\n            if self.condition == \"label\":\n                # Condition depends on the target.\n                converter_function = self._cat_input_label\n            elif isinstance(self.condition, torch.Tensor):\n                converter_function = self._cat_input_condition\n            else:\n                raise NotImplementedError(\"Error: support for this type of condition is not added.\")\n\n        elif self.condition is None:  # non-conditional autoencoder\n            converter_function = self._only_replace_target_with_data\n        return converter_function\n\n    def _only_replace_target_with_data(\n        self, data: torch.Tensor, target: torch.Tensor | None\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"The data converter function used for simple autoencoders or variational autoencoders.\"\"\"\n        # Target in self-supervised training for autoencoder is the data.\n        return data, data\n\n    def _cat_input_condition(\n        self, data: torch.Tensor, target: torch.Tensor | None\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        The data converter function used for conditional autoencoders.\n\n        This converter is used when we have a torch tensor as condition for all the data samples.\n        \"\"\"\n        # We can flatten the data since self.data_shape is already saved.\n        # Target should be the original data.\n        assert isinstance(self.condition, torch.Tensor), \"Error: condition should be a torch tensor\"\n        return torch.cat([data.view(-1), self.condition]), data\n\n    def _cat_input_label(self, data: torch.Tensor, target: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        The data converter function used for conditional autoencoders.\n\n        This converter is used when we want to condition each data sample on its label.\n        \"\"\"\n        if self.do_one_hot_encoding:  # If condition needs to be one-hot encoded.\n            # Create the condition vector by getting the one_hot encoded target.\n            target = torch.nn.functional.one_hot(target, num_classes=self.get_condition_vector_size())\n        # Concatenate the data and target (target is the condition)\n        # We can flatten the data since self.data_shape is already saved.\n        return torch.cat([data.view(-1), target]), data\n\n    def get_unpacking_function(self) -&gt; Callable[[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n        condition_vector_size = self.get_condition_vector_size()\n        return partial(\n            AutoEncoderDatasetConverter.unpack_input_condition,\n            cond_vec_size=condition_vector_size,\n            data_shape=self.data_shape,\n        )\n\n    @staticmethod\n    def unpack_input_condition(\n        packed_data: torch.Tensor, cond_vec_size: int, data_shape: torch.Size\n    ) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Unpacks model inputs (data and condition) from a tensor used in the training loop regardless of the\n        converter function used to pack them. Unpacking relies on the size of the condition vector, and the original\n        data shape which is saved before the packing process.\n\n        Args:\n            packed_data (torch.Tensor): Data tensor used in the training loop as the input to the model.\n            cond_vec_size (int): Size of the conditional vector that has been packed into the ``packed_data`` variable.\n            data_shape (torch.Size): Expected shape of the original data tensor after unpacking the conditioning\n                vector.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor]): Data in its original shape, and the condition vector to be fed into\n                the model.\n        \"\"\"\n        # We assume data is \"batch first\".\n        x = packed_data[:, : -1 * cond_vec_size]  # Exclude the condition from input\n        condition_matrix = packed_data[:, -1 * cond_vec_size :]  # Save the conditions\n        # Data shape is restored in the unpacking process.\n        assert data_shape is not None\n        return x.view(-1, *data_shape), condition_matrix\n</code></pre>"},{"location":"api/#fl4health.utils.dataset_converter.AutoEncoderDatasetConverter.__init__","title":"<code>__init__(condition=None, do_one_hot_encoding=False, custom_converter_function=None, condition_vector_size=None)</code>","text":"<p>A dataset converter specific to formatting supervised data such as MNIST for self-supervised training in autoencoder-based models, and potentially handling the existence of additional input (i.e. condition).</p> <p>This class includes three converter functions that are chosen based on the condition, other converter functions can be added or passed to support other conditions.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>str | Tensor | None</code> <p>Could be a fixed tensor used for all the data samples, None for non-conditional models, or a name (str) passed for other custom conversions like \"label\". Defaults to None.</p> <code>None</code> <code>do_one_hot_encoding</code> <code>bool</code> <p>Should converter perform one hot encoding on the condition or not. Defaults to False.</p> <code>False</code> <code>custom_converter_function</code> <code>Callable | None</code> <p>User can define a new converter function. Defaults to None.</p> <code>None</code> <code>condition_vector_size</code> <code>int | None</code> <p>Size of the conditioning vector if available. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/utils/dataset_converter.py</code> <pre><code>def __init__(\n    self,\n    condition: str | torch.Tensor | None = None,\n    do_one_hot_encoding: bool = False,\n    custom_converter_function: Callable | None = None,\n    condition_vector_size: int | None = None,\n) -&gt; None:\n    \"\"\"\n    A dataset converter specific to formatting supervised data such as MNIST for self-supervised training in\n    autoencoder-based models, and potentially handling the existence of additional input (i.e. condition).\n\n    This class includes three converter functions that are chosen based on the condition, other converter\n    functions can be added or passed to support other conditions.\n\n    Args:\n        condition (str | torch.Tensor | None, optional): Could be a fixed tensor used for all the data samples,\n            None for non-conditional models, or a name (str) passed for other custom conversions like \"label\".\n            Defaults to None.\n        do_one_hot_encoding (bool, optional): Should converter perform one hot encoding on the condition or not.\n            Defaults to False.\n        custom_converter_function (Callable | None, optional): User can define a new converter function. Defaults\n            to None.\n        condition_vector_size (int | None, optional): Size of the conditioning vector if available. Defaults to\n            None.\n    \"\"\"\n    self.condition = condition\n    if isinstance(self.condition, torch.Tensor):\n        # Condition should be a ready to use 1D tensor set in the client.\n        assert self.condition.dim() == 1, (\n            f\"Error: condition should be a 1D vector instead of {self.condition.dim()}D tensor.\"\n        )\n    # Will be set in convert_dataset.\n    self.data_shape: torch.Size\n    self.do_one_hot_encoding = do_one_hot_encoding\n    # If no converter function is passed, it should be defined here.\n    if custom_converter_function is None:\n        self.converter_function = self._setup_converter_function()\n    else:\n        self.converter_function = custom_converter_function\n        assert condition_vector_size is not None, (\n            \"Error: The condition should be specified for a custom converter function.\"\n        )\n        self.condition_vector_size = condition_vector_size\n    super().__init__(self.converter_function, dataset=None)\n</code></pre>"},{"location":"api/#fl4health.utils.dataset_converter.AutoEncoderDatasetConverter.unpack_input_condition","title":"<code>unpack_input_condition(packed_data, cond_vec_size, data_shape)</code>  <code>staticmethod</code>","text":"<p>Unpacks model inputs (data and condition) from a tensor used in the training loop regardless of the converter function used to pack them. Unpacking relies on the size of the condition vector, and the original data shape which is saved before the packing process.</p> <p>Parameters:</p> Name Type Description Default <code>packed_data</code> <code>Tensor</code> <p>Data tensor used in the training loop as the input to the model.</p> required <code>cond_vec_size</code> <code>int</code> <p>Size of the conditional vector that has been packed into the <code>packed_data</code> variable.</p> required <code>data_shape</code> <code>Size</code> <p>Expected shape of the original data tensor after unpacking the conditioning vector.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>Data in its original shape, and the condition vector to be fed into the model.</p> Source code in <code>fl4health/utils/dataset_converter.py</code> <pre><code>@staticmethod\ndef unpack_input_condition(\n    packed_data: torch.Tensor, cond_vec_size: int, data_shape: torch.Size\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Unpacks model inputs (data and condition) from a tensor used in the training loop regardless of the\n    converter function used to pack them. Unpacking relies on the size of the condition vector, and the original\n    data shape which is saved before the packing process.\n\n    Args:\n        packed_data (torch.Tensor): Data tensor used in the training loop as the input to the model.\n        cond_vec_size (int): Size of the conditional vector that has been packed into the ``packed_data`` variable.\n        data_shape (torch.Size): Expected shape of the original data tensor after unpacking the conditioning\n            vector.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor]): Data in its original shape, and the condition vector to be fed into\n            the model.\n    \"\"\"\n    # We assume data is \"batch first\".\n    x = packed_data[:, : -1 * cond_vec_size]  # Exclude the condition from input\n    condition_matrix = packed_data[:, -1 * cond_vec_size :]  # Save the conditions\n    # Data shape is restored in the unpacking process.\n    assert data_shape is not None\n    return x.view(-1, *data_shape), condition_matrix\n</code></pre>"},{"location":"api/#fl4health.utils.early_stopper","title":"<code>early_stopper</code>","text":""},{"location":"api/#fl4health.utils.early_stopper.EarlyStopper","title":"<code>EarlyStopper</code>","text":"Source code in <code>fl4health/utils/early_stopper.py</code> <pre><code>class EarlyStopper:\n    def __init__(\n        self,\n        client: BasicClient,\n        train_loop_checkpoint_dir: Path,\n        patience: int | None = 1,\n        interval_steps: int = 5,\n    ) -&gt; None:\n        \"\"\"\n        Early stopping class is a plugin for the client that allows to stop local training based on the validation\n        loss. At each training step this class saves the best state of the client and restores it if the client is\n        stopped. If the client starts to overfit, the early stopper will stop the training process and restore the best\n        state of the client before sending the model to the server.\n\n        Args:\n            client (BasicClient): The client to be monitored.\n            train_loop_checkpoint_dir (Path): Directory to checkpoint the \"best\" state seen so far.\n            patience (int, optional): Number of validation cycles to wait before stopping the training. If it is equal\n                to None client never stops, but still loads the best state before sending the model to the server.\n                Defaults to 1.\n            interval_steps (int): Specifies the frequency, in terms of training intervals, at which the early\n                stopping mechanism should evaluate the validation loss. Defaults to 5.\n        \"\"\"\n        self.client = client\n\n        self.patience = patience\n        self.count_down = patience\n        self.interval_steps = interval_steps\n\n        # Early stopper uses a default name for the state\n        checkpoint_name = f\"temp_{self.client.client_name}.pt\"\n\n        self.state_checkpointer = ClientStateCheckpointer(\n            checkpoint_dir=train_loop_checkpoint_dir, checkpoint_name=checkpoint_name\n        )\n\n        self.best_score: float | None = None\n\n    def load_snapshot(self, attributes: list[str] | None = None) -&gt; None:\n        \"\"\"\n        Load the best snapshot of the client state from the checkpoint directory.\n\n        Args:\n            attributes (list[str] | None, optional): List of attributes to load from the checkpoint.\n                If None, all attributes as defined in ``state_checkpointer`` are loaded. Defaults to None.\n        \"\"\"\n        # Load the best snapshot, and update self.client with the values\n        self.state_checkpointer.maybe_load_client_state(self.client, attributes)\n\n    def should_stop(self, steps: int) -&gt; bool:\n        \"\"\"\n        Determine if the client should stop training based on early stopping criteria.\n\n        Args:\n            steps (int): Number of steps since the start of the training.\n\n        Returns:\n            (bool): True if training should stop, otherwise False.\n        \"\"\"\n        if steps % self.interval_steps != 0:\n            return False\n\n        val_loss, _ = self.client._fully_validate_or_test(\n            loader=self.client.val_loader,\n            loss_meter=self.client.val_loss_meter,\n            metric_manager=self.client.val_metric_manager,\n            logging_mode=LoggingMode.EARLY_STOP_VALIDATION,\n            include_losses_in_metrics=False,\n        )\n\n        if val_loss is None:\n            return False\n\n        if self.best_score is None or val_loss &lt; self.best_score:\n            self.best_score = val_loss\n            self.count_down = self.patience\n            self.state_checkpointer.save_client_state(self.client)\n            return False\n\n        if self.count_down is not None:\n            self.count_down -= 1\n            if self.count_down &lt;= 0:\n                return True\n\n        return False\n</code></pre>"},{"location":"api/#fl4health.utils.early_stopper.EarlyStopper.__init__","title":"<code>__init__(client, train_loop_checkpoint_dir, patience=1, interval_steps=5)</code>","text":"<p>Early stopping class is a plugin for the client that allows to stop local training based on the validation loss. At each training step this class saves the best state of the client and restores it if the client is stopped. If the client starts to overfit, the early stopper will stop the training process and restore the best state of the client before sending the model to the server.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>BasicClient</code> <p>The client to be monitored.</p> required <code>train_loop_checkpoint_dir</code> <code>Path</code> <p>Directory to checkpoint the \"best\" state seen so far.</p> required <code>patience</code> <code>int</code> <p>Number of validation cycles to wait before stopping the training. If it is equal to None client never stops, but still loads the best state before sending the model to the server. Defaults to 1.</p> <code>1</code> <code>interval_steps</code> <code>int</code> <p>Specifies the frequency, in terms of training intervals, at which the early stopping mechanism should evaluate the validation loss. Defaults to 5.</p> <code>5</code> Source code in <code>fl4health/utils/early_stopper.py</code> <pre><code>def __init__(\n    self,\n    client: BasicClient,\n    train_loop_checkpoint_dir: Path,\n    patience: int | None = 1,\n    interval_steps: int = 5,\n) -&gt; None:\n    \"\"\"\n    Early stopping class is a plugin for the client that allows to stop local training based on the validation\n    loss. At each training step this class saves the best state of the client and restores it if the client is\n    stopped. If the client starts to overfit, the early stopper will stop the training process and restore the best\n    state of the client before sending the model to the server.\n\n    Args:\n        client (BasicClient): The client to be monitored.\n        train_loop_checkpoint_dir (Path): Directory to checkpoint the \"best\" state seen so far.\n        patience (int, optional): Number of validation cycles to wait before stopping the training. If it is equal\n            to None client never stops, but still loads the best state before sending the model to the server.\n            Defaults to 1.\n        interval_steps (int): Specifies the frequency, in terms of training intervals, at which the early\n            stopping mechanism should evaluate the validation loss. Defaults to 5.\n    \"\"\"\n    self.client = client\n\n    self.patience = patience\n    self.count_down = patience\n    self.interval_steps = interval_steps\n\n    # Early stopper uses a default name for the state\n    checkpoint_name = f\"temp_{self.client.client_name}.pt\"\n\n    self.state_checkpointer = ClientStateCheckpointer(\n        checkpoint_dir=train_loop_checkpoint_dir, checkpoint_name=checkpoint_name\n    )\n\n    self.best_score: float | None = None\n</code></pre>"},{"location":"api/#fl4health.utils.early_stopper.EarlyStopper.load_snapshot","title":"<code>load_snapshot(attributes=None)</code>","text":"<p>Load the best snapshot of the client state from the checkpoint directory.</p> <p>Parameters:</p> Name Type Description Default <code>attributes</code> <code>list[str] | None</code> <p>List of attributes to load from the checkpoint. If None, all attributes as defined in <code>state_checkpointer</code> are loaded. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/utils/early_stopper.py</code> <pre><code>def load_snapshot(self, attributes: list[str] | None = None) -&gt; None:\n    \"\"\"\n    Load the best snapshot of the client state from the checkpoint directory.\n\n    Args:\n        attributes (list[str] | None, optional): List of attributes to load from the checkpoint.\n            If None, all attributes as defined in ``state_checkpointer`` are loaded. Defaults to None.\n    \"\"\"\n    # Load the best snapshot, and update self.client with the values\n    self.state_checkpointer.maybe_load_client_state(self.client, attributes)\n</code></pre>"},{"location":"api/#fl4health.utils.early_stopper.EarlyStopper.should_stop","title":"<code>should_stop(steps)</code>","text":"<p>Determine if the client should stop training based on early stopping criteria.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of steps since the start of the training.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if training should stop, otherwise False.</p> Source code in <code>fl4health/utils/early_stopper.py</code> <pre><code>def should_stop(self, steps: int) -&gt; bool:\n    \"\"\"\n    Determine if the client should stop training based on early stopping criteria.\n\n    Args:\n        steps (int): Number of steps since the start of the training.\n\n    Returns:\n        (bool): True if training should stop, otherwise False.\n    \"\"\"\n    if steps % self.interval_steps != 0:\n        return False\n\n    val_loss, _ = self.client._fully_validate_or_test(\n        loader=self.client.val_loader,\n        loss_meter=self.client.val_loss_meter,\n        metric_manager=self.client.val_metric_manager,\n        logging_mode=LoggingMode.EARLY_STOP_VALIDATION,\n        include_losses_in_metrics=False,\n    )\n\n    if val_loss is None:\n        return False\n\n    if self.best_score is None or val_loss &lt; self.best_score:\n        self.best_score = val_loss\n        self.count_down = self.patience\n        self.state_checkpointer.save_client_state(self.client)\n        return False\n\n    if self.count_down is not None:\n        self.count_down -= 1\n        if self.count_down &lt;= 0:\n            return True\n\n    return False\n</code></pre>"},{"location":"api/#fl4health.utils.functions","title":"<code>functions</code>","text":""},{"location":"api/#fl4health.utils.functions.BernoulliSample","title":"<code>BernoulliSample</code>","text":"<p>               Bases: <code>Function</code></p> <p>Bernoulli sampling function that allows for gradient computation.</p> <p>Bernoulli sampling is by itself not differentiable, so in order to integrate it with autograd, this implementation follows the paper \"Estimating or propagating gradients through stochastic neurons for conditional computation\" and simply returns the Bernoulli probabilities themselves as the \"gradient.\" This is called the \"straight-through estimator.\" For more details, please see Section 4 of the aforementioned paper (https://arxiv.org/pdf/1308.3432).</p> Source code in <code>fl4health/utils/functions.py</code> <pre><code>class BernoulliSample(torch.autograd.Function):\n    \"\"\"\n    Bernoulli sampling function that allows for gradient computation.\n\n    Bernoulli sampling is by itself not differentiable, so in order to integrate it with autograd,\n    this implementation follows the paper \"Estimating or propagating gradients through stochastic neurons for\n    conditional computation\" and simply returns the Bernoulli probabilities themselves as the \"gradient.\" This is\n    called the \"straight-through estimator.\" For more details, please see Section 4 of the aforementioned paper\n    (https://arxiv.org/pdf/1308.3432).\n    \"\"\"\n\n    @staticmethod\n    def forward(bernoulli_probs: torch.Tensor) -&gt; torch.Tensor:\n        return torch.bernoulli(input=bernoulli_probs)\n\n    @staticmethod\n    # inputs is a Tuple of all of the inputs passed to forward.\n    # output is the output of the forward().\n    def setup_context(ctx: Any, inputs: tuple[torch.Tensor], output: torch.Tensor) -&gt; None:\n        assert len(inputs) == 1\n        (bernoulli_probs,) = inputs\n        ctx.save_for_backward(bernoulli_probs)\n\n    # This method determines the \"gradient\" of the BernoulliSample function.\n    # grad_output is supposed to be the gradient w.r.t. the output of the forward method.\n    @staticmethod\n    def backward(ctx: Any, grad_output: torch.Tensor) -&gt; torch.Tensor:  # type: ignore\n        # ctx.saved_tensors is a tuple (of length 1 in this case). Hence the indexing here.\n        bernoulli_probs = ctx.saved_tensors[0]\n        return bernoulli_probs * grad_output\n</code></pre>"},{"location":"api/#fl4health.utils.functions.select_zeroeth_element","title":"<code>select_zeroeth_element(array)</code>","text":"<p>Helper function that simply selects the first element of an array (index 0 across all dimensions).</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Array from which the very first element is selected.</p> required <p>Returns:</p> Type Description <code>float</code> <p>zeroeth element value.</p> Source code in <code>fl4health/utils/functions.py</code> <pre><code>def select_zeroeth_element(array: np.ndarray) -&gt; float:\n    \"\"\"\n    Helper function that simply selects the first element of an array (index 0 across all dimensions).\n\n    Args:\n        array (np.ndarray): Array from which the very first element is selected.\n\n    Returns:\n        (float): zeroeth element value.\n    \"\"\"\n    indices = tuple(0 for _ in array.shape)\n    return array[indices]\n</code></pre>"},{"location":"api/#fl4health.utils.functions.pseudo_sort_scoring_function","title":"<code>pseudo_sort_scoring_function(client_result)</code>","text":"<p>This function provides the \"score\" that is used to sort a list of <code>tuple[ClientProxy, NDArrays, int]</code>. We select the zeroeth (index 0 across all dimensions) element from each of the arrays in the <code>NDArrays</code> list, sum them, and add the integer (client sample counts) to the sum to come up with a score for sorting. Note that the underlying numpy arrays in <code>NDArrays</code> may not all be of numerical type. So we limit to selecting elements from arrays of floats.</p> <p>Parameters:</p> Name Type Description Default <code>client_result</code> <code>tuple[ClientProxy, NDArrays, int]]</code> <p>Elements to use to determine the score.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Sum of a the zeroeth elements of each array in the <code>NDArrays</code> and the int of the tuple.</p> Source code in <code>fl4health/utils/functions.py</code> <pre><code>def pseudo_sort_scoring_function(client_result: tuple[ClientProxy, NDArrays, int]) -&gt; float:\n    \"\"\"\n    This function provides the \"score\" that is used to sort a list of ``tuple[ClientProxy, NDArrays, int]``. We select\n    the zeroeth (index 0 across all dimensions) element from each of the arrays in the ``NDArrays`` list, sum them, and\n    add the integer (client sample counts) to the sum to come up with a score for sorting. Note that the underlying\n    numpy arrays in ``NDArrays`` may not all be of numerical type. So we limit to selecting elements from arrays of\n    floats.\n\n    Args:\n        client_result (tuple[ClientProxy, NDArrays, int]]): Elements to use to determine the score.\n\n    Returns:\n        (float): Sum of a the zeroeth elements of each array in the ``NDArrays`` and the int of the tuple.\n    \"\"\"\n    _, client_arrays, sample_count = client_result\n    zeroeth_params = [\n        select_zeroeth_element(array) for array in client_arrays if np.issubdtype(array.dtype, np.floating)\n    ]\n    return np.sum(zeroeth_params) + sample_count\n</code></pre>"},{"location":"api/#fl4health.utils.functions.decode_and_pseudo_sort_results","title":"<code>decode_and_pseudo_sort_results(results)</code>","text":"<p>This function is used to convert the results of client training into <code>NDArrays</code> and to apply a pseudo sort based on the zeroeth elements in the weights and the sample counts. As long as the numpy seed has been set on the server this process should be deterministic when repeatedly running the same server code leading to deterministic sorting (assuming the clients are deterministically training their weights as well). This allows, for example, for weights from the clients to be summed in a deterministic order during aggregation.</p> <p>NOTE: Client proxies would be nice to use for this task, but the CIDs are set by uuid deep in the flower library and are, therefore, not pinnable without a ton of work.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>list[tuple[ClientProxy, FitRes]]</code> <p>Results from a federated training round.</p> required <p>Returns:</p> Type Description <code>list[tuple[ClientProxy, NDArrays, int]]</code> <p>The ordered set of weights as <code>NDarrays</code> and the corresponding</p> <code>list[tuple[ClientProxy, NDArrays, int]]</code> <p>number of examples.</p> Source code in <code>fl4health/utils/functions.py</code> <pre><code>def decode_and_pseudo_sort_results(\n    results: list[tuple[ClientProxy, FitRes]],\n) -&gt; list[tuple[ClientProxy, NDArrays, int]]:\n    \"\"\"\n    This function is used to convert the results of client training into ``NDArrays`` and to apply a pseudo sort\n    based on the zeroeth elements in the weights and the sample counts. As long as the numpy seed has been set on the\n    server this process should be deterministic when repeatedly running the same server code leading to deterministic\n    sorting (assuming the clients are deterministically training their weights as well). This allows, for example,\n    for weights from the clients to be summed in a deterministic order during aggregation.\n\n    **NOTE**: Client proxies would be nice to use for this task, but the CIDs are set by uuid deep in the flower\n    library and are, therefore, not pinnable without a ton of work.\n\n    Args:\n        results (list[tuple[ClientProxy, FitRes]]): Results from a federated training round.\n\n    Returns:\n        (list[tuple[ClientProxy, NDArrays, int]]): The ordered set of weights as ``NDarrays`` and the corresponding\n        number of examples.\n    \"\"\"\n    ndarrays_results = [\n        (client_proxy, parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)\n        for client_proxy, fit_res in results\n    ]\n    return sorted(ndarrays_results, key=pseudo_sort_scoring_function)\n</code></pre>"},{"location":"api/#fl4health.utils.load_data","title":"<code>load_data</code>","text":""},{"location":"api/#fl4health.utils.load_data.load_mnist_data","title":"<code>load_mnist_data(data_dir, batch_size, sampler=None, transform=None, target_transform=None, dataset_converter=None, validation_proportion=0.2, hash_key=None)</code>","text":"<p>Load MNIST Dataset (training and validation set).</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the MNIST dataset locally. Dataset is downloaded to this location if it does not already exist.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use for the train and validation dataloader.</p> required <code>sampler</code> <code>LabelBasedSampler | None</code> <p>Optional sampler to subsample dataset based on labels.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Optional transform to be applied to input samples.</p> <code>None</code> <code>target_transform</code> <code>Callable | None</code> <p>Optional transform to be applied to targets.</p> <code>None</code> <code>dataset_converter</code> <code>DatasetConverter | None</code> <p>Optional dataset converter used to convert the input and/or target of train and validation dataset.</p> <code>None</code> <code>validation_proportion</code> <code>float</code> <p>A float between 0 and 1 specifying the proportion of samples to allocate to the validation dataset. Defaults to 0.2.</p> <code>0.2</code> <code>hash_key</code> <code>int | None</code> <p>Optional hash key to create a reproducible split for train and validation datasets.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader, dict[str, int]]</code> <p>The train data loader, validation data loader and a dictionary</p> <code>DataLoader</code> <p>with the sample counts of datasets underpinning the respective data loaders.</p> Source code in <code>fl4health/utils/load_data.py</code> <pre><code>def load_mnist_data(\n    data_dir: Path,\n    batch_size: int,\n    sampler: LabelBasedSampler | None = None,\n    transform: Callable | None = None,\n    target_transform: Callable | None = None,\n    dataset_converter: DatasetConverter | None = None,\n    validation_proportion: float = 0.2,\n    hash_key: int | None = None,\n) -&gt; tuple[DataLoader, DataLoader, dict[str, int]]:\n    \"\"\"\n    Load MNIST Dataset (training and validation set).\n\n    Args:\n        data_dir (Path): The path to the MNIST dataset locally. Dataset is downloaded to this location if it does\n            not already exist.\n        batch_size (int): The batch size to use for the train and validation dataloader.\n        sampler (LabelBasedSampler | None): Optional sampler to subsample dataset based on labels.\n        transform (Callable | None): Optional transform to be applied to input samples.\n        target_transform (Callable | None): Optional transform to be applied to targets.\n        dataset_converter (DatasetConverter | None): Optional dataset converter used to convert the input and/or\n            target of train and validation dataset.\n        validation_proportion (float): A float between 0 and 1 specifying the proportion of samples\n            to allocate to the validation dataset. Defaults to 0.2.\n        hash_key (int | None): Optional hash key to create a reproducible split for train and validation\n            datasets.\n\n    Returns:\n        (tuple[DataLoader, DataLoader, dict[str, int]]): The train data loader, validation data loader and a dictionary\n        with the sample counts of datasets underpinning the respective data loaders.\n    \"\"\"\n    log(INFO, f\"Data directory: {str(data_dir)}\")\n\n    if transform is None:\n        transform = transforms.Compose(\n            [\n                ToNumpy(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5), (0.5)),\n            ]\n        )\n    training_set, validation_set = get_train_and_val_mnist_datasets(\n        data_dir, transform, target_transform, validation_proportion, hash_key\n    )\n\n    if sampler is not None:\n        training_set = sampler.subsample(training_set)\n        validation_set = sampler.subsample(validation_set)\n\n    if dataset_converter is not None:\n        training_set = dataset_converter.convert_dataset(training_set)\n        validation_set = dataset_converter.convert_dataset(validation_set)\n\n    train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n    validation_loader = DataLoader(validation_set, batch_size=batch_size)\n\n    num_examples = {\"train_set\": len(training_set), \"validation_set\": len(validation_set)}\n    return train_loader, validation_loader, num_examples\n</code></pre>"},{"location":"api/#fl4health.utils.load_data.load_mnist_test_data","title":"<code>load_mnist_test_data(data_dir, batch_size, sampler=None, transform=None)</code>","text":"<p>Load MNIST Test Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the MNIST dataset locally. Dataset is downloaded to this location if it does not already exist.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use for the test dataloader.</p> required <code>sampler</code> <code>LabelBasedSampler | None</code> <p>Optional sampler to subsample dataset based on labels.</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Optional transform to be applied to input samples.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, dict[str, int]]</code> <p>The test data loader and a dictionary containing the sample count</p> <code>dict[str, int]</code> <p>of the test dataset.</p> Source code in <code>fl4health/utils/load_data.py</code> <pre><code>def load_mnist_test_data(\n    data_dir: Path,\n    batch_size: int,\n    sampler: LabelBasedSampler | None = None,\n    transform: Callable | None = None,\n) -&gt; tuple[DataLoader, dict[str, int]]:\n    \"\"\"\n    Load MNIST Test Dataset.\n\n    Args:\n        data_dir (Path): The path to the MNIST dataset locally. Dataset is downloaded to this location if it does not\n            already exist.\n        batch_size (int): The batch size to use for the test dataloader.\n        sampler (LabelBasedSampler | None): Optional sampler to subsample dataset based on labels.\n        transform (Callable | None): Optional transform to be applied to input samples.\n\n    Returns:\n        (tuple[DataLoader, dict[str, int]]): The test data loader and a dictionary containing the sample count\n        of the test dataset.\n    \"\"\"\n    log(INFO, f\"Data directory: {str(data_dir)}\")\n\n    if transform is None:\n        transform = transforms.Compose(\n            [\n                ToNumpy(),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5), (0.5)),\n            ]\n        )\n\n    data, targets = get_mnist_data_and_target_tensors(data_dir, False)\n    evaluation_set = TensorDataset(data, targets, transform)\n\n    if sampler is not None:\n        evaluation_set = sampler.subsample(evaluation_set)\n\n    evaluation_loader = DataLoader(evaluation_set, batch_size=batch_size, shuffle=False)\n    num_examples = {\"eval_set\": len(evaluation_set)}\n    return evaluation_loader, num_examples\n</code></pre>"},{"location":"api/#fl4health.utils.load_data.load_cifar10_data","title":"<code>load_cifar10_data(data_dir, batch_size, sampler=None, validation_proportion=0.2, hash_key=None)</code>","text":"<p>Load CIFAR10 Dataset (training and validation set).</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the CIFAR10 dataset locally. Dataset is downloaded to this location if it does not already exist.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use for the train and validation dataloader.</p> required <code>sampler</code> <code>LabelBasedSampler | None</code> <p>Optional sampler to subsample dataset based on labels.</p> <code>None</code> <code>validation_proportion</code> <code>float</code> <p>A float between 0 and 1 specifying the proportion of samples to allocate to the validation dataset. Defaults to 0.2.</p> <code>0.2</code> <code>hash_key</code> <code>int | None</code> <p>Optional hash key to create a reproducible split for train and validation datasets.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader, dict[str, int]]</code> <p>The train data loader, validation data loader and a dictionary</p> <code>DataLoader</code> <p>with the sample counts of datasets underpinning the respective data loaders.</p> Source code in <code>fl4health/utils/load_data.py</code> <pre><code>def load_cifar10_data(\n    data_dir: Path,\n    batch_size: int,\n    sampler: LabelBasedSampler | None = None,\n    validation_proportion: float = 0.2,\n    hash_key: int | None = None,\n) -&gt; tuple[DataLoader, DataLoader, dict[str, int]]:\n    \"\"\"\n    Load CIFAR10 Dataset (training and validation set).\n\n    Args:\n        data_dir (Path): The path to the CIFAR10 dataset locally. Dataset is downloaded to this location if it does\n            not already exist.\n        batch_size (int): The batch size to use for the train and validation dataloader.\n        sampler (LabelBasedSampler | None): Optional sampler to subsample dataset based on labels.\n        validation_proportion (float): A float between 0 and 1 specifying the proportion of samples to allocate to the\n            validation dataset. Defaults to 0.2.\n        hash_key (int | None): Optional hash key to create a reproducible split for train and validation\n            datasets.\n\n    Returns:\n        (tuple[DataLoader, DataLoader, dict[str, int]]): The train data loader, validation data loader and a dictionary\n        with the sample counts of datasets underpinning the respective data loaders.\n    \"\"\"\n    log(INFO, f\"Data directory: {str(data_dir)}\")\n\n    transform = transforms.Compose(\n        [\n            ToNumpy(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )\n    training_set, validation_set = get_train_and_val_cifar10_datasets(\n        data_dir, transform, None, validation_proportion, hash_key\n    )\n\n    if sampler is not None:\n        training_set = sampler.subsample(training_set)\n        validation_set = sampler.subsample(validation_set)\n\n    train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n    validation_loader = DataLoader(validation_set, batch_size=batch_size)\n    num_examples = {\n        \"train_set\": len(training_set),\n        \"validation_set\": len(validation_set),\n    }\n    return train_loader, validation_loader, num_examples\n</code></pre>"},{"location":"api/#fl4health.utils.load_data.load_cifar10_test_data","title":"<code>load_cifar10_test_data(data_dir, batch_size, sampler=None)</code>","text":"<p>Load CIFAR10 Test Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>The path to the CIFAR10 dataset locally. Dataset is downloaded to this location if it does not already exist.</p> required <code>batch_size</code> <code>int</code> <p>The batch size to use for the test dataloader.</p> required <code>sampler</code> <code>LabelBasedSampler | None</code> <p>Optional sampler to subsample dataset based on labels.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, dict[str, int]]</code> <p>The test data loader and a dictionary containing the sample count of the</p> <code>dict[str, int]</code> <p>test dataset.</p> Source code in <code>fl4health/utils/load_data.py</code> <pre><code>def load_cifar10_test_data(\n    data_dir: Path, batch_size: int, sampler: LabelBasedSampler | None = None\n) -&gt; tuple[DataLoader, dict[str, int]]:\n    \"\"\"\n    Load CIFAR10 Test Dataset.\n\n    Args:\n        data_dir (Path): The path to the CIFAR10 dataset locally. Dataset is downloaded to this location if it does\n            not already exist.\n        batch_size (int): The batch size to use for the test dataloader.\n        sampler (LabelBasedSampler | None): Optional sampler to subsample dataset based on labels.\n\n    Returns:\n        (tuple[DataLoader, dict[str, int]]): The test data loader and a dictionary containing the sample count of the\n        test dataset.\n    \"\"\"\n    log(INFO, f\"Data directory: {str(data_dir)}\")\n    transform = transforms.Compose(\n        [\n            ToNumpy(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )\n    data, targets = get_cifar10_data_and_target_tensors(data_dir, False)\n    evaluation_set = TensorDataset(data, targets, transform)\n\n    if sampler is not None:\n        evaluation_set = sampler.subsample(evaluation_set)\n\n    evaluation_loader = DataLoader(evaluation_set, batch_size=batch_size, shuffle=False)\n    num_examples = {\"eval_set\": len(evaluation_set)}\n    return evaluation_loader, num_examples\n</code></pre>"},{"location":"api/#fl4health.utils.load_data.load_msd_dataset","title":"<code>load_msd_dataset(data_path, msd_dataset_name)</code>","text":"<p>Downloads and extracts one of the 10 Medical Segmentation Decathelon (MSD) datasets.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Path to the folder in which to extract the dataset. The data itself will be in a subfolder named after the dataset, not in the <code>data_path</code> directory itself. The name of the folder will be the name of the dataset as defined by the values of the <code>MsdDataset</code> enum returned by <code>get_msd_dataset_enum</code></p> required <code>msd_dataset_name</code> <code>str</code> <p>One of the 10 msd datasets</p> required Source code in <code>fl4health/utils/load_data.py</code> <pre><code>def load_msd_dataset(data_path: str, msd_dataset_name: str) -&gt; None:\n    \"\"\"\n    Downloads and extracts one of the 10 Medical Segmentation Decathelon (MSD) datasets.\n\n    Args:\n        data_path (str): Path to the folder in which to extract the dataset. The data itself will be in a subfolder\n            named after the dataset, not in the ``data_path`` directory itself. The name of the folder will be the\n            name of the dataset as defined by the values of the ``MsdDataset`` enum returned by\n            ``get_msd_dataset_enum``\n        msd_dataset_name (str): One of the 10 msd datasets\n    \"\"\"\n    msd_enum = get_msd_dataset_enum(msd_dataset_name)\n    msd_hash = msd_md5_hashes[msd_enum]\n    url = msd_urls[msd_enum]\n    download_and_extract(url=url, output_dir=data_path, hash_val=msd_hash, hash_type=\"md5\", progress=True)\n</code></pre>"},{"location":"api/#fl4health.utils.losses","title":"<code>losses</code>","text":""},{"location":"api/#fl4health.utils.losses.Losses","title":"<code>Losses</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>class Losses(ABC):\n    def __init__(self, additional_losses: dict[str, torch.Tensor] | None = None) -&gt; None:\n        \"\"\"\n        An abstract class to store the losses.\n\n        Args:\n            additional_losses (dict[str, torch.Tensor] | None): Optional dictionary of additional losses.\n        \"\"\"\n        self.additional_losses = additional_losses if additional_losses else {}\n\n    def as_dict(self) -&gt; dict[str, float]:\n        \"\"\"\n        Produces a dictionary representation of the object with all of the losses.\n\n        Returns:\n            (dict[str, float]): A dictionary with the additional losses if they exist.\n        \"\"\"\n        loss_dict: dict[str, float] = {}\n\n        if self.additional_losses is not None:\n            for key, val in self.additional_losses.items():\n                loss_dict[key] = float(val.item())\n\n        return loss_dict\n\n    @staticmethod\n    @abstractmethod\n    def aggregate(loss_meter: LossMeter) -&gt; Losses:\n        \"\"\"\n        Aggregates the losses in the given ``LossMeter`` into an instance of Losses.\n\n        Args:\n            loss_meter (LossMeter): The loss meter object with the collected losses.\n\n        Raises:\n            NotImplementedError: To be implemented by child classes.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.utils.losses.Losses.__init__","title":"<code>__init__(additional_losses=None)</code>","text":"<p>An abstract class to store the losses.</p> <p>Parameters:</p> Name Type Description Default <code>additional_losses</code> <code>dict[str, Tensor] | None</code> <p>Optional dictionary of additional losses.</p> <code>None</code> Source code in <code>fl4health/utils/losses.py</code> <pre><code>def __init__(self, additional_losses: dict[str, torch.Tensor] | None = None) -&gt; None:\n    \"\"\"\n    An abstract class to store the losses.\n\n    Args:\n        additional_losses (dict[str, torch.Tensor] | None): Optional dictionary of additional losses.\n    \"\"\"\n    self.additional_losses = additional_losses if additional_losses else {}\n</code></pre>"},{"location":"api/#fl4health.utils.losses.Losses.as_dict","title":"<code>as_dict()</code>","text":"<p>Produces a dictionary representation of the object with all of the losses.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dictionary with the additional losses if they exist.</p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>def as_dict(self) -&gt; dict[str, float]:\n    \"\"\"\n    Produces a dictionary representation of the object with all of the losses.\n\n    Returns:\n        (dict[str, float]): A dictionary with the additional losses if they exist.\n    \"\"\"\n    loss_dict: dict[str, float] = {}\n\n    if self.additional_losses is not None:\n        for key, val in self.additional_losses.items():\n            loss_dict[key] = float(val.item())\n\n    return loss_dict\n</code></pre>"},{"location":"api/#fl4health.utils.losses.Losses.aggregate","title":"<code>aggregate(loss_meter)</code>  <code>abstractmethod</code> <code>staticmethod</code>","text":"<p>Aggregates the losses in the given <code>LossMeter</code> into an instance of Losses.</p> <p>Parameters:</p> Name Type Description Default <code>loss_meter</code> <code>LossMeter</code> <p>The loss meter object with the collected losses.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>To be implemented by child classes.</p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>@staticmethod\n@abstractmethod\ndef aggregate(loss_meter: LossMeter) -&gt; Losses:\n    \"\"\"\n    Aggregates the losses in the given ``LossMeter`` into an instance of Losses.\n\n    Args:\n        loss_meter (LossMeter): The loss meter object with the collected losses.\n\n    Raises:\n        NotImplementedError: To be implemented by child classes.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.utils.losses.EvaluationLosses","title":"<code>EvaluationLosses</code>","text":"<p>               Bases: <code>Losses</code></p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>class EvaluationLosses(Losses):\n    def __init__(self, checkpoint: torch.Tensor, additional_losses: dict[str, torch.Tensor] | None = None) -&gt; None:\n        \"\"\"\n        A class to store the checkpoint and ``additional_losses`` of a model along with a method to return a\n        dictionary representation.\n\n        Args:\n            checkpoint (torch.Tensor): The loss used to checkpoint model (if checkpointing is enabled).\n            additional_losses (dict[str, torch.Tensor] | None): Optional dictionary of additional losses.\n        \"\"\"\n        super().__init__(additional_losses)\n        self.checkpoint = checkpoint\n\n    def as_dict(self) -&gt; dict[str, float]:\n        \"\"\"\n        Produces a dictionary representation of the object with all of the losses.\n\n        Returns:\n            (dict[str, float]): A dictionary with the checkpoint loss, plus each one of the keys in additional losses\n                if they exist.\n        \"\"\"\n        loss_dict = super().as_dict()\n        loss_dict[\"checkpoint\"] = float(self.checkpoint.item())\n        return loss_dict\n\n    @staticmethod\n    def aggregate(loss_meter: LossMeter[EvaluationLosses]) -&gt; EvaluationLosses:\n        \"\"\"\n        Aggregates the losses in the given ``LossMeter`` into An instance of ``EvaluationLosses``.\n\n        Args:\n            loss_meter (LossMeter[EvaluationLosses]): The loss meter object with the collected evaluation losses.\n\n        Returns:\n            (EvaluationLosses): An instance of ``EvaluationLosses`` with the aggregated losses.\n        \"\"\"\n        checkpoint_loss = torch.sum(\n            torch.FloatTensor([losses.checkpoint for losses in loss_meter.losses_list])  # type: ignore\n        )\n        if loss_meter.loss_meter_type == LossMeterType.AVERAGE:\n            checkpoint_loss /= len(loss_meter.losses_list)\n\n        additional_losses_list = [losses.additional_losses for losses in loss_meter.losses_list]\n        additional_losses_dict = LossMeter.aggregate_losses_dict(additional_losses_list, loss_meter.loss_meter_type)\n\n        return EvaluationLosses(checkpoint=checkpoint_loss, additional_losses=additional_losses_dict)\n</code></pre>"},{"location":"api/#fl4health.utils.losses.EvaluationLosses.__init__","title":"<code>__init__(checkpoint, additional_losses=None)</code>","text":"<p>A class to store the checkpoint and <code>additional_losses</code> of a model along with a method to return a dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>Tensor</code> <p>The loss used to checkpoint model (if checkpointing is enabled).</p> required <code>additional_losses</code> <code>dict[str, Tensor] | None</code> <p>Optional dictionary of additional losses.</p> <code>None</code> Source code in <code>fl4health/utils/losses.py</code> <pre><code>def __init__(self, checkpoint: torch.Tensor, additional_losses: dict[str, torch.Tensor] | None = None) -&gt; None:\n    \"\"\"\n    A class to store the checkpoint and ``additional_losses`` of a model along with a method to return a\n    dictionary representation.\n\n    Args:\n        checkpoint (torch.Tensor): The loss used to checkpoint model (if checkpointing is enabled).\n        additional_losses (dict[str, torch.Tensor] | None): Optional dictionary of additional losses.\n    \"\"\"\n    super().__init__(additional_losses)\n    self.checkpoint = checkpoint\n</code></pre>"},{"location":"api/#fl4health.utils.losses.EvaluationLosses.as_dict","title":"<code>as_dict()</code>","text":"<p>Produces a dictionary representation of the object with all of the losses.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dictionary with the checkpoint loss, plus each one of the keys in additional losses if they exist.</p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>def as_dict(self) -&gt; dict[str, float]:\n    \"\"\"\n    Produces a dictionary representation of the object with all of the losses.\n\n    Returns:\n        (dict[str, float]): A dictionary with the checkpoint loss, plus each one of the keys in additional losses\n            if they exist.\n    \"\"\"\n    loss_dict = super().as_dict()\n    loss_dict[\"checkpoint\"] = float(self.checkpoint.item())\n    return loss_dict\n</code></pre>"},{"location":"api/#fl4health.utils.losses.EvaluationLosses.aggregate","title":"<code>aggregate(loss_meter)</code>  <code>staticmethod</code>","text":"<p>Aggregates the losses in the given <code>LossMeter</code> into An instance of <code>EvaluationLosses</code>.</p> <p>Parameters:</p> Name Type Description Default <code>loss_meter</code> <code>LossMeter[EvaluationLosses]</code> <p>The loss meter object with the collected evaluation losses.</p> required <p>Returns:</p> Type Description <code>EvaluationLosses</code> <p>An instance of <code>EvaluationLosses</code> with the aggregated losses.</p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>@staticmethod\ndef aggregate(loss_meter: LossMeter[EvaluationLosses]) -&gt; EvaluationLosses:\n    \"\"\"\n    Aggregates the losses in the given ``LossMeter`` into An instance of ``EvaluationLosses``.\n\n    Args:\n        loss_meter (LossMeter[EvaluationLosses]): The loss meter object with the collected evaluation losses.\n\n    Returns:\n        (EvaluationLosses): An instance of ``EvaluationLosses`` with the aggregated losses.\n    \"\"\"\n    checkpoint_loss = torch.sum(\n        torch.FloatTensor([losses.checkpoint for losses in loss_meter.losses_list])  # type: ignore\n    )\n    if loss_meter.loss_meter_type == LossMeterType.AVERAGE:\n        checkpoint_loss /= len(loss_meter.losses_list)\n\n    additional_losses_list = [losses.additional_losses for losses in loss_meter.losses_list]\n    additional_losses_dict = LossMeter.aggregate_losses_dict(additional_losses_list, loss_meter.loss_meter_type)\n\n    return EvaluationLosses(checkpoint=checkpoint_loss, additional_losses=additional_losses_dict)\n</code></pre>"},{"location":"api/#fl4health.utils.losses.TrainingLosses","title":"<code>TrainingLosses</code>","text":"<p>               Bases: <code>Losses</code></p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>class TrainingLosses(Losses):\n    def __init__(\n        self,\n        backward: torch.Tensor | dict[str, torch.Tensor],\n        additional_losses: dict[str, torch.Tensor] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        A class to store the backward and ``additional_losses`` of a model along with a method to return a dictionary\n        representation.\n\n        Args:\n            backward (torch.Tensor | dict[str, torch.Tensor]): The backward loss or\n                losses to optimize. In the normal case, backward is a Tensor corresponding to the\n                loss of a model. In the case of an ``ensemble_model``, backward is dictionary of losses.\n            additional_losses (dict[str, torch.Tensor] | None): Optional dictionary of additional losses.\n        \"\"\"\n        super().__init__(additional_losses)\n        self.backward = backward if isinstance(backward, dict) else {\"backward\": backward}\n\n    def as_dict(self) -&gt; dict[str, float]:\n        \"\"\"\n        Produces a dictionary representation of the object with all of the losses.\n\n        Returns:\n            (dict[str, float]): A dictionary where each key represents one of the  backward losses, plus additional\n                losses if they exist.\n        \"\"\"\n        loss_dict = super().as_dict()\n\n        backward = {key: float(loss.item()) for key, loss in self.backward.items()}\n        loss_dict.update(backward)\n\n        return loss_dict\n\n    @staticmethod\n    def aggregate(loss_meter: LossMeter[TrainingLosses]) -&gt; TrainingLosses:\n        \"\"\"\n        Aggregates the losses in the given ``LossMeter`` into An instance of ``TrainingLosses``.\n\n        Args:\n            loss_meter (LossMeter[TrainingLosses]): The loss meter object with the collected training losses.\n\n        Returns:\n            (TrainingLosses): An instance of ``TrainingLosses`` with the aggregated losses.\n        \"\"\"\n        additional_losses_list = [losses.additional_losses for losses in loss_meter.losses_list]\n        additional_losses_dict = LossMeter.aggregate_losses_dict(additional_losses_list, loss_meter.loss_meter_type)\n\n        backward_losses_list = [losses.backward for losses in loss_meter.losses_list]  # type: ignore\n        if len(backward_losses_list) &gt; 0 and isinstance(backward_losses_list[0], dict):\n            # if backward losses is a dictionary, aggregate the dictionary keys\n            backward_losses_dict = LossMeter.aggregate_losses_dict(backward_losses_list, loss_meter.loss_meter_type)\n            return TrainingLosses(backward=backward_losses_dict, additional_losses=additional_losses_dict)\n\n        # otherwise, calculate the average tensor\n        backward_losses = torch.sum(torch.FloatTensor(backward_losses_list))\n        if loss_meter.loss_meter_type == LossMeterType.AVERAGE:\n            backward_losses /= len(loss_meter.losses_list)\n\n        return TrainingLosses(backward=backward_losses, additional_losses=additional_losses_dict)\n</code></pre>"},{"location":"api/#fl4health.utils.losses.TrainingLosses.__init__","title":"<code>__init__(backward, additional_losses=None)</code>","text":"<p>A class to store the backward and <code>additional_losses</code> of a model along with a method to return a dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>backward</code> <code>Tensor | dict[str, Tensor]</code> <p>The backward loss or losses to optimize. In the normal case, backward is a Tensor corresponding to the loss of a model. In the case of an <code>ensemble_model</code>, backward is dictionary of losses.</p> required <code>additional_losses</code> <code>dict[str, Tensor] | None</code> <p>Optional dictionary of additional losses.</p> <code>None</code> Source code in <code>fl4health/utils/losses.py</code> <pre><code>def __init__(\n    self,\n    backward: torch.Tensor | dict[str, torch.Tensor],\n    additional_losses: dict[str, torch.Tensor] | None = None,\n) -&gt; None:\n    \"\"\"\n    A class to store the backward and ``additional_losses`` of a model along with a method to return a dictionary\n    representation.\n\n    Args:\n        backward (torch.Tensor | dict[str, torch.Tensor]): The backward loss or\n            losses to optimize. In the normal case, backward is a Tensor corresponding to the\n            loss of a model. In the case of an ``ensemble_model``, backward is dictionary of losses.\n        additional_losses (dict[str, torch.Tensor] | None): Optional dictionary of additional losses.\n    \"\"\"\n    super().__init__(additional_losses)\n    self.backward = backward if isinstance(backward, dict) else {\"backward\": backward}\n</code></pre>"},{"location":"api/#fl4health.utils.losses.TrainingLosses.as_dict","title":"<code>as_dict()</code>","text":"<p>Produces a dictionary representation of the object with all of the losses.</p> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>A dictionary where each key represents one of the  backward losses, plus additional losses if they exist.</p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>def as_dict(self) -&gt; dict[str, float]:\n    \"\"\"\n    Produces a dictionary representation of the object with all of the losses.\n\n    Returns:\n        (dict[str, float]): A dictionary where each key represents one of the  backward losses, plus additional\n            losses if they exist.\n    \"\"\"\n    loss_dict = super().as_dict()\n\n    backward = {key: float(loss.item()) for key, loss in self.backward.items()}\n    loss_dict.update(backward)\n\n    return loss_dict\n</code></pre>"},{"location":"api/#fl4health.utils.losses.TrainingLosses.aggregate","title":"<code>aggregate(loss_meter)</code>  <code>staticmethod</code>","text":"<p>Aggregates the losses in the given <code>LossMeter</code> into An instance of <code>TrainingLosses</code>.</p> <p>Parameters:</p> Name Type Description Default <code>loss_meter</code> <code>LossMeter[TrainingLosses]</code> <p>The loss meter object with the collected training losses.</p> required <p>Returns:</p> Type Description <code>TrainingLosses</code> <p>An instance of <code>TrainingLosses</code> with the aggregated losses.</p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>@staticmethod\ndef aggregate(loss_meter: LossMeter[TrainingLosses]) -&gt; TrainingLosses:\n    \"\"\"\n    Aggregates the losses in the given ``LossMeter`` into An instance of ``TrainingLosses``.\n\n    Args:\n        loss_meter (LossMeter[TrainingLosses]): The loss meter object with the collected training losses.\n\n    Returns:\n        (TrainingLosses): An instance of ``TrainingLosses`` with the aggregated losses.\n    \"\"\"\n    additional_losses_list = [losses.additional_losses for losses in loss_meter.losses_list]\n    additional_losses_dict = LossMeter.aggregate_losses_dict(additional_losses_list, loss_meter.loss_meter_type)\n\n    backward_losses_list = [losses.backward for losses in loss_meter.losses_list]  # type: ignore\n    if len(backward_losses_list) &gt; 0 and isinstance(backward_losses_list[0], dict):\n        # if backward losses is a dictionary, aggregate the dictionary keys\n        backward_losses_dict = LossMeter.aggregate_losses_dict(backward_losses_list, loss_meter.loss_meter_type)\n        return TrainingLosses(backward=backward_losses_dict, additional_losses=additional_losses_dict)\n\n    # otherwise, calculate the average tensor\n    backward_losses = torch.sum(torch.FloatTensor(backward_losses_list))\n    if loss_meter.loss_meter_type == LossMeterType.AVERAGE:\n        backward_losses /= len(loss_meter.losses_list)\n\n    return TrainingLosses(backward=backward_losses, additional_losses=additional_losses_dict)\n</code></pre>"},{"location":"api/#fl4health.utils.losses.LossMeter","title":"<code>LossMeter</code>","text":"<p>               Bases: <code>Generic[LossesType]</code></p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>class LossMeter(Generic[LossesType]):\n    def __init__(self, loss_meter_type: LossMeterType, losses_type: type[LossesType]) -&gt; None:\n        \"\"\"\n        A meter to store a list of losses.\n\n        Args:\n            loss_meter_type (LossMeterType): The type of this loss meter\n            losses_type (type[Losses]): The type of the loss that will be stored. Should be one of the subclasses of\n                Losses\n\n        \"\"\"\n        self.losses_list: list[LossesType] = []\n        self.loss_meter_type = loss_meter_type\n        self.losses_type = losses_type\n\n    def update(self, losses: LossesType) -&gt; None:\n        \"\"\"\n        Appends loss to list of losses.\n\n        Args:\n            losses (LossesType): A losses object with checkpoint, backward and additional losses.\n        \"\"\"\n        self.losses_list.append(losses)\n\n    def clear(self) -&gt; None:\n        \"\"\"Resets the meter by re-initializing ``losses_list`` to be empty.\"\"\"\n        self.losses_list = []\n\n    def compute(self) -&gt; LossesType:\n        \"\"\"\n        Computes the aggregation of current list of losses if non-empty.\n\n        Returns:\n            (LossesType): New Losses object with the aggregation of losses in ``losses_list``.\n        \"\"\"\n        assert len(self.losses_list) &gt; 0\n        return self.losses_type.aggregate(self)  # type: ignore\n\n    @staticmethod\n    def aggregate_losses_dict(\n        loss_list: list[dict[str, torch.Tensor]],\n        loss_meter_type: LossMeterType,\n    ) -&gt; dict[str, torch.Tensor]:\n        \"\"\"\n        Aggregates a list of losses dictionaries into a single dictionary according to the loss meter aggregation type.\n\n        Args:\n            loss_list (list[dict[str, torch.Tensor]]): A list of loss dictionaries.\n            loss_meter_type (LossMeterType): The type of the loss meter to perform the aggregation.\n\n        Returns:\n            (dict[str, torch.Tensor]): A single dictionary with the aggregated losses according to the given loss\n                meter type.\n        \"\"\"\n        # We don't know the keys of the dict (backward or additional losses) beforehand. We don't obtain them\n        # from the first entry because losses can have different keys. We get list of all the keys from\n        # all the losses.\n        loss_keys = {key for loss_dict_ in loss_list for key in loss_dict_}\n        loss_dict: dict[str, torch.Tensor] = {}\n        for key in loss_keys:\n            if loss_meter_type == LossMeterType.AVERAGE:\n                loss = torch.mean(torch.FloatTensor([loss[key] for loss in loss_list if key in loss]))\n            else:\n                loss = torch.sum(torch.FloatTensor([loss[key] for loss in loss_list if key in loss]))\n            loss_dict[key] = loss\n\n        return loss_dict\n</code></pre>"},{"location":"api/#fl4health.utils.losses.LossMeter.__init__","title":"<code>__init__(loss_meter_type, losses_type)</code>","text":"<p>A meter to store a list of losses.</p> <p>Parameters:</p> Name Type Description Default <code>loss_meter_type</code> <code>LossMeterType</code> <p>The type of this loss meter</p> required <code>losses_type</code> <code>type[Losses]</code> <p>The type of the loss that will be stored. Should be one of the subclasses of Losses</p> required Source code in <code>fl4health/utils/losses.py</code> <pre><code>def __init__(self, loss_meter_type: LossMeterType, losses_type: type[LossesType]) -&gt; None:\n    \"\"\"\n    A meter to store a list of losses.\n\n    Args:\n        loss_meter_type (LossMeterType): The type of this loss meter\n        losses_type (type[Losses]): The type of the loss that will be stored. Should be one of the subclasses of\n            Losses\n\n    \"\"\"\n    self.losses_list: list[LossesType] = []\n    self.loss_meter_type = loss_meter_type\n    self.losses_type = losses_type\n</code></pre>"},{"location":"api/#fl4health.utils.losses.LossMeter.update","title":"<code>update(losses)</code>","text":"<p>Appends loss to list of losses.</p> <p>Parameters:</p> Name Type Description Default <code>losses</code> <code>LossesType</code> <p>A losses object with checkpoint, backward and additional losses.</p> required Source code in <code>fl4health/utils/losses.py</code> <pre><code>def update(self, losses: LossesType) -&gt; None:\n    \"\"\"\n    Appends loss to list of losses.\n\n    Args:\n        losses (LossesType): A losses object with checkpoint, backward and additional losses.\n    \"\"\"\n    self.losses_list.append(losses)\n</code></pre>"},{"location":"api/#fl4health.utils.losses.LossMeter.clear","title":"<code>clear()</code>","text":"<p>Resets the meter by re-initializing <code>losses_list</code> to be empty.</p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Resets the meter by re-initializing ``losses_list`` to be empty.\"\"\"\n    self.losses_list = []\n</code></pre>"},{"location":"api/#fl4health.utils.losses.LossMeter.compute","title":"<code>compute()</code>","text":"<p>Computes the aggregation of current list of losses if non-empty.</p> <p>Returns:</p> Type Description <code>LossesType</code> <p>New Losses object with the aggregation of losses in <code>losses_list</code>.</p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>def compute(self) -&gt; LossesType:\n    \"\"\"\n    Computes the aggregation of current list of losses if non-empty.\n\n    Returns:\n        (LossesType): New Losses object with the aggregation of losses in ``losses_list``.\n    \"\"\"\n    assert len(self.losses_list) &gt; 0\n    return self.losses_type.aggregate(self)  # type: ignore\n</code></pre>"},{"location":"api/#fl4health.utils.losses.LossMeter.aggregate_losses_dict","title":"<code>aggregate_losses_dict(loss_list, loss_meter_type)</code>  <code>staticmethod</code>","text":"<p>Aggregates a list of losses dictionaries into a single dictionary according to the loss meter aggregation type.</p> <p>Parameters:</p> Name Type Description Default <code>loss_list</code> <code>list[dict[str, Tensor]]</code> <p>A list of loss dictionaries.</p> required <code>loss_meter_type</code> <code>LossMeterType</code> <p>The type of the loss meter to perform the aggregation.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>A single dictionary with the aggregated losses according to the given loss meter type.</p> Source code in <code>fl4health/utils/losses.py</code> <pre><code>@staticmethod\ndef aggregate_losses_dict(\n    loss_list: list[dict[str, torch.Tensor]],\n    loss_meter_type: LossMeterType,\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Aggregates a list of losses dictionaries into a single dictionary according to the loss meter aggregation type.\n\n    Args:\n        loss_list (list[dict[str, torch.Tensor]]): A list of loss dictionaries.\n        loss_meter_type (LossMeterType): The type of the loss meter to perform the aggregation.\n\n    Returns:\n        (dict[str, torch.Tensor]): A single dictionary with the aggregated losses according to the given loss\n            meter type.\n    \"\"\"\n    # We don't know the keys of the dict (backward or additional losses) beforehand. We don't obtain them\n    # from the first entry because losses can have different keys. We get list of all the keys from\n    # all the losses.\n    loss_keys = {key for loss_dict_ in loss_list for key in loss_dict_}\n    loss_dict: dict[str, torch.Tensor] = {}\n    for key in loss_keys:\n        if loss_meter_type == LossMeterType.AVERAGE:\n            loss = torch.mean(torch.FloatTensor([loss[key] for loss in loss_list if key in loss]))\n        else:\n            loss = torch.sum(torch.FloatTensor([loss[key] for loss in loss_list if key in loss]))\n        loss_dict[key] = loss\n\n    return loss_dict\n</code></pre>"},{"location":"api/#fl4health.utils.metrics","title":"<code>metrics</code>","text":""},{"location":"api/#fl4health.utils.metrics.F1","title":"<code>F1</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class F1(SimpleMetric):\n    def __init__(\n        self,\n        name: str = \"F1 score\",\n        average: str | None = \"weighted\",\n    ):\n        \"\"\"\n        Computes the F1 score using the ``sklearn f1_score`` function. As such, the values of average correspond to\n        those of that function.\n\n        Args:\n            name (str, optional): Name of the metric. Defaults to \"F1 score\".\n            average (str | None, optional): Whether to perform averaging of the F1 scores and how. The values of this\n                string corresponds to those of the ``sklearn f1_score function``. See:\n\n                https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n\n                Defaults to \"weighted\".\n        \"\"\"\n        super().__init__(name)\n        self.average = average\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        assert logits.shape[0] == target.shape[0]\n        target = target.cpu().detach()\n        logits = logits.cpu().detach()\n        y_true = target.reshape(-1)\n        preds = np.argmax(logits, axis=1)\n        return sklearn_metrics.f1_score(y_true, preds, average=self.average)\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.F1.__init__","title":"<code>__init__(name='F1 score', average='weighted')</code>","text":"<p>Computes the F1 score using the <code>sklearn f1_score</code> function. As such, the values of average correspond to those of that function.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric. Defaults to \"F1 score\".</p> <code>'F1 score'</code> <code>average</code> <code>str | None</code> <p>Whether to perform averaging of the F1 scores and how. The values of this string corresponds to those of the <code>sklearn f1_score function</code>. See:</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html</p> <p>Defaults to \"weighted\".</p> <code>'weighted'</code> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"F1 score\",\n    average: str | None = \"weighted\",\n):\n    \"\"\"\n    Computes the F1 score using the ``sklearn f1_score`` function. As such, the values of average correspond to\n    those of that function.\n\n    Args:\n        name (str, optional): Name of the metric. Defaults to \"F1 score\".\n        average (str | None, optional): Whether to perform averaging of the F1 scores and how. The values of this\n            string corresponds to those of the ``sklearn f1_score function``. See:\n\n            https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n\n            Defaults to \"weighted\".\n    \"\"\"\n    super().__init__(name)\n    self.average = average\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.Accuracy","title":"<code>Accuracy</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class Accuracy(SimpleMetric):\n    def __init__(self, name: str = \"accuracy\"):\n        \"\"\"\n        Accuracy metric for classification tasks.\n\n        Args:\n            name (str): The name of the metric.\n\n        \"\"\"\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor, threshold: float = 0.5) -&gt; Scalar:\n        # assuming batch first\n        assert logits.shape[0] == target.shape[0]\n        # Single value output, assume binary logits\n        preds = (\n            (logits &gt; threshold).int() if len(logits.shape) == 1 or logits.shape[1] == 1 else torch.argmax(logits, 1)\n        )\n        target = target.cpu().detach()\n        preds = preds.cpu().detach()\n        return sklearn_metrics.accuracy_score(target, preds)\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.Accuracy.__init__","title":"<code>__init__(name='accuracy')</code>","text":"<p>Accuracy metric for classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> <code>'accuracy'</code> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str = \"accuracy\"):\n    \"\"\"\n    Accuracy metric for classification tasks.\n\n    Args:\n        name (str): The name of the metric.\n\n    \"\"\"\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.BalancedAccuracy","title":"<code>BalancedAccuracy</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class BalancedAccuracy(SimpleMetric):\n    def __init__(self, name: str = \"balanced_accuracy\"):\n        \"\"\"\n        Balanced accuracy metric for classification tasks. Used for the evaluation of imbalanced datasets.\n\n        For more information:\n\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n        \"\"\"\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        # assuming batch first\n        assert logits.shape[0] == target.shape[0]\n        target = target.cpu().detach()\n        logits = logits.cpu().detach()\n        y_true = target.reshape(-1)\n        preds = np.argmax(logits, axis=1)\n        return sklearn_metrics.balanced_accuracy_score(y_true, preds)\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.BalancedAccuracy.__init__","title":"<code>__init__(name='balanced_accuracy')</code>","text":"<p>Balanced accuracy metric for classification tasks. Used for the evaluation of imbalanced datasets.</p> <p>For more information:</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str = \"balanced_accuracy\"):\n    \"\"\"\n    Balanced accuracy metric for classification tasks. Used for the evaluation of imbalanced datasets.\n\n    For more information:\n\n    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html\n    \"\"\"\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.BinarySoftDiceCoefficient","title":"<code>BinarySoftDiceCoefficient</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class BinarySoftDiceCoefficient(SimpleMetric):\n    def __init__(\n        self,\n        name: str = \"BinarySoftDiceCoefficient\",\n        epsilon: float = 1.0e-7,\n        spatial_dimensions: tuple[int, ...] = (2, 3, 4),\n        logits_threshold: float | None = 0.5,\n    ):\n        \"\"\"\n        Binary DICE Coefficient Metric with configurable spatial dimensions and logits threshold.\n\n        Args:\n            name (str): Name of the metric.\n            epsilon (float): Small float to add to denominator of DICE calculation to avoid divide by 0.\n            spatial_dimensions (tuple[int, ...]): The spatial dimensions of the image within the prediction tensors.\n                The default assumes that the images are 3D and have shape:\n                (``batch_size``, ``channel``, ``spatial``, ``spatial``, ``spatial``)\n            logits_threshold: This is a threshold value where values above are classified as 1 and those below are\n                mapped to 0. If the threshold is None, then no thresholding is performed and a continuous or \"soft\"\n                DICE coefficient is computed.\n        \"\"\"\n        self.epsilon = epsilon\n        self.spatial_dimensions = spatial_dimensions\n\n        self.logits_threshold = logits_threshold\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        # Assuming the logits are to be mapped to binary. Note that this assumes the logits have already been\n        # constrained to [0, 1]. The metric still functions if not, but results will be unpredictable.\n        y_pred = (logits &gt; self.logits_threshold).int() if self.logits_threshold else logits\n        intersection = (y_pred * target).sum(dim=self.spatial_dimensions)\n        union = (0.5 * (y_pred + target)).sum(dim=self.spatial_dimensions)\n        dice = intersection / (union + self.epsilon)\n        # If both inputs are empty the dice coefficient should be equal 1\n        dice[union == 0] = 1\n        return torch.mean(dice).item()\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.BinarySoftDiceCoefficient.__init__","title":"<code>__init__(name='BinarySoftDiceCoefficient', epsilon=1e-07, spatial_dimensions=(2, 3, 4), logits_threshold=0.5)</code>","text":"<p>Binary DICE Coefficient Metric with configurable spatial dimensions and logits threshold.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> <code>'BinarySoftDiceCoefficient'</code> <code>epsilon</code> <code>float</code> <p>Small float to add to denominator of DICE calculation to avoid divide by 0.</p> <code>1e-07</code> <code>spatial_dimensions</code> <code>tuple[int, ...]</code> <p>The spatial dimensions of the image within the prediction tensors. The default assumes that the images are 3D and have shape: (<code>batch_size</code>, <code>channel</code>, <code>spatial</code>, <code>spatial</code>, <code>spatial</code>)</p> <code>(2, 3, 4)</code> <code>logits_threshold</code> <code>float | None</code> <p>This is a threshold value where values above are classified as 1 and those below are mapped to 0. If the threshold is None, then no thresholding is performed and a continuous or \"soft\" DICE coefficient is computed.</p> <code>0.5</code> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(\n    self,\n    name: str = \"BinarySoftDiceCoefficient\",\n    epsilon: float = 1.0e-7,\n    spatial_dimensions: tuple[int, ...] = (2, 3, 4),\n    logits_threshold: float | None = 0.5,\n):\n    \"\"\"\n    Binary DICE Coefficient Metric with configurable spatial dimensions and logits threshold.\n\n    Args:\n        name (str): Name of the metric.\n        epsilon (float): Small float to add to denominator of DICE calculation to avoid divide by 0.\n        spatial_dimensions (tuple[int, ...]): The spatial dimensions of the image within the prediction tensors.\n            The default assumes that the images are 3D and have shape:\n            (``batch_size``, ``channel``, ``spatial``, ``spatial``, ``spatial``)\n        logits_threshold: This is a threshold value where values above are classified as 1 and those below are\n            mapped to 0. If the threshold is None, then no thresholding is performed and a continuous or \"soft\"\n            DICE coefficient is computed.\n    \"\"\"\n    self.epsilon = epsilon\n    self.spatial_dimensions = spatial_dimensions\n\n    self.logits_threshold = logits_threshold\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.RocAuc","title":"<code>RocAuc</code>","text":"<p>               Bases: <code>SimpleMetric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class RocAuc(SimpleMetric):\n    def __init__(self, name: str = \"ROC_AUC score\"):\n        \"\"\"\n        Area under the Receiver Operator Curve (AUCROC) metric for classification.\n\n        For more information:\n\n        https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n        \"\"\"\n        super().__init__(name)\n\n    def __call__(self, logits: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        assert logits.shape[0] == target.shape[0]\n        prob = torch.nn.functional.softmax(logits, dim=1)\n        prob = prob.cpu().detach()\n        target = target.cpu().detach()\n        y_true = target.reshape(-1)\n        return sklearn_metrics.roc_auc_score(y_true, prob, average=\"weighted\", multi_class=\"ovr\")\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.RocAuc.__init__","title":"<code>__init__(name='ROC_AUC score')</code>","text":"<p>Area under the Receiver Operator Curve (AUCROC) metric for classification.</p> <p>For more information:</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str = \"ROC_AUC score\"):\n    \"\"\"\n    Area under the Receiver Operator Curve (AUCROC) metric for classification.\n\n    For more information:\n\n    https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html\n    \"\"\"\n    super().__init__(name)\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.SimpleMetric","title":"<code>SimpleMetric</code>","text":"<p>               Bases: <code>Metric</code>, <code>ABC</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class SimpleMetric(Metric, ABC):\n    def __init__(self, name: str) -&gt; None:\n        \"\"\"\n        Abstract metric class with base functionality to update, compute and clear metrics. User needs to define\n        ``__call__`` method which returns metric given inputs and target.\n\n        Args:\n            name (str): Name of the metric.\n        \"\"\"\n        super().__init__(name)\n        self.accumulated_inputs: list[torch.Tensor] = []\n        self.accumulated_targets: list[torch.Tensor] = []\n\n    def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n        \"\"\"\n        This method updates the state of the metric by appending the passed input and target pairing to their\n        respective list.\n\n        Args:\n            input (torch.Tensor): The predictions of the model to be evaluated.\n            target (torch.Tensor): The ground truth target to evaluate predictions against.\n        \"\"\"\n        self.accumulated_inputs.append(input)\n        self.accumulated_targets.append(target)\n\n    def compute(self, name: str | None = None) -&gt; Metrics:\n        \"\"\"\n        Compute metric on accumulated input and output over updates.\n\n        Args:\n            name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n                dictionary.\n\n        Raises:\n            AssertionError: Input and target lists must be non empty.\n\n        Returns:\n            (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n        \"\"\"\n        assert len(self.accumulated_inputs) &gt; 0 and len(self.accumulated_targets) &gt; 0\n        stacked_inputs = torch.cat(self.accumulated_inputs)\n        stacked_targets = torch.cat(self.accumulated_targets)\n        result = self.__call__(stacked_inputs, stacked_targets)\n        result_key = f\"{name} - {self.name}\" if name is not None else self.name\n\n        return {result_key: result}\n\n    def clear(self) -&gt; None:\n        \"\"\"Resets metrics by clearing input and target lists.\"\"\"\n        self.accumulated_inputs = []\n        self.accumulated_targets = []\n\n    @abstractmethod\n    def __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n        \"\"\"\n        User defined method that calculates the desired metric given the predictions and target.\n\n        Raises:\n            NotImplementedError: User must define this method.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.SimpleMetric.__init__","title":"<code>__init__(name)</code>","text":"<p>Abstract metric class with base functionality to update, compute and clear metrics. User needs to define <code>__call__</code> method which returns metric given inputs and target.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the metric.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str) -&gt; None:\n    \"\"\"\n    Abstract metric class with base functionality to update, compute and clear metrics. User needs to define\n    ``__call__`` method which returns metric given inputs and target.\n\n    Args:\n        name (str): Name of the metric.\n    \"\"\"\n    super().__init__(name)\n    self.accumulated_inputs: list[torch.Tensor] = []\n    self.accumulated_targets: list[torch.Tensor] = []\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.SimpleMetric.update","title":"<code>update(input, target)</code>","text":"<p>This method updates the state of the metric by appending the passed input and target pairing to their respective list.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The predictions of the model to be evaluated.</p> required <code>target</code> <code>Tensor</code> <p>The ground truth target to evaluate predictions against.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n    \"\"\"\n    This method updates the state of the metric by appending the passed input and target pairing to their\n    respective list.\n\n    Args:\n        input (torch.Tensor): The predictions of the model to be evaluated.\n        target (torch.Tensor): The ground truth target to evaluate predictions against.\n    \"\"\"\n    self.accumulated_inputs.append(input)\n    self.accumulated_targets.append(target)\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.SimpleMetric.compute","title":"<code>compute(name=None)</code>","text":"<p>Compute metric on accumulated input and output over updates.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Optional name used in conjunction with class attribute name to define key in metrics dictionary.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>Input and target lists must be non empty.</p> <p>Returns:</p> Type Description <code>Metrics</code> <p>A dictionary of string and <code>Scalar</code> representing the computed metric and its associated key.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def compute(self, name: str | None = None) -&gt; Metrics:\n    \"\"\"\n    Compute metric on accumulated input and output over updates.\n\n    Args:\n        name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n            dictionary.\n\n    Raises:\n        AssertionError: Input and target lists must be non empty.\n\n    Returns:\n        (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n    \"\"\"\n    assert len(self.accumulated_inputs) &gt; 0 and len(self.accumulated_targets) &gt; 0\n    stacked_inputs = torch.cat(self.accumulated_inputs)\n    stacked_targets = torch.cat(self.accumulated_targets)\n    result = self.__call__(stacked_inputs, stacked_targets)\n    result_key = f\"{name} - {self.name}\" if name is not None else self.name\n\n    return {result_key: result}\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.SimpleMetric.clear","title":"<code>clear()</code>","text":"<p>Resets metrics by clearing input and target lists.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Resets metrics by clearing input and target lists.\"\"\"\n    self.accumulated_inputs = []\n    self.accumulated_targets = []\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.SimpleMetric.__call__","title":"<code>__call__(input, target)</code>  <code>abstractmethod</code>","text":"<p>User defined method that calculates the desired metric given the predictions and target.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>User must define this method.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>@abstractmethod\ndef __call__(self, input: torch.Tensor, target: torch.Tensor) -&gt; Scalar:\n    \"\"\"\n    User defined method that calculates the desired metric given the predictions and target.\n\n    Raises:\n        NotImplementedError: User must define this method.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.TorchMetric","title":"<code>TorchMetric</code>","text":"<p>               Bases: <code>Metric</code></p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>class TorchMetric(Metric):\n    def __init__(self, name: str, metric: TMetric) -&gt; None:\n        \"\"\"\n        Thin wrapper on ``TorchMetric`` to make it compatible with our ``Metric`` interface.\n\n        Args:\n            name (str): The name of the metric.\n            metric (TMetric): ``TorchMetric`` class based metric.\n        \"\"\"\n        super().__init__(name)\n        self.metric = metric\n\n    def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n        \"\"\"\n        Updates the state of the underlying ``TorchMetric``.\n\n        Args:\n            input (torch.Tensor): The predictions of the model to be evaluated.\n            target (torch.Tensor): The ground truth target to evaluate predictions against.\n        \"\"\"\n        self.metric.update(input, target.long())\n\n    def compute(self, name: str | None = None) -&gt; Metrics:\n        \"\"\"\n        Compute value of underlying ``TorchMetric``.\n\n        Args:\n            name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n                dictionary.\n\n        Returns:\n           (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n        \"\"\"\n        result_key = f\"{name} - {self.name}\" if name is not None else self.name\n        result = self.metric.compute().item()\n        return {result_key: result}\n\n    def clear(self) -&gt; None:\n        self.metric.reset()\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.TorchMetric.__init__","title":"<code>__init__(name, metric)</code>","text":"<p>Thin wrapper on <code>TorchMetric</code> to make it compatible with our <code>Metric</code> interface.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the metric.</p> required <code>metric</code> <code>Metric</code> <p><code>TorchMetric</code> class based metric.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def __init__(self, name: str, metric: TMetric) -&gt; None:\n    \"\"\"\n    Thin wrapper on ``TorchMetric`` to make it compatible with our ``Metric`` interface.\n\n    Args:\n        name (str): The name of the metric.\n        metric (TMetric): ``TorchMetric`` class based metric.\n    \"\"\"\n    super().__init__(name)\n    self.metric = metric\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.TorchMetric.update","title":"<code>update(input, target)</code>","text":"<p>Updates the state of the underlying <code>TorchMetric</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The predictions of the model to be evaluated.</p> required <code>target</code> <code>Tensor</code> <p>The ground truth target to evaluate predictions against.</p> required Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def update(self, input: torch.Tensor, target: torch.Tensor) -&gt; None:\n    \"\"\"\n    Updates the state of the underlying ``TorchMetric``.\n\n    Args:\n        input (torch.Tensor): The predictions of the model to be evaluated.\n        target (torch.Tensor): The ground truth target to evaluate predictions against.\n    \"\"\"\n    self.metric.update(input, target.long())\n</code></pre>"},{"location":"api/#fl4health.utils.metrics.TorchMetric.compute","title":"<code>compute(name=None)</code>","text":"<p>Compute value of underlying <code>TorchMetric</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str | None</code> <p>Optional name used in conjunction with class attribute name to define key in metrics dictionary.</p> <code>None</code> <p>Returns:</p> Type Description <code>Metrics</code> <p>A dictionary of string and <code>Scalar</code> representing the computed metric and its associated key.</p> Source code in <code>fl4health/metrics/metrics.py</code> <pre><code>def compute(self, name: str | None = None) -&gt; Metrics:\n    \"\"\"\n    Compute value of underlying ``TorchMetric``.\n\n    Args:\n        name (str | None): Optional name used in conjunction with class attribute name to define key in metrics\n            dictionary.\n\n    Returns:\n       (Metrics): A dictionary of string and ``Scalar`` representing the computed metric and its associated key.\n    \"\"\"\n    result_key = f\"{name} - {self.name}\" if name is not None else self.name\n    result = self.metric.compute().item()\n    return {result_key: result}\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils","title":"<code>nnunet_utils</code>","text":""},{"location":"api/#fl4health.utils.nnunet_utils.NnunetConfig","title":"<code>NnunetConfig</code>","text":"<p>               Bases: <code>Enum</code></p> <p>The possible nnunet model configs as of nnunetv2 version 2.5.1.</p> <p>See https://github.com/MIC-DKFZ/nnUNet/tree/v2.5.1</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>class NnunetConfig(Enum):\n    \"\"\"\n    The possible nnunet model configs as of nnunetv2 version 2.5.1.\n\n    See https://github.com/MIC-DKFZ/nnUNet/tree/v2.5.1\n    \"\"\"\n\n    _2D = \"2d\"\n    _3D_FULLRES = \"3d_fullres\"\n    _3D_CASCADE = \"3d_cascade_fullres\"\n    _3D_LOWRES = \"3d_lowres\"\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.NnUNetDataLoaderWrapper","title":"<code>NnUNetDataLoaderWrapper</code>","text":"<p>               Bases: <code>DataLoader</code></p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>class NnUNetDataLoaderWrapper(DataLoader):\n    def __init__(\n        self,\n        nnunet_augmenter: SingleThreadedAugmenter | NonDetMultiThreadedAugmenter | MultiThreadedAugmenter,\n        nnunet_config: NnunetConfig | str,\n        infinite: bool = False,\n        set_len: int | None = None,\n        ref_image_shape: Sequence | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Wraps nnunet dataloader classes using the pytorch dataloader to make them pytorch compatible. Also handles\n        some unique stuff specific to nnunet such as deep supervision and infinite dataloaders. The nnunet dataloaders\n        should only be used for training and validation, not final testing.\n\n        Args:\n            nnunet_augmenter (SingleThreadedAugmenter | NonDetMultiThreadedAugmenter | MultiThreadedAugmenter): The\n                dataloader used by nnunet\n            nnunet_config (NnunetConfig | str): The nnunet config. Enum type helps ensure that nnunet config is valid\n            infinite (bool, optional): Whether or not to treat the dataset as infinite. The dataloaders sample data\n                with replacement either way. The only difference is that if set to False, a ``StopIteration`` is\n                generated after ``num_samples``/``batch_size`` steps. Defaults to False.\n            set_len (int | None, optional): If specified overrides the dataloaders estimate of its own length with the\n                provided value. A ``StopIteration`` will be raised after ``set_len`` steps. If not specified the\n                length is determined by scaling the number of samples by the ratio of image size to the networks input\n                patch size. Defaults to None.\n            ref_image_shape (Sequence | None, optional): The image shape to use when computing the scaling factor used\n                in determining the length of the dataloader. Should be representative of the median or average image\n                size in the data set. If not specified a random image is loaded and its shape is used in the\n                calculation of the scaling factor. Defaults to None.\n        \"\"\"\n        # The augmenter is a wrapper on the nnunet dataloader\n        self.nnunet_augmenter = nnunet_augmenter\n\n        if isinstance(self.nnunet_augmenter, SingleThreadedAugmenter):\n            self.nnunet_dataloader = self.nnunet_augmenter.data_loader\n        else:\n            self.nnunet_dataloader = self.nnunet_augmenter.generator\n\n        # Figure out if dataloader is 2d or 3d\n        self.num_spatial_dims = NNUNET_N_SPATIAL_DIMS[NnunetConfig(nnunet_config)]\n\n        # nnUNetDataloaders store their datasets under the self.data attribute\n        self.dataset: nnUNetDataset = self.nnunet_dataloader._data\n        super().__init__(dataset=self.dataset, batch_size=self.nnunet_dataloader.batch_size)\n\n        # nnunet dataloaders are infinite by default so we have to track steps to stop iteration\n        self.current_step = 0\n        self.infinite = infinite\n        self.ref_image_shape = ref_image_shape\n        self.set_len = set_len\n\n    def __next__(self) -&gt; tuple[torch.Tensor, torch.Tensor | dict[str, torch.Tensor]]:\n        \"\"\"\n        Define how the NnUNetDataLoaderWrapper selects the next item as part of standard iteration through the data\n        in the data loader. This is slightly more complicated due to the potentially \"infinite\" nature of these\n        data loaders within nnUnet and the use of deep supervision. See class description for more information.\n\n        Raises:\n            StopIteration: When we hit the \"end\" of the dataset through iteration.\n            TypeError: Raised when the targets extracted from the batch objects are not of the right types.\n\n        Returns:\n            (tuple[torch.Tensor, torch.Tensor | dict[str, torch.Tensor]]): A batch of input and target data.\n        \"\"\"\n        if not self.infinite and self.current_step == self.__len__():\n            self.reset()\n            raise StopIteration  # Raise stop iteration after epoch has completed\n        self.current_step += 1\n        batch = next(self.nnunet_augmenter)  # This returns a dictionary\n        # Note: When deep supervision is on, target is a list of ground truth\n        # segmentations at various spatial scales/resolutions\n        # nnUNet has a wrapper for loss functions to enable deep supervision\n        inputs: torch.Tensor = batch[\"data\"]\n        targets: torch.Tensor | list[torch.Tensor] = batch[\"target\"]\n        if isinstance(targets, list):\n            target_dict = convert_deep_supervision_list_to_dict(targets, self.num_spatial_dims)\n            return inputs, target_dict\n        if isinstance(targets, torch.Tensor):\n            return inputs, targets\n        raise TypeError(\"Was expecting the target generated by the nnunet dataloader to be a list or a torch.Tensor\")\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        nnunetv2 v2.5.1 hardcodes an 'epoch' as 250 steps. We could set the len to ``n_samples``/``batch_size``, but\n        this gets complicated as nnunet models operate on patches of the input images, and therefore can have batch\n        sizes larger than the dataset. We would then have epochs with only 1 step!\n\n        Here we go through the hassle of computing the ratio between the number of voxels in a sample and the number\n        of voxels in a patch and then using that factor to scale ``n_samples``. This is particularly important for\n        training 2d models on 3d data.\n        \"\"\"\n        if self.set_len is not None:\n            return self.set_len\n\n        if self.ref_image_shape is None:\n            # Sample will have shape (n_channels, x, y, ...)\n            sample, _, _ = self.dataset.load_case(self.nnunet_dataloader.indices[0])\n            self.ref_image_shape = sample.shape[1:]  # Must remove the channel dimension\n\n        n_image_voxels = np.prod(self.ref_image_shape)\n        n_patch_voxels = np.prod(self.nnunet_dataloader.final_patch_size)\n\n        # Scale factor is at least one to prevent shrinking the dataset. We can have a\n        # larger patch size sometimes because nnunet will do padding\n        scale = max(n_image_voxels / n_patch_voxels, 1)\n\n        # Scale n_samples and then divide by batch size to get n_steps per epoch\n        return round((len(self.dataset) * scale) / self.nnunet_dataloader.batch_size)\n\n    def reset(self) -&gt; None:\n        self.current_step = 0\n\n    def __iter__(self) -&gt; DataLoader:  # type: ignore\n        \"\"\"\n        Define the iter conversion for an NnUNetDataLoaderWrapper.\n\n        Returns:\n            (DataLoader): The iterator, which is just the NnUNetDataLoaderWrapper itself.\n        \"\"\"\n        # mypy gets angry that the return type is different\n        return self\n\n    def shutdown(self) -&gt; None:\n        \"\"\"The multithreaded augmenters used by nnunet need to be shutdown gracefully to avoid errors.\"\"\"\n        if isinstance(\n            self.nnunet_augmenter,\n            (NonDetMultiThreadedAugmenter, MultiThreadedAugmenter),\n        ):\n            self.nnunet_augmenter._finish()\n        else:\n            del self.nnunet_augmenter\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.NnUNetDataLoaderWrapper.__init__","title":"<code>__init__(nnunet_augmenter, nnunet_config, infinite=False, set_len=None, ref_image_shape=None)</code>","text":"<p>Wraps nnunet dataloader classes using the pytorch dataloader to make them pytorch compatible. Also handles some unique stuff specific to nnunet such as deep supervision and infinite dataloaders. The nnunet dataloaders should only be used for training and validation, not final testing.</p> <p>Parameters:</p> Name Type Description Default <code>nnunet_augmenter</code> <code>SingleThreadedAugmenter | NonDetMultiThreadedAugmenter | MultiThreadedAugmenter</code> <p>The dataloader used by nnunet</p> required <code>nnunet_config</code> <code>NnunetConfig | str</code> <p>The nnunet config. Enum type helps ensure that nnunet config is valid</p> required <code>infinite</code> <code>bool</code> <p>Whether or not to treat the dataset as infinite. The dataloaders sample data with replacement either way. The only difference is that if set to False, a <code>StopIteration</code> is generated after <code>num_samples</code>/<code>batch_size</code> steps. Defaults to False.</p> <code>False</code> <code>set_len</code> <code>int | None</code> <p>If specified overrides the dataloaders estimate of its own length with the provided value. A <code>StopIteration</code> will be raised after <code>set_len</code> steps. If not specified the length is determined by scaling the number of samples by the ratio of image size to the networks input patch size. Defaults to None.</p> <code>None</code> <code>ref_image_shape</code> <code>Sequence | None</code> <p>The image shape to use when computing the scaling factor used in determining the length of the dataloader. Should be representative of the median or average image size in the data set. If not specified a random image is loaded and its shape is used in the calculation of the scaling factor. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def __init__(\n    self,\n    nnunet_augmenter: SingleThreadedAugmenter | NonDetMultiThreadedAugmenter | MultiThreadedAugmenter,\n    nnunet_config: NnunetConfig | str,\n    infinite: bool = False,\n    set_len: int | None = None,\n    ref_image_shape: Sequence | None = None,\n) -&gt; None:\n    \"\"\"\n    Wraps nnunet dataloader classes using the pytorch dataloader to make them pytorch compatible. Also handles\n    some unique stuff specific to nnunet such as deep supervision and infinite dataloaders. The nnunet dataloaders\n    should only be used for training and validation, not final testing.\n\n    Args:\n        nnunet_augmenter (SingleThreadedAugmenter | NonDetMultiThreadedAugmenter | MultiThreadedAugmenter): The\n            dataloader used by nnunet\n        nnunet_config (NnunetConfig | str): The nnunet config. Enum type helps ensure that nnunet config is valid\n        infinite (bool, optional): Whether or not to treat the dataset as infinite. The dataloaders sample data\n            with replacement either way. The only difference is that if set to False, a ``StopIteration`` is\n            generated after ``num_samples``/``batch_size`` steps. Defaults to False.\n        set_len (int | None, optional): If specified overrides the dataloaders estimate of its own length with the\n            provided value. A ``StopIteration`` will be raised after ``set_len`` steps. If not specified the\n            length is determined by scaling the number of samples by the ratio of image size to the networks input\n            patch size. Defaults to None.\n        ref_image_shape (Sequence | None, optional): The image shape to use when computing the scaling factor used\n            in determining the length of the dataloader. Should be representative of the median or average image\n            size in the data set. If not specified a random image is loaded and its shape is used in the\n            calculation of the scaling factor. Defaults to None.\n    \"\"\"\n    # The augmenter is a wrapper on the nnunet dataloader\n    self.nnunet_augmenter = nnunet_augmenter\n\n    if isinstance(self.nnunet_augmenter, SingleThreadedAugmenter):\n        self.nnunet_dataloader = self.nnunet_augmenter.data_loader\n    else:\n        self.nnunet_dataloader = self.nnunet_augmenter.generator\n\n    # Figure out if dataloader is 2d or 3d\n    self.num_spatial_dims = NNUNET_N_SPATIAL_DIMS[NnunetConfig(nnunet_config)]\n\n    # nnUNetDataloaders store their datasets under the self.data attribute\n    self.dataset: nnUNetDataset = self.nnunet_dataloader._data\n    super().__init__(dataset=self.dataset, batch_size=self.nnunet_dataloader.batch_size)\n\n    # nnunet dataloaders are infinite by default so we have to track steps to stop iteration\n    self.current_step = 0\n    self.infinite = infinite\n    self.ref_image_shape = ref_image_shape\n    self.set_len = set_len\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.NnUNetDataLoaderWrapper.__next__","title":"<code>__next__()</code>","text":"<p>Define how the NnUNetDataLoaderWrapper selects the next item as part of standard iteration through the data in the data loader. This is slightly more complicated due to the potentially \"infinite\" nature of these data loaders within nnUnet and the use of deep supervision. See class description for more information.</p> <p>Raises:</p> Type Description <code>StopIteration</code> <p>When we hit the \"end\" of the dataset through iteration.</p> <code>TypeError</code> <p>Raised when the targets extracted from the batch objects are not of the right types.</p> <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor | dict[str, Tensor]]</code> <p>A batch of input and target data.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def __next__(self) -&gt; tuple[torch.Tensor, torch.Tensor | dict[str, torch.Tensor]]:\n    \"\"\"\n    Define how the NnUNetDataLoaderWrapper selects the next item as part of standard iteration through the data\n    in the data loader. This is slightly more complicated due to the potentially \"infinite\" nature of these\n    data loaders within nnUnet and the use of deep supervision. See class description for more information.\n\n    Raises:\n        StopIteration: When we hit the \"end\" of the dataset through iteration.\n        TypeError: Raised when the targets extracted from the batch objects are not of the right types.\n\n    Returns:\n        (tuple[torch.Tensor, torch.Tensor | dict[str, torch.Tensor]]): A batch of input and target data.\n    \"\"\"\n    if not self.infinite and self.current_step == self.__len__():\n        self.reset()\n        raise StopIteration  # Raise stop iteration after epoch has completed\n    self.current_step += 1\n    batch = next(self.nnunet_augmenter)  # This returns a dictionary\n    # Note: When deep supervision is on, target is a list of ground truth\n    # segmentations at various spatial scales/resolutions\n    # nnUNet has a wrapper for loss functions to enable deep supervision\n    inputs: torch.Tensor = batch[\"data\"]\n    targets: torch.Tensor | list[torch.Tensor] = batch[\"target\"]\n    if isinstance(targets, list):\n        target_dict = convert_deep_supervision_list_to_dict(targets, self.num_spatial_dims)\n        return inputs, target_dict\n    if isinstance(targets, torch.Tensor):\n        return inputs, targets\n    raise TypeError(\"Was expecting the target generated by the nnunet dataloader to be a list or a torch.Tensor\")\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.NnUNetDataLoaderWrapper.__len__","title":"<code>__len__()</code>","text":"<p>nnunetv2 v2.5.1 hardcodes an 'epoch' as 250 steps. We could set the len to <code>n_samples</code>/<code>batch_size</code>, but this gets complicated as nnunet models operate on patches of the input images, and therefore can have batch sizes larger than the dataset. We would then have epochs with only 1 step!</p> <p>Here we go through the hassle of computing the ratio between the number of voxels in a sample and the number of voxels in a patch and then using that factor to scale <code>n_samples</code>. This is particularly important for training 2d models on 3d data.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    nnunetv2 v2.5.1 hardcodes an 'epoch' as 250 steps. We could set the len to ``n_samples``/``batch_size``, but\n    this gets complicated as nnunet models operate on patches of the input images, and therefore can have batch\n    sizes larger than the dataset. We would then have epochs with only 1 step!\n\n    Here we go through the hassle of computing the ratio between the number of voxels in a sample and the number\n    of voxels in a patch and then using that factor to scale ``n_samples``. This is particularly important for\n    training 2d models on 3d data.\n    \"\"\"\n    if self.set_len is not None:\n        return self.set_len\n\n    if self.ref_image_shape is None:\n        # Sample will have shape (n_channels, x, y, ...)\n        sample, _, _ = self.dataset.load_case(self.nnunet_dataloader.indices[0])\n        self.ref_image_shape = sample.shape[1:]  # Must remove the channel dimension\n\n    n_image_voxels = np.prod(self.ref_image_shape)\n    n_patch_voxels = np.prod(self.nnunet_dataloader.final_patch_size)\n\n    # Scale factor is at least one to prevent shrinking the dataset. We can have a\n    # larger patch size sometimes because nnunet will do padding\n    scale = max(n_image_voxels / n_patch_voxels, 1)\n\n    # Scale n_samples and then divide by batch size to get n_steps per epoch\n    return round((len(self.dataset) * scale) / self.nnunet_dataloader.batch_size)\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.NnUNetDataLoaderWrapper.__iter__","title":"<code>__iter__()</code>","text":"<p>Define the iter conversion for an NnUNetDataLoaderWrapper.</p> <p>Returns:</p> Type Description <code>DataLoader</code> <p>The iterator, which is just the NnUNetDataLoaderWrapper itself.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def __iter__(self) -&gt; DataLoader:  # type: ignore\n    \"\"\"\n    Define the iter conversion for an NnUNetDataLoaderWrapper.\n\n    Returns:\n        (DataLoader): The iterator, which is just the NnUNetDataLoaderWrapper itself.\n    \"\"\"\n    # mypy gets angry that the return type is different\n    return self\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.NnUNetDataLoaderWrapper.shutdown","title":"<code>shutdown()</code>","text":"<p>The multithreaded augmenters used by nnunet need to be shutdown gracefully to avoid errors.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def shutdown(self) -&gt; None:\n    \"\"\"The multithreaded augmenters used by nnunet need to be shutdown gracefully to avoid errors.\"\"\"\n    if isinstance(\n        self.nnunet_augmenter,\n        (NonDetMultiThreadedAugmenter, MultiThreadedAugmenter),\n    ):\n        self.nnunet_augmenter._finish()\n    else:\n        del self.nnunet_augmenter\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.Module2LossWrapper","title":"<code>Module2LossWrapper</code>","text":"<p>               Bases: <code>_Loss</code></p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>class Module2LossWrapper(_Loss):\n    def __init__(self, loss: nn.Module, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Converts a ``nn.Module`` subclass to a ``_Loss`` subclass. NnUnet defines their loss functions as modules\n        rather than true losses. This provides a type conversion.\n\n        Args:\n            loss (nn.Module): Loss to be wrapped.\n            **kwargs (Any): Any other key word arguments that need to go to the ``_Loss`` base class.\n        \"\"\"\n        super().__init__(**kwargs)\n        self.loss = loss\n\n    def forward(self, pred: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Push the pred and target tensors through the wrapped loss.\n\n        Args:\n            pred (torch.Tensor): Predictions tensor.\n            target (torch.Tensor): Target tensor.\n\n        Returns:\n            (torch.Tensor): Loss output.\n        \"\"\"\n        return self.loss(pred, target)\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.Module2LossWrapper.__init__","title":"<code>__init__(loss, **kwargs)</code>","text":"<p>Converts a <code>nn.Module</code> subclass to a <code>_Loss</code> subclass. NnUnet defines their loss functions as modules rather than true losses. This provides a type conversion.</p> <p>Parameters:</p> Name Type Description Default <code>loss</code> <code>Module</code> <p>Loss to be wrapped.</p> required <code>**kwargs</code> <code>Any</code> <p>Any other key word arguments that need to go to the <code>_Loss</code> base class.</p> <code>{}</code> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def __init__(self, loss: nn.Module, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Converts a ``nn.Module`` subclass to a ``_Loss`` subclass. NnUnet defines their loss functions as modules\n    rather than true losses. This provides a type conversion.\n\n    Args:\n        loss (nn.Module): Loss to be wrapped.\n        **kwargs (Any): Any other key word arguments that need to go to the ``_Loss`` base class.\n    \"\"\"\n    super().__init__(**kwargs)\n    self.loss = loss\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.Module2LossWrapper.forward","title":"<code>forward(pred, target)</code>","text":"<p>Push the pred and target tensors through the wrapped loss.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predictions tensor.</p> required <code>target</code> <code>Tensor</code> <p>Target tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss output.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def forward(self, pred: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Push the pred and target tensors through the wrapped loss.\n\n    Args:\n        pred (torch.Tensor): Predictions tensor.\n        target (torch.Tensor): Target tensor.\n\n    Returns:\n        (torch.Tensor): Loss output.\n    \"\"\"\n    return self.loss(pred, target)\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.StreamToLogger","title":"<code>StreamToLogger</code>","text":"<p>               Bases: <code>StringIO</code></p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>class StreamToLogger(io.StringIO):\n    def __init__(self, logger: Logger, level: LogLevel | int) -&gt; None:\n        \"\"\"\n        File-like stream object that redirects writes to a logger. Useful for redirecting stdout to a logger.\n\n        Args:\n            logger (Logger): The logger to redirect writes to.\n            level (LogLevel): The log level at which to redirect the writes.\n        \"\"\"\n        self.logger = logger\n        self.level = level if isinstance(level, int) else level.value\n        self.linebuf = \"\"  # idk why this is needed. Got this class from stack overflow\n\n    def write(self, buf: str) -&gt; int:\n        char_count = 0\n        for line in buf.rstrip().splitlines():\n            self.logger.log(self.level, line.rstrip())\n            char_count += len(line.rstrip())\n        return char_count\n\n    def flush(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.StreamToLogger.__init__","title":"<code>__init__(logger, level)</code>","text":"<p>File-like stream object that redirects writes to a logger. Useful for redirecting stdout to a logger.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger</code> <p>The logger to redirect writes to.</p> required <code>level</code> <code>LogLevel</code> <p>The log level at which to redirect the writes.</p> required Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def __init__(self, logger: Logger, level: LogLevel | int) -&gt; None:\n    \"\"\"\n    File-like stream object that redirects writes to a logger. Useful for redirecting stdout to a logger.\n\n    Args:\n        logger (Logger): The logger to redirect writes to.\n        level (LogLevel): The log level at which to redirect the writes.\n    \"\"\"\n    self.logger = logger\n    self.level = level if isinstance(level, int) else level.value\n    self.linebuf = \"\"  # idk why this is needed. Got this class from stack overflow\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.PolyLRSchedulerWrapper","title":"<code>PolyLRSchedulerWrapper</code>","text":"<p>               Bases: <code>_LRScheduler</code></p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>class PolyLRSchedulerWrapper(_LRScheduler):\n    def __init__(\n        self,\n        optimizer: torch.optim.Optimizer,\n        initial_lr: float,\n        max_steps: int,\n        exponent: float = 0.9,\n        steps_per_lr: int = 250,\n    ) -&gt; None:\n        \"\"\"\n        Learning rate (LR) scheduler with polynomial decay across fixed windows of size ``steps_per_lr``.\n\n        Args:\n            optimizer (Optimizer): The optimizer to apply LR scheduler to.\n            initial_lr (float): The initial learning rate of the optimizer.\n            max_steps (int): The maximum total number of steps across all FL rounds.\n            exponent (float): Controls how quickly LR decreases over time. Higher values lead to more rapid descent.\n                Defaults to 0.9.\n            steps_per_lr (int): The number of steps per LR before decaying. (ie 10 means the LR will be constant for\n                10 steps prior to being decreased to the subsequent value). Defaults to 250 as that is the default for\n                nnunet (decay LR once an epoch and epoch is 250 steps).\n        \"\"\"\n        self.optimizer = optimizer\n        self.initial_lr = initial_lr\n        self.max_steps = max_steps\n        self.exponent = exponent\n        self.steps_per_lr = steps_per_lr\n        # Number of windows with constant LR across training\n        self.num_windows = ceil(max_steps / self.steps_per_lr)\n        self._step_count: int\n        super().__init__(optimizer, -1, False)\n\n    # mypy incorrectly infers get_lr returns a float\n    # Documented issue https://github.com/pytorch/pytorch/issues/100804\n    @no_type_check\n    def get_lr(self) -&gt; Sequence[float]:\n        \"\"\"\n        Get the current LR of the scheduler.\n\n        Returns:\n            (Sequence[float]): A uniform sequence of LR for each of the parameter groups in the optimizer.\n        \"\"\"\n        if self._step_count - 1 == self.max_steps + 1:\n            log(\n                WARN,\n                f\"Current LR step of {self._step_count} reached Max Steps of {self.max_steps}. LR will remain fixed.\",\n            )\n\n        # Subtract 1 from step count since it starts at 1 (imposed by PyTorch)\n        curr_step = min(self._step_count - 1, self.max_steps)\n        curr_window = int(curr_step / self.steps_per_lr)\n\n        new_lr = self.initial_lr * (1 - curr_window / self.num_windows) ** self.exponent\n\n        if curr_step % self.steps_per_lr == 0 and curr_step not in {0, self.max_steps}:\n            log(INFO, f\"Decaying LR of optimizer to {new_lr} at step {curr_step}\")\n\n        return [new_lr] * len(self.optimizer.param_groups)\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.PolyLRSchedulerWrapper.__init__","title":"<code>__init__(optimizer, initial_lr, max_steps, exponent=0.9, steps_per_lr=250)</code>","text":"<p>Learning rate (LR) scheduler with polynomial decay across fixed windows of size <code>steps_per_lr</code>.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to apply LR scheduler to.</p> required <code>initial_lr</code> <code>float</code> <p>The initial learning rate of the optimizer.</p> required <code>max_steps</code> <code>int</code> <p>The maximum total number of steps across all FL rounds.</p> required <code>exponent</code> <code>float</code> <p>Controls how quickly LR decreases over time. Higher values lead to more rapid descent. Defaults to 0.9.</p> <code>0.9</code> <code>steps_per_lr</code> <code>int</code> <p>The number of steps per LR before decaying. (ie 10 means the LR will be constant for 10 steps prior to being decreased to the subsequent value). Defaults to 250 as that is the default for nnunet (decay LR once an epoch and epoch is 250 steps).</p> <code>250</code> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def __init__(\n    self,\n    optimizer: torch.optim.Optimizer,\n    initial_lr: float,\n    max_steps: int,\n    exponent: float = 0.9,\n    steps_per_lr: int = 250,\n) -&gt; None:\n    \"\"\"\n    Learning rate (LR) scheduler with polynomial decay across fixed windows of size ``steps_per_lr``.\n\n    Args:\n        optimizer (Optimizer): The optimizer to apply LR scheduler to.\n        initial_lr (float): The initial learning rate of the optimizer.\n        max_steps (int): The maximum total number of steps across all FL rounds.\n        exponent (float): Controls how quickly LR decreases over time. Higher values lead to more rapid descent.\n            Defaults to 0.9.\n        steps_per_lr (int): The number of steps per LR before decaying. (ie 10 means the LR will be constant for\n            10 steps prior to being decreased to the subsequent value). Defaults to 250 as that is the default for\n            nnunet (decay LR once an epoch and epoch is 250 steps).\n    \"\"\"\n    self.optimizer = optimizer\n    self.initial_lr = initial_lr\n    self.max_steps = max_steps\n    self.exponent = exponent\n    self.steps_per_lr = steps_per_lr\n    # Number of windows with constant LR across training\n    self.num_windows = ceil(max_steps / self.steps_per_lr)\n    self._step_count: int\n    super().__init__(optimizer, -1, False)\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.PolyLRSchedulerWrapper.get_lr","title":"<code>get_lr()</code>","text":"<p>Get the current LR of the scheduler.</p> <p>Returns:</p> Type Description <code>Sequence[float]</code> <p>A uniform sequence of LR for each of the parameter groups in the optimizer.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>@no_type_check\ndef get_lr(self) -&gt; Sequence[float]:\n    \"\"\"\n    Get the current LR of the scheduler.\n\n    Returns:\n        (Sequence[float]): A uniform sequence of LR for each of the parameter groups in the optimizer.\n    \"\"\"\n    if self._step_count - 1 == self.max_steps + 1:\n        log(\n            WARN,\n            f\"Current LR step of {self._step_count} reached Max Steps of {self.max_steps}. LR will remain fixed.\",\n        )\n\n    # Subtract 1 from step count since it starts at 1 (imposed by PyTorch)\n    curr_step = min(self._step_count - 1, self.max_steps)\n    curr_window = int(curr_step / self.steps_per_lr)\n\n    new_lr = self.initial_lr * (1 - curr_window / self.num_windows) ** self.exponent\n\n    if curr_step % self.steps_per_lr == 0 and curr_step not in {0, self.max_steps}:\n        log(INFO, f\"Decaying LR of optimizer to {new_lr} at step {curr_step}\")\n\n    return [new_lr] * len(self.optimizer.param_groups)\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.use_default_signal_handlers","title":"<code>use_default_signal_handlers(fn)</code>","text":"<p>This is a decorator that resets the <code>SIGINT</code> and <code>SIGTERM</code> signal handlers back to the python defaults for the execution of the method.</p> <p>flwr 1.9.0 overrides the default signal handlers with handlers that raise an error on any interruption or termination. Since nnunet spawns child processes which inherit these handlers, when those subprocesses are terminated (which is expected behavior), the flwr signal handlers raise an error (which we don't want).</p> <p>Flwr is expected to fix this in the next release. See the following issue:</p> <p>https://github.com/adap/flower/issues/3837</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def use_default_signal_handlers(fn: Callable) -&gt; Callable:\n    \"\"\"\n    This is a decorator that resets the ``SIGINT`` and ``SIGTERM`` signal handlers back to the python defaults for the\n    execution of the method.\n\n    flwr 1.9.0 overrides the default signal handlers with handlers that raise an error on any interruption or\n    termination. Since nnunet spawns child processes which inherit these handlers, when those subprocesses are\n    terminated (which is expected behavior), the flwr signal handlers raise an error (which we don't want).\n\n    Flwr is expected to fix this in the next release. See the following issue:\n\n    https://github.com/adap/flower/issues/3837\n    \"\"\"\n\n    def new_fn(*args: Any, **kwargs: Any) -&gt; Any:\n        # Set SIGINT and SIGTERM back to defaults. Method returns previous handler\n        sigint_old = signal.signal(signal.SIGINT, signal.default_int_handler)\n        sigterm_old = signal.signal(signal.SIGTERM, signal.SIG_DFL)\n        # Execute function\n        output = fn(*args, **kwargs)\n        # Reset handlers back to what they were before function call\n        signal.signal(signal.SIGINT, sigint_old)\n        signal.signal(signal.SIGTERM, sigterm_old)\n        return output\n\n    return new_fn\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.reload_modules","title":"<code>reload_modules(packages)</code>","text":"<p>Given the names of one or more packages, subpackages or modules, reloads all the modules within the scope of each package or the modules themselves if a module was specified.</p> <p>Parameters:</p> Name Type Description Default <code>packages</code> <code>Sequence[str]</code> <p>The absolute names of the packages, subpackages or modules to reload. The entire import hierarchy must be specified. Eg. <code>package.subpackage</code> to reload all modules in subpackage, not just <code>subpackage</code>. Packages are reloaded in the order they are given.</p> required Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def reload_modules(packages: Sequence[str]) -&gt; None:\n    \"\"\"\n    Given the names of one or more packages, subpackages or modules, reloads all the modules within the scope of each\n    package or the modules themselves if a module was specified.\n\n    Args:\n        packages (Sequence[str]): The absolute names of the packages, subpackages or modules to reload. The entire\n            import hierarchy must be specified. Eg. ``package.subpackage`` to reload all modules in subpackage, not\n            just ``subpackage``. Packages are reloaded in the order they are given.\n    \"\"\"\n    for m_name, module in list(sys.modules.items()):\n        for package in packages:\n            if m_name.startswith(package):\n                try:\n                    reload(module)\n                except Exception as e:\n                    log(DEBUG, f\"Failed to reload module {m_name}: {e}\")\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.set_nnunet_env_and_reload_modules","title":"<code>set_nnunet_env_and_reload_modules(verbose=False, **kwargs)</code>","text":"<p>For each keyword argument name and value sets the current environment variable with the same name to that value and then reloads nnunet. Values must be strings. This is necessary because nnunet checks some environment variables on import, and therefore it must be imported or reloaded after they are set.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>description. Whether or not logging is enabled.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>dict containing environment variables.</p> <code>{}</code> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def set_nnunet_env_and_reload_modules(verbose: bool = False, **kwargs: Any) -&gt; None:\n    \"\"\"\n    For each keyword argument name and value sets the current environment variable with the same name to that value\n    and then reloads nnunet. Values must be strings. This is necessary because nnunet checks some environment\n    variables on import, and therefore it must be imported or reloaded after they are set.\n\n    Args:\n        verbose (bool, optional): _description_. Whether or not logging is enabled.\n        kwargs (Any): dict containing environment variables.\n    \"\"\"\n    # Set environment variables\n    for key, val in kwargs.items():\n        os.environ[key] = str(val)\n        if verbose:\n            log(INFO, f\"Resetting env var '{key}' to '{val}'\")\n\n    # Its necessary to reload nnunetv2.paths first, then other modules with env vars\n    reload_modules([\"nnunetv2.paths\"])\n    reload_modules([\"nnunetv2.default_n_proc_DA\", \"nnunetv2.configuration\"])\n    # Reload whatever depends on nnunetv2 environment variables\n    # Be careful. If you reload something with an enum in it, things get messed up.\n    reload_modules(\n        [\n            \"nnunetv2\",\n            \"fl4health.clients.nnunet_client\",\n            \"fl4health.clients.flexible.nnunet\",\n        ]\n    )\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.set_nnunet_env","title":"<code>set_nnunet_env(verbose=False, **kwargs)</code>","text":"<p>Maintain for backwards compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>description. Whether or not logging is enabled.</p> <code>False</code> <code>kwargs</code> <code>Any</code> <p>dict containing environment variables.</p> <code>{}</code> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def set_nnunet_env(verbose: bool = False, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Maintain for backwards compatibility.\n\n    Args:\n        verbose (bool, optional): _description_. Whether or not logging is enabled.\n        kwargs (Any): dict containing environment variables.\n    \"\"\"\n    msg = (\n        \"`set_nnunet_env` is deprecated and will be removed in a future version. \"\n        \"Use `set_nnunet_env_and_reload_modules` instead.\"\n    )\n    warnings.warn(\n        msg,\n        DeprecationWarning,\n        stacklevel=1,\n    )\n    set_nnunet_env_and_reload_modules(verbose=verbose, **kwargs)\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.convert_deep_supervision_list_to_dict","title":"<code>convert_deep_supervision_list_to_dict(tensor_list, num_spatial_dims)</code>","text":"<p>Converts a list of <code>torch.Tensors</code> to a dictionary. Names the keys for each tensor based on the spatial resolution of the tensor and its index in the list. Useful for nnUNet models with deep supervision where model outputs and targets loaded by the dataloader are lists. Assumes the spatial dimensions of the tensors are last.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_list</code> <code>list[Tensor]</code> <p>A list of tensors, usually either nnunet model outputs or targets, to be converted into a dictionary.</p> required <code>num_spatial_dims</code> <code>int</code> <p>The number of spatial dimensions. Assumes the spatial dimensions are last.</p> required <p>Returns:</p> Type Description <code>dict[str, Tensor]</code> <p>A dictionary containing the tensors as values where the keys are 'i-XxYxZ' where i</p> <code>dict[str, Tensor]</code> <p>was the tensor's index in the list and X,Y,Z are the spatial dimensions of the tensor.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def convert_deep_supervision_list_to_dict(\n    tensor_list: list[torch.Tensor] | tuple[torch.Tensor], num_spatial_dims: int\n) -&gt; dict[str, torch.Tensor]:\n    \"\"\"\n    Converts a list of ``torch.Tensors`` to a dictionary. Names the keys for each tensor based on the spatial\n    resolution of the tensor and its index in the list. Useful for nnUNet models with deep supervision where model\n    outputs and targets loaded by the dataloader are lists. Assumes the spatial dimensions of the tensors are last.\n\n    Args:\n        tensor_list (list[torch.Tensor]): A list of tensors, usually either nnunet model outputs or targets, to be\n            converted into a dictionary.\n        num_spatial_dims (int): The number of spatial dimensions. Assumes the spatial dimensions are last.\n\n    Returns:\n        (dict[str, torch.Tensor]): A dictionary containing the tensors as values where the keys are 'i-XxYxZ' where i\n        was the tensor's index in the list and X,Y,Z are the spatial dimensions of the tensor.\n    \"\"\"\n    # Convert list of targets into a dictionary\n    tensors = {}\n\n    for i, tensor in enumerate(tensor_list):\n        # generate a key based on the spatial dimension and index\n        key = str(i) + \"-\" + \"x\".join([str(s) for s in tensor.shape[-num_spatial_dims:]])\n        tensors[key] = tensor\n\n    return tensors\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.convert_deep_supervision_dict_to_list","title":"<code>convert_deep_supervision_dict_to_list(tensor_dict)</code>","text":"<p>Converts a dictionary of tensors back into a list so that it can be used by nnunet deep supervision loss functions.</p> <p>Parameters:</p> Name Type Description Default <code>tensor_dict</code> <code>dict[str, Tensor]</code> <p>Dictionary containing <code>torch.Tensors</code>. The key values must start with 'X-' where X is an integer representing the index at which the tensor should be placed in the output list.</p> required <p>Returns:</p> Type Description <code>list[Tensor]</code> <p>A list of <code>torch.Tensors</code>.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def convert_deep_supervision_dict_to_list(\n    tensor_dict: dict[str, torch.Tensor],\n) -&gt; list[torch.Tensor]:\n    \"\"\"\n    Converts a dictionary of tensors back into a list so that it can be used by nnunet deep supervision loss functions.\n\n    Args:\n        tensor_dict (dict[str, torch.Tensor]): Dictionary containing ``torch.Tensors``. The key values must start\n            with 'X-' where X is an integer representing the index at which the tensor should be placed in the output\n            list.\n\n    Returns:\n        (list[torch.Tensor]): A list of ``torch.Tensors``.\n    \"\"\"\n    sorted_list = sorted(tensor_dict.items(), key=lambda x: int(x[0].split(\"-\")[0]))\n    return [tensor for key, tensor in sorted_list]\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.get_segs_from_probs","title":"<code>get_segs_from_probs(preds, has_regions=False, threshold=0.5)</code>","text":"<p>Converts the nnunet model output probabilities to predicted segmentations.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>The one hot encoded model output probabilities with shape (batch, classes, *additional_dims). The background should be a separate class.</p> required <code>has_regions</code> <code>bool</code> <p>If True, predicted segmentations can be multiple classes at once. The exception is the background class which is assumed to be the first class (class 0). If False, each value in predicted segmentations has only a single class. Defaults to False.</p> <code>False</code> <code>threshold</code> <code>float</code> <p>When <code>has_regions</code> is True, this is the threshold value used to determine whether or not an output is a part of a class.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tensor containing the predicted segmentations as a one hot encoded binary tensor of 64-bit integers.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def get_segs_from_probs(preds: torch.Tensor, has_regions: bool = False, threshold: float = 0.5) -&gt; torch.Tensor:\n    \"\"\"\n    Converts the nnunet model output probabilities to predicted segmentations.\n\n    Args:\n        preds (torch.Tensor): The one hot encoded model output probabilities with shape (batch, classes,\n            \\\\*additional_dims). The background should be a separate class.\n        has_regions (bool, optional): If True, predicted segmentations can be multiple classes at once. The exception\n            is the background class which is assumed to be the first class (class 0). If False, each value in\n            predicted segmentations has only a single class. Defaults to False.\n        threshold (float): When ``has_regions`` is True, this is the threshold value used to determine whether or not\n            an output is a part of a class.\n\n    Returns:\n        (torch.Tensor): tensor containing the predicted segmentations as a one hot encoded binary tensor of 64-bit\n            integers.\n    \"\"\"\n    if has_regions:\n        pred_segs = preds &gt; threshold\n        # Mask is the inverse of the background class. Ensures that values\n        # classified as background are not part of another class\n        mask = ~pred_segs[:, 0]\n        return pred_segs * mask\n    pred_segs = preds.argmax(1)[:, None]  # shape (batch, 1, additional_dims)\n    # one hot encode (OHE) predicted segmentations again\n    # WARNING: Note the '_' after scatter. scatter_ and scatter are both\n    # functions with different functionality. It is easy to introduce a bug\n    # here by using the wrong one\n    pred_segs_one_hot = torch.zeros(preds.shape, device=preds.device, dtype=torch.float32)\n    pred_segs_one_hot.scatter_(1, pred_segs, 1)  # ohe -&gt; One Hot Encoded\n    # convert output preds to long since it is binary\n    return pred_segs_one_hot.long()\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.collapse_one_hot_tensor","title":"<code>collapse_one_hot_tensor(input, dim=0)</code>","text":"<p>Collapses a one hot encoded tensor so that they are no longer one hot encoded.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The binary one hot encoded tensor.</p> required <code>dim</code> <code>int</code> <p>Dimension over which to collapse the one-hot tensor. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Integer tensor with the specified dim collapsed.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def collapse_one_hot_tensor(input: torch.Tensor, dim: int = 0) -&gt; torch.Tensor:\n    \"\"\"\n    Collapses a one hot encoded tensor so that they are no longer one hot encoded.\n\n    Args:\n        input (torch.Tensor): The binary one hot encoded tensor.\n        dim (int, optional): Dimension over which to collapse the one-hot tensor. Defaults to 0.\n\n    Returns:\n        (torch.Tensor): Integer tensor with the specified dim collapsed.\n    \"\"\"\n    return torch.argmax(input.long(), dim=dim).to(input.device)\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.get_dataset_n_voxels","title":"<code>get_dataset_n_voxels(source_plans, n_cases)</code>","text":"<p>Determines the total number of voxels in the dataset. Used by <code>NnunetClient</code> to determine the maximum batch size.</p> <p>Parameters:</p> Name Type Description Default <code>source_plans</code> <code>Dict</code> <p>The nnunet plans dict that is being modified.</p> required <code>n_cases</code> <code>int</code> <p>The number of cases in the dataset.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The total number of voxels in the local client dataset.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def get_dataset_n_voxels(source_plans: dict, n_cases: int) -&gt; float:\n    \"\"\"\n    Determines the total number of voxels in the dataset. Used by ``NnunetClient`` to determine the maximum batch size.\n\n    Args:\n        source_plans (Dict): The nnunet plans dict that is being modified.\n        n_cases (int): The number of cases in the dataset.\n\n    Returns:\n        (float): The total number of voxels in the local client dataset.\n    \"\"\"\n    # Need to determine input dimensionality\n    if NnunetConfig._3D_FULLRES.value in source_plans[\"configurations\"]:\n        cfg = source_plans[\"configurations\"][NnunetConfig._3D_FULLRES.value]\n    else:\n        cfg = source_plans[\"configurations\"][NnunetConfig._2D.value]\n\n    # Get total number of voxels in dataset\n    image_shape = cfg[\"median_image_size_in_voxels\"]\n    return float(np.prod(image_shape, dtype=np.float64) * n_cases)\n</code></pre>"},{"location":"api/#fl4health.utils.nnunet_utils.prepare_loss_arg","title":"<code>prepare_loss_arg(tensor)</code>","text":"<p>Converts pred and target tensors into the proper data type to be passed to the nnunet loss functions.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor | dict[str, Tensor]</code> <p>The input tensor.</p> required <p>Returns:</p> Type Description <code>Tensor | list[Tensor]</code> <p>The tensor ready to be passed to the loss function. A single tensor if not using deep supervision and a list of tensors if deep supervision is on.</p> Source code in <code>fl4health/utils/nnunet_utils.py</code> <pre><code>def prepare_loss_arg(\n    tensor: torch.Tensor | dict[str, torch.Tensor],\n) -&gt; torch.Tensor | list[torch.Tensor]:\n    \"\"\"\n    Converts pred and target tensors into the proper data type to be passed to the nnunet loss functions.\n\n    Args:\n        tensor (torch.Tensor | dict[str, torch.Tensor]): The input tensor.\n\n    Returns:\n        (torch.Tensor | list[torch.Tensor]): The tensor ready to be passed to the loss function. A single tensor if not\n            using deep supervision and a list of tensors if deep supervision is on.\n    \"\"\"\n    # TODO: IDK why we have to make assumptions when we could just have a boolean state\n    if isinstance(tensor, torch.Tensor):\n        return tensor  # If input is a tensor then no changes required\n    if isinstance(tensor, dict):\n        if len(tensor) &gt; 1:  # Assume deep supervision is on and return a list\n            return convert_deep_supervision_dict_to_list(tensor)\n        # If dict has only one item, assume deep supervision is off\n        return list(tensor.values())[0]  # return the torch.Tensor\n    raise ValueError(f\"Unrecognized type for tensor: {type(tensor)}\")\n</code></pre>"},{"location":"api/#fl4health.utils.parameter_extraction","title":"<code>parameter_extraction</code>","text":""},{"location":"api/#fl4health.utils.parameter_extraction.get_all_model_parameters","title":"<code>get_all_model_parameters(model)</code>","text":"<p>Function to extract ALL parameters associated with a pytorch module, including any state parameters. These values are converted from numpy arrays into a Flower Parameters object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model whose parameters are to be extracted.</p> required <p>Returns:</p> Type Description <code>Parameters</code> <p>Flower Parameters object containing all of the target models state.</p> Source code in <code>fl4health/utils/parameter_extraction.py</code> <pre><code>def get_all_model_parameters(model: nn.Module) -&gt; Parameters:\n    \"\"\"\n    Function to extract **ALL** parameters associated with a pytorch module, including any state parameters. These\n    values are converted from numpy arrays into a Flower Parameters object.\n\n    Args:\n        model (nn.Module): PyTorch model whose parameters are to be extracted.\n\n    Returns:\n        (Parameters): Flower Parameters object containing all of the target models state.\n    \"\"\"\n    # Extracting all model parameters and converting to Parameters object\n    return ndarrays_to_parameters([val.cpu().numpy() for _, val in model.state_dict().items()])\n</code></pre>"},{"location":"api/#fl4health.utils.parameter_extraction.check_shape_match","title":"<code>check_shape_match(params1, params2, error_message)</code>","text":"<p>Check if the shapes of parameters from two models match.</p> <p>Parameters:</p> Name Type Description Default <code>params1</code> <code>Iterable[Tensor]</code> <p>Parameters from the first model.</p> required <code>params2</code> <code>Iterable[Tensor]</code> <p>Parameters from the second model.</p> required <code>error_message</code> <code>str</code> <p>Error message to display if the shapes do not match.</p> required Source code in <code>fl4health/utils/parameter_extraction.py</code> <pre><code>def check_shape_match(params1: Iterable[torch.Tensor], params2: Iterable[torch.Tensor], error_message: str) -&gt; None:\n    \"\"\"\n    Check if the shapes of parameters from two models match.\n\n    Args:\n        params1 (Iterable[torch.Tensor]): Parameters from the first model.\n        params2 (Iterable[torch.Tensor]): Parameters from the second model.\n        error_message (str): Error message to display if the shapes do not match.\n    \"\"\"\n    params1_list = list(params1)\n    params2_list = list(params2)\n\n    # Check if the number of parameters match\n    assert len(params1_list) == len(params2_list), (\n        f\"Parameter length mismatch: {len(params1_list)} vs {len(params2_list)}. {error_message}\"\n    )\n\n    # Check if each corresponding parameter shape matches\n    for param1, param2 in zip(params1_list, params2_list):\n        assert param1.shape == param2.shape, error_message\n</code></pre>"},{"location":"api/#fl4health.utils.partitioners","title":"<code>partitioners</code>","text":""},{"location":"api/#fl4health.utils.partitioners.DirichletLabelBasedAllocation","title":"<code>DirichletLabelBasedAllocation</code>","text":"<p>               Bases: <code>Generic[T]</code></p> Source code in <code>fl4health/utils/partitioners.py</code> <pre><code>class DirichletLabelBasedAllocation(Generic[T]):\n    def __init__(\n        self,\n        number_of_partitions: int,\n        unique_labels: list[T],\n        min_label_examples: int | None = None,\n        beta: float | None = None,\n        prior_distribution: dict[T, np.ndarray] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        The class supports partitioning of a dataset into a set of datasets (of the same type) via Dirichlet\n        allocation. That is, for each label, a Dirichlet distribution is constructed using beta across a requested\n        number of partitions. Data associated with the label are then assigned to each partition according to the\n        distribution. Another distribution is sampled for the next label, and so on.\n\n        **NOTE**: This differs in kind from label-based Dirichlet sampling. There, an existing dataset is subsampled in\n        place (rather than partitioned) such that its labels match a Dirichlet distribution.\n\n        **NOTE**: The range for beta is (0, \\\\(\\\\infty\\\\)). The larger the value of beta, the more uniform the\n        multinomial probability of the clients will be. The smaller beta is the more heterogeneous it is.\n\n        `#!python np.random.dirichlet([1]*5): array([0.23645891, 0.08857052, 0.29519184, 0.2999956 , 0.07978313])`\n\n        `#!python np.random.dirichlet([1000]*5): array([0.2066252 , 0.19644968, 0.20080513, 0.19992536, 0.19619462])`\n\n        Example Usage:\n\n        ```python\n\n        original_dataset = SyntheticDataset(\n\n        ```\n                torch.rand((10000, 3, 3)), torch.randint(low=0, high=10, size=(10000, 1))\n            )\n\n            heterogeneous_partitioner = DirichletLabelBasedAllocation(\n                number_of_partitions=10, unique_labels=list(range(10)), beta=10.0, min_label_examples=2\n            )\n\n            partitioned_datasets = heterogeneous_partitioner.partition_dataset(original_dataset, max_retries=5)\n\n        Args:\n            number_of_partitions (int): Number of new datasets that we want to break the current dataset into.\n            unique_labels (list[T]): This is the set of labels through which we'll iterate to perform allocation.\n            min_label_examples (int | None, optional): This is an optional input if you want to ensure a minimum\n                number of labels is present on each partition. If prior distribution is provided, this is ignored.\n\n                **NOTE**: This does not guarantee feasibility. That is, if you have a very small beta and request a\n                large minimum number here, you are unlikely to satisfy this request. In partitioning, if the minimum\n                isn't satisfied, we resample from the Dirichlet distribution. This is repeated some limited number of\n                times. Otherwise the partitioner \"gives up\".\n\n                Defaults to None.\n            beta (float | None): This controls the heterogeneity of the partition allocations. The smaller the beta,\n                the more skewed the label assignments will be to different clients. It is mutually exclusive with given\n                prior distribution.\n            prior_distribution (dict[T, np.ndarray] | None, optional): This is an optional input if you want to\n                provide a prior distribution for the Dirichlet distribution. This is useful if you want to make\n                sure that the partitioning of test data is similar to the partitioning of the training data.\n                Defaults to None. It is mutually exclusive with the beta parameter and ``min_label_examples``.\n        \"\"\"\n        assert (beta is not None) ^ (prior_distribution is not None), (\n            \"Either beta or a prior distribution must be provided, but not both.\"\n        )\n        self.number_of_partitions = number_of_partitions\n        self.unique_labels = unique_labels\n        self.n_unique_labels = len(unique_labels)\n        self.beta = beta\n        self.min_label_examples = min_label_examples if min_label_examples else 0\n        self.prior_distribution = prior_distribution\n        if self.prior_distribution is not None:\n            assert len(self.prior_distribution) == self.n_unique_labels, (\n                \"The length of the prior must match the number of labels\"\n            )\n            if self.min_label_examples &gt; 0:\n                log(\n                    WARN,\n                    \"A prior distribution has been provided for the partitioner\",\n                    \"so min_label_examples will be ignored.\",\n                )\n\n    def partition_label_indices(\n        self, label: T, label_indices: torch.Tensor\n    ) -&gt; tuple[list[torch.Tensor], int, np.ndarray]:\n        \"\"\"\n        Given a set of indices from the dataset corresponding to a particular label, the indices are allocated using\n        a Dirichlet distribution, to the partitions.\n\n        Args:\n            label (T): Label is passed for logging transparency. It must be convertible to a string through ``str()``.\n            label_indices (torch.Tensor): Indices from the dataset corresponding to a particular label. This assumes\n                that the tensor is 1D and it's len constitutes the number of total datapoints with the label.\n\n        Raises:\n            ValueError: Either beta or a prior distribution must be provided.\n\n        Returns:\n            (tuple[list[torch.Tensor], int, np.ndarray]): Tuple of:\n\n                - ``torch.Tensor``: partitioned indices of datapoints with the corresponding label.\n                - ``int``: The minimum number of data points assigned to a partition.\n                - ``np.ndarray``: The Dirichlet distribution used to partition the data points.\n        \"\"\"\n        if self.prior_distribution is not None:\n            label_prior_distribution = self.prior_distribution[label]\n            assert len(label_prior_distribution) == self.number_of_partitions, (\n                f\"The length of the prior distribution for label ({str(label)}) must match the number of partitions\"\n            )\n            if sum(label_prior_distribution) != 1:\n                log(\n                    WARN,\n                    f\"The provided prior distribution for label ({str(label)}) does not sum to 1. \"\n                    \"It will be normalized to sum to 1.\",\n                )\n            partition_allocations = label_prior_distribution / sum(label_prior_distribution)\n            log(\n                INFO,\n                f\"The allocation distribution for label ({str(label)}) is {partition_allocations} \"\n                \"using the provided prior distribution\",\n            )\n        elif self.beta is not None:\n            # These are the percentages of the label indices to be distributed for each partition\n            partition_allocations = np.random.dirichlet(np.repeat(self.beta, self.number_of_partitions))\n            log(\n                INFO,\n                (\n                    f\"The allocation distribution for label ({str(label)}) is {partition_allocations} \"\n                    f\"using a beta of {self.beta}\",\n                ),\n            )\n        else:\n            raise ValueError(\"Either beta or a prior distribution must be provided.\")\n        total_data_points_with_label = len(label_indices)\n        num_samples_per_partition = [\n            math.floor(probability * total_data_points_with_label) for probability in partition_allocations\n        ]\n        log(INFO, f\"Assignment of datapoints to partitions as {num_samples_per_partition}\")\n        # Getting the smallest sample across the partitions to decided if it was acceptable.\n        min_samples = min(num_samples_per_partition)\n        total_samples_in_partitions = sum(num_samples_per_partition)\n        # Due to rounding, we need to make sure the partitions we're asking for sum to the total data points size.\n        # So our final partition will \"fill\" the difference and be discarded\n        num_samples_per_partition.append(total_data_points_with_label - total_samples_in_partitions)\n\n        shuffled_indices = label_indices[torch.randperm(total_data_points_with_label)]\n\n        # Partitioning of the indices according to the Dirichlet distribution.\n        partitioned_indices = list(torch.split(shuffled_indices, num_samples_per_partition))\n\n        # Dropping the last partition as they are \"excess\" indices\n        return partitioned_indices[:-1], min_samples, partition_allocations\n\n    def partition_dataset(\n        self, original_dataset: D, max_retries: int | None = 5\n    ) -&gt; tuple[list[D], dict[T, np.ndarray]]:\n        \"\"\"\n        Attempts partitioning of the original dataset up to ``max_retries`` times. Retries are potentially required if\n        the user requests a minimum number of labels be assigned to each of the partitions. If the drawn Dirichlet\n        distribution violates this minimum, a new distribution is drawn. This is repeated until the number of retries\n        is exceeded or the minimum threshold is met.\n\n        Args:\n            original_dataset (D): The dataset to be partitioned.\n            max_retries (int | None, optional): Number of times to attempt to satisfy a user provided minimum\n                label-associated data points per partition. Set this value to None if you want to retry indefinitely.\n                Defaults to 5.\n\n        Raises:\n            ValueError: Throws this error if the retries have been exhausted and the user provided minimum is not met.\n\n        Returns:\n            (tuple[list[D], dict[T, np.ndarray]]): ``list[D]`` is the partitioned datasets, length should correspond to\n                ``self.number_of_partitions``. ``dict[T, np.ndarray]`` is the Dirichlet distribution used to partition\n                the data points for each label.\n        \"\"\"\n        targets = original_dataset.targets\n        assert targets is not None, \"A label-based partitioner requires targets but this dataset has no targets\"\n        partitioned_indices = [torch.Tensor([]).int() for _ in range(self.number_of_partitions)]\n\n        partition_attempts = 0\n        partitioned_probabilities: dict[T, np.ndarray] = {}\n        for label in self.unique_labels:\n            label_indices = torch.where(targets == label)[0].int()\n            min_selected_labels = -1\n            while min_selected_labels &lt; self.min_label_examples:\n                partitioned_indices_for_label, min_selected_labels, partitioned_probability = (\n                    self.partition_label_indices(label, label_indices)\n                )\n                # If the minimum number of labels is satisfied or if there is a prior distribution, we accept the\n                # partition. Otherwise, we retry.\n                if self.prior_distribution is not None or min_selected_labels &gt;= self.min_label_examples:\n                    partitioned_probabilities[label] = partitioned_probability\n                    for i, indices_for_label_partition in enumerate(partitioned_indices_for_label):\n                        partitioned_indices[i] = torch.cat((partitioned_indices[i], indices_for_label_partition))\n                else:\n                    partition_attempts += 1\n                    log(\n                        INFO,\n                        (\n                            f\"Too few datapoints in a partition. One partition had {min_selected_labels} but the \"\n                            f\"minimum requested was {self.min_label_examples}. Resampling the partition...\"\n                        ),\n                    )\n                    if max_retries is not None and partition_attempts &gt;= max_retries:\n                        raise ValueError(\n                            (\n                                f\"Max Retries: {max_retries} reached. Partitioning failed to \"\n                                \"satisfy the minimum label threshold\"\n                            )\n                        )\n\n        return [\n            select_by_indices(original_dataset, indices) for indices in partitioned_indices\n        ], partitioned_probabilities\n</code></pre>"},{"location":"api/#fl4health.utils.partitioners.DirichletLabelBasedAllocation.__init__","title":"<code>__init__(number_of_partitions, unique_labels, min_label_examples=None, beta=None, prior_distribution=None)</code>","text":"<p>The class supports partitioning of a dataset into a set of datasets (of the same type) via Dirichlet allocation. That is, for each label, a Dirichlet distribution is constructed using beta across a requested number of partitions. Data associated with the label are then assigned to each partition according to the distribution. Another distribution is sampled for the next label, and so on.</p> <p>NOTE: This differs in kind from label-based Dirichlet sampling. There, an existing dataset is subsampled in place (rather than partitioned) such that its labels match a Dirichlet distribution.</p> <p>NOTE: The range for beta is (0, \\(\\infty\\)). The larger the value of beta, the more uniform the multinomial probability of the clients will be. The smaller beta is the more heterogeneous it is.</p> <p><code>np.random.dirichlet([1]*5): array([0.23645891, 0.08857052, 0.29519184, 0.2999956 , 0.07978313])</code></p> <p><code>np.random.dirichlet([1000]*5): array([0.2066252 , 0.19644968, 0.20080513, 0.19992536, 0.19619462])</code></p> <p>Example Usage:</p> <p><pre><code>original_dataset = SyntheticDataset(\n</code></pre>         torch.rand((10000, 3, 3)), torch.randint(low=0, high=10, size=(10000, 1))     )</p> <pre><code>heterogeneous_partitioner = DirichletLabelBasedAllocation(\n    number_of_partitions=10, unique_labels=list(range(10)), beta=10.0, min_label_examples=2\n)\n\npartitioned_datasets = heterogeneous_partitioner.partition_dataset(original_dataset, max_retries=5)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>number_of_partitions</code> <code>int</code> <p>Number of new datasets that we want to break the current dataset into.</p> required <code>unique_labels</code> <code>list[T]</code> <p>This is the set of labels through which we'll iterate to perform allocation.</p> required <code>min_label_examples</code> <code>int | None</code> <p>This is an optional input if you want to ensure a minimum number of labels is present on each partition. If prior distribution is provided, this is ignored.</p> <p>NOTE: This does not guarantee feasibility. That is, if you have a very small beta and request a large minimum number here, you are unlikely to satisfy this request. In partitioning, if the minimum isn't satisfied, we resample from the Dirichlet distribution. This is repeated some limited number of times. Otherwise the partitioner \"gives up\".</p> <p>Defaults to None.</p> <code>None</code> <code>beta</code> <code>float | None</code> <p>This controls the heterogeneity of the partition allocations. The smaller the beta, the more skewed the label assignments will be to different clients. It is mutually exclusive with given prior distribution.</p> <code>None</code> <code>prior_distribution</code> <code>dict[T, ndarray] | None</code> <p>This is an optional input if you want to provide a prior distribution for the Dirichlet distribution. This is useful if you want to make sure that the partitioning of test data is similar to the partitioning of the training data. Defaults to None. It is mutually exclusive with the beta parameter and <code>min_label_examples</code>.</p> <code>None</code> Source code in <code>fl4health/utils/partitioners.py</code> <pre><code>def __init__(\n    self,\n    number_of_partitions: int,\n    unique_labels: list[T],\n    min_label_examples: int | None = None,\n    beta: float | None = None,\n    prior_distribution: dict[T, np.ndarray] | None = None,\n) -&gt; None:\n    \"\"\"\n    The class supports partitioning of a dataset into a set of datasets (of the same type) via Dirichlet\n    allocation. That is, for each label, a Dirichlet distribution is constructed using beta across a requested\n    number of partitions. Data associated with the label are then assigned to each partition according to the\n    distribution. Another distribution is sampled for the next label, and so on.\n\n    **NOTE**: This differs in kind from label-based Dirichlet sampling. There, an existing dataset is subsampled in\n    place (rather than partitioned) such that its labels match a Dirichlet distribution.\n\n    **NOTE**: The range for beta is (0, \\\\(\\\\infty\\\\)). The larger the value of beta, the more uniform the\n    multinomial probability of the clients will be. The smaller beta is the more heterogeneous it is.\n\n    `#!python np.random.dirichlet([1]*5): array([0.23645891, 0.08857052, 0.29519184, 0.2999956 , 0.07978313])`\n\n    `#!python np.random.dirichlet([1000]*5): array([0.2066252 , 0.19644968, 0.20080513, 0.19992536, 0.19619462])`\n\n    Example Usage:\n\n    ```python\n\n    original_dataset = SyntheticDataset(\n\n    ```\n            torch.rand((10000, 3, 3)), torch.randint(low=0, high=10, size=(10000, 1))\n        )\n\n        heterogeneous_partitioner = DirichletLabelBasedAllocation(\n            number_of_partitions=10, unique_labels=list(range(10)), beta=10.0, min_label_examples=2\n        )\n\n        partitioned_datasets = heterogeneous_partitioner.partition_dataset(original_dataset, max_retries=5)\n\n    Args:\n        number_of_partitions (int): Number of new datasets that we want to break the current dataset into.\n        unique_labels (list[T]): This is the set of labels through which we'll iterate to perform allocation.\n        min_label_examples (int | None, optional): This is an optional input if you want to ensure a minimum\n            number of labels is present on each partition. If prior distribution is provided, this is ignored.\n\n            **NOTE**: This does not guarantee feasibility. That is, if you have a very small beta and request a\n            large minimum number here, you are unlikely to satisfy this request. In partitioning, if the minimum\n            isn't satisfied, we resample from the Dirichlet distribution. This is repeated some limited number of\n            times. Otherwise the partitioner \"gives up\".\n\n            Defaults to None.\n        beta (float | None): This controls the heterogeneity of the partition allocations. The smaller the beta,\n            the more skewed the label assignments will be to different clients. It is mutually exclusive with given\n            prior distribution.\n        prior_distribution (dict[T, np.ndarray] | None, optional): This is an optional input if you want to\n            provide a prior distribution for the Dirichlet distribution. This is useful if you want to make\n            sure that the partitioning of test data is similar to the partitioning of the training data.\n            Defaults to None. It is mutually exclusive with the beta parameter and ``min_label_examples``.\n    \"\"\"\n    assert (beta is not None) ^ (prior_distribution is not None), (\n        \"Either beta or a prior distribution must be provided, but not both.\"\n    )\n    self.number_of_partitions = number_of_partitions\n    self.unique_labels = unique_labels\n    self.n_unique_labels = len(unique_labels)\n    self.beta = beta\n    self.min_label_examples = min_label_examples if min_label_examples else 0\n    self.prior_distribution = prior_distribution\n    if self.prior_distribution is not None:\n        assert len(self.prior_distribution) == self.n_unique_labels, (\n            \"The length of the prior must match the number of labels\"\n        )\n        if self.min_label_examples &gt; 0:\n            log(\n                WARN,\n                \"A prior distribution has been provided for the partitioner\",\n                \"so min_label_examples will be ignored.\",\n            )\n</code></pre>"},{"location":"api/#fl4health.utils.partitioners.DirichletLabelBasedAllocation.partition_label_indices","title":"<code>partition_label_indices(label, label_indices)</code>","text":"<p>Given a set of indices from the dataset corresponding to a particular label, the indices are allocated using a Dirichlet distribution, to the partitions.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>T</code> <p>Label is passed for logging transparency. It must be convertible to a string through <code>str()</code>.</p> required <code>label_indices</code> <code>Tensor</code> <p>Indices from the dataset corresponding to a particular label. This assumes that the tensor is 1D and it's len constitutes the number of total datapoints with the label.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Either beta or a prior distribution must be provided.</p> <p>Returns:</p> Type Description <code>tuple[list[Tensor], int, ndarray]</code> <p>Tuple of:</p> <ul> <li><code>torch.Tensor</code>: partitioned indices of datapoints with the corresponding label.</li> <li><code>int</code>: The minimum number of data points assigned to a partition.</li> <li><code>np.ndarray</code>: The Dirichlet distribution used to partition the data points.</li> </ul> Source code in <code>fl4health/utils/partitioners.py</code> <pre><code>def partition_label_indices(\n    self, label: T, label_indices: torch.Tensor\n) -&gt; tuple[list[torch.Tensor], int, np.ndarray]:\n    \"\"\"\n    Given a set of indices from the dataset corresponding to a particular label, the indices are allocated using\n    a Dirichlet distribution, to the partitions.\n\n    Args:\n        label (T): Label is passed for logging transparency. It must be convertible to a string through ``str()``.\n        label_indices (torch.Tensor): Indices from the dataset corresponding to a particular label. This assumes\n            that the tensor is 1D and it's len constitutes the number of total datapoints with the label.\n\n    Raises:\n        ValueError: Either beta or a prior distribution must be provided.\n\n    Returns:\n        (tuple[list[torch.Tensor], int, np.ndarray]): Tuple of:\n\n            - ``torch.Tensor``: partitioned indices of datapoints with the corresponding label.\n            - ``int``: The minimum number of data points assigned to a partition.\n            - ``np.ndarray``: The Dirichlet distribution used to partition the data points.\n    \"\"\"\n    if self.prior_distribution is not None:\n        label_prior_distribution = self.prior_distribution[label]\n        assert len(label_prior_distribution) == self.number_of_partitions, (\n            f\"The length of the prior distribution for label ({str(label)}) must match the number of partitions\"\n        )\n        if sum(label_prior_distribution) != 1:\n            log(\n                WARN,\n                f\"The provided prior distribution for label ({str(label)}) does not sum to 1. \"\n                \"It will be normalized to sum to 1.\",\n            )\n        partition_allocations = label_prior_distribution / sum(label_prior_distribution)\n        log(\n            INFO,\n            f\"The allocation distribution for label ({str(label)}) is {partition_allocations} \"\n            \"using the provided prior distribution\",\n        )\n    elif self.beta is not None:\n        # These are the percentages of the label indices to be distributed for each partition\n        partition_allocations = np.random.dirichlet(np.repeat(self.beta, self.number_of_partitions))\n        log(\n            INFO,\n            (\n                f\"The allocation distribution for label ({str(label)}) is {partition_allocations} \"\n                f\"using a beta of {self.beta}\",\n            ),\n        )\n    else:\n        raise ValueError(\"Either beta or a prior distribution must be provided.\")\n    total_data_points_with_label = len(label_indices)\n    num_samples_per_partition = [\n        math.floor(probability * total_data_points_with_label) for probability in partition_allocations\n    ]\n    log(INFO, f\"Assignment of datapoints to partitions as {num_samples_per_partition}\")\n    # Getting the smallest sample across the partitions to decided if it was acceptable.\n    min_samples = min(num_samples_per_partition)\n    total_samples_in_partitions = sum(num_samples_per_partition)\n    # Due to rounding, we need to make sure the partitions we're asking for sum to the total data points size.\n    # So our final partition will \"fill\" the difference and be discarded\n    num_samples_per_partition.append(total_data_points_with_label - total_samples_in_partitions)\n\n    shuffled_indices = label_indices[torch.randperm(total_data_points_with_label)]\n\n    # Partitioning of the indices according to the Dirichlet distribution.\n    partitioned_indices = list(torch.split(shuffled_indices, num_samples_per_partition))\n\n    # Dropping the last partition as they are \"excess\" indices\n    return partitioned_indices[:-1], min_samples, partition_allocations\n</code></pre>"},{"location":"api/#fl4health.utils.partitioners.DirichletLabelBasedAllocation.partition_dataset","title":"<code>partition_dataset(original_dataset, max_retries=5)</code>","text":"<p>Attempts partitioning of the original dataset up to <code>max_retries</code> times. Retries are potentially required if the user requests a minimum number of labels be assigned to each of the partitions. If the drawn Dirichlet distribution violates this minimum, a new distribution is drawn. This is repeated until the number of retries is exceeded or the minimum threshold is met.</p> <p>Parameters:</p> Name Type Description Default <code>original_dataset</code> <code>D</code> <p>The dataset to be partitioned.</p> required <code>max_retries</code> <code>int | None</code> <p>Number of times to attempt to satisfy a user provided minimum label-associated data points per partition. Set this value to None if you want to retry indefinitely. Defaults to 5.</p> <code>5</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Throws this error if the retries have been exhausted and the user provided minimum is not met.</p> <p>Returns:</p> Type Description <code>tuple[list[D], dict[T, ndarray]]</code> <p><code>list[D]</code> is the partitioned datasets, length should correspond to <code>self.number_of_partitions</code>. <code>dict[T, np.ndarray]</code> is the Dirichlet distribution used to partition the data points for each label.</p> Source code in <code>fl4health/utils/partitioners.py</code> <pre><code>def partition_dataset(\n    self, original_dataset: D, max_retries: int | None = 5\n) -&gt; tuple[list[D], dict[T, np.ndarray]]:\n    \"\"\"\n    Attempts partitioning of the original dataset up to ``max_retries`` times. Retries are potentially required if\n    the user requests a minimum number of labels be assigned to each of the partitions. If the drawn Dirichlet\n    distribution violates this minimum, a new distribution is drawn. This is repeated until the number of retries\n    is exceeded or the minimum threshold is met.\n\n    Args:\n        original_dataset (D): The dataset to be partitioned.\n        max_retries (int | None, optional): Number of times to attempt to satisfy a user provided minimum\n            label-associated data points per partition. Set this value to None if you want to retry indefinitely.\n            Defaults to 5.\n\n    Raises:\n        ValueError: Throws this error if the retries have been exhausted and the user provided minimum is not met.\n\n    Returns:\n        (tuple[list[D], dict[T, np.ndarray]]): ``list[D]`` is the partitioned datasets, length should correspond to\n            ``self.number_of_partitions``. ``dict[T, np.ndarray]`` is the Dirichlet distribution used to partition\n            the data points for each label.\n    \"\"\"\n    targets = original_dataset.targets\n    assert targets is not None, \"A label-based partitioner requires targets but this dataset has no targets\"\n    partitioned_indices = [torch.Tensor([]).int() for _ in range(self.number_of_partitions)]\n\n    partition_attempts = 0\n    partitioned_probabilities: dict[T, np.ndarray] = {}\n    for label in self.unique_labels:\n        label_indices = torch.where(targets == label)[0].int()\n        min_selected_labels = -1\n        while min_selected_labels &lt; self.min_label_examples:\n            partitioned_indices_for_label, min_selected_labels, partitioned_probability = (\n                self.partition_label_indices(label, label_indices)\n            )\n            # If the minimum number of labels is satisfied or if there is a prior distribution, we accept the\n            # partition. Otherwise, we retry.\n            if self.prior_distribution is not None or min_selected_labels &gt;= self.min_label_examples:\n                partitioned_probabilities[label] = partitioned_probability\n                for i, indices_for_label_partition in enumerate(partitioned_indices_for_label):\n                    partitioned_indices[i] = torch.cat((partitioned_indices[i], indices_for_label_partition))\n            else:\n                partition_attempts += 1\n                log(\n                    INFO,\n                    (\n                        f\"Too few datapoints in a partition. One partition had {min_selected_labels} but the \"\n                        f\"minimum requested was {self.min_label_examples}. Resampling the partition...\"\n                    ),\n                )\n                if max_retries is not None and partition_attempts &gt;= max_retries:\n                    raise ValueError(\n                        (\n                            f\"Max Retries: {max_retries} reached. Partitioning failed to \"\n                            \"satisfy the minimum label threshold\"\n                        )\n                    )\n\n    return [\n        select_by_indices(original_dataset, indices) for indices in partitioned_indices\n    ], partitioned_probabilities\n</code></pre>"},{"location":"api/#fl4health.utils.peft_parameter_extraction","title":"<code>peft_parameter_extraction</code>","text":""},{"location":"api/#fl4health.utils.peft_parameter_extraction.get_all_peft_parameters_from_model","title":"<code>get_all_peft_parameters_from_model(model)</code>","text":"<p>Function to extract peft parameters associated with a pytorch module. These values are converted from numpy arrays into a Flower Parameters object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model whose parameters are to be extracted.</p> required <p>Returns:</p> Type Description <code>Parameters</code> <p>Flower Parameters object containing all of the target models state.</p> Source code in <code>fl4health/utils/peft_parameter_extraction.py</code> <pre><code>def get_all_peft_parameters_from_model(model: nn.Module) -&gt; Parameters:\n    \"\"\"\n    Function to extract peft parameters associated with a pytorch module. These values are converted\n    from numpy arrays into a Flower Parameters object.\n\n    Args:\n        model (nn.Module): PyTorch model whose parameters are to be extracted.\n\n    Returns:\n        (Parameters): Flower Parameters object containing all of the target models state.\n    \"\"\"\n    state_dict = get_peft_model_state_dict(model)\n    return ndarrays_to_parameters([val.cpu().numpy() for _, val in state_dict.items()])\n</code></pre>"},{"location":"api/#fl4health.utils.privacy_utilities","title":"<code>privacy_utilities</code>","text":""},{"location":"api/#fl4health.utils.privacy_utilities.privacy_validate_and_fix_modules","title":"<code>privacy_validate_and_fix_modules(model)</code>","text":"<p>This function runs Opacus model validation to ensure that the provided models layers are compatible with the privacy mechanisms in Opacus. The function attempts to use Opacus to replace any incompatible layers if possible. For example <code>BatchNormalization</code> layers are not \"DP Compliant\" and will be replaced by compliant layers such as <code>GroupNormalization</code> with this function. Note that this uses the default \"fix\" functionality in Opacus. For more custom options, defining your own <code>setup_opacus_objects</code> function is required.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be validated and potentially modified to be Opacus compliant.</p> required <p>Returns:</p> Type Description <code>tuple[Module, bool]</code> <p>Returns a (possibly) modified pytorch model and a boolean indicating whether a</p> <code>bool</code> <p>reinitialization of any optimizers associated with the model will be required. Reinitialization of the</p> <code>tuple[Module, bool]</code> <p>optimizer parameters is required, for example, when the model layers are modified, yielding a mismatch</p> <code>tuple[Module, bool]</code> <p>in the optimizer parameters and the new model parameters.</p> Source code in <code>fl4health/utils/privacy_utilities.py</code> <pre><code>def privacy_validate_and_fix_modules(model: nn.Module) -&gt; tuple[nn.Module, bool]:\n    \"\"\"\n    This function runs Opacus model validation to ensure that the provided models layers are compatible with the\n    privacy mechanisms in Opacus. The function attempts to use Opacus to replace any incompatible layers if possible.\n    For example ``BatchNormalization`` layers are not \"DP Compliant\" and will be replaced by compliant layers such as\n    ``GroupNormalization`` with this function. Note that this uses the default \"fix\" functionality in Opacus. For more\n    custom options, defining your own ``setup_opacus_objects`` function is required.\n\n    Args:\n        model (nn.Module): The model to be validated and potentially modified to be Opacus compliant.\n\n    Returns:\n        (tuple[nn.Module, bool]): Returns a (possibly) modified pytorch model and a boolean indicating whether a\n        reinitialization of any optimizers associated with the model will be required. Reinitialization of the\n        optimizer parameters is required, for example, when the model layers are modified, yielding a mismatch\n        in the optimizer parameters and the new model parameters.\n    \"\"\"\n    errors = ModuleValidator.validate(model, strict=False)\n    reinitialize_optimizer = len(errors) &gt; 0\n    # Due to a bug in Opacus, it's possible that we need to run multiple rounds fo module validator fix for\n    # complex nested models to fully replace all layers within a model (for example, in the Fed-IXI model)\n    while len(errors) != 0:\n        for error in errors:\n            opacus_warning = (\n                \"Opacus has found layers within your model that do not comply with DP training. \"\n                \"These layers will automatically be replaced with DP compliant layers. \"\n                \"If you would like to perform this replacement yourself, please adjust your model manually.\"\n            )\n            log(WARNING, f\"{opacus_warning}\")\n            log(WARNING, f\"Opacus error: {error}\")\n        model = ModuleValidator.fix(model)\n        errors = ModuleValidator.validate(model, strict=False)\n    # If we made changes to the underlying model, we may need to reinitialize an optimizer\n    return model, reinitialize_optimizer\n</code></pre>"},{"location":"api/#fl4health.utils.privacy_utilities.convert_model_to_opacus_model","title":"<code>convert_model_to_opacus_model(model, grad_sample_mode='hooks', *args, **kwargs)</code>","text":"<p>This function converts a standard pytorch model to an Opacus <code>GradSampleModule</code>, which Opacus uses to perform efficient DP-SGD operations. It uses the wrap_model functionality and mimics its defaults.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Pytorch model to be converted to an Opacus <code>GradSampleModule</code>.</p> required <code>grad_sample_mode</code> <code>str</code> <p>This determines how Opacus performs the conversion under the hood. The standard mechanism is indicated by \"hooks\" but other approaches may be necessary depending on how the pytorch module is defined. Defaults to \"hooks\".</p> <code>'hooks'</code> <code>*args</code> <code>Any</code> <p>Any other args that need to go to the wrap function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Another other kwargs that need to go to the wrap function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GradSampleModule</code> <p>The Opacus wrapped <code>GradSampleModule</code></p> Source code in <code>fl4health/utils/privacy_utilities.py</code> <pre><code>def convert_model_to_opacus_model(\n    model: nn.Module, grad_sample_mode: str = \"hooks\", *args: Any, **kwargs: Any\n) -&gt; GradSampleModule:\n    \"\"\"\n    This function converts a standard pytorch model to an Opacus ``GradSampleModule``, which Opacus uses to perform\n    efficient DP-SGD operations. It uses the wrap_model functionality and mimics its defaults.\n\n    Args:\n        model (nn.Module): Pytorch model to be converted to an Opacus ``GradSampleModule``.\n        grad_sample_mode (str, optional): This determines how Opacus performs the conversion under the hood. The\n            standard mechanism is indicated by \"hooks\" but other approaches may be necessary depending on how the\n            pytorch module is defined. Defaults to \"hooks\".\n        *args (Any): Any other args that need to go to the wrap function.\n        **kwargs (Any): Another other kwargs that need to go to the wrap function.\n\n    Returns:\n        (GradSampleModule): The Opacus wrapped ``GradSampleModule``\n    \"\"\"\n    if isinstance(model, GradSampleModule):\n        log(INFO, f\"Provided model is already of type {type(model)}, skipping conversion to Opacus model type\")\n        return model\n    return wrap_model(model, grad_sample_mode, *args, **kwargs)\n</code></pre>"},{"location":"api/#fl4health.utils.privacy_utilities.map_model_to_opacus_model","title":"<code>map_model_to_opacus_model(model, grad_sample_mode='hooks', *args, **kwargs)</code>","text":"<p>Performs an validation and modifications necessary to make the provided pytorch model \"Opacus Compliant\" via the call to <code>privacy_validate_and_fix_modules</code>. The resulting model is then converted to an Opacus <code>GradSampleModule</code> via <code>convert_model_to_opacus_model</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Pytorch model to be converted to an Opacus compliant <code>GradSampleModule</code>.</p> required <code>grad_sample_mode</code> <code>str</code> <p>This determines how Opacus performs the conversion under the hood. The standard mechanism is indicated by \"hooks\" but other approaches may be necessary depending on how the pytorch module is defined. Defaults to \"hooks\".</p> <code>'hooks'</code> <code>*args</code> <code>Any</code> <p>Any other args that need to go to the conversion function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Another other kwargs that need to go to the conversion function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GradSampleModule</code> <p>The Opacus-compliant, wrapped <code>GradSampleModule</code>.</p> Source code in <code>fl4health/utils/privacy_utilities.py</code> <pre><code>def map_model_to_opacus_model(\n    model: nn.Module, grad_sample_mode: str = \"hooks\", *args: Any, **kwargs: Any\n) -&gt; GradSampleModule:\n    \"\"\"\n    Performs an validation and modifications necessary to make the provided pytorch model \"Opacus Compliant\" via the\n    call to ``privacy_validate_and_fix_modules``. The resulting model is then converted to an Opacus\n    ``GradSampleModule`` via ``convert_model_to_opacus_model``.\n\n    Args:\n        model (nn.Module): Pytorch model to be converted to an Opacus compliant ``GradSampleModule``.\n        grad_sample_mode (str, optional): This determines how Opacus performs the conversion under the hood. The\n            standard mechanism is indicated by \"hooks\" but other approaches may be necessary depending on how the\n            pytorch module is defined. Defaults to \"hooks\".\n        *args (Any): Any other args that need to go to the conversion function.\n        **kwargs (Any): Another other kwargs that need to go to the conversion function.\n\n    Returns:\n        (GradSampleModule): The Opacus-compliant, wrapped ``GradSampleModule``.\n    \"\"\"\n    model, _ = privacy_validate_and_fix_modules(model)\n    return convert_model_to_opacus_model(model, grad_sample_mode, *args, **kwargs)\n</code></pre>"},{"location":"api/#fl4health.utils.random","title":"<code>random</code>","text":""},{"location":"api/#fl4health.utils.random.set_all_random_seeds","title":"<code>set_all_random_seeds(seed=42, use_deterministic_torch_algos=False, disable_torch_benchmarking=False)</code>","text":"<p>Set seeds for python random, numpy random, and pytorch random. It also offers the option to force pytorch to use deterministic algorithm for certain methods and layers.</p> <p>See:</p> <p>https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)</p> <p>for more details. Finally, it allows one to disable cuda benchmarking, which can also affect the determinism of pytorch training outside of random seeding. For more information on reproducibility in pytorch see:</p> <p>https://pytorch.org/docs/stable/notes/randomness.html</p> <p>NOTE: If the <code>use_deterministic_torch_algos</code> flag is set to True, you may need to set the environment variable <code>CUBLAS_WORKSPACE_CONFIG</code> to something like <code>:4096:8</code>, to avoid CUDA errors. Additional documentation may be found here:</p> <p>https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>The seed value to be used for random number generators. Default is 42. Seed setting will no-op if the seed is explicitly set to None.</p> <code>42</code> <code>use_deterministic_torch_algos</code> <code>bool</code> <p>Whether or not to set <code>torch.use_deterministic_algorithms</code> to True. Defaults to False.</p> <code>False</code> <code>disable_torch_benchmarking</code> <code>bool</code> <p>Whether to explicitly disable cuda benchmarking in torch processes. Defaults to False.</p> <code>False</code> Source code in <code>fl4health/utils/random.py</code> <pre><code>def set_all_random_seeds(\n    seed: int | None = 42, use_deterministic_torch_algos: bool = False, disable_torch_benchmarking: bool = False\n) -&gt; None:\n    \"\"\"\n    Set seeds for python random, numpy random, and pytorch random. It also offers the option to force pytorch to use\n    deterministic algorithm for certain methods and layers.\n\n    See:\n\n    https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)\n\n    for more details. Finally, it allows one to disable cuda benchmarking, which can also affect the determinism of\n    pytorch training outside of random seeding. For more information on reproducibility in pytorch see:\n\n    https://pytorch.org/docs/stable/notes/randomness.html\n\n    **NOTE**: If the ``use_deterministic_torch_algos`` flag is set to True, you may need to set the environment\n    variable ``CUBLAS_WORKSPACE_CONFIG`` to something like ``:4096:8``, to avoid CUDA errors. Additional documentation\n    may be found here:\n\n    https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility\n\n    Args:\n        seed (int | None, optional): The seed value to be used for random number generators. Default is 42. Seed\n            setting will no-op if the seed is explicitly set to None.\n        use_deterministic_torch_algos (bool, optional): Whether or not to set ``torch.use_deterministic_algorithms`` to\n            True. Defaults to False.\n        disable_torch_benchmarking (bool, optional): Whether to explicitly disable cuda benchmarking in torch\n            processes. Defaults to False.\n    \"\"\"\n    if seed is None:\n        log(INFO, \"No seed provided. Using random seed.\")\n    else:\n        log(INFO, f\"Setting random seeds to {seed}.\")\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n    if use_deterministic_torch_algos:\n        log(INFO, \"Setting torch.use_deterministic_algorithms to True.\")\n        # warn_only is set to true so that layers and components without deterministic algorithms available will\n        # warn the user that they don't exist, but won't take down the process with an exception.\n        torch.use_deterministic_algorithms(True, warn_only=True)\n    if disable_torch_benchmarking:\n        log(INFO, \"Disabling CUDA algorithm benchmarking.\")\n        torch.backends.cudnn.benchmark = False\n</code></pre>"},{"location":"api/#fl4health.utils.random.unset_all_random_seeds","title":"<code>unset_all_random_seeds()</code>","text":"<p>Set random seeds for Python random, NumPy, and PyTorch to None. Running this function would undo the effects of <code>set_all_random_seeds</code>.</p> Source code in <code>fl4health/utils/random.py</code> <pre><code>def unset_all_random_seeds() -&gt; None:\n    \"\"\"\n    Set random seeds for Python random, NumPy, and PyTorch to None. Running this function would undo\n    the effects of ``set_all_random_seeds``.\n    \"\"\"\n    log(INFO, \"Setting all random seeds to None. Reverting torch determinism settings\")\n    random.seed(None)\n    np.random.seed(None)\n    torch.seed()\n    torch.use_deterministic_algorithms(False)\n</code></pre>"},{"location":"api/#fl4health.utils.random.save_random_state","title":"<code>save_random_state()</code>","text":"<p>Save the state of the random number generators for Python, NumPy, and PyTorch. This will allow you to restore the state of the random number generators at a later time.</p> <p>Returns:</p> Type Description <code>tuple[tuple[Any, ...], dict[str, Any], Tensor]</code> <p>A tuple containing the state of the random number</p> <code>dict[str, Any]</code> <p>generators for Python, NumPy, and PyTorch.</p> Source code in <code>fl4health/utils/random.py</code> <pre><code>def save_random_state() -&gt; tuple[tuple[Any, ...], dict[str, Any], torch.Tensor]:\n    \"\"\"\n    Save the state of the random number generators for Python, NumPy, and PyTorch. This will allow you to restore the\n    state of the random number generators at a later time.\n\n    Returns:\n        (tuple[tuple[Any, ...], dict[str, Any], torch.Tensor]): A tuple containing the state of the random number\n        generators for Python, NumPy, and PyTorch.\n    \"\"\"\n    log(INFO, \"Saving random state.\")\n    random_state = random.getstate()\n    numpy_state = np.random.get_state()\n    torch_state = torch.get_rng_state()\n    return random_state, numpy_state, torch_state\n</code></pre>"},{"location":"api/#fl4health.utils.random.restore_random_state","title":"<code>restore_random_state(random_state, numpy_state, torch_state)</code>","text":"<p>Restore the state of the random number generators for Python, NumPy, and PyTorch. This will allow you to restore the state of the random number generators to a previously saved state.</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>tuple[Any, ...]</code> <p>The state of the Python random number generator.</p> required <code>numpy_state</code> <code>dict[str, Any]</code> <p>The state of the NumPy random number generator.</p> required <code>torch_state</code> <code>Tensor</code> <p>The state of the PyTorch random number generator.</p> required Source code in <code>fl4health/utils/random.py</code> <pre><code>def restore_random_state(\n    random_state: tuple[Any, ...], numpy_state: dict[str, Any], torch_state: torch.Tensor\n) -&gt; None:\n    \"\"\"\n    Restore the state of the random number generators for Python, NumPy, and PyTorch. This will allow you to restore\n    the state of the random number generators to a previously saved state.\n\n    Args:\n        random_state (tuple[Any, ...]): The state of the Python random number generator.\n        numpy_state (dict[str, Any]): The state of the NumPy random number generator.\n        torch_state (torch.Tensor): The state of the PyTorch random number generator.\n    \"\"\"\n    log(INFO, \"Restoring random state.\")\n    random.setstate(random_state)\n    np.random.set_state(numpy_state)\n    torch.set_rng_state(torch_state)\n</code></pre>"},{"location":"api/#fl4health.utils.random.generate_hash","title":"<code>generate_hash(length=8)</code>","text":"<p>Generates unique hash used as id for client.</p> <p>NOTE: This generation is unaffected by setting of random seeds.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>The length of the hash generated. Maximum length is 32.</p> <code>8</code> <p>Returns:</p> Type Description <code>str</code> <p>hash</p> Source code in <code>fl4health/utils/random.py</code> <pre><code>def generate_hash(length: int = 8) -&gt; str:\n    \"\"\"\n    Generates unique hash used as id for client.\n\n    **NOTE**: This generation is unaffected by setting of random seeds.\n\n    Args:\n       length (int): The length of the hash generated. Maximum length is 32.\n\n    Returns:\n        (str): hash\n    \"\"\"\n    return str(uuid.uuid4()).replace(\"-\", \"\")[:length]\n</code></pre>"},{"location":"api/#fl4health.utils.sampler","title":"<code>sampler</code>","text":""},{"location":"api/#fl4health.utils.sampler.LabelBasedSampler","title":"<code>LabelBasedSampler</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>fl4health/utils/sampler.py</code> <pre><code>class LabelBasedSampler(ABC):\n    def __init__(self, unique_labels: list[Any]) -&gt; None:\n        \"\"\"\n        This is an abstract class to be extended to create dataset samplers based on the class of samples.\n\n        Args:\n            unique_labels (list[Any]): The full set of labels contained in the dataset.\n        \"\"\"\n        self.unique_labels = unique_labels\n        self.num_classes = len(self.unique_labels)\n\n    @abstractmethod\n    def subsample(self, dataset: D) -&gt; D:\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#fl4health.utils.sampler.LabelBasedSampler.__init__","title":"<code>__init__(unique_labels)</code>","text":"<p>This is an abstract class to be extended to create dataset samplers based on the class of samples.</p> <p>Parameters:</p> Name Type Description Default <code>unique_labels</code> <code>list[Any]</code> <p>The full set of labels contained in the dataset.</p> required Source code in <code>fl4health/utils/sampler.py</code> <pre><code>def __init__(self, unique_labels: list[Any]) -&gt; None:\n    \"\"\"\n    This is an abstract class to be extended to create dataset samplers based on the class of samples.\n\n    Args:\n        unique_labels (list[Any]): The full set of labels contained in the dataset.\n    \"\"\"\n    self.unique_labels = unique_labels\n    self.num_classes = len(self.unique_labels)\n</code></pre>"},{"location":"api/#fl4health.utils.sampler.MinorityLabelBasedSampler","title":"<code>MinorityLabelBasedSampler</code>","text":"<p>               Bases: <code>LabelBasedSampler</code></p> Source code in <code>fl4health/utils/sampler.py</code> <pre><code>class MinorityLabelBasedSampler(LabelBasedSampler):\n    def __init__(self, unique_labels: list[T], downsampling_ratio: float, minority_labels: Set[T]) -&gt; None:\n        \"\"\"\n        This class is used to subsample a dataset so the classes are distributed in a non-IID way. In particular, the\n        ``MinorityLabelBasedSampler`` explicitly downsamples classes based on the ``downsampling_ratio`` and\n        ``minority_labels`` args used to construct the object. Subsampling a dataset is accomplished by calling the\n        subsample method and passing a ``BaseDataset`` object. This will return the resulting subsampled dataset.\n\n        Args:\n            unique_labels (list[T]): The full set of labels contained in the dataset.\n            downsampling_ratio (float): The percentage to which the specified \"minority\" labels are downsampled. For\n                example, if a label ``L`` has 10 examples and the ``downsampling_ratio`` is 0.2, then 8 of the\n                datapoints with label ``L`` are discarded.\n            minority_labels (Set[T]): The labels subject to downsampling.\n        \"\"\"\n        super().__init__(unique_labels)\n        self.downsampling_ratio = downsampling_ratio\n        self.minority_labels = minority_labels\n\n    def subsample(self, dataset: D) -&gt; D:\n        \"\"\"\n        Returns a new dataset where samples part of ``minority_labels`` are downsampled.\n\n        Args:\n            dataset (D): Dataset to be modified, through downsampling on specified labels.\n\n        Returns:\n            (D): New dataset with downsampled labels.\n        \"\"\"\n        assert dataset.targets is not None, \"A label-based sampler requires targets but this dataset has no targets\"\n        selected_indices_list: list[torch.Tensor] = []\n        for label in self.unique_labels:\n            # Get indices of samples equal to the current label\n            indices_of_label = (dataset.targets == label).nonzero()\n            if label in self.minority_labels:\n                subsample_size = int(indices_of_label.shape[0] * self.downsampling_ratio)\n                subsampled_indices = self._get_random_subsample(indices_of_label, subsample_size)\n                selected_indices_list.append(subsampled_indices.squeeze())\n            else:\n                selected_indices_list.append(indices_of_label.squeeze())\n\n        selected_indices = torch.cat(selected_indices_list, dim=0)\n\n        return select_by_indices(dataset, selected_indices)\n\n    def _get_random_subsample(self, tensor_to_subsample: torch.Tensor, subsample_size: int) -&gt; torch.Tensor:\n        \"\"\"\n        Given a tensor a new tensor is created by selecting a set of rows from the original tensor of size\n        ``subsample_size``.\n\n        Args:\n            tensor_to_subsample (torch.Tensor): Tensor to be subsampled. Assumes that we're subsampling rows of the\n                tensor\n            subsample_size (int): How many rows we want to extract from the tensor.\n\n        Returns:\n            (torch.Tensor): New tensor with subsampled rows\n        \"\"\"\n        # NOTE: Assumes subsampling on rows\n        tensor_size = tensor_to_subsample.shape[0]\n        assert subsample_size &lt; tensor_size\n        permutation = torch.randperm(tensor_size)\n        return tensor_to_subsample[permutation[:subsample_size]]\n</code></pre>"},{"location":"api/#fl4health.utils.sampler.MinorityLabelBasedSampler.__init__","title":"<code>__init__(unique_labels, downsampling_ratio, minority_labels)</code>","text":"<p>This class is used to subsample a dataset so the classes are distributed in a non-IID way. In particular, the <code>MinorityLabelBasedSampler</code> explicitly downsamples classes based on the <code>downsampling_ratio</code> and <code>minority_labels</code> args used to construct the object. Subsampling a dataset is accomplished by calling the subsample method and passing a <code>BaseDataset</code> object. This will return the resulting subsampled dataset.</p> <p>Parameters:</p> Name Type Description Default <code>unique_labels</code> <code>list[T]</code> <p>The full set of labels contained in the dataset.</p> required <code>downsampling_ratio</code> <code>float</code> <p>The percentage to which the specified \"minority\" labels are downsampled. For example, if a label <code>L</code> has 10 examples and the <code>downsampling_ratio</code> is 0.2, then 8 of the datapoints with label <code>L</code> are discarded.</p> required <code>minority_labels</code> <code>Set[T]</code> <p>The labels subject to downsampling.</p> required Source code in <code>fl4health/utils/sampler.py</code> <pre><code>def __init__(self, unique_labels: list[T], downsampling_ratio: float, minority_labels: Set[T]) -&gt; None:\n    \"\"\"\n    This class is used to subsample a dataset so the classes are distributed in a non-IID way. In particular, the\n    ``MinorityLabelBasedSampler`` explicitly downsamples classes based on the ``downsampling_ratio`` and\n    ``minority_labels`` args used to construct the object. Subsampling a dataset is accomplished by calling the\n    subsample method and passing a ``BaseDataset`` object. This will return the resulting subsampled dataset.\n\n    Args:\n        unique_labels (list[T]): The full set of labels contained in the dataset.\n        downsampling_ratio (float): The percentage to which the specified \"minority\" labels are downsampled. For\n            example, if a label ``L`` has 10 examples and the ``downsampling_ratio`` is 0.2, then 8 of the\n            datapoints with label ``L`` are discarded.\n        minority_labels (Set[T]): The labels subject to downsampling.\n    \"\"\"\n    super().__init__(unique_labels)\n    self.downsampling_ratio = downsampling_ratio\n    self.minority_labels = minority_labels\n</code></pre>"},{"location":"api/#fl4health.utils.sampler.MinorityLabelBasedSampler.subsample","title":"<code>subsample(dataset)</code>","text":"<p>Returns a new dataset where samples part of <code>minority_labels</code> are downsampled.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>D</code> <p>Dataset to be modified, through downsampling on specified labels.</p> required <p>Returns:</p> Type Description <code>D</code> <p>New dataset with downsampled labels.</p> Source code in <code>fl4health/utils/sampler.py</code> <pre><code>def subsample(self, dataset: D) -&gt; D:\n    \"\"\"\n    Returns a new dataset where samples part of ``minority_labels`` are downsampled.\n\n    Args:\n        dataset (D): Dataset to be modified, through downsampling on specified labels.\n\n    Returns:\n        (D): New dataset with downsampled labels.\n    \"\"\"\n    assert dataset.targets is not None, \"A label-based sampler requires targets but this dataset has no targets\"\n    selected_indices_list: list[torch.Tensor] = []\n    for label in self.unique_labels:\n        # Get indices of samples equal to the current label\n        indices_of_label = (dataset.targets == label).nonzero()\n        if label in self.minority_labels:\n            subsample_size = int(indices_of_label.shape[0] * self.downsampling_ratio)\n            subsampled_indices = self._get_random_subsample(indices_of_label, subsample_size)\n            selected_indices_list.append(subsampled_indices.squeeze())\n        else:\n            selected_indices_list.append(indices_of_label.squeeze())\n\n    selected_indices = torch.cat(selected_indices_list, dim=0)\n\n    return select_by_indices(dataset, selected_indices)\n</code></pre>"},{"location":"api/#fl4health.utils.sampler.DirichletLabelBasedSampler","title":"<code>DirichletLabelBasedSampler</code>","text":"<p>               Bases: <code>LabelBasedSampler</code></p> Source code in <code>fl4health/utils/sampler.py</code> <pre><code>class DirichletLabelBasedSampler(LabelBasedSampler):\n    def __init__(\n        self,\n        unique_labels: list[Any],\n        hash_key: int | None = None,\n        sample_percentage: float = 0.5,\n        beta: float = 100,\n    ) -&gt; None:\n        \"\"\"\n        Class used to subsample a dataset so the classes of samples are distributed in a non-IID way. In particular,\n        the ``DirichletLabelBasedSampler`` uses a Dirichlet distribution to determine the number of samples from each\n        class. The sampler is constructed by passing a beta parameter that determines the level of heterogeneity and a\n        ``sample_percentage`` that determines the relative size of the modified dataset. Subsampling a dataset is\n        accomplished by calling the subsample method and passing a ``BaseDataset`` object. This will return the\n        resulting  subsampled dataset.\n\n        **NOTE**: The range for beta is (0, \\\\(\\\\infty\\\\)). The larger the value of beta, the more evenly the\n        multinomial probability of the labels will be. The smaller beta is the more heterogeneous it is.\n\n        `#!python np.random.dirichlet([1]*5): array([0.23645891, 0.08857052, 0.29519184, 0.2999956 , 0.07978313])`\n\n        `#!python np.random.dirichlet([1000]*5): array([0.2066252 , 0.19644968, 0.20080513, 0.19992536, 0.19619462])`\n\n        Args:\n            unique_labels (list[Any]): The full set of labels contained in the dataset.\n            sample_percentage (float, optional): The downsampling of the entire dataset to do. For example, if this\n                value is 0.5 and the dataset is of size 100, we will end up with 50 total data points. Defaults to 0.5.\n            beta (float, optional): This controls the heterogeneity of the label sampling. The smaller the beta, the\n                more skewed the label assignments will be for the dataset. Defaults to 100.\n            hash_key (int | None, optional): Seed for the random number generators and samplers. Defaults to None.\n        \"\"\"\n        super().__init__(unique_labels)\n\n        self.hash_key = hash_key\n\n        self.torch_generator = None\n        if self.hash_key is not None:\n            log(INFO, f\"Setting seed to {self.hash_key} for the Torch and Numpy Generators\")\n            log(WARN, \"Note that setting a hash key here will override any torch and numpy seeds that you have set\")\n            self.torch_generator = torch.Generator().manual_seed(self.hash_key)\n            np_generator = np.random.default_rng(self.hash_key)\n            self.probabilities = np_generator.dirichlet(np.repeat(beta, self.num_classes))\n        else:\n            self.probabilities = np.random.dirichlet(np.repeat(beta, self.num_classes))\n        log(INFO, f\"Setting probabilities to {self.probabilities}\")\n\n        self.sample_percentage = sample_percentage\n\n    def subsample(self, dataset: D) -&gt; D:\n        \"\"\"\n        Returns a new dataset where samples are selected based on a Dirichlet distribution over labels.\n\n        Args:\n            dataset (D): Dataset to be modified, through downsampling on specified labels.\n\n        Returns:\n            (D): New dataset with downsampled labels.\n        \"\"\"\n        assert dataset.targets is not None, \"A label-based sampler requires targets but this dataset has no targets\"\n        assert self.sample_percentage &lt;= 1.0\n\n        total_num_samples = int(len(dataset) * self.sample_percentage)\n        targets = dataset.targets\n\n        class_idx_list = [torch.where(targets == target)[0].float() for target in self.unique_labels]\n\n        num_samples_per_class = [math.ceil(prob * total_num_samples) for prob in self.probabilities]\n\n        # For each class, sample the given number of samples from the class specific indices\n        # torch.multinomial is used to uniformly sample indices the size of given number of samples\n        sampled_class_idx_list = [\n            class_idx[\n                torch.multinomial(\n                    torch.ones(class_idx.size(0)), num_samples, replacement=True, generator=self.torch_generator\n                )\n            ]\n            for class_idx, num_samples in zip(class_idx_list, num_samples_per_class)\n        ]\n        selected_indices = torch.cat(sampled_class_idx_list, dim=0).long()\n\n        # Due to precision errors with previous rounding, sum of sample counts\n        # may differ from total_num_samples so we resample to ensure correct count\n        selected_indices = selected_indices[:total_num_samples]\n\n        return select_by_indices(dataset, selected_indices)\n</code></pre>"},{"location":"api/#fl4health.utils.sampler.DirichletLabelBasedSampler.__init__","title":"<code>__init__(unique_labels, hash_key=None, sample_percentage=0.5, beta=100)</code>","text":"<p>Class used to subsample a dataset so the classes of samples are distributed in a non-IID way. In particular, the <code>DirichletLabelBasedSampler</code> uses a Dirichlet distribution to determine the number of samples from each class. The sampler is constructed by passing a beta parameter that determines the level of heterogeneity and a <code>sample_percentage</code> that determines the relative size of the modified dataset. Subsampling a dataset is accomplished by calling the subsample method and passing a <code>BaseDataset</code> object. This will return the resulting  subsampled dataset.</p> <p>NOTE: The range for beta is (0, \\(\\infty\\)). The larger the value of beta, the more evenly the multinomial probability of the labels will be. The smaller beta is the more heterogeneous it is.</p> <p><code>np.random.dirichlet([1]*5): array([0.23645891, 0.08857052, 0.29519184, 0.2999956 , 0.07978313])</code></p> <p><code>np.random.dirichlet([1000]*5): array([0.2066252 , 0.19644968, 0.20080513, 0.19992536, 0.19619462])</code></p> <p>Parameters:</p> Name Type Description Default <code>unique_labels</code> <code>list[Any]</code> <p>The full set of labels contained in the dataset.</p> required <code>sample_percentage</code> <code>float</code> <p>The downsampling of the entire dataset to do. For example, if this value is 0.5 and the dataset is of size 100, we will end up with 50 total data points. Defaults to 0.5.</p> <code>0.5</code> <code>beta</code> <code>float</code> <p>This controls the heterogeneity of the label sampling. The smaller the beta, the more skewed the label assignments will be for the dataset. Defaults to 100.</p> <code>100</code> <code>hash_key</code> <code>int | None</code> <p>Seed for the random number generators and samplers. Defaults to None.</p> <code>None</code> Source code in <code>fl4health/utils/sampler.py</code> <pre><code>def __init__(\n    self,\n    unique_labels: list[Any],\n    hash_key: int | None = None,\n    sample_percentage: float = 0.5,\n    beta: float = 100,\n) -&gt; None:\n    \"\"\"\n    Class used to subsample a dataset so the classes of samples are distributed in a non-IID way. In particular,\n    the ``DirichletLabelBasedSampler`` uses a Dirichlet distribution to determine the number of samples from each\n    class. The sampler is constructed by passing a beta parameter that determines the level of heterogeneity and a\n    ``sample_percentage`` that determines the relative size of the modified dataset. Subsampling a dataset is\n    accomplished by calling the subsample method and passing a ``BaseDataset`` object. This will return the\n    resulting  subsampled dataset.\n\n    **NOTE**: The range for beta is (0, \\\\(\\\\infty\\\\)). The larger the value of beta, the more evenly the\n    multinomial probability of the labels will be. The smaller beta is the more heterogeneous it is.\n\n    `#!python np.random.dirichlet([1]*5): array([0.23645891, 0.08857052, 0.29519184, 0.2999956 , 0.07978313])`\n\n    `#!python np.random.dirichlet([1000]*5): array([0.2066252 , 0.19644968, 0.20080513, 0.19992536, 0.19619462])`\n\n    Args:\n        unique_labels (list[Any]): The full set of labels contained in the dataset.\n        sample_percentage (float, optional): The downsampling of the entire dataset to do. For example, if this\n            value is 0.5 and the dataset is of size 100, we will end up with 50 total data points. Defaults to 0.5.\n        beta (float, optional): This controls the heterogeneity of the label sampling. The smaller the beta, the\n            more skewed the label assignments will be for the dataset. Defaults to 100.\n        hash_key (int | None, optional): Seed for the random number generators and samplers. Defaults to None.\n    \"\"\"\n    super().__init__(unique_labels)\n\n    self.hash_key = hash_key\n\n    self.torch_generator = None\n    if self.hash_key is not None:\n        log(INFO, f\"Setting seed to {self.hash_key} for the Torch and Numpy Generators\")\n        log(WARN, \"Note that setting a hash key here will override any torch and numpy seeds that you have set\")\n        self.torch_generator = torch.Generator().manual_seed(self.hash_key)\n        np_generator = np.random.default_rng(self.hash_key)\n        self.probabilities = np_generator.dirichlet(np.repeat(beta, self.num_classes))\n    else:\n        self.probabilities = np.random.dirichlet(np.repeat(beta, self.num_classes))\n    log(INFO, f\"Setting probabilities to {self.probabilities}\")\n\n    self.sample_percentage = sample_percentage\n</code></pre>"},{"location":"api/#fl4health.utils.sampler.DirichletLabelBasedSampler.subsample","title":"<code>subsample(dataset)</code>","text":"<p>Returns a new dataset where samples are selected based on a Dirichlet distribution over labels.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>D</code> <p>Dataset to be modified, through downsampling on specified labels.</p> required <p>Returns:</p> Type Description <code>D</code> <p>New dataset with downsampled labels.</p> Source code in <code>fl4health/utils/sampler.py</code> <pre><code>def subsample(self, dataset: D) -&gt; D:\n    \"\"\"\n    Returns a new dataset where samples are selected based on a Dirichlet distribution over labels.\n\n    Args:\n        dataset (D): Dataset to be modified, through downsampling on specified labels.\n\n    Returns:\n        (D): New dataset with downsampled labels.\n    \"\"\"\n    assert dataset.targets is not None, \"A label-based sampler requires targets but this dataset has no targets\"\n    assert self.sample_percentage &lt;= 1.0\n\n    total_num_samples = int(len(dataset) * self.sample_percentage)\n    targets = dataset.targets\n\n    class_idx_list = [torch.where(targets == target)[0].float() for target in self.unique_labels]\n\n    num_samples_per_class = [math.ceil(prob * total_num_samples) for prob in self.probabilities]\n\n    # For each class, sample the given number of samples from the class specific indices\n    # torch.multinomial is used to uniformly sample indices the size of given number of samples\n    sampled_class_idx_list = [\n        class_idx[\n            torch.multinomial(\n                torch.ones(class_idx.size(0)), num_samples, replacement=True, generator=self.torch_generator\n            )\n        ]\n        for class_idx, num_samples in zip(class_idx_list, num_samples_per_class)\n    ]\n    selected_indices = torch.cat(sampled_class_idx_list, dim=0).long()\n\n    # Due to precision errors with previous rounding, sum of sample counts\n    # may differ from total_num_samples so we resample to ensure correct count\n    selected_indices = selected_indices[:total_num_samples]\n\n    return select_by_indices(dataset, selected_indices)\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter","title":"<code>snapshotter</code>","text":""},{"location":"api/#fl4health.utils.snapshotter.AbstractSnapshotter","title":"<code>AbstractSnapshotter</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[T]</code></p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>class AbstractSnapshotter(ABC, Generic[T]):\n    @abstractmethod\n    def save_attribute(self, attribute: dict[str, T]) -&gt; dict[str, Any]:\n        \"\"\"\n        Abstract method used to save the state of the attribute. This method should be implemented based on the type of\n        the attribute and the way it should be saved.\n\n        Args:\n            attribute (dict[str, T]): The attribute to be saved.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the attribute.\n        \"\"\"\n\n    @abstractmethod\n    def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, T]) -&gt; None:\n        \"\"\"\n        Abstract method to load the state of the attribute. This method should be implemented based on the type of\n        the attribute and the way it should be loaded.\n\n        Args:\n            attribute_snapshot (dict[str, Any]): The snapshot containing the state of the attribute.\n            attribute (dict[str, T]): The attribute to be loaded.\n        \"\"\"\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.AbstractSnapshotter.save_attribute","title":"<code>save_attribute(attribute)</code>  <code>abstractmethod</code>","text":"<p>Abstract method used to save the state of the attribute. This method should be implemented based on the type of the attribute and the way it should be saved.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>dict[str, T]</code> <p>The attribute to be saved.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the state of the attribute.</p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>@abstractmethod\ndef save_attribute(self, attribute: dict[str, T]) -&gt; dict[str, Any]:\n    \"\"\"\n    Abstract method used to save the state of the attribute. This method should be implemented based on the type of\n    the attribute and the way it should be saved.\n\n    Args:\n        attribute (dict[str, T]): The attribute to be saved.\n\n    Returns:\n        (dict[str, Any]): A dictionary containing the state of the attribute.\n    \"\"\"\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.AbstractSnapshotter.load_attribute","title":"<code>load_attribute(attribute_snapshot, attribute)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to load the state of the attribute. This method should be implemented based on the type of the attribute and the way it should be loaded.</p> <p>Parameters:</p> Name Type Description Default <code>attribute_snapshot</code> <code>dict[str, Any]</code> <p>The snapshot containing the state of the attribute.</p> required <code>attribute</code> <code>dict[str, T]</code> <p>The attribute to be loaded.</p> required Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>@abstractmethod\ndef load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, T]) -&gt; None:\n    \"\"\"\n    Abstract method to load the state of the attribute. This method should be implemented based on the type of\n    the attribute and the way it should be loaded.\n\n    Args:\n        attribute_snapshot (dict[str, Any]): The snapshot containing the state of the attribute.\n        attribute (dict[str, T]): The attribute to be loaded.\n    \"\"\"\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.OptimizerSnapshotter","title":"<code>OptimizerSnapshotter</code>","text":"<p>               Bases: <code>AbstractSnapshotter[Optimizer]</code></p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>class OptimizerSnapshotter(AbstractSnapshotter[Optimizer]):\n    def save_attribute(self, attribute: dict[str, Optimizer]) -&gt; dict[str, Any]:\n        \"\"\"\n        Save the state of the optimizers by saving \"state\" attribute of the optimizers.\n\n        Args:\n            attribute (dict[str, Optimizer]): The optimizers to be saved.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the optimizers.\n        \"\"\"\n        output = {}\n        for key, optimizer in attribute.items():\n            output[key] = optimizer.state_dict()[\"state\"]\n        return output\n\n    def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, Optimizer]) -&gt; None:\n        \"\"\"\n        Load the state of the optimizers by loading \"state\" attribute of the optimizers.\n\n        Args:\n            attribute_snapshot (dict[str, Any]): The snapshot containing the state of the optimizers.\n            attribute (dict[str, Optimizer]): The optimizers to be loaded.\n        \"\"\"\n        for key, optimizer in attribute.items():\n            optimizer_state_dict = optimizer.state_dict()\n            optimizer_state_dict[\"state\"] = attribute_snapshot[key]\n            optimizer.load_state_dict(optimizer_state_dict)\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.OptimizerSnapshotter.save_attribute","title":"<code>save_attribute(attribute)</code>","text":"<p>Save the state of the optimizers by saving \"state\" attribute of the optimizers.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>dict[str, Optimizer]</code> <p>The optimizers to be saved.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the state of the optimizers.</p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def save_attribute(self, attribute: dict[str, Optimizer]) -&gt; dict[str, Any]:\n    \"\"\"\n    Save the state of the optimizers by saving \"state\" attribute of the optimizers.\n\n    Args:\n        attribute (dict[str, Optimizer]): The optimizers to be saved.\n\n    Returns:\n        (dict[str, Any]): A dictionary containing the state of the optimizers.\n    \"\"\"\n    output = {}\n    for key, optimizer in attribute.items():\n        output[key] = optimizer.state_dict()[\"state\"]\n    return output\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.OptimizerSnapshotter.load_attribute","title":"<code>load_attribute(attribute_snapshot, attribute)</code>","text":"<p>Load the state of the optimizers by loading \"state\" attribute of the optimizers.</p> <p>Parameters:</p> Name Type Description Default <code>attribute_snapshot</code> <code>dict[str, Any]</code> <p>The snapshot containing the state of the optimizers.</p> required <code>attribute</code> <code>dict[str, Optimizer]</code> <p>The optimizers to be loaded.</p> required Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, Optimizer]) -&gt; None:\n    \"\"\"\n    Load the state of the optimizers by loading \"state\" attribute of the optimizers.\n\n    Args:\n        attribute_snapshot (dict[str, Any]): The snapshot containing the state of the optimizers.\n        attribute (dict[str, Optimizer]): The optimizers to be loaded.\n    \"\"\"\n    for key, optimizer in attribute.items():\n        optimizer_state_dict = optimizer.state_dict()\n        optimizer_state_dict[\"state\"] = attribute_snapshot[key]\n        optimizer.load_state_dict(optimizer_state_dict)\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.LRSchedulerSnapshotter","title":"<code>LRSchedulerSnapshotter</code>","text":"<p>               Bases: <code>AbstractSnapshotter[LRScheduler]</code></p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>class LRSchedulerSnapshotter(AbstractSnapshotter[LRScheduler]):\n    def save_attribute(self, attribute: dict[str, LRScheduler]) -&gt; dict[str, Any]:\n        \"\"\"\n        Save the state of the learning rate schedulers.\n\n        Args:\n            attribute (dict[str, LRScheduler]): The learning rate schedulers to be saved.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the learning rate schedulers.\n        \"\"\"\n        output = {}\n        for key, lr_scheduler in attribute.items():\n            output[key] = lr_scheduler.state_dict()\n        return output\n\n    def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, LRScheduler]) -&gt; None:\n        \"\"\"\n        Load the state of the learning rate schedulers.\n\n        Args:\n            attribute_snapshot (dict[str, Any]): The snapshot containing the state of the learning rate schedulers.\n            attribute (dict[str, LRScheduler]): The learning rate schedulers to be loaded.\n        \"\"\"\n        for key, lr_scheduler in attribute.items():\n            lr_scheduler.load_state_dict(attribute_snapshot[key])\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.LRSchedulerSnapshotter.save_attribute","title":"<code>save_attribute(attribute)</code>","text":"<p>Save the state of the learning rate schedulers.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>dict[str, LRScheduler]</code> <p>The learning rate schedulers to be saved.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the state of the learning rate schedulers.</p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def save_attribute(self, attribute: dict[str, LRScheduler]) -&gt; dict[str, Any]:\n    \"\"\"\n    Save the state of the learning rate schedulers.\n\n    Args:\n        attribute (dict[str, LRScheduler]): The learning rate schedulers to be saved.\n\n    Returns:\n        (dict[str, Any]): A dictionary containing the state of the learning rate schedulers.\n    \"\"\"\n    output = {}\n    for key, lr_scheduler in attribute.items():\n        output[key] = lr_scheduler.state_dict()\n    return output\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.LRSchedulerSnapshotter.load_attribute","title":"<code>load_attribute(attribute_snapshot, attribute)</code>","text":"<p>Load the state of the learning rate schedulers.</p> <p>Parameters:</p> Name Type Description Default <code>attribute_snapshot</code> <code>dict[str, Any]</code> <p>The snapshot containing the state of the learning rate schedulers.</p> required <code>attribute</code> <code>dict[str, LRScheduler]</code> <p>The learning rate schedulers to be loaded.</p> required Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, LRScheduler]) -&gt; None:\n    \"\"\"\n    Load the state of the learning rate schedulers.\n\n    Args:\n        attribute_snapshot (dict[str, Any]): The snapshot containing the state of the learning rate schedulers.\n        attribute (dict[str, LRScheduler]): The learning rate schedulers to be loaded.\n    \"\"\"\n    for key, lr_scheduler in attribute.items():\n        lr_scheduler.load_state_dict(attribute_snapshot[key])\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.TorchModuleSnapshotter","title":"<code>TorchModuleSnapshotter</code>","text":"<p>               Bases: <code>AbstractSnapshotter[Module]</code></p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>class TorchModuleSnapshotter(AbstractSnapshotter[nn.Module]):\n    def save_attribute(self, attribute: dict[str, nn.Module]) -&gt; dict[str, Any]:\n        \"\"\"\n        Save the state of the ``nn.Modules``.\n\n        Args:\n            attribute (dict[str, nn.Module]): The ``nn.Modules`` to be saved.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the ``nn.Modules``.\n        \"\"\"\n        output = {}\n        for key, model in attribute.items():\n            output[key] = model.state_dict()\n        return output\n\n    def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, nn.Module]) -&gt; None:\n        \"\"\"\n        Load the state of the ``nn.Modules``.\n\n        Args:\n            attribute_snapshot (dict[str, Any]): The snapshot containing the state of the ``nn.Modules``.\n            attribute (dict[str, nn.Module]): The ``nn.Modules`` to be loaded.\n        \"\"\"\n        for key, model in attribute.items():\n            model.load_state_dict(attribute_snapshot[key])\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.TorchModuleSnapshotter.save_attribute","title":"<code>save_attribute(attribute)</code>","text":"<p>Save the state of the <code>nn.Modules</code>.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>dict[str, Module]</code> <p>The <code>nn.Modules</code> to be saved.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the state of the <code>nn.Modules</code>.</p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def save_attribute(self, attribute: dict[str, nn.Module]) -&gt; dict[str, Any]:\n    \"\"\"\n    Save the state of the ``nn.Modules``.\n\n    Args:\n        attribute (dict[str, nn.Module]): The ``nn.Modules`` to be saved.\n\n    Returns:\n        (dict[str, Any]): A dictionary containing the state of the ``nn.Modules``.\n    \"\"\"\n    output = {}\n    for key, model in attribute.items():\n        output[key] = model.state_dict()\n    return output\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.TorchModuleSnapshotter.load_attribute","title":"<code>load_attribute(attribute_snapshot, attribute)</code>","text":"<p>Load the state of the <code>nn.Modules</code>.</p> <p>Parameters:</p> Name Type Description Default <code>attribute_snapshot</code> <code>dict[str, Any]</code> <p>The snapshot containing the state of the <code>nn.Modules</code>.</p> required <code>attribute</code> <code>dict[str, Module]</code> <p>The <code>nn.Modules</code> to be loaded.</p> required Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, nn.Module]) -&gt; None:\n    \"\"\"\n    Load the state of the ``nn.Modules``.\n\n    Args:\n        attribute_snapshot (dict[str, Any]): The snapshot containing the state of the ``nn.Modules``.\n        attribute (dict[str, nn.Module]): The ``nn.Modules`` to be loaded.\n    \"\"\"\n    for key, model in attribute.items():\n        model.load_state_dict(attribute_snapshot[key])\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.SerializableObjectSnapshotter","title":"<code>SerializableObjectSnapshotter</code>","text":"<p>               Bases: <code>AbstractSnapshotter[MetricManager | LossMeter | ReportsManager]</code></p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>class SerializableObjectSnapshotter(AbstractSnapshotter[MetricManager | LossMeter | ReportsManager]):\n    def save_attribute(self, attribute: dict[str, MetricManager | LossMeter | ReportsManager]) -&gt; dict[str, Any]:\n        \"\"\"\n        Save the state of the serializable objects (either single or dictionary of them).\n\n        Args:\n            attribute (dict[str, MetricManager | LossMeter | ReportsManager]): The serializable objects to be saved.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the serializable objects.\n        \"\"\"\n        return attribute\n\n    def load_attribute(\n        self, attribute_snapshot: dict[str, Any], attribute: dict[str, MetricManager | LossMeter | ReportsManager]\n    ) -&gt; None:\n        \"\"\"\n        Load the state of the serializable objects (either single or dictionary of them).\n\n        Args:\n            attribute_snapshot (dict[str, Any]): The snapshot containing the state of the serializable objects.\n            attribute (dict[str, MetricManager | LossMeter | ReportsManager]): The serializable objects to be loaded.\n        \"\"\"\n        for key in attribute:\n            attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.SerializableObjectSnapshotter.save_attribute","title":"<code>save_attribute(attribute)</code>","text":"<p>Save the state of the serializable objects (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>dict[str, MetricManager | LossMeter | ReportsManager]</code> <p>The serializable objects to be saved.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the state of the serializable objects.</p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def save_attribute(self, attribute: dict[str, MetricManager | LossMeter | ReportsManager]) -&gt; dict[str, Any]:\n    \"\"\"\n    Save the state of the serializable objects (either single or dictionary of them).\n\n    Args:\n        attribute (dict[str, MetricManager | LossMeter | ReportsManager]): The serializable objects to be saved.\n\n    Returns:\n        (dict[str, Any]): A dictionary containing the state of the serializable objects.\n    \"\"\"\n    return attribute\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.SerializableObjectSnapshotter.load_attribute","title":"<code>load_attribute(attribute_snapshot, attribute)</code>","text":"<p>Load the state of the serializable objects (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute_snapshot</code> <code>dict[str, Any]</code> <p>The snapshot containing the state of the serializable objects.</p> required <code>attribute</code> <code>dict[str, MetricManager | LossMeter | ReportsManager]</code> <p>The serializable objects to be loaded.</p> required Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def load_attribute(\n    self, attribute_snapshot: dict[str, Any], attribute: dict[str, MetricManager | LossMeter | ReportsManager]\n) -&gt; None:\n    \"\"\"\n    Load the state of the serializable objects (either single or dictionary of them).\n\n    Args:\n        attribute_snapshot (dict[str, Any]): The snapshot containing the state of the serializable objects.\n        attribute (dict[str, MetricManager | LossMeter | ReportsManager]): The serializable objects to be loaded.\n    \"\"\"\n    for key in attribute:\n        attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.SingletonSnapshotter","title":"<code>SingletonSnapshotter</code>","text":"<p>               Bases: <code>AbstractSnapshotter[int | float | bool]</code></p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>class SingletonSnapshotter(AbstractSnapshotter[int | float | bool]):\n    def save_attribute(self, attribute: dict[str, int | float | bool]) -&gt; dict[str, Any]:\n        \"\"\"\n        Save the state of a singleton which could be a number or a boolean (either single or dictionary of them).\n\n        Args:\n            attribute (dict[str, int | float | bool]): The singleton to be saved.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the singletons.\n        \"\"\"\n        return attribute\n\n    def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, int | float | bool]) -&gt; None:\n        \"\"\"\n        Load the state of the singleton (either single or dictionary of them).\n\n        Args:\n            attribute_snapshot (dict[str, Any]): The snapshot containing the state of the singleton.\n            attribute (dict[str, int | float | bool]): The singletons to be loaded.\n        \"\"\"\n        for key in attribute:\n            attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.SingletonSnapshotter.save_attribute","title":"<code>save_attribute(attribute)</code>","text":"<p>Save the state of a singleton which could be a number or a boolean (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>dict[str, int | float | bool]</code> <p>The singleton to be saved.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the state of the singletons.</p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def save_attribute(self, attribute: dict[str, int | float | bool]) -&gt; dict[str, Any]:\n    \"\"\"\n    Save the state of a singleton which could be a number or a boolean (either single or dictionary of them).\n\n    Args:\n        attribute (dict[str, int | float | bool]): The singleton to be saved.\n\n    Returns:\n        (dict[str, Any]): A dictionary containing the state of the singletons.\n    \"\"\"\n    return attribute\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.SingletonSnapshotter.load_attribute","title":"<code>load_attribute(attribute_snapshot, attribute)</code>","text":"<p>Load the state of the singleton (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute_snapshot</code> <code>dict[str, Any]</code> <p>The snapshot containing the state of the singleton.</p> required <code>attribute</code> <code>dict[str, int | float | bool]</code> <p>The singletons to be loaded.</p> required Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, int | float | bool]) -&gt; None:\n    \"\"\"\n    Load the state of the singleton (either single or dictionary of them).\n\n    Args:\n        attribute_snapshot (dict[str, Any]): The snapshot containing the state of the singleton.\n        attribute (dict[str, int | float | bool]): The singletons to be loaded.\n    \"\"\"\n    for key in attribute:\n        attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.HistorySnapshotter","title":"<code>HistorySnapshotter</code>","text":"<p>               Bases: <code>AbstractSnapshotter[History]</code></p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>class HistorySnapshotter(AbstractSnapshotter[History]):\n    def save_attribute(self, attribute: dict[str, History]) -&gt; dict[str, Any]:\n        \"\"\"\n        Save the state of the history objects (either single or dictionary of them).\n\n        Args:\n            attribute (dict[str, History]): The history to be saved.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the history.\n        \"\"\"\n        return attribute\n\n    def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, History]) -&gt; None:\n        \"\"\"\n        Load the state of the history (either single or dictionary of them).\n\n        Args:\n            attribute_snapshot (dict[str, Any]): The snapshot containing the state of the history.\n            attribute (dict[str, History]): The history to be loaded.\n        \"\"\"\n        for key in attribute:\n            attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.HistorySnapshotter.save_attribute","title":"<code>save_attribute(attribute)</code>","text":"<p>Save the state of the history objects (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>dict[str, History]</code> <p>The history to be saved.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the state of the history.</p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def save_attribute(self, attribute: dict[str, History]) -&gt; dict[str, Any]:\n    \"\"\"\n    Save the state of the history objects (either single or dictionary of them).\n\n    Args:\n        attribute (dict[str, History]): The history to be saved.\n\n    Returns:\n        (dict[str, Any]): A dictionary containing the state of the history.\n    \"\"\"\n    return attribute\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.HistorySnapshotter.load_attribute","title":"<code>load_attribute(attribute_snapshot, attribute)</code>","text":"<p>Load the state of the history (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute_snapshot</code> <code>dict[str, Any]</code> <p>The snapshot containing the state of the history.</p> required <code>attribute</code> <code>dict[str, History]</code> <p>The history to be loaded.</p> required Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, History]) -&gt; None:\n    \"\"\"\n    Load the state of the history (either single or dictionary of them).\n\n    Args:\n        attribute_snapshot (dict[str, Any]): The snapshot containing the state of the history.\n        attribute (dict[str, History]): The history to be loaded.\n    \"\"\"\n    for key in attribute:\n        attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.StringSnapshotter","title":"<code>StringSnapshotter</code>","text":"<p>               Bases: <code>AbstractSnapshotter[str]</code></p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>class StringSnapshotter(AbstractSnapshotter[str]):\n    def save_attribute(self, attribute: dict[str, str]) -&gt; dict[str, Any]:\n        \"\"\"\n        Save the state of the strings (either single or dictionary of them).\n\n        Args:\n            attribute (dict[str, str]): The string to be saved.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the strings.\n        \"\"\"\n        return attribute\n\n    def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, str]) -&gt; None:\n        \"\"\"\n        Load the state of the strings (either single or dictionary of them).\n\n        Args:\n            attribute_snapshot (dict[str, Any]): The snapshot containing the state of the strings.\n            attribute (dict[str, str]): The strings to be loaded.\n        \"\"\"\n        for key in attribute:\n            attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.StringSnapshotter.save_attribute","title":"<code>save_attribute(attribute)</code>","text":"<p>Save the state of the strings (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>dict[str, str]</code> <p>The string to be saved.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the state of the strings.</p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def save_attribute(self, attribute: dict[str, str]) -&gt; dict[str, Any]:\n    \"\"\"\n    Save the state of the strings (either single or dictionary of them).\n\n    Args:\n        attribute (dict[str, str]): The string to be saved.\n\n    Returns:\n        (dict[str, Any]): A dictionary containing the state of the strings.\n    \"\"\"\n    return attribute\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.StringSnapshotter.load_attribute","title":"<code>load_attribute(attribute_snapshot, attribute)</code>","text":"<p>Load the state of the strings (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute_snapshot</code> <code>dict[str, Any]</code> <p>The snapshot containing the state of the strings.</p> required <code>attribute</code> <code>dict[str, str]</code> <p>The strings to be loaded.</p> required Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, str]) -&gt; None:\n    \"\"\"\n    Load the state of the strings (either single or dictionary of them).\n\n    Args:\n        attribute_snapshot (dict[str, Any]): The snapshot containing the state of the strings.\n        attribute (dict[str, str]): The strings to be loaded.\n    \"\"\"\n    for key in attribute:\n        attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.BytesSnapshotter","title":"<code>BytesSnapshotter</code>","text":"<p>               Bases: <code>AbstractSnapshotter[bytes]</code></p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>class BytesSnapshotter(AbstractSnapshotter[bytes]):\n    def save_attribute(self, attribute: dict[str, bytes]) -&gt; dict[str, Any]:\n        \"\"\"\n        Save the state of the bytes (either single or dictionary of them).\n\n        Args:\n            attribute (dict[str, str]): The string to be saved.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the bytes.\n        \"\"\"\n        return attribute\n\n    def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, bytes]) -&gt; None:\n        \"\"\"\n        Load the state of the bytes (either single or dictionary of them).\n\n        Args:\n            attribute_snapshot (dict[str, Any]): The snapshot containing the state of the bytes.\n            attribute (dict[str, str]): The bytes to be loaded.\n        \"\"\"\n        for key in attribute:\n            attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.BytesSnapshotter.save_attribute","title":"<code>save_attribute(attribute)</code>","text":"<p>Save the state of the bytes (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>dict[str, str]</code> <p>The string to be saved.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the state of the bytes.</p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def save_attribute(self, attribute: dict[str, bytes]) -&gt; dict[str, Any]:\n    \"\"\"\n    Save the state of the bytes (either single or dictionary of them).\n\n    Args:\n        attribute (dict[str, str]): The string to be saved.\n\n    Returns:\n        (dict[str, Any]): A dictionary containing the state of the bytes.\n    \"\"\"\n    return attribute\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.BytesSnapshotter.load_attribute","title":"<code>load_attribute(attribute_snapshot, attribute)</code>","text":"<p>Load the state of the bytes (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute_snapshot</code> <code>dict[str, Any]</code> <p>The snapshot containing the state of the bytes.</p> required <code>attribute</code> <code>dict[str, str]</code> <p>The bytes to be loaded.</p> required Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, bytes]) -&gt; None:\n    \"\"\"\n    Load the state of the bytes (either single or dictionary of them).\n\n    Args:\n        attribute_snapshot (dict[str, Any]): The snapshot containing the state of the bytes.\n        attribute (dict[str, str]): The bytes to be loaded.\n    \"\"\"\n    for key in attribute:\n        attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.EnumSnapshotter","title":"<code>EnumSnapshotter</code>","text":"<p>               Bases: <code>AbstractSnapshotter[Enum]</code></p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>class EnumSnapshotter(AbstractSnapshotter[Enum]):\n    def save_attribute(self, attribute: dict[str, Enum]) -&gt; dict[str, Any]:\n        \"\"\"\n        Save the state of the Enum (either single or dictionary of them).\n\n        Args:\n            attribute (dict[str, Enum]): The enum to be saved.\n\n        Returns:\n            (dict[str, Any]): A dictionary containing the state of the enum.\n        \"\"\"\n        return attribute\n\n    def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, Enum]) -&gt; None:\n        \"\"\"\n        Load the state of the num (either single or dictionary of them).\n\n        Args:\n            attribute_snapshot (dict[str, Any]): The snapshot containing the state of the enum.\n            attribute (dict[str, Enum]): The enum to be loaded.\n        \"\"\"\n        for key in attribute:\n            attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.EnumSnapshotter.save_attribute","title":"<code>save_attribute(attribute)</code>","text":"<p>Save the state of the Enum (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>dict[str, Enum]</code> <p>The enum to be saved.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing the state of the enum.</p> Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def save_attribute(self, attribute: dict[str, Enum]) -&gt; dict[str, Any]:\n    \"\"\"\n    Save the state of the Enum (either single or dictionary of them).\n\n    Args:\n        attribute (dict[str, Enum]): The enum to be saved.\n\n    Returns:\n        (dict[str, Any]): A dictionary containing the state of the enum.\n    \"\"\"\n    return attribute\n</code></pre>"},{"location":"api/#fl4health.utils.snapshotter.EnumSnapshotter.load_attribute","title":"<code>load_attribute(attribute_snapshot, attribute)</code>","text":"<p>Load the state of the num (either single or dictionary of them).</p> <p>Parameters:</p> Name Type Description Default <code>attribute_snapshot</code> <code>dict[str, Any]</code> <p>The snapshot containing the state of the enum.</p> required <code>attribute</code> <code>dict[str, Enum]</code> <p>The enum to be loaded.</p> required Source code in <code>fl4health/utils/snapshotter.py</code> <pre><code>def load_attribute(self, attribute_snapshot: dict[str, Any], attribute: dict[str, Enum]) -&gt; None:\n    \"\"\"\n    Load the state of the num (either single or dictionary of them).\n\n    Args:\n        attribute_snapshot (dict[str, Any]): The snapshot containing the state of the enum.\n        attribute (dict[str, Enum]): The enum to be loaded.\n    \"\"\"\n    for key in attribute:\n        attribute[key] = attribute_snapshot[key]\n</code></pre>"},{"location":"contributing/","title":"Contributing to FL4Health","text":"<p>Thanks for your interest in contributing to the FL4Health library!</p> <p>To submit PRs, please fill out the PR template along with the PR. If the PR fixes an issue, please include a link to the PR to the issue, if possible. Below are some details around important things to consider before contributing to the library. A table of contents also appears below for navigation.</p> <ul> <li>Development Practices</li> <li>Development Requirements</li> <li>Coding Guidelines, Formatters, and Checks</li> <li>Code Documentation</li> <li>Tests</li> </ul>"},{"location":"contributing/#development-practices","title":"Development Practices","text":"<p>We use the standard git development flow of branch and merge to main with PRs on GitHub. At least one member of the core team needs to approve a PR before it can be merged into main. As mentioned above, tests are run automatically on PRs with a merge target of main. Furthermore, a suite of static code checkers and formatters are also run on said PRs. These also need to pass for a PR to be eligible for merging into the main branch of the library. Currently, such checks run on python3.9.</p>"},{"location":"contributing/#development-requirements","title":"Development Requirements","text":"<p>For development and testing, we use uv for dependency management. The library dependencies and those for development and testing are listed in the <code>pyproject.toml</code> file.</p> <p>The easiest way to set up the development environment: <pre><code># Install uv (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Clone the repository\ngit clone https://github.com/VectorInstitute/FL4Health.git\ncd FL4Health\n\n# Install all dependencies\nuv sync --group dev --group test --group codestyle\n</code></pre></p> <p>Note that the <code>--group</code> flag installs optional dependency groups required for the full development workflow. See the <code>pyproject.toml</code> file for details about each group.</p> <p>If you need to update dependencies, you should change the requirements in <code>pyproject.toml</code> and then update the <code>uv.lock</code> using the command <code>uv lock</code>.</p>"},{"location":"contributing/#coding-guidelines-formatters-and-checks","title":"Coding Guidelines, Formatters, and Checks","text":"<p>For code style, we recommend the google style guide.</p> <p>We use ruff for items such as code formatting and static code analysis. Ruff checks various rules including flake8. The pre-commit hooks (installation described below) show errors which you need to fix before submitting a PR, as these checks will fail and prevent PR merger. This project's configuration details for ruff are found in the <code>pyproject.toml</code> under headings prefixed with <code>tool.ruff.</code>. In addition to code checks, ruff also has a number of features for imposing documentation formatting that we leverage.</p> <p>If you want to run run (independent of the other pre-commit checks), you can run <pre><code>ruff check .\nruff format .\n</code></pre> from the top level directory.</p> <p>Throughout the codebase, we use type hints which are checked using mypy. The mypy checks are strictly enforced. That is, all mypy checks must pass or the associated PR will not be merge-able.</p> <p>The settings for <code>mypy</code> are in the <code>mypy.ini</code>, settings for <code>ruff</code> come from the <code>pyproject.toml</code>, and some standard checks are defined directly in the <code>.pre-commit-config.yaml</code> settings.</p> <p>All of these checks and formatters are invoked by pre-commit hooks. These hooks are run remotely on GitHub. In order to ensure that your code conforms to these standards, and, therefore, passes the remote checks, you can install the pre-commit hooks to be run locally. This is done by running (with your environment active)</p> <p>Note: We use the modern mypy types introduced in Python 3.10 and above. See some of the documentation here</p> <p>For example, this means that we're using <code>list[str], tuple[int, int], tuple[int, ...], dict[str, int], type[C]</code> as built-in types and <code>Iterable[int], Sequence[bool], Mapping[str, int], Callable[[...], ...]</code> from collections.abc (as now recommended by mypy).</p> <p>We are also moving to the new Optional and Union specification style: <pre><code>Optional[typing_stuff] -&gt; typing_stuff | None\nUnion[typing1, typing2] -&gt; typing1 | typing2\nOptional[Union[typing1, typing2]] -&gt; typing1 | typing2 | None\n</code></pre></p> <pre><code>pre-commit install\n</code></pre> <p>To run the checks, some of which will automatically re-format your code to fit the standards, you can run <pre><code>pre-commit run --all-files\n</code></pre> It can also be run on a subset of files by omitting the <code>--all-files</code> option and pointing to specific files or folders.</p> <p>If you're using VS Code for development, pre-commit should setup git hooks that execute the pre-commit checks each time you check code into your branch through the integrated source-control as well. This will ensure that each of your commits conform to the desired format before they are run remotely and without needing to remember to run the checks before pushing to a remote. If this isn't done automatically, you can find instructions for setting up these hooks manually online.</p>"},{"location":"contributing/#code-documentation","title":"Code Documentation","text":"<p>For code documentation, we try to adhere to the Google docstring style (See here, Section: Comments and Doc-strings). The implementation of an extensive set of comments for the code in this repository is a work-in-progress. However, we are continuing to work towards a better commented state for the code. For development, as stated in the style guide, any non-trivial or non-obvious methods added to the library should have a doc string. For our library this applies only to code added to the main library in <code>fl4health</code>. Examples, research code, and tests need not incorporate the strict rules of documentation, though clarifying and helpful comments in that code is also strongly encouraged.</p> <p>Note</p> <p>As a matter of convention choice, classes are documented through their <code>__init__</code> functions rather than at the \"class\" level.</p> <p>If you are using VS Code a very helpful integration is available to facilitate the creation of properly formatted doc-strings called autoDocstring VS Code Page and Documentation. This tool will automatically generate a docstring template when starting a docstring with triple quotation marks (<code>\"\"\"</code>). To get the correct format, the following settings should be prescribed in your VS Code settings JSON:</p> <pre><code>{\n    \"autoDocstring.customTemplatePath\": \"\",\n    \"autoDocstring.docstringFormat\": \"google\",\n    \"autoDocstring.generateDocstringOnEnter\": true,\n    \"autoDocstring.guessTypes\": true,\n    \"autoDocstring.includeExtendedSummary\": false,\n    \"autoDocstring.includeName\": false,\n    \"autoDocstring.logLevel\": \"Info\",\n    \"autoDocstring.quoteStyle\": \"\\\"\\\"\\\"\",\n    \"autoDocstring.startOnNewLine\": true\n}\n</code></pre>"},{"location":"contributing/#tests","title":"Tests","text":"<p>All tests for the library are housed in the tests folder. The unit and integration tests are run using <code>pytest</code>. These tests are automatically run through GitHub integrations on PRs to the main branch of this repository. PRs that fail any of the tests will not be eligible to be merged until they are fixed.</p> <p>To run all tests in the tests folder one only needs to run (with the venv active) <pre><code>pytest .\n</code></pre> To run a specific test with pytest, one runs <pre><code>pytest tests/checkpointing/test_best_checkpointer.py\n</code></pre></p> <p>If you use VS Code for development, you can setup the tests with the testing integration so that you can run debugging and other IDE features. Setup will vary depending on your VS Code environment, but in your .vscode folder your <code>settings.json</code> might look something like</p> <pre><code>{\n    \"python.testing.unittestArgs\": [\n        \"-v\",\n        \"-s\",\n        \".\",\n        \"-p\",\n        \"test_*.py\"\n    ],\n    \"python.testing.pytestEnabled\": true,\n    \"python.testing.unittestEnabled\": false,\n    \"python.testing.pytestArgs\": [\n        \".\"\n    ]\n}\n</code></pre> <p>In addition to the unit and integration tests through <code>pytest</code> a number of smoke tests have been implemented and are run remotely on github as well. These tests are housed in <code>tests/smoke_tests</code>. All of these smoke tests must also pass for a PR to be eligible for merging to main. These smoke tests ensure that changes to note unintentionally break or alter the current functionality of many of our examples. This helps to ensure that code changes do not have unintended side-effects on already tested and/or working code.</p>"},{"location":"contributing/#code-coverage","title":"Code Coverage","text":"<p>For code coverage, we use Codecov (by Sentry) and have configured this tool to pass only if a PR's overall code coverage is above 80%.</p> <p>Note</p> <p>The contents of the tests folder is not packed with the FL4Health library on release to PyPI</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<p>First, we need to install the <code>fl4health</code> package. The easiest and recommended way to do this is via <code>pip</code>.</p> <pre><code>pip install fl4health\n</code></pre>"},{"location":"quickstart/#a-simple-fl-task","title":"A simple FL task","text":"<p>With federated learning, the model is trained collaboratively by a set of distributed nodes called <code>clients</code>. This collaboration is facilitated by another node, namely the <code>server</code> node. To setup an FL task we need to define our <code>Client</code> as well as our <code>Server</code> in the scripts <code>client.py</code> and <code>server.py</code>, respectively.</p>"},{"location":"quickstart/#clientpy","title":"<code>client.py</code>","text":"<pre><code>from pathlib import Path\n\nimport flwr as fl\nimport torch\nimport torch.nn as nn\nfrom flwr.common.typing import Config\nfrom torch.nn.modules.loss import _Loss\nfrom torch.optim import Optimizer\nfrom torch.utils.data import DataLoader\n\nfrom examples.models.cnn_model import Net\nfrom fl4health.clients.basic_client import BasicClient\nfrom fl4health.utils.load_data import load_cifar10_data\nfrom fl4health.metrics import Accuracy\n\n\nclass CifarClient(BasicClient):\n    def get_data_loaders(self, config: Config) -&gt; tuple[DataLoader, DataLoader]:\n        train_loader, val_loader, _ = load_cifar10_data(self.data_path, batch_size=64)\n        return train_loader, val_loader\n\n    def get_criterion(self, config: Config) -&gt; _Loss:\n        return torch.nn.CrossEntropyLoss()\n\n    def get_optimizer(self, config: Config) -&gt; Optimizer:\n        return torch.optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n\n    def get_model(self, config: Config) -&gt; nn.Module:\n        return Net().to(self.device)\n\n\ndef main(dataset_path: str) -&gt; None:\n    client = CifarClient(data_path=Path(dataset_path), metrics=[Accuracy(\"accuracy\")], device=torch.device(\"cpu\"))\n    fl.client.start_client(server_address=\"0.0.0.0:8080\", client=client.to_client())\n    client.shutdown()\n</code></pre>"},{"location":"quickstart/#serverpy","title":"<code>server.py</code>","text":"<pre><code>from functools import partial\n\nimport flwr as fl\nfrom flwr.common.typing import Config\nfrom flwr.server.client_manager import SimpleClientManager\nfrom flwr.server.strategy import FedAvg\n\nfrom examples.models.cnn_model import Net\nfrom fl4health.servers.base_server import FlServer\nfrom fl4health.metrics.metric_aggregation import evaluate_metrics_aggregation_fn, fit_metrics_aggregation_fn\nfrom fl4health.utils.parameter_extraction import get_all_model_parameters\n\n\ndef fit_config(current_server_round: int) -&gt; Config:\n    return {\"local_epochs\": 3, \"batch_size\": 64, \"current_server_round\": current_server_round}\n\n\ndef main() -&gt; None:\n\n    fit_config_fn = partial(fit_config)\n    model = Net()\n    strategy = FedAvg(\n        min_fit_clients=2,\n        min_evaluate_clients=2,\n        # Server waits for min_available_clients before starting FL rounds\n        min_available_clients=2,\n        on_fit_config_fn=fit_config_fn,\n        # We use the same fit config function, as nothing changes for eval\n        on_evaluate_config_fn=fit_config_fn,\n        fit_metrics_aggregation_fn=fit_metrics_aggregation_fn,\n        evaluate_metrics_aggregation_fn=evaluate_metrics_aggregation_fn,\n        initial_parameters=get_all_model_parameters(model),\n    )\n    server = FlServer(SimpleClientManager(), {}, strategy)\n\n    fl.server.start_server(\n        server=server,\n        server_address=\"0.0.0.0:8080\",\n        config=fl.server.ServerConfig(num_rounds=20),\n    )\n</code></pre>"},{"location":"quickstart/#running-the-fl-task","title":"Running the FL task","text":"<p>Now that we have our server and clients defined, we can run the FL system!</p>"},{"location":"quickstart/#starting-server","title":"Starting Server","text":"<p>The next step is to start the server by running</p> <pre><code>python -m examples.basic_example.server\n</code></pre>"},{"location":"quickstart/#starting-clients","title":"Starting Clients","text":"<p>Once the server has started and logged \"FL starting,\" the next step, in separate terminals, is to start the two clients. This is done by simply running (remembering to activate your environment)</p> <pre><code>python -m examples.basic_example.client --dataset_path /path/to/data\n</code></pre> <p>NOTE: The argument <code>dataset_path</code> has two functions, depending on whether the dataset exists locally or not. If the dataset already exists at the path specified, it will be loaded from there. Otherwise, the dataset will be automatically downloaded to the path specified and used in the run.</p> <p>After both clients have been started federated learning should commence.</p>"},{"location":"examples/","title":"Examples","text":"<p>This section provides examples demonstrating how to use FL4Health for various federated learning scenarios.</p>"},{"location":"examples/#available-examples","title":"Available Examples","text":"<ul> <li>Basic Example - A basic federated learning example to get started</li> </ul>"},{"location":"examples/basic/","title":"Basic Example","text":""},{"location":"module_guides/","title":"Module Guides","text":"<p>This section provides detailed guides for the various modules in FL4Health.</p>"},{"location":"module_guides/#available-guides","title":"Available Guides","text":"<ul> <li>Checkpointing - Guide to checkpointing models during federated learning</li> </ul>"},{"location":"module_guides/checkpointing/","title":"Checkpointing","text":""}]}